Source: https://de.wikipedia.org/wiki/Python_(Programmiersprache)
Content: Python ([ˈpʰaɪθn̩], [ˈpʰaɪθɑn], auf Deutsch auch [ˈpʰyːtɔn]) ist eine universelle, üblicherweise interpretierte, höhere Programmiersprache.[11] Sie hat den Anspruch, einen gut lesbaren, knappen Programmierstil zu fördern.[12] So werden beispielsweise Blöcke nicht durch geschweifte Klammern, sondern durch Einrückungen strukturiert.
 Python unterstützt mehrere Programmierparadigmen, z. B. die objektorientierte, die aspektorientierte und die funktionale Programmierung. Ferner bietet es eine dynamische Typisierung. Wie viele dynamische Sprachen wird Python oft als Skriptsprache genutzt. Die Sprache weist ein offenes, gemeinschaftsbasiertes Entwicklungsmodell auf, das durch die gemeinnützige Python Software Foundation gestützt wird, die die Definition der Sprache in der Referenzumsetzung CPython pflegt. Python zählt zu den Allzweck-Programmiersprachen.
 Die Sprache wurde Anfang der 1990er Jahre von Guido van Rossum am Centrum Wiskunde & Informatica in Amsterdam als Nachfolger für die Programmier-Lehrsprache ABC entwickelt und war ursprünglich für das verteilte Betriebssystem Amoeba gedacht.
 Der Name geht nicht, wie das Logo vermuten lässt, auf die gleichnamige Schlangengattung Python zurück, sondern bezog sich ursprünglich auf die englische Komikergruppe Monty Python. In der Dokumentation finden sich daher auch einige Anspielungen auf Sketche aus dem Flying Circus.[13] Trotzdem etablierte sich die Assoziation zur Schlange, was sich unter anderem in der Programmiersprache Cobra[14] sowie dem Python-Toolkit „Boa“[15] äußert.
Die erste Vollversion erschien im Januar 1994 unter der Bezeichnung Python 1.0. Gegenüber früheren Versionen wurden einige Konzepte der funktionalen Programmierung implementiert, die allerdings später wieder aufgegeben wurden.[16] Von 1995 bis 2000 erschienen neue Versionen, die fortlaufend als Python 1.1, 1.2 etc. bezeichnet wurden.
 Python 2.0 erschien am 16. Oktober 2000. Neue Funktionen umfassten eine voll funktionsfähige Garbage Collection (automatische Speicherbereinigung) und die Unterstützung für den Unicode-Zeichensatz.[17]
 Python 3.0 (auch Python 3000) erschien am 3. Dezember 2008 nach längerer Entwicklungszeit. Es beinhaltete einige tiefgreifende Änderungen an der Sprache, etwa das Entfernen von Redundanzen bei Befehlssätzen und veralteten Konstrukten. Da Python 3.0 hierdurch teilweise inkompatibel zu früheren Versionen wurde,[18] beschloss die Python Software Foundation, Python 2.7 parallel zu Python 3 bis Ende 2019 weiter mit neuen Versionen zu unterstützen (für Hinweise zur letzten 2er-Version, zum Support-Ende und zur Migration siehe Abschnitt Ende von Python 2).
 Python wurde mit dem Ziel größter Einfachheit und Übersichtlichkeit entworfen. Dies wird vor allem durch zwei Maßnahmen erreicht. Zum einen kommt die Sprache mit relativ wenigen Schlüsselwörtern aus.[44] Zum anderen ist die Syntax reduziert und auf Übersichtlichkeit optimiert. Dadurch lassen sich Python-basierte Skripte deutlich knapper formulieren als in anderen Sprachen.[45]
 Van Rossum legte bei der Entwicklung großen Wert auf eine Standardbibliothek, die überschaubar und leicht erweiterbar ist. Dies war Ergebnis seiner schlechten Erfahrung mit der Sprache ABC, in der das Gegenteil der Fall ist.[46] Dieses Konzept ermöglicht, in Python Module aufzurufen, die in anderen Programmiersprachen geschrieben wurden, etwa um Schwächen von Python auszugleichen. Beispielsweise können für zeitkritische Teile in maschinennäheren Sprachen wie C implementierte Routinen aufgerufen werden.[47] Umgekehrt lassen sich mit Python Module und Plug-ins für andere Programme schreiben, die die entsprechende Unterstützung bieten. Dies ist unter anderem bei Blender, Cinema 4D, GIMP, Maya, OpenOffice bzw. LibreOffice, PyMOL, SPSS, QGIS oder KiCad der Fall.
 Python ist eine Multiparadigmensprache. Das bedeutet, Python zwingt den Programmierer nicht zu einem einzigen Programmierstil, sondern erlaubt, das für die jeweilige Aufgabe am besten geeignete Paradigma zu wählen. Objektorientierte und strukturierte Programmierung werden vollständig unterstützt, funktionale und aspektorientierte Programmierung werden durch einzelne Elemente der Sprache unterstützt.
Die Freigabe nicht mehr benutzter Speicherbereiche erfolgt durch Referenzzählung.
Datentypen werden dynamisch verwaltet, eine automatische statische Typprüfung wie z. B. bei C++ gibt es nicht. Jedoch unterstützt Python ab Version 3.5 optionale Typ-Annotationen, um eine statische Typprüfung mithilfe externer Software, wie zum Beispiel Mypy, zu vereinfachen.[48][49]
 Python besitzt eine größere Anzahl von grundlegenden Datentypen. Neben der herkömmlichen Arithmetik unterstützt es transparent auch beliebig große Ganzzahlen und komplexe Zahlen.
 Die üblichen Zeichenkettenoperationen werden unterstützt. Zeichenketten sind in Python allerdings unveränderliche Objekte (wie auch in Java). Daher geben Operationen, die eine Zeichenkette verändern sollen – wie z. B. durch Ersetzen von Zeichen – immer eine neue Zeichenkette zurück.
 In Python ist alles ein Objekt: Klassen, Typen, Methoden, Module etc. Der Datentyp ist jeweils an das Objekt (den Wert) gebunden und nicht an eine Variable, d. h. Datentypen werden dynamisch vergeben, so wie bei Smalltalk oder Lisp – und nicht wie bei Java.
 Trotz der dynamischen Typverwaltung enthält Python eine gewisse Typprüfung. Diese ist strenger als bei Perl, aber weniger strikt als etwa bei Objective CAML.
Implizite Umwandlungen nach dem Duck-Typing-Prinzip sind unter anderem für numerische Typen definiert, sodass man beispielsweise eine komplexe Zahl mit einer langen Ganzzahl ohne explizite Typumwandlung multiplizieren kann.
 Mit dem Format-Operator % gibt es eine implizite Umwandlung eines Objekts in eine Zeichenkette. Der Operator == überprüft zwei Objekte auf (Wert-)Gleichheit. Der Operator is überprüft die tatsächliche Identität zweier Objekte.[50]
 Python besitzt mehrere Sammeltypen, darunter Listen, Tupel, Mengen (Sets) und assoziative Arrays (Dictionaries). Listen, Tupel und Zeichenketten sind Folgen (Sequenzen, Felder) und kennen fast alle die gleichen Methoden: Über die Zeichen einer Kette kann man ebenso iterieren wie über die Elemente einer Liste. Außerdem gibt es die unveränderlichen Objekte, die nach ihrer Erzeugung nicht mehr geändert werden können. Listen sind z. B. erweiterbare Felder, wohingegen Tupel und Zeichenketten eine feste Länge haben und unveränderlich sind.
 Der Zweck solcher Unveränderlichkeit hängt z. B. mit den Dictionaries zusammen, einem Datentyp, der auch als assoziatives Array bezeichnet wird. Um die Datenkonsistenz zu sichern, müssen die Schlüssel eines Dictionary vom Typ „unveränderlich“ sein. Die ins Dictionary eingetragenen Werte können dagegen von beliebigem Typ sein.
 Sets sind Mengen von Objekten und in CPython ab Version 2.4 im Standardsprachumfang enthalten. Diese Datenstruktur kann beliebige (paarweise unterschiedliche) Objekte aufnehmen und stellt Mengenoperationen wie beispielsweise Durchschnitt, Differenz und Vereinigung zur Verfügung.
 Das Typsystem von Python ist auf das Klassensystem abgestimmt. Obwohl die eingebauten Datentypen genau genommen keine Klassen sind, können Klassen von einem Typ erben. So kann man die Eigenschaften von Zeichenketten oder Wörterbüchern erweitern – auch von Ganzzahlen. Python unterstützt Mehrfachvererbung.
 Die Sprache unterstützt direkt den Umgang mit Typen und Klassen. Typen können ausgelesen (ermittelt) und verglichen werden und verhalten sich wie Objekte – tatsächlich sind die Typen (wie in Smalltalk) selbst ein Objekt. Die Attribute eines Objektes können als Wörterbuch extrahiert werden.
 Eines der Entwurfsziele für Python war die gute Lesbarkeit des Quellcodes. Die Anweisungen benutzen häufig englische Schlüsselwörter, wo andere Sprachen Symbole einsetzen (z. B. or statt ||). Für strukturierte Programmierung besitzt Python die folgenden Elemente:
 Im Gegensatz zu vielen anderen Sprachen können for- und while-Schleifen einen else-Zweig haben. Dieser wird nur ausgeführt, wenn die Schleife vollständig durchlaufen und nicht mittels break, return oder einer Ausnahme abgebrochen wurde.
 Python benutzt wie Miranda und Haskell Einrückungen als Strukturierungselement. Diese Idee wurde erstmals von Peter J. Landin vorgeschlagen und von ihm off-side rule („Abseitsregel“) genannt. In den meisten anderen Programmiersprachen werden Blöcke durch Klammern oder Schlüsselwörter markiert, während unterschiedlich große Leerräume außerhalb von Zeichenketten keine spezielle Semantik tragen. Bei diesen Sprachen ist die Einrückung zur optischen Hervorhebung eines Blockes zwar erlaubt und in der Regel auch erwünscht, aber nicht vorgeschrieben. Für Programmierneulinge wird der Zwang zu lesbarem Stil aber als Vorteil gesehen.
 Hierzu als Beispiel die Berechnung der Fakultät einer Ganzzahl, einmal in C und einmal in Python:
 Fakultätsfunktion in C:
 Die gleiche Funktion in Python:
 Es ist jedoch darauf zu achten, die Einrückungen im gesamten Programmtext gleich zu gestalten. Die gemischte Verwendung von Leerzeichen und Tabulatorzeichen kann zu Problemen führen, da der Python-Interpreter Tabulatoren im Abstand von acht Leerzeichen annimmt. Je nach Konfiguration des Editors können Tabulatoren optisch mit weniger als acht Leerzeichen dargestellt werden, was zu Syntaxfehlern oder ungewollter Programmstrukturierung führen kann. Als vorbeugende Maßnahme kann man den Editor Tabulatorzeichen durch eine feste Anzahl von Leerzeichen ersetzen lassen. Die Python-Distribution enthält in der Standardbibliothek das Modul tabnanny, welches die Vermischung von Tabulator- und Leerzeichen zu erkennen und beheben hilft.
 Man kann die Fakultätsfunktion aber auch wie in C einzeilig mit ternärem Operator formulieren:
 Die Fakultätsfunktion in C:
 Die Fakultätsfunktion in Python:
 Coconut[51] und andere Erweiterungen erleichtern das funktionale Programmieren in Python. Darüber hinaus lässt sich dies auch mit dem herkömmlichen Python realisieren:
 Ausdrucksstarke syntaktische Elemente zur funktionalen Programmierung vereinfachen das Arbeiten mit Listen und anderen Sammeltypen. Eine solche Vereinfachung ist die Listennotation, die aus der funktionalen Programmiersprache Haskell stammt; hier bei der Berechnung der ersten fünf Zweierpotenzen:
 Weil in Python Funktionen als Argumente auftreten dürfen, kann man auch ausgeklügeltere Konstruktionen ausdrücken, wie den Continuation-Passing Style.
 Pythons Schlüsselwort lambda könnte manche Anhänger der funktionalen Programmierung fehlleiten. Solche lambda-Blöcke in Python können nur Ausdrücke enthalten, aber keine Anweisungen. Damit werden solche Anweisungen generell nicht verwendet, um eine Funktion zurückzugeben. Die übliche Vorgehensweise ist stattdessen, den Namen einer lokalen Funktion zurückzugeben. Das folgende Beispiel zeigt dies anhand einer einfachen Funktion nach den Ideen von Haskell Brooks Curry:
 Damit ist auch Currying auf einfache Art möglich, um generische Funktionsobjekte auf problemspezifische herunterzubrechen. Hier ein einfaches Beispiel:
 Wird die curry-Funktion aufgerufen, erwartet diese eine Funktion mit zwei notwendigen Parametern sowie die Parameterbelegung für den zweiten Parameter dieser Funktion. Der Rückgabewert von curry ist eine Funktion, die das Gleiche tut wie func, aber nur noch einen Parameter benötigt.
 Closures sind mit den o. g. Mechanismen in Python ebenfalls einfach möglich. Ein simples Beispiel für einen Stack, intern durch eine Liste repräsentiert:
 Auf diese Weise erhält man die drei Funktionsobjekte pop, push, is_empty, um den Stack zu modifizieren bzw. auf enthaltene Elemente zu prüfen, ohne dabei auf l direkt zuzugreifen.
 Python nutzt ausgiebig die Ausnahmebehandlung (englisch exception handling) als ein Mittel, um Fehlerbedingungen zu testen. Dies ist so weit in Python integriert, dass es teilweise sogar möglich ist, Syntaxfehler abzufangen und zur Laufzeit zu behandeln.
 Ausnahmen haben einige Vorteile gegenüber anderen beim Programmieren üblichen Verfahren der Fehlerbehandlung (wie z. B. Fehler-Rückgabewerte und globale Statusvariablen). Sie sind Thread-sicher und können leicht bis in die höchste Programmebene weitergegeben oder an einer beliebigen anderen Ebene der Funktionsaufruffolge behandelt werden. Der korrekte Einsatz von Ausnahmebehandlungen beim Zugriff auf dynamische Ressourcen erleichtert es zudem, bestimmte auf Race Conditions basierende Sicherheitslücken zu vermeiden, die entstehen können, wenn Zugriffe auf bereits veralteten Statusabfragen basieren.
 Der Python-Ansatz legt den Einsatz von Ausnahmen nahe, wann immer eine Fehlerbedingung entstehen könnte. Nützlich ist dieses Prinzip beispielsweise bei der Konstruktion robuster Eingabeaufforderungen:
 Dieses Programmstück fragt den Benutzer so lange nach einer Zahl, bis dieser eine Zeichenfolge eingibt, die sich per int() in eine Ganzzahl konvertieren lässt. Durch die Ausnahmebehandlung wird hier vermieden, dass eine Fehleingabe zu einem Laufzeitfehler führt, der das Programm zum Abbruch zwingt.
 Ebenso kann auch das hier nicht berücksichtigte Interrupt-Signal (SIGINT, häufig Strg+C) mittels Ausnahmebehandlung in Python abgefangen und behandelt werden (except KeyboardInterrupt: …).
 Die mächtige Standardbibliothek ist eine der größten Stärken von Python, wodurch es sich für viele Anwendungen eignet. Der überwiegende Teil davon ist plattformunabhängig, so dass auch größere Python-Programme oft auf Unix, Windows, macOS und anderen Plattformen ohne Änderung laufen. Die Module der Standardbibliothek können mit in C oder Python selbst geschriebenen Modulen ergänzt werden.
 Die Standardbibliothek ist besonders auf Internetanwendungen zugeschnitten, mit der Unterstützung einer großen Anzahl von Standardformaten und -protokollen (wie MIME und HTTP). Module zur Schaffung grafischer Benutzeroberflächen, zur Verbindung mit relationalen Datenbanken und zur Manipulation regulärer Ausdrücke sind ebenfalls enthalten.
 Mit Hilfe des mitgelieferten Moduls Tkinter kann in Python (wie in Perl und Tcl) schnell eine grafische Benutzeroberfläche (GUI) mit Tk erzeugt werden. Es gibt darüber hinaus eine Vielzahl von weiteren Wrappern von anderen Anbietern. Sie stellen Anbindungen (englisch language bindings) zu GUI-Toolkits wie z. B. PyGTK, PyQt, wxPython, PyObjC und PyFLTK zur Verfügung.
 Neben Tkinter wird auch ein Modul zum Zeichnen von Turtle-Grafiken mitgeliefert.
 Als nicht triviales Beispiel sei hier der kompakte Sortieralgorithmus Quicksort angegeben:
 Hier ermöglicht insbesondere die Listennotation für die Variablen links und rechts eine kompakte Darstellung. Zum Vergleich eine iterative Formulierung dieser zwei Zeilen:
 Dies ist nur ein Beispiel für die gesparte Schreibarbeit durch die Listennotation. Tatsächlich ist in diesem Fall die iterative Formulierung die schnellere, da pro Durchgang nur einmal über das Feld „liste“ iteriert wird und nicht zweimal wie in der Listennotation.
 So wie Lisp, Ruby, Groovy und Perl unterstützt der Python-Interpreter auch einen interaktiven Modus, in dem Ausdrücke am Terminal eingegeben und die Ergebnisse sofort betrachtet werden können. Das ist nicht nur für Neulinge angenehm, die die Sprache lernen, sondern auch für erfahrene Programmierer: Code-Stückchen können interaktiv ausgiebig getestet werden, bevor man sie in ein geeignetes Programm aufnimmt.
 Darüber hinaus steht mit Python Shell ein Kommandozeileninterpreter für verschiedene unixoide Computer-Betriebssysteme zur Verfügung, der neben klassischen Unix-Shellkommandos auch direkte Eingaben in Python-Form verarbeiten kann. IPython ist eine populäre interaktive Python-Shell mit stark erweiterter Funktionalität.
 CPython ist die offizielle oder Referenzimplementierung der Programmiersprache Python und deren Interpreter.
 Daneben gibt es einen in Java implementierten Python-Interpreter namens Jython, mit dem die Bibliothek der Java-Laufzeitumgebung für Python verfügbar gemacht wird.
 Außer den oben genannten Interpretern existieren Compiler, die Python-Code in eine andere Programmiersprache übersetzen.
 Mit Cython kann Python-Code in effiziente C-Erweiterungen übersetzt oder externer C++- oder C-Code angebunden werden.
 Ebenso existiert der Compiler IronPython für die .Net-Framework- bzw. Mono-Plattform.
 Um Python als Skriptsprache für Programme in C++ zu nutzen, werden zumeist die Boost-Python-Bibliothek oder (in neueren Projekten) Cython verwendet.
 Ein Python-Parser für Parrot und ein in Python geschriebener Just-in-time-Compiler für Python, PyPy, welcher von der EU gefördert wurde, sind ebenfalls in Entwicklung.
 Auch die Python-Variante Pyston verwendet Just-in-time-Compilierung und beschleunigt so die Ausführung von Python-Programmen.[52]
 Außerdem existiert ein Python-Interpreter für Mikrocontroller namens MicroPython.[53]
 Neben IDLE, das oft mit Python installiert wird und hauptsächlich aus einer Textumgebung und einer Shell besteht, wurden auch einige vollwertige Entwicklungsumgebungen (IDEs) für Python entwickelt, beispielsweise Eric Python IDE, Spyder oder PyCharm. Weiterhin gibt es Plug-ins für größere IDEs wie Eclipse, Visual Studio, IntelliJ IDEA[54] und NetBeans. Texteditoren für Programmierer wie Vim und Emacs lassen sich auch für Python anpassen: Ein einfacher Python-Mode ist bereits integriert, und komfortablere Erweiterungen können hinzugefügt werden.
 Für die verschiedenen GUI-Toolkits, wie z. B. Tkinter (GUI-Builder), WxPython (wxGlade), PyQt (Qt Designer), PySide, PyGTK (Glade), Kivy oder PyFLTK gibt es teils eigene Editoren, mit denen sich grafische Benutzeroberflächen auf vergleichsweise einfache Art aufbauen lassen.
 Python unterstützt die Erstellung von Paketen; dabei helfen distutils und setuptools. Die Pakete werden auf PyPI, dem Python Package Index, gespeichert und von dort zur Installation abgerufen. Als Paketmanager wird üblicherweise pip oder auf alten Systemen auch easy_install eingesetzt. Paketversionen der Anaconda (Python-Distribution) werden von der Paketverwaltung conda verwaltet.[55]
 Python ist für die meisten gängigen Betriebssysteme frei erhältlich und bei den meisten Linux-Distributionen im Standardumfang enthalten. Um Python in Webserver einzubinden, wird Webserver-umgreifend WSGI verwendet, welches die Nachteile von CGI umgeht. WSGI stellt eine universelle Schnittstelle zwischen Webserver und Python(-Framework) zur Verfügung.
 Eine Reihe von Web-Application-Frameworks nutzt Python, darunter Django, Pylons, SQLAlchemy, TurboGears, web2py, Flask und Zope. Ferner gibt es einen Python-Interpreter für das Symbian-Betriebssystem, so dass Python auf verschiedenen Mobiltelefonen verfügbar ist. In der Version 2.5.1 ist Python ein Bestandteil von AmigaOS 4.0.
 Bekannte kommerzielle Projekte, etwa Google Suche und YouTube, basieren in Teilen auf Python.[56] Auch in der Spieleindustrie findet die Sprache bisweilen Einsatz, etwa in EVE Online, World in Conflict und Civilization IV.
 Python wird gern in der Lehre eingesetzt, da Python auf der einen Seite einsteigerfreundlich ist, auf der anderen Seite aber auch leistungsfähig und mächtig genug, um theoretische Grundlagen der Programmierung zu vermitteln und um moderne Anwendungen bis hin zu komplexen Datenanalysen, grafischer Programmierung oder Datenbankanwendungen zu entwickeln.[57][58][59][60][61][62] Lehrbücher, die sich explizit an junge Menschen ohne Programmiererfahrung wenden, unterstützen und unterstreichen diese Entwicklung.[63]
 Im Rahmen des Projektes 100-Dollar-Laptop wird Python als Standardsprache der Benutzeroberfläche verwendet. Da der 100-Dollar-Rechner für die Schulausbildung von Kindern konzipiert ist, soll bei Benutzung der dafür gestalteten grafischen Benutzeroberfläche „Sugar“ auf Knopfdruck der gerade laufende Python-Quellcode angezeigt werden.[64] Damit soll Kindern die Möglichkeit gegeben werden, die dahinter liegende Informationstechnik real zu erleben und nach Belieben „hinter die Kulissen“ zu schauen.
 Der Einplatinen-Computer Raspberry Pi (Python Interpreter) sollte ursprünglich mit einem im ROM integrierten Python-Interpreter ausgeliefert werden.[65] Auch heute ist Python eine der bevorzugtesten Sprachen für den Raspberry Pi. Sein Standard-Betriebssystem Raspberry Pi OS kommt mit einer großen Python-Bibliothek zur Ansteuerung der Hardware.
 Python wird weltweit in der Informatikausbildung an Schulen und Universitäten eingesetzt. So steht inzwischen eine Reihe von (kostenlosen) didaktisch konzipierten Online-Lernplattformen zu Python für Schule und Selbststudium ab dem 6. Schuljahr zur Verfügung – meist in mehreren Sprachen. Der Online-Kurs Computer Science Circles z. B. wird von der Universität Waterloo in Kanada bereitgestellt. Die deutsche Version wird betrieben von den deutschen Bundesweiten Informatikwettbewerben.[66] TigerJython, gehostet von der Pädagogischen Hochschule Bern, wird vor allem in der Schweiz im Informatikunterricht eingesetzt.[67]
 In der Wissenschaftsgemeinde genießt Python große Verbreitung, hauptsächlich wegen des einfachen Einstiegs in die Programmierung und der großen Auswahl wissenschaftlicher Bibliotheken. Oft wird Python hier innerhalb eines Jupyter Notebooks genutzt.[68] Numerische Rechnungen und die visuelle Aufbereitung der Ergebnisse in Graphen werden meist mit NumPy und der Matplotlib erledigt. Anaconda und SciPy bündeln viele wissenschaftliche Python-Bibliotheken und machen sie somit einfacher zugänglich. Mit TensorFlow, Keras, Scikit-learn, PyTorch u. a. gibt es große Bibliotheken zur Forschung und Nutzung von maschinellem Lernen und Deep Learning (Künstliche Intelligenz).
 Die Unterstützung für Python 2 ist beendet. Die letzte 2er-Version war die 2.7.18 vom 20. April 2020;[69][70] seit diesem Datum wird Python 2 nicht mehr unterstützt.[71][72] Es gibt aber vielfältige und umfangreiche Dokumentationen zum Umstieg[73][74][75] und auch Tools, die bei der Migration helfen[76] oder es ermöglichen, Code zu schreiben, der mit Python 2 und 3 funktioniert.[77][78][79][80][81]
 Bei der Definition von Methoden muss der Parameter self, der der Instanz entspricht, deren Methode aufgerufen wird, explizit als Parameter angegeben werden. Dies wird von Andrew Kuchling, Autor und langjähriger Python-Entwickler[82], als unelegant und nicht objektorientiert empfunden.[83] Python-Schöpfer van Rossum verweist hingegen darauf, dass es nötig sei, um bestimmte wichtige Konstrukte zu ermöglichen.[84] Einer der Python-Grundsätze lautet zudem „Explicit is better than implicit“.[85]
 Bis zur Version 3.0 wurde kritisiert, dass in einer Methodendefinition der Aufruf der Basisklassenversion derselben Methode die explizite Angabe der Klasse und Instanz erfordert. Dies wurde als Verletzung des DRY-Prinzips („Don’t repeat yourself“) gesehen; außerdem behinderte es Umbenennungen. In Python 3.0 wurde dieser Kritikpunkt behoben.[86]
 Auf Multiprozessor-Systemen behindert der sogenannte Global Interpreter Lock (GIL) von CPython die Effizienz von Python-Anwendungen, die softwareseitiges Multithreading benutzen. Diese Beschränkung existiert unter Jython oder IronPython allerdings nicht. Bislang ist von offizieller Seite nicht geplant, den GIL zu ersetzen. Stattdessen wird empfohlen, mehrere miteinander kommunizierende Prozesse anstelle von Threads zu verwenden.[87][88]
 In den vorherrschenden Implementationen ist die Ausführungsgeschwindigkeit niedriger als bei vielen kompilierbaren Sprachen,[89] aber ähnlich wie bei Perl,[90] PHP,[91] Dart[92] und Ruby.[93] Das liegt zum Teil daran, dass bei der Entwicklung von CPython der Klarheit des Codes gegenüber der Geschwindigkeit Vorrang eingeräumt wird.[94] Man beruft sich dabei auf Autoritäten wie Donald Knuth und Tony Hoare, die von verfrühter Optimierung abraten. Wenn Geschwindigkeitsprobleme auftreten, die nicht durch Optimierung des Python-Codes gelöst werden können,[95] werden stattdessen JIT-Compiler wie PyPy verwendet oder zeitkritische Funktionen in maschinennähere Sprachen wie C oder Cython ausgelagert.
 Für den Einstieg
 Referenzen
 Weiterführendes
 Dieser Artikel ist als Audiodatei verfügbar:
 Mehr Informationen zur gesprochenen Wikipedia


Source: https://en.wikipedia.org/wiki/Datei:Python_logo_and_wordmark.svg
Content: Other reasons this message may be displayed:


Source: https://en.wikipedia.org/wiki/Programmierparadigma
Content: Other reasons this message may be displayed:


Source: https://en.wikipedia.org/wiki/Objektorientierte_Programmierung
Content: Other reasons this message may be displayed:


Source: https://en.wikipedia.org/wiki/Prozedurale_Programmierung
Content: Other reasons this message may be displayed:


Source: https://en.wikipedia.org/wiki/Imperative_Programmierung
Content: Other reasons this message may be displayed:


Source: https://en.wikipedia.org/wiki/Funktionale_Programmierung
Content: Other reasons this message may be displayed:


Source: https://en.wikipedia.org/wiki/Strukturierte_Programmierung
Content: Other reasons this message may be displayed:


Source: https://en.wikipedia.org/wiki/Reflexion_(Programmierung)
Content: Other reasons this message may be displayed:


Source: https://en.wikipedia.org/wiki/Guido_van_Rossum
Content: 
 Guido van Rossum (Dutch: [ˈɣido vɑn ˈrɔsʏm, -səm]; born 31 January 1956) is a Dutch programmer best known as the creator of the Python programming language, for which he was the "benevolent dictator for life" (BDFL) until he stepped down from the position on 12 July 2018.[4][5] He remained a member of the Python Steering Council through 2019, and withdrew from nominations for the 2020 election.[6]
 Van Rossum was born and raised in the Netherlands, where he received a master's degree in mathematics and computer science from the University of Amsterdam in 1982. He received a bronze medal in 1974 in the International Mathematical Olympiad.[7] He has a brother, Just van Rossum, who is a type designer and programmer who designed the typeface used in the "Python Powered" logo.[8]
 Van Rossum lives in Belmont, California, with his wife, Kim Knapp,[9] and their son.[10][11][12] According to his home page and Dutch naming conventions, the "van" in his name is capitalized when he is referred to by surname alone, but not when using his first and last name together.[13]
 While working at the Centrum Wiskunde & Informatica (CWI), Van Rossum wrote and contributed a glob() routine to BSD Unix in 1986[14][15] and helped develop the ABC programming language. He once stated, "I try to mention ABC's influence because I'm indebted to everything I learned during that project and to the people who worked on it."[16] He also created Grail, an early web browser written in Python, and engaged in discussions about the HTML standard.[17]
 He has worked for various research institutes, including the Centrum Wiskunde & Informatica (CWI) in the Netherlands, the U.S. National Institute of Standards and Technology (NIST), and the Corporation for National Research Initiatives (CNRI). In May 2000, he left CNRI along with three other Python core developers to work for tech startup BeOpen.com, which subsequently collapsed by October of the same year.[18][19] From late 2000 until 2003 he worked for Zope Corporation. In 2003 Van Rossum left Zope for Elemental Security. While there he worked on a custom programming language for the organization.[20]
 From 2005 to December 2012, he worked at Google, where he spent half of his time developing the Python language. 
At Google, Van Rossum developed Mondrian, a web-based code review system written in Python and used within the company. He named the software after the Dutch painter Piet Mondrian.[21] He named another related software project after Gerrit Rietveld, a Dutch designer.[22] On 7 December 2012, Van Rossum left Google.[23]
 In January 2013, Van Rossum started working at the cloud file storage company Dropbox.[24][25]
 In October 2019, Van Rossum left Dropbox and officially retired.[26][27][28]
 On 12 November 2020 Van Rossum announced that he was coming out of retirement to join the Developer Division at Microsoft. He currently holds the title Distinguished Engineer at Microsoft.[29][30][31]
 In December 1989, Van Rossum had been looking for a "'hobby' programming project that would keep [him] occupied during the week around Christmas" as his office was closed when he decided to write an interpreter for a "new scripting language [he] had been thinking about lately: a descendant of ABC that would appeal to Unix/C hackers". He attributes choosing the name "Python" to "being in a slightly irreverent mood (and a big fan of Monty Python's Flying Circus)".[32]
 He has explained that Python's predecessor, ABC, was inspired by SETL, noting that ABC co-developer Lambert Meertens had "spent a year with the SETL group at NYU before coming up with the final ABC design".[33]
 On 12 July 2018, Van Rossum announced that he would be stepping down from the position of BDFL of the Python programming language.[34]
 In 1999, Van Rossum submitted a funding proposal to DARPA called "Computer Programming for Everybody", in which he further defined his goals for Python:
 In 2019, Python became the second most popular language on GitHub, the largest source code management website on the internet, second only to JavaScript.[35] According to a programming language popularity survey[36] it is consistently among the top 10 most mentioned languages in job postings. Furthermore, Python has been among the 10 most popular programming languages every year since 2004 according to the TIOBE Programming Community Index and got the number one spot on the index in October 2021.[37]


Source: https://en.wikipedia.org/wiki/Python_Software_Foundation
Content: The Python Software Foundation (PSF) is an American nonprofit organization devoted to the Python programming language,[3] launched on March 6, 2001. The mission of the foundation is to foster development of the Python community and is responsible for various processes within the Python community, including developing the core Python distribution, managing intellectual rights, developer conferences including the Python Conference (PyCon), and raising funds.
 In 2005, the Python Software Foundation received the Computerworld Horizon Award for "cutting-edge" technology.[4][5]
 The PSF focuses on empowering and supporting people within the Python community with grant programs that support sprints, conferences, meetups, user groups, and Python development.  The PSF runs Python Conference (PyCon) US, the leading Python community conference.  The PSF is the primary point of contact for organizations that wish to work with Python, to support Python, or sponsor Python development. The PSF provides a structure by which work, donations, and sponsorships are coordinated worldwide. The PSF also possesses and protects intellectual property associated with Python and the Python community, such as the word "Python," the two-snakes logo, and the terms "PyLadies" and "PyCon."[6]
 There are five tiers of membership within the PSF. These tiers include:
 Since late 2012, the Python Software Foundation started recommending that all Python conferences create and apply a code of conduct. This is mandatory to any event to be granted funds by the Python Software Foundation.[8]
 
 This article about an organization in the United States is a stub. You can help Wikipedia by expanding it.

Source: https://en.wikipedia.org/wiki/Guido_van_Rossum
Content: 
 Guido van Rossum (Dutch: [ˈɣido vɑn ˈrɔsʏm, -səm]; born 31 January 1956) is a Dutch programmer best known as the creator of the Python programming language, for which he was the "benevolent dictator for life" (BDFL) until he stepped down from the position on 12 July 2018.[4][5] He remained a member of the Python Steering Council through 2019, and withdrew from nominations for the 2020 election.[6]
 Van Rossum was born and raised in the Netherlands, where he received a master's degree in mathematics and computer science from the University of Amsterdam in 1982. He received a bronze medal in 1974 in the International Mathematical Olympiad.[7] He has a brother, Just van Rossum, who is a type designer and programmer who designed the typeface used in the "Python Powered" logo.[8]
 Van Rossum lives in Belmont, California, with his wife, Kim Knapp,[9] and their son.[10][11][12] According to his home page and Dutch naming conventions, the "van" in his name is capitalized when he is referred to by surname alone, but not when using his first and last name together.[13]
 While working at the Centrum Wiskunde & Informatica (CWI), Van Rossum wrote and contributed a glob() routine to BSD Unix in 1986[14][15] and helped develop the ABC programming language. He once stated, "I try to mention ABC's influence because I'm indebted to everything I learned during that project and to the people who worked on it."[16] He also created Grail, an early web browser written in Python, and engaged in discussions about the HTML standard.[17]
 He has worked for various research institutes, including the Centrum Wiskunde & Informatica (CWI) in the Netherlands, the U.S. National Institute of Standards and Technology (NIST), and the Corporation for National Research Initiatives (CNRI). In May 2000, he left CNRI along with three other Python core developers to work for tech startup BeOpen.com, which subsequently collapsed by October of the same year.[18][19] From late 2000 until 2003 he worked for Zope Corporation. In 2003 Van Rossum left Zope for Elemental Security. While there he worked on a custom programming language for the organization.[20]
 From 2005 to December 2012, he worked at Google, where he spent half of his time developing the Python language. 
At Google, Van Rossum developed Mondrian, a web-based code review system written in Python and used within the company. He named the software after the Dutch painter Piet Mondrian.[21] He named another related software project after Gerrit Rietveld, a Dutch designer.[22] On 7 December 2012, Van Rossum left Google.[23]
 In January 2013, Van Rossum started working at the cloud file storage company Dropbox.[24][25]
 In October 2019, Van Rossum left Dropbox and officially retired.[26][27][28]
 On 12 November 2020 Van Rossum announced that he was coming out of retirement to join the Developer Division at Microsoft. He currently holds the title Distinguished Engineer at Microsoft.[29][30][31]
 In December 1989, Van Rossum had been looking for a "'hobby' programming project that would keep [him] occupied during the week around Christmas" as his office was closed when he decided to write an interpreter for a "new scripting language [he] had been thinking about lately: a descendant of ABC that would appeal to Unix/C hackers". He attributes choosing the name "Python" to "being in a slightly irreverent mood (and a big fan of Monty Python's Flying Circus)".[32]
 He has explained that Python's predecessor, ABC, was inspired by SETL, noting that ABC co-developer Lambert Meertens had "spent a year with the SETL group at NYU before coming up with the final ABC design".[33]
 On 12 July 2018, Van Rossum announced that he would be stepping down from the position of BDFL of the Python programming language.[34]
 In 1999, Van Rossum submitted a funding proposal to DARPA called "Computer Programming for Everybody", in which he further defined his goals for Python:
 In 2019, Python became the second most popular language on GitHub, the largest source code management website on the internet, second only to JavaScript.[35] According to a programming language popularity survey[36] it is consistently among the top 10 most mentioned languages in job postings. Furthermore, Python has been among the 10 most popular programming languages every year since 2004 according to the TIOBE Programming Community Index and got the number one spot on the index in October 2021.[37]


Source: https://en.wikipedia.org/wiki/Version_(Software)
Content: Other reasons this message may be displayed:


Source: https://en.wikipedia.org/wiki/Typisierung_(Informatik)
Content: Other reasons this message may be displayed:


Source: https://en.wikipedia.org/wiki/Starke_Typisierung
Content: Other reasons this message may be displayed:


Source: https://en.wikipedia.org/wiki/Dynamische_Typisierung
Content: Other reasons this message may be displayed:


Source: https://en.wikipedia.org/wiki/Duck-Typing
Content: Other reasons this message may be displayed:


Source: https://en.wikipedia.org/wiki/Implementierung
Content: Other reasons this message may be displayed:


Source: https://en.wikipedia.org/wiki/Jython
Content: 
 Jython is an implementation of the Python programming language designed to run on the Java platform. It was known as JPython until 1999.[3]
 Jython programs can import and use any Java class. Except for some standard modules, Jython programs use Java classes instead of Python modules. Jython includes almost all of the modules in the standard Python programming language distribution, lacking only some of the modules implemented originally in C. For example, a user interface in Jython could be written with Swing, AWT or SWT. Jython compiles Python source code to Java bytecode (an intermediate language) either on demand or statically.
 Jython was initially created in late 1997 to replace C with Java for performance-intensive code accessed by Python programs, moving to SourceForge in October 2000. The Python Software Foundation awarded a grant in January 2005.  Jython 2.5 was released in June 2009.[4]
 The most recent release is Jython 2.7.3. It was released on September 10, 2022 and is compatible with Python 2.7.[5]
 Python 3 compatible changes are planned in Jython 3 Roadmap.[6]
 Although Jython implements the Python language specification, it has some differences and incompatibilities with CPython, which is the reference implementation of Python.[7][8]
 From version 2.2 on, Jython (including the standard library) is released under the Python Software Foundation License (v2). Older versions are covered by the Jython 2.0, 2.1 license and the JPython 1.1.x Software License.[9]
 The command-line interpreter is available under the Apache Software License.


Source: https://en.wikipedia.org/wiki/IronPython
Content: IronPython is an implementation of the Python programming language targeting the .NET and Mono frameworks. The project is currently maintained by a group of volunteers at GitHub. It is free and open-source software, and can be implemented with Python Tools for Visual Studio, which is a free and open-source extension for Microsoft's Visual Studio IDE.[2][3]
 IronPython is written entirely in C#, although some of its code is automatically generated by a code generator written in Python.
 IronPython is implemented on top of the Dynamic Language Runtime (DLR), a library running on top of the Common Language Infrastructure that provides dynamic typing and dynamic method dispatch, among other things, for dynamic languages.[4] The DLR is part of the .NET Framework 4.0 and is also a part of Mono since version 2.4 from 2009.[5] The DLR can also be used as a library on older CLI implementations.
 Jim Hugunin created the project and actively contributed to it up until Version 1.0 which was released on September 5, 2006.[6] IronPython 2.0 was released on December 10, 2008.[7] After version 1.0 it was maintained by a small team at Microsoft until the 2.7 Beta 1 release. Microsoft abandoned IronPython (and its sister project IronRuby) in late 2010, after which Hugunin left to work at Google.[8] The project is currently maintained by a group of volunteers at GitHub.
 There are some differences between the Python reference implementation CPython and IronPython.[23] Some projects built on top of IronPython are known not to work under CPython.[24] Conversely, CPython applications that depend on extensions to the language that are implemented in C are not compatible with IronPython
,[25] unless they are implemented in a .NET interop. For example, NumPy was wrapped by Microsoft in 2011, allowing code and libraries dependent on it to be run directly from .NET Framework.[26]
 IronPython is supported on Silverlight (which is deprecated by Microsoft and already has lost support in most web browsers[27]). It can be used as a scripting engine in the browser just like the JavaScript engine.[28] IronPython scripts are passed like simple client-side JavaScript scripts in <script>-tags. It is then also possible to modify embedded XAML markup.
 The technology behind this is called Gestalt.[citation needed]
 The same works for IronRuby.
 Until version 0.6, IronPython was released under the terms of Common Public License.[29] Following recruitment of the project lead in August 2004, IronPython was made available as part of Microsoft's Shared Source initiative. This license is not OSI-approved but the authors claim it meets the open-source definition.[30] With the 2.0 alpha release, the license was changed to the Microsoft Public License,[31] which the OSI has approved. The latest versions are released under the terms of the Apache License 2.0.
 One of IronPython's key advantages is in its function as an extensibility layer to application frameworks written in a .NET language. It is relatively simple to integrate an IronPython interpreter into an existing .NET application framework. Once in place, downstream developers can use scripts written in IronPython that interact with .NET objects in the framework, thereby extending the functionality in the framework's interface, without having to change any of the framework's code base.[32]
 IronPython makes extensive use of reflection. When passed in a reference to a .NET object, it will automatically import the types and methods available to that object. This results in a highly intuitive experience when working with .NET objects from within an IronPython script.
 The following IronPython script manipulates .NET Framework objects. This script can be supplied by a third-party client-side application developer and passed into the server-side framework through an interface. Note that neither the interface, nor the server-side code is modified to support the analytics required by the client application.
 In this case, assume that the .NET Framework implements a class, BookDictionary, in a module called BookService, and publishes an interface into which IronPython scripts can be sent and executed.
 This script, when sent to that interface, will iterate over the entire list of books maintained by the framework, and pick out those written by Booker Prize-winning authors.
 What's interesting is that the responsibility for writing the actual analytics reside with the client-side developer. The demands on the server-side developer are minimal, essentially just providing access to the data maintained by the server. This design pattern greatly simplifies the deployment and maintenance of complex application frameworks.
 The following script uses the .NET Framework to create a simple Hello World message.
 The performance characteristics of IronPython compared to CPython, the reference implementation of Python, depends on the exact benchmark used. IronPython performs worse than CPython on most benchmarks taken with the PyStone script but better on other benchmarks.[33]
IronPython may perform better in Python programs that use threads or multiple cores, as it has a JIT compiler, and also because it doesn't have the Global Interpreter Lock.[34][35]


Source: https://en.wikipedia.org/wiki/PyPy
Content: PyPy (/ˈpaɪpaɪ/) is an implementation of the Python programming language.[2] PyPy often runs faster than the standard implementation CPython because PyPy uses a just-in-time compiler.[3] Most Python code runs well on PyPy except for code that depends on CPython extensions, which either does not work or incurs some overhead when run in PyPy.
 PyPy itself is built using a technique known as meta-tracing, which is a mostly automatic transformation that takes an interpreter as input and produces a tracing just-in-time compiler as output. Since interpreters are usually easier to write than compilers, but run slower, this technique can make it easier to produce efficient implementations of programming languages. PyPy's meta-tracing toolchain is called RPython.
 PyPy does not have full compatibility with more recent versions of the CPython ecosystem. While it claims compatibility with Python 2.7, 3.7, 3.8 and 3.9 ("a drop-in replacement for CPython"), it lacks some of the newer features and syntax in Python 3.10, such as syntax for pattern matching.[4]
 PyPy aims to provide a common translation and support framework for producing implementations of dynamic languages, emphasizing a clean separation between language specification and implementation aspects. It also aims to provide a compliant, flexible and fast implementation of the Python programming language using the above framework to enable new advanced features without having to encode low-level details into it.[5][6]
 The PyPy interpreter itself is written in a restricted subset of Python called RPython (Restricted Python).[7] RPython puts some constraints on the Python language such that a variable's type can be inferred at compile time.[8]
 The PyPy project has developed a toolchain that analyzes RPython code and translates it into a form of byte code, which can be lowered into C. There used to be other backends in addition to C (Java, C#, and Javascript), but those suffered from bitrot and have been removed. Thus, the recursive logo of PyPy is a snake swallowing itself since the RPython is translated by a Python interpreter. The code can also be run untranslated for testing and analysis, which provides a nice test-bed for research into dynamic languages.
 It allows for pluggable garbage collectors, as well as optionally enabling Stackless Python features. Finally, it includes a just-in-time (JIT) generator that builds a just-in-time compiler into the interpreter, given a few annotations in the interpreter source code. The generated JIT compiler is a tracing JIT.[9]
 RPython is now also used to write non-Python language implementations, such as Pixie.[10]
 PyPy as of version 7.3.7 is compatible with three CPython versions: 2.7, 3.7 and 3.8.[11][12] The first PyPy version compatible with CPython v3 is PyPy v2.3.1 (2014).[13] The PyPy interpreter compatible with CPython v3 is also known as PyPy3.
 PyPy has JIT compilation support on 32-bit/64-bit x86 and 32-bit/64-bit ARM processors.[14] It is tested nightly on Windows, Linux, OpenBSD and Mac OS X. PyPy is able to run pure Python software that does not rely on implementation-specific features.[15]
 There is a compatibility layer for CPython C API extensions called CPyExt, but it is incomplete and experimental. The preferred way of interfacing with C shared libraries is through the built-in C foreign function interface (CFFI) or ctypes libraries.
 PyPy is a followup to the Psyco project, a just-in-time specializing compiler for Python, developed by Armin Rigo between 2002 and 2010. PyPy's aim is to have a just-in-time specializing compiler with scope, which was not available for Psyco.[clarification needed] Initially, the RPython could also be compiled into Java bytecode, CIL and JavaScript, but these backends were removed due to lack of interest.
 PyPy was initially a research and development-oriented project. Reaching a mature state of development and an official 1.0 release in mid-2007, its next focus was on releasing a production-ready version with more CPython compatibility. Many of PyPy's changes have been made during coding sprints.
 PyPy was funded by the European Union being a Specific Targeted Research Project[31] between December 2004 and March 2007. In June 2008, PyPy announced funding being part of the Google Open Source programs and has agreed to focus on making PyPy more compatible with CPython. In 2009 Eurostars, a European Union funding agency specially focused on SMEs,[32] accepted a proposal from PyPy project members titled "PYJIT – a fast and flexible toolkit for dynamic programming languages based on PyPy". Eurostars funding lasted until August 2011.[33]
At PyCon US 2011, the Python Software Foundation provided a $10,000 grant for PyPy to continue work on performance and compatibility with newer versions of the language.[34]
The port to ARM architecture was sponsored in part by the Raspberry Pi Foundation.[22]
 The PyPy project also accepts donations through its status blog pages.[35] As of 2013, a variety of sub-projects had funding: Python 3 version compatibility, built-in optimized NumPy support for numerical calculations and software transactional memory support to allow better parallelism.[22]


Source: https://en.wikipedia.org/wiki/Algol_68
Content: 
 ALGOL 68 (short for Algorithmic Language 1968) is an imperative programming language that was conceived as a successor to the ALGOL 60 programming language, designed with the goal of a much wider scope of application and more rigorously defined syntax and semantics.
 The complexity of the language's definition, which runs to several hundred pages filled with non-standard terminology, made compiler implementation difficult and it was said it had "no implementations and no users". This was only partly true; ALGOL 68 did find use in several niche markets, notably in the United Kingdom where it was popular on International Computers Limited (ICL) machines, and in teaching roles. Outside these fields, use was relatively limited.
 Nevertheless, the contributions of ALGOL 68 to the field of computer science have been deep, wide-ranging and enduring, although many of these contributions were only publicly identified when they had reappeared in subsequently developed programming languages. Many languages were developed specifically as a response to the perceived complexity of the language, the most notable being Pascal, or were reimplementations for specific roles, like Ada.
 Many languages of the 1970s trace their design specifically to ALGOL 68, selecting some features while abandoning others that were considered too complex or out-of-scope for given roles. Among these is the language C, which was directly influenced by ALGOL 68, especially by its strong typing and structures. Most modern languages trace at least some of their syntax to either C or Pascal, and thus directly or indirectly to ALGOL 68.
 ALGOL 68 features include expression-based syntax, user-declared types and structures/tagged-unions, a reference model of variables and reference parameters, string, array and matrix slicing, and concurrency.
 ALGOL 68 was designed by the International Federation for Information Processing (IFIP) IFIP Working Group 2.1 on Algorithmic Languages and Calculi. On December 20, 1968, the language was formally adopted by the group, and then approved for publication by the General Assembly of IFIP.
 ALGOL 68 was defined using a formalism, a two-level formal grammar, invented by Adriaan van Wijngaarden. Van Wijngaarden grammars use a context-free grammar to generate an infinite set of productions that will recognize a particular ALGOL 68 program; notably, they are able to express the kind of requirements that in many other programming language technical standards are labelled semantics, and must be expressed in ambiguity-prone natural language prose, and then implemented in compilers as ad hoc code attached to the formal language parser.
 ALGOL 68 was the first (and possibly one of the last) major language for which a full formal definition was made before it was implemented. 
 C. H. A. Koster[8] The main aims and principles of design of ALGOL 68:
 ALGOL 68 has been criticized, most prominently by some members of its design committee such as C. A. R. Hoare and Edsger Dijkstra, for abandoning the simplicity of ALGOL 60, becoming a vehicle for complex or overly general ideas, and doing little to make the compiler writer's task easier, in contrast to deliberately simple contemporaries (and competitors) such as C, S-algol and Pascal.
 In 1970, ALGOL 68-R became the first working compiler for ALGOL 68.
 In the 1973 revision, certain features — such as proceduring, gommas[13] and formal bounds — were omitted.[14] C.f. The language of the unrevised report.r0
 Though European defence agencies (in Britain Royal Signals and Radar Establishment (RSRE)) promoted the use of ALGOL 68 for its expected security advantages, the American side of the NATO alliance decided to develop a different project, the language Ada, making its use obligatory for US defense contracts.
 ALGOL 68 also had a notable influence in the Soviet Union, details of which can be found in Andrey Terekhov's 2014 paper: "ALGOL 68 and Its Impact on the USSR and Russian Programming",[15] and "Алгол 68 и его влияние на программирование в СССР и России".[16]
 Steve Bourne, who was on the ALGOL 68 revision committee, took some of its ideas to his Bourne shell (and thereby, to descendant Unix shells such as Bash) and to C (and thereby to descendants such as C++).
 The complete history of the project can be found in C. H. Lindsey's A History of ALGOL 68.[17][18]
 For a full-length treatment of the language, see "Programming ALGOL 68 Made Easy"[19] by Dr. Sian Mountbatten, or "Learning ALGOL 68 Genie"[20] by Marcel van der Veer which includes the Revised Report.
 ALGOL 68, as the name implies, is a follow-on to the ALGOL language that was first formalized in 1960. That same year the International Federation for Information Processing (IFIP) formed and started the Working Group on ALGOL, or WG2.1. This group released an updated ALGOL 60 specification in Rome in April 1962. At a follow-up meeting in March 1964, it was agreed that the group should begin work on two follow-on standards, ALGOL X which would be a redefinition of the language with some additions, and an ALGOL Y, which would have the ability to modify its own programs in the style of the language LISP.[21]
 The first meeting of the ALGOL X group was held in Princeton University in May 1965. A report of the meeting noted two broadly supported themes, the introduction of strong typing and interest in Euler's concepts of 'trees' or 'lists' for handling collections.[22]
 At the second meeting in October in France, three formal proposals were presented, Niklaus Wirth's ALGOL W along with comments about record structures by C.A.R. (Tony) Hoare, a similar language by Gerhard Seegmüller, and a paper by Adriaan van Wijngaarden on "Orthogonal design and description of a formal language". The latter, written in almost indecipherable "W-Grammar", proved to be a decisive shift in the evolution of the language. The meeting closed with an agreement that van Wijngaarden would re-write the Wirth/Hoare submission using his W-Grammar.[22]
 This seemingly simple task ultimately proved more difficult than expected, and the follow-up meeting had to be delayed six months. When it met in April 1966 in Kootwijk, van Wijngaarden's draft remained incomplete and Wirth and Hoare presented a version using more traditional descriptions. It was generally agreed that their paper was "the right language in the wrong formalism".[23] As these approaches were explored, it became clear there was a difference in the way parameters were described that would have real-world effects, and while Wirth and Hoare protested that further delays might become endless, the committee decided to wait for van Wijngaarden's version. Wirth then implemented their current definition as ALGOL W.[24]
 At the next meeting in Warsaw in October 1966,[25] there was an initial report from the I/O Subcommittee who had met at the Oak Ridge National Laboratory and the University of Illinois but had not yet made much progress. The two proposals from the previous meeting were again explored, and this time a new debate emerged about the use of pointers; ALGOL W used them only to refer to records, while van Wijngaarden's version could point to any object. To add confusion, John McCarthy presented a new proposal for operator overloading and the ability to string together and or constructs, and Klaus Samelson wanted to allow anonymous functions. In the resulting confusion, there was some discussion of abandoning the entire effort.[24] The confusion continued through what was supposed to be the ALGOL Y meeting in Zandvoort in May 1967.[22]
 A draft report was finally published in February 1968. This was met by "shock, horror and dissent",[22] mostly due to the hundreds of pages of unreadable grammar and odd terminology. Charles H. Lindsey attempted to figure out what "language was hidden inside of it",[26] a process that took six man-weeks of effort. The resulting paper, "ALGOL 68 with fewer tears", was widely circulated. At a wider information processing meeting in Zürich in May 1968, attendees complained that the language was being forced upon them and that IFIP was "the true villain of this unreasonable situation" as the meetings were mostly closed and there was no formal feedback mechanism. Wirth and Peter Naur formally resigned their authorship positions in WG2.1 at that time.[26]
 The next WG2.1 meeting took place in Tirrenia in June 1968. It was supposed to discuss the release of compilers and other issues, but instead devolved into a discussion on the language. van Wijngaarden responded by saying (or threatening) that he would release only one more version of the report. By this point Naur, Hoare, and Wirth had left the effort, and several more were threatening to do so.[27] Several more meetings followed, North Berwick in August 1968, Munich in December which produced the release of the official Report in January 1969 but also resulted in a contentious Minority Report being written. Finally, at Banff, Alberta in September 1969, the project was generally considered complete and the discussion was primarily on errata and a greatly expanded Introduction to the Report.[28]
 The effort took five years, burned out many of the greatest names in computer science, and on several occasions became deadlocked over issues both in the definition and the group as a whole. Hoare released a "Critique of ALGOL 68" almost immediately,[29] which has been widely referenced in many works. Wirth went on to further develop the ALGOL W concept and released this as Pascal in 1970.
 The first implementation of the standard, based on the late-1968 draft Report, was introduced by the Royal Radar Establishment in the UK as ALGOL 68-R in July 1970. This was, however, a subset of the full language, and Barry Mailloux, the final editor of the Report, joked that "It is a question of morality.  We have a Bible and you are sinning!"[30] This version nevertheless became very popular on the ICL machines, and became a widely-used language in military coding, especially in the UK.[31]
 Among the changes in 68-R was the requirement for all variables to be declared before their first use. This had a significant advantage that it allowed the compiler to be one-pass, as space for the variables in the activation record was set aside before it was used. However, this change also had the side-effect of demanding the PROCs be declared twice, once as a declaration of the types, and then again as the body of code. Another change was to eliminate the assumed VOID mode, an expression that returns no value (named a statement in other languages) and demanding the word VOID be added where it would have been assumed. Further, 68-R eliminated the explicit parallel processing commands based on PAR.[30]
 The first full implementation of the language was introduced in 1974 by CDC Netherlands for the Control Data mainframe series. This saw limited use, mostly teaching in Germany and the Netherlands.[31]
 A version similar to 68-R was introduced from Carnegie Mellon University in 1976 as 68S, and was again a one-pass compiler based on various simplifications of the original and intended for use on smaller machines like the DEC PDP-11. It too was used mostly for teaching purposes.[31]
 A version for IBM mainframes did not become available until 1978, when one was released from Cambridge University. This was "nearly complete". Lindsey released a version for small machines including the IBM PC in 1984.[31]
 Three open source Algol 68 implementations are known:[32]
 "Van Wijngaarden once characterized the four authors, somewhat tongue-in-cheek, as: Koster: transputter, Peck: syntaxer, Mailloux: implementer, Van Wijngaarden: party ideologist." – Koster. 1968: On 20 December 1968, the "Final Report" (MR 101) was adopted by the Working Group, then subsequently approved by the General Assembly of UNESCO's IFIP for publication. Translations of the standard were made for Russian, German, French and Bulgarian, and then later Japanese and Chinese.[46] The standard was also made available in Braille.
 1984: TC 97 considered ALGOL 68 for standardisation as "New Work Item" TC97/N1642 [2][3]. West Germany, Belgium, Netherlands, USSR and Czechoslovakia willing to participate in preparing the standard but the USSR and Czechoslovakia "were not the right kinds of member of the right ISO committees"[4] and Algol 68's ISO standardisation stalled.[5]
 1988: Subsequently ALGOL 68 became one of the GOST standards in Russia.
 The standard language contains about sixty reserved words, typically bolded in print, and some with "brief symbol" equivalents:
 The basic language construct is the unit. A unit may be a formula, an enclosed clause, a routine text or one of several technically needed constructs (assignation, jump, skip, nihil). The technical term enclosed clause unifies some of the inherently bracketing constructs known as block, do statement, switch statement in other contemporary languages. When keywords are used, generally the reversed character sequence of the introducing keyword is used for terminating the enclosure, e.g. ( IF ~ THEN ~ ELSE ~ FI, CASE ~ IN ~ OUT ~ ESAC, FOR ~ WHILE ~ DO ~ OD ). This Guarded Command syntax was reused by Stephen Bourne in the common Unix Bourne shell. An expression may also yield a multiple value, which is constructed from other values by a collateral clause. This construct just looks like the parameter pack of a procedure call.
 The basic data types (called modes in Algol 68 parlance) are real, int, compl (complex number), bool, char, bits and bytes. For example:
 However, the declaration REAL x; is just syntactic sugar for REF REAL x = LOC REAL;. That is, x is really the constant identifier for a reference to a newly generated local REAL variable.
 Furthermore, instead of defining both float and double, or int and long and short, etc., ALGOL 68 provides modifiers, so that the presently common double would be written as LONG REAL or LONG LONG REAL instead, for example. The prelude constants max real and min long int are provided to adapt programs to different implementations.
 All variables need to be declared, but declaration does not have to precede the first use.
 primitive-declarer: INT, REAL, COMPL, COMPLEXG, BOOL, CHAR, STRING, BITS, BYTES, FORMAT, FILE, PIPEG, CHANNEL, SEMA
 Complex types can be created from simpler ones using various type constructors:
 For some examples, see Comparison of ALGOL 68 and C++.
 Other declaration symbols include:  FLEX, HEAP, LOC, REF, LONG, SHORT, EVENTS
 A name for a mode (type) can be declared using a MODE declaration,
which is similar to TYPEDEF in C/C++ and TYPE in Pascal:
 This is similar to the following C code:
 For ALGOL 68, only the NEWMODE mode-indication appears to the left of the equals symbol, and most notably the construction is made, and can be read, from left to right without regard to priorities. Also, the lower bound of Algol 68 arrays is one by default, but can be any integer from -max int to max int.
 Mode declarations allow types to be recursive: defined directly or indirectly in terms of themselves.
This is subject to some restrictions – for instance, these declarations are illegal:
 while these are valid:
 The coercions produce a coercee from a coercend according to three criteria: the a priori mode of the coercend before the application of any coercion, the a posteriori mode of the coercee required after those coercions, and the syntactic position or "sort" of the coercee. Coercions may be cascaded.
 The six possible coercions are termed deproceduring, dereferencing, uniting, widening, rowing, and voiding. Each coercion, except for uniting, prescribes a corresponding dynamic effect on the associated values. Hence, many primitive actions can be programmed implicitly by coercions.
 Context strength – allowed coercions:
 ALGOL 68 has a hierarchy of contexts which determine the kind of coercions available at a particular point in the program. These contexts are:
 Also:
 Widening occurs if there is no loss of precision. For example: An INT will be coerced to a REAL, and a REAL will be coerced to a LONG REAL. But not vice versa.  Examples:
 A variable can also be coerced (rowed) to an array of length 1.
 For example:
 UNION(INT,REAL) var := 1
 IF ~ THEN ... FI and 
FROM ~ BY ~ TO ~ WHILE ~ DO ... OD etc
 For more details about Primaries, Secondaries, Tertiary & Quaternaries refer to Operator precedence.
 Pragmats are directives in the program, typically hints to the compiler; in newer languages these are called "pragmas" (no 't'). e.g.
 Comments can be inserted in a variety of ways:
 Normally, comments cannot be nested in ALGOL 68. This restriction can be circumvented by using different comment delimiters (e.g. use hash only for temporary code deletions).
 ALGOL 68 being an expression-oriented programming language, the value returned by an assignment statement is a reference to the destination. Thus, the following is valid ALGOL 68 code:
 This notion is present in C and Perl, among others. Note that as in earlier languages such as Algol 60 and FORTRAN, spaces are allowed in identifiers, so that half pi is a single identifier (thus avoiding the underscores versus camel case versus all lower-case issues).
 As another example, to express the mathematical idea of a sum of f(i) from i=1 to n, the following ALGOL 68 integer expression suffices:
 Note that, being an integer expression, the former block of code can be used in any context where an integer value can be used. A block of code returns the value of the last expression it evaluated; this idea is present in Lisp, among other languages.
 Compound statements are all terminated by distinctive closing brackets:
 This scheme not only avoids the dangling else problem but also avoids having to use BEGIN and END in embedded statement sequences.
 Choice clause example with Brief symbols:
 Choice clause example with Bold symbols:
 Choice clause example mixing Bold and Brief symbols:
 Algol68 allowed the switch to be of either type INT or (uniquely) UNION. The latter allows the enforcing strong typing onto UNION variables. c.f. union below for example.
 This was considered the "universal" loop, the full syntax is:
 The construct have several unusual aspects:
 Subsequent "extensions" to the standard Algol68 allowed the TO syntactic element to be replaced with UPTO and DOWNTO to achieve a small optimisation.  The same compilers also incorporated:
 Further examples can be found in the code examples below.
 ALGOL 68 supports arrays with any number of dimensions, and it allows for the slicing of whole or partial rows or columns.
 Matrices can be sliced either way, e.g.:
 ALGOL 68 supports multiple field structures (STRUCT) and united modes. Reference variables may point to any MODE including array slices and structure fields.
 For an example of all this, here is the traditional linked list declaration:
 Usage example for UNION CASE of NODE:
 Procedure (PROC) declarations require type specifications for both the parameters and the result (VOID if none):
 or, using the "brief" form of the conditional statement:
 The return value of a proc is the value of the last expression evaluated in the procedure. References to procedures (ref proc) are also permitted. Call-by-reference parameters are provided by specifying references (such as ref real) in the formal argument list. The following example defines a procedure that applies a function (specified as a parameter) to each element of an array:
 This simplicity of code was unachievable in ALGOL 68's predecessor ALGOL 60.
 The programmer may define new operators and both those and the pre-defined ones may be overloaded and their priorities may be changed by the coder. The following example defines operator MAX with both dyadic and monadic versions (scanning across the elements of an array).
 These are technically not operators, rather they are considered "units associated with names"
 -, ABS, ARG, BIN, ENTIER, LENG, LEVEL, ODD, REPR, ROUND, SHORTEN
 -:=, +:=, *:=, /:=, %:=, %*:=, +=:
 Specific details:
 These are technically not operators, rather they are considered "units associated with names"
 Note: Quaternaries include names SKIP and ~.
 ":=:" (alternatively "IS") tests if two pointers are equal; ":/=:" (alternatively "ISNT") tests if they are unequal.
 Why :=: and :/=: are needed: Consider trying to compare two pointer values, such as the following variables, declared as pointers-to-integer:
 Now consider how to decide whether these two are pointing to the same location, or whether one of them is pointing to NIL. The following expression
 will dereference both pointers down to values of type INT, and compare those, since the "=" operator is defined for INT, but not REF INT. It is not legal to define "=" for operands of type REF INT and INT at the same time, because then calls become ambiguous, due to the implicit coercions that can be applied: should the operands be left as REF INT and that version of the operator called? Or should they be dereferenced further to INT and that version used instead? Therefore the following expression can never be made legal:
 Hence the need for separate constructs not subject to the normal coercion rules for operands to operators. But there is a gotcha. The following expressions:
 while legal, will probably not do what might be expected. They will always return FALSE, because they are comparing the actual addresses of the variables ip and jp, rather than what they point to. To achieve the right effect, one would have to write
 Most of Algol's "special" characters (⊂, ≡, ␣, ×, ÷, ≤, ≥, ≠, ¬, ⊃, ≡, ∨, ∧, →, ↓, ↑, ⌊, ⌈, ⎩, ⎧, ⊥, ⏨, ¢, ○ and □) can be found on the IBM 2741 keyboard with the APL "golf-ball" print head inserted; these became available in the mid-1960s while ALGOL 68 was being drafted. These characters are also part of the Unicode standard and most of them are available in several popular fonts.
 Transput is the term used to refer to ALGOL 68's input and output facilities. It includes pre-defined procedures for unformatted, formatted and binary transput. Files and other transput devices are handled in a consistent and machine-independent manner. The following example prints out some unformatted output to the standard output device:
 Note the predefined procedures newpage and newline passed as arguments.
 The TRANSPUT is considered to be of BOOKS, CHANNELS and FILES:
 "Formatted transput" in ALGOL 68's transput has its own syntax and patterns (functions), with FORMATs embedded between two $ characters.[49]
 Examples:
 ALGOL 68 supports programming of parallel processing. Using the keyword PAR, a collateral clause is converted to a parallel clause, where the synchronisation of actions is controlled using semaphores. In A68G the parallel actions are mapped to threads when available on the hosting operating system. In A68S a different paradigm of parallel processing was implemented (see below).
 For its technical intricacies, ALGOL 68 needs a cornucopia of methods to deny the existence of something:
 The term NIL IS var always evaluates to TRUE for any variable (but see above for correct use of IS :/=:), whereas it is not known to which value a comparison x < SKIP evaluates for any integer x.
 ALGOL 68 leaves intentionally undefined what happens in case of integer overflow, the integer bit representation, and the degree of numerical accuracy for floating point.
 Both official reports included some advanced features that were not part of the standard language.  These were indicated with an ℵ and considered effectively private.  Examples include "≮" and "≯" for templates, the  OUTTYPE/INTYPE for crude duck typing, and the STRAIGHTOUT and STRAIGHTIN operators for "straightening" nested arrays and structures
 This sample program implements the Sieve of Eratosthenes to find all the prime numbers that are less than 100. NIL is the ALGOL 68 analogue of the null pointer in other languages. The notation x OF y accesses a member x of a STRUCT y.
 Note: The Soviet Era computers Эльбрус-1 (Elbrus-1) and Эльбрус-2 were created using high-level language Эль-76 (AL-76), rather than the traditional assembly. Эль-76 resembles Algol-68, The main difference is the dynamic binding types in Эль-76 supported at the hardware level. Эль-76 is used for application, job control, system programming.[53]
 Both ALGOL 68C and ALGOL 68-R are written in ALGOL 68, effectively making ALGOL 68 an application of itself. Other applications include:
 A feature of ALGOL 68, inherited from the ALGOL tradition, is its different representations. There is a representation language used to describe algorithms in printed work, a strict language (rigorously defined in the Report), and an official reference language intended to be used in compiler input. The examples contain BOLD typeface words, this is the STRICT language. ALGOL 68's reserved words are effectively in a different namespace from identifiers, and spaces are allowed in identifiers, so this next fragment is legal:
 The programmer who writes executable code does not always have an option of BOLD typeface or underlining in the code as this may depend on hardware and cultural issues. Different methods to denote these identifiers have been devised. This is called a stropping regime. For example, all or some of the following may be available programming representations:
 All implementations must recognize at least POINT, UPPER and RES inside PRAGMAT sections. Of these, POINT and UPPER stropping are quite common, while RES stropping is a contradiction to the specification (as there are no reserved words). QUOTE (single apostrophe quoting) was the original recommendation, while matched apostrophe quoting, common in ALGOL 60, is not used much in ALGOL 68.[56]
 The following characters were recommended for portability, and termed "worthy characters" in the Report on the Standard Hardware Representation of Algol 68 Archived 2014-01-02 at the Wayback Machine:
 This reflected a problem in the 1960s where some hardware didn't support lower-case, nor some other non-ASCII characters, indeed in the 1973 report it was written: "Four worthy characters — "|", "_", "[", and "]" — are often coded differently, even at installations which nominally use the same character set."
 ALGOL 68 allows for every natural language to define its own set of keywords Algol-68. As a result, programmers are able to write programs using keywords from their native language. Below is an example of a simple procedure that calculates "the day following", the code is in two languages: English and German.[citation needed]
 Russian/Soviet example:
In English Algol68's case statement reads CASE ~ IN ~ OUT ~ ESAC, in Cyrillic this reads выб ~ в ~ либо ~ быв.
 Except where noted (with a superscript), the language described above is that of the "Revised Report(r1)".
 The original language (As per the "Final Report"r0) differs in syntax of the mode cast, and it had the feature of proceduring, i.e. coercing the value of a term into a procedure which evaluates the term. Proceduring would be intended to make evaluations lazy. The most useful application could have been the short-circuited evaluation of boolean operators. In:
 b is only evaluated if a is true.
 As defined in ALGOL 68, it did not work as expected, for example in the code:
 against the programmers naïve expectations the print would be executed as it is only the value of the elaborated enclosed-clause after ANDF that was procedured. Textual insertion of the commented-out PROC BOOL: makes it work.
 Some implementations emulate the expected behaviour for this special case by extension of the language.
 Before revision, the programmer could decide to have the arguments of a procedure evaluated serially instead of collaterally by using semicolons instead of commas (gommas).
 For example in:
 The first argument to test is guaranteed to be evaluated before the second, but in the usual:
 then the compiler could evaluate the arguments in whatever order it felt like.
 After the revision of the report, some extensions to the language have been proposed to widen the applicability:
 So far, only partial parametrisation has been implemented, in Algol 68 Genie.
 The S3 language that was used to write the ICL VME operating system and much other system software on the ICL 2900 Series was a direct derivative of Algol 68. However, it omitted many of the more complex features, and replaced the basic modes with a set of data types that mapped directly to the 2900 Series hardware architecture.
 ALGOL 68R(R) from RRE was the first ALGOL 68 subset implementation, running on the ICL 1900. Based on the original language, the main subset restrictions were definition before use and no parallel processing. This compiler was popular in UK universities in the 1970s, where many computer science students learnt ALGOL 68 as their first programming language; the compiler was renowned for good error messages.
 ALGOL 68RS(RS) from RSRE was a portable compiler system written in ALGOL 68RS (bootstrapped from ALGOL 68R), and implemented on a variety of systems including the ICL 2900/Series 39, Multics and DEC VAX/VMS. The language was based on the Revised Report, but with similar subset restrictions to ALGOL 68R. This compiler survives in the form of an Algol68-to-C compiler.
 In ALGOL 68S(S) from Carnegie Mellon University the power of parallel processing was improved by adding an orthogonal extension, eventing. Any variable declaration containing keyword EVENT made assignments to this variable eligible for parallel evaluation, i.e. the right hand side was made into a procedure which was moved to one of the processors of the C.mmp multiprocessor system. Accesses to such variables were delayed after termination of the assignment.
 Cambridge ALGOL 68C(C) was a portable compiler that implemented a subset of ALGOL 68, restricting operator definitions and omitting garbage collection, flexible rows and formatted transput.
 Algol 68 Genie(G) by M. van der Veer is an ALGOL 68 implementation for today's computers and operating systems.
 "Despite good intentions, a programmer may violate portability by inadvertently employing a local extension.  To guard against this, each implementation should provide a PORTCHECK pragmat option.  While this option is in force, the compiler prints a message for each construct that it recognizes as violating some portability constraint."[66]


Source: https://en.wikipedia.org/wiki/ABC_(Programmiersprache)
Content: Other reasons this message may be displayed:


Source: https://en.wikipedia.org/wiki/Modula-3
Content: Modula-3 is a programming language conceived as a successor to an upgraded version of Modula-2 known as Modula-2+. While it has been influential in research circles (influencing the designs of languages such as Java, C#, Python[8] and Nim) it has not been adopted widely in industry. It was designed by Luca Cardelli, James Donahue, Lucille Glassman, Mick Jordan (before at the Olivetti Software Technology Laboratory), Bill Kalsow and Greg Nelson at the Digital Equipment Corporation (DEC) Systems Research Center (SRC) and the Olivetti Research Center (ORC) in the late 1980s.
 Modula-3's main features are modularity, simplicity and safety while preserving the power of a systems-programming language. Modula-3 aimed to continue the Pascal tradition of type safety, while introducing new constructs for practical real-world programming. In particular Modula-3 added support for generic programming (similar to templates), multithreading, exception handling, garbage collection, object-oriented programming, partial revelation, and explicit marking of unsafe code. The design goal of Modula-3 was a language that implements the most important features of modern imperative programming languages in quite basic forms. Thus allegedly dangerous and complicating features such as multiple inheritance and operator overloading were omitted.
 The Modula-3 project started in November 1986 when Maurice Wilkes wrote to Niklaus Wirth with some ideas for a new version of Modula. Wilkes had been working at DEC just prior to this point, and had returned to England and joined Olivetti's Research Strategy Board. Wirth had already moved on to Oberon, but had no problems with Wilkes's team continuing development under the Modula name. The language definition was completed in August 1988, and an updated version in January 1989. Compilers from DEC and Olivetti soon followed, and 3rd party implementations after that.
 Its design was heavily influenced by work on the Modula-2+ language in use at SRC and at the Acorn Computers Research Center (ARC, later ORC when Olivetti acquired Acorn) at the time, which was the language in which the operating system for the DEC Firefly multiprocessor VAX workstation was written and in which the Acorn Compiler for Acorn C and Modula Execution Library (CAMEL) at ARC for the ARX operating system project of ARM based Acorn Archimedes range of computers was written. As the revised Modula-3 Report states, the language was influenced by other languages such as Mesa, Cedar, Object Pascal, Oberon and Euclid.[9]
 During the 1990s, Modula-3 gained considerable currency as a teaching language, but it was never widely adopted for industrial use. Contributing to this may have been the demise of DEC, a key Modula-3 supporter (especially when it ceased to maintain it effectively before DEC was sold to Compaq in 1998). In any case, in spite of Modula-3's simplicity and power, it appears that there was little demand for a procedural compiled language with restricted implementation of object-oriented programming. For a time, a commercial compiler named CM3 maintained by one of the chief implementors prior at DEC SRC who was hired before DEC being sold to Compaq, an integrated development environment (IDE) named Reactor and an extensible Java virtual machine (licensed in binary code and source code formats and buildable with Reactor) were offered by Critical Mass, Inc., but that company ceased active operations in 2000 and gave some of the source code of its products to elego Software Solutions GmbH. Modula-3 is now taught in universities mostly in comparative programming language courses, and its textbooks are out of print. Essentially the only corporate supporter of Modula-3 is elego, which inherited the sources from Critical Mass and has since made several releases of the CM3 system in source and binary code. The Reactor IDE has been open source released after several years it had not, with the new name CM3-IDE. In March 2002, elego also took over the repository of another active Modula-3 distribution, PM3, until then maintained at the École Polytechnique de Montréal but which later continued by the work on HM3 improved over the years later until it was obsoleted.
 A common example of a language's syntax is the "Hello, World!" program.
 
All programs in Modula-3 have at least a module file, while most also include an interface file that is used by clients to access data from the module. As in some other languages, a Modula-3 program must export a Main module, which can either be a file named Main.m3, or a file can call EXPORT to export the Main module.   Module file names are advised to be the same as the name in source code. If they differ, the compiler only emits a warning.
 Other conventions in the syntax include naming the exported type of an interface T, since types are usually qualified by their full names, so a type T inside a module named Foo will be named Foo.T. This aids in readability. Another similar convention is naming a public object Public as in the OOP examples below.
 First and foremost, all compiled units are either INTERFACE or implementation MODULEs, of one flavor or another. An interface compiled unit, starting with the keyword INTERFACE, defines constants, types, variables, exceptions, and procedures. The implementation module, starting with the keyword MODULE, provides the code, and any further constants, types, or variables needed to implement the interface. By default, an implementation module will implement the interface of the same name, but a module may explicitly EXPORT to a module not of the same name. For example, the main program exports an implementation module for the Main interface.
 Any compiled unit may IMPORT other interfaces, although circular imports are forbidden. This may be resolved by doing the import from the implementation MODULE. The entities within the imported module may be imported, instead of only the module name, using the FROM Module IMPORT Item [, Item]* syntax:
 Typically, one only imports the interface, and uses the 'dot' notation to access the items within the interface (similar to accessing the fields within a record). A typical use is to define one data structure (record or object) per interface along with any support procedures. Here the main type will get the name 'T', and one uses as in MyModule.T.
 In the event of a name collision between an imported module and other entity within the module, the reserved word
AS can be used as in IMPORT CollidingModule AS X;
 Some ability is deemed unsafe, where the compiler can no longer guarantee that results will be consistent; for example, when interfacing to the C language. The keyword UNSAFE prefixed in front of INTERFACE or MODULE, may be used to tell the compiler to enable certain low level features of the language. For example, an unsafe operation is bypassing the type system using LOOPHOLE to copy the bits of an integer into a floating point REAL number.
 An interface that imports an unsafe module must also be unsafe. A safe interface may be exported by an unsafe implementation module. This is the typical use when interfacing to external libraries, where two interfaces are built: one unsafe, the other safe.
 A generic interface and its corresponding generic module, 
prefix the INTERFACE or MODULE keyword with GENERIC, and take as formal arguments other interfaces. Thus (like C++ templates) one can easily define and use abstract data types, but unlike C++, the granularity is at the module level. An interface is passed to the generic interface and implementation modules as arguments, and the compiler will generate concrete modules.
 For example, one could define a GenericStack, then instantiate it with interfaces such as IntegerElem, or RealElem, or even interfaces to Objects, as long as each of those interfaces defines the properties needed by the generic modules.
 The bare types INTEGER, or REAL can't be used, because they are not modules, and the system of generics is based on using modules as arguments. By comparison, in a C++ template, a bare type would be used.
 FILE: IntegerElem.i3
 FILE: GenericStack.ig
 FILE: GenericStack.mg
 FILE: IntegerStack.i3
 FILE: IntegerStack.m3
 Any identifier can be traced back to where it originated, unlike the 'include' feature of other languages. A compiled unit must import identifiers from other compiled units, using an IMPORT statement. Even enumerations make use of the same 'dot' notation as used when accessing a field of a record.
 Modula-3 supports the allocation of data at runtime. There are two kinds of memory that can be allocated, TRACED and UNTRACED, the difference being whether the garbage collector can see it or not. NEW() is used to allocate data of either of these classes of memory. In an UNSAFE module, DISPOSE is available to free untraced memory.
 Object-oriented programming techniques may be used in Modula-3, but their use is not a requirement. Many of the other features provided in Modula-3 (modules, generics) can usually take the place of object-orientation.
 Object support is intentionally kept to its simplest terms. An object type (termed a "class" in other object-oriented languages) is introduced with the OBJECT declaration, which has essentially the same syntax as a RECORD declaration, although an object type is a reference type, whereas RECORDs in Modula-3 are not (similar to structs in C). Exported types are usually named T by convention, and create a separate "Public" type to expose the methods and data. For example:
 This defines an interface Person with two types, T, and Public, which is defined as an object with two methods, getAge() and init(). T is defined as a subtype of Public by the use of the <: operator.
 To create a new Person.T object, use the built in procedure NEW with the method init() as 
 Modula-3's REVEAL construct provides a conceptually simple and clean yet very powerful mechanism for hiding implementation details from clients, with arbitrarily many levels of friendliness. A full revelation of the form REVEAL T = V can be used to show the full implementation of the Person interface from above. A partial revelation of the form REVEAL T <: V merely reveals that T is a supertype of V without revealing any additional information on T.[10]
 Note the use of the BRANDED keyword, which "brands" objects to make them unique as to avoid structural equivalence. BRANDED can also take a string as an argument, but when omitted, a unique string is generated for you.
 Modula-3 is one of a few programming languages which requires external references from a module to be strictly qualified. That is, a reference in module A to the object x exported from module B must take the form B.x. In Modula-3, it is impossible to import all exported names from a module.
 Because of the language's requirements on name qualification and method overriding, it is impossible to break a working program simply by adding new declarations to an interface (any interface). This makes it possible for large programs to be edited concurrently by many programmers with no worries about naming conflicts; and it also makes it possible to edit core language libraries with the firm knowledge that no extant program will be broken in the process.
 Exception handling is based on a TRY...EXCEPT block system, which has since[citation needed] become common.  One feature that has not been adopted in other languages[citation needed], with the notable exceptions of Delphi, Python[1], Scala[2] and Visual Basic.NET, is that the EXCEPT construct defined a form of switch statement with each possible exception as a case in its own EXCEPT clause. Modula-3 also supports a LOOP...EXIT...END construct that loops until an EXIT occurs, a structure equivalent to a simple loop inside a TRY...EXCEPT clause.
 The language supports the use of multi-threading, and synchronization between threads.
There is a standard module within the runtime library (m3core) named Thread, which supports the use of multi-threaded applications. The Modula-3 runtime may make use of a separate thread for internal tasks such as garbage collection.
 A built-in data structure MUTEX is used to synchronize multiple threads and protect data structures from simultaneous access with possible corruption or race conditions. The LOCK statement introduces a block in which the mutex is locked. Unlocking a MUTEX is implicit by the code execution locus's leaving the block. The MUTEX is an object, and as such, other objects may be derived from it.
 For example, in the input/output (I/O) section of the library libm3, readers and writers (Rd.T, and Wr.T) are derived from MUTEX, and they lock themselves before accessing or modifying any internal data such as buffers.
 In summary, the language features:
 Modula-3 is one of the rare languages whose evolution of features is documented.
 In Systems Programming with Modula-3, four essential points of the language design are intensively discussed. These topics are: structural vs. name equivalence, subtyping rules, generic modules, and parameter modes like READONLY.
 Continuing a trend started with the C language, many of the features needed to write real programs were left out of the language definition and instead provided via a standard library set. Most of the interfaces below are described in detail in[11]
 Standard libraries providing the following features. These are called standard interfaces and are required (must be provided) in the language.
 Some recommended interfaces implemented in the available implementations but are not required
 As in C, I/O is also provided via libraries, in Modula-3 called Rd and Wr. The object-oriented design of the Rd (readers) and Wr (writers) libraries is covered in detail in the book by Greg Nelson. An interesting aspect of Modula-3 is that it is one of few programming languages which standard libraries have been formally verified not to contain various types of bugs, including locking bugs. This was done under the auspices of the Larch/Modula-3 (see Larch family)[12] and Extended static checking[13] projects at DEC Systems Research Center.
 Several compilers are available, most of them open source.
 Since the only aspect of C data structures that is missing from Modula-3 is the union type, all extant Modula-3 implementations are able to provide good binary code compatibility with C language type declarations of arrays and structs.
 None of these books are still in print, although used copies are obtainable and some are digitized, partly or fully, and some chapters of one them have prior or posterior versions obtainable as research reports from the web.
 Software which is programmed Modula-3 includes:
 Although Modula-3 did not gain mainstream status, several parts of the DEC-SRC M3 distribution did. Probably the most influential part was the Network Objects library, which formed the basis for Java's first Remote Method Invocation (RMI) implementation, including the network protocol. Only when Sun moved from the Common Object Request Broker Architecture (CORBA) standard to the IIOP based protocol was it dropped. The Java documentation on garbage collection of remote objects still refer to the pioneering work done for Modula-3 Network Objects.[21]  Python's implementation of classes was also inspired by the class mechanism found in C++ and Modula-3.[22]
Also the language Nim makes use of some aspects of Modula-3, such as traced vs untraced pointers.


Source: https://en.wikipedia.org/wiki/C_(Programmiersprache)
Content: Other reasons this message may be displayed:


Source: https://en.wikipedia.org/wiki/C%2B%2B
Content: 
 C++ (/ˈsiː plʌs plʌs/, pronounced "C plus plus" and sometimes abbreviated as CPP) is a high-level, general-purpose programming language created by Danish computer scientist Bjarne Stroustrup. First released in 1985 as an extension of the C programming language, it has since expanded significantly over time; as of 1997[update], C++ has object-oriented, generic, and functional features, in addition to facilities for low-level memory manipulation for making things like microcomputers or to make operating systems like Linux or Windows. It is almost always implemented as a compiled language, and many vendors provide C++ compilers, including the Free Software Foundation, LLVM, Microsoft, Intel, Embarcadero, Oracle, and IBM.[14]
 C++ was designed with systems programming and embedded, resource-constrained software and large systems in mind, with performance, efficiency, and flexibility of use as its design highlights.[15] C++ has also been found useful in many other contexts, with key strengths being software infrastructure and resource-constrained applications,[15] including desktop applications, video games, servers (e.g., e-commerce, web search, or databases), and performance-critical applications (e.g., telephone switches or space probes).[16]
 C++ is standardized by the International Organization for Standardization (ISO), with the latest standard version ratified and published by ISO in December 2020 as ISO/IEC 14882:2020 (informally known as C++20).[17] The C++ programming language was initially standardized in 1998 as ISO/IEC 14882:1998, which was then amended by the C++03, C++11, C++14, and C++17 standards. The current C++20 standard supersedes these with new features and an enlarged standard library. Before the initial standardization in 1998, C++ was developed by Stroustrup at Bell Labs since 1979 as an extension of the C language; he wanted an efficient and flexible language similar to C that also provided high-level features for program organization.[18] Since 2012, C++ has been on a three-year release schedule[19] with C++23 as the next planned standard.[20]
 In 1979, Bjarne Stroustrup, a Danish computer scientist, began work on "C with Classes", the predecessor to C++.[21] The motivation for creating a new language originated from Stroustrup's experience in programming for his PhD thesis. Stroustrup found that Simula had features that were very helpful for large software development, but the language was too slow for practical use, while BCPL was fast but too low-level to be suitable for large software development. When Stroustrup started working in AT&T Bell Labs, he had the problem of analyzing the UNIX kernel with respect to distributed computing. Remembering his PhD experience, Stroustrup set out to enhance the C language with Simula-like features.[22] C was chosen because it was general-purpose, fast, portable, and widely used. In addition to C and Simula's influences, other languages influenced this new language, including ALGOL 68, Ada, CLU, and ML.[citation needed]
 Initially, Stroustrup's "C with Classes" added features to the C compiler, Cpre, including classes, derived classes, strong typing, inlining, and default arguments.[23]
 In 1982, Stroustrup started to develop a successor to C with Classes, which he named "C++" (++ being the increment operator in C) after going through several other names. New features were added, including virtual functions, function name and operator overloading, references, constants, type-safe free-store memory allocation (new/delete), improved type checking, and BCPL-style single-line comments with two forward slashes (//). Furthermore, Stroustrup developed a new, standalone compiler for C++, Cfront.
 In 1984, Stroustrup implemented the first stream input/output library. The idea of providing an output operator rather than a named output function was suggested by Doug McIlroy[2] (who had previously suggested Unix pipes).
 In 1985, the first edition of The C++ Programming Language was released, which became the definitive reference for the language, as there was not yet an official standard.[24] The first commercial implementation of C++ was released in October of the same year.[21]
 In 1989, C++ 2.0 was released, followed by the updated second edition of The C++ Programming Language in 1991.[25] New features in 2.0 included multiple inheritance, abstract classes, static member functions, const member functions, and protected members. In 1990, The Annotated C++ Reference Manual was published. This work became the basis for the future standard. Later feature additions included templates, exceptions, namespaces, new casts, and a Boolean type.
 In 1998, C++98 was released, standardizing the language, and a minor update (C++03) was released in 2003.
 After C++98, C++ evolved relatively slowly until, in 2011, the C++11 standard was released, adding numerous new features, enlarging the standard library further, and providing more facilities to C++ programmers. After a minor C++14 update released in December 2014, various new additions were introduced in C++17.[26] After becoming finalized in February 2020,[27] a draft of the C++20 standard was approved on 4 September 2020, and officially published on 15 December 2020.[28][29]
 On January 3, 2018, Stroustrup was announced as the 2018 winner of the Charles Stark Draper Prize for Engineering, "for conceptualizing and developing the C++ programming language".[30]
 As of December 2022[update], C++ ranked third on the TIOBE index, surpassing Java for the first time in the history of the index. It ranks third, after Python and C.[31]
 According to Stroustrup, "the name signifies the evolutionary nature of the changes from C."[32] This name is credited to Rick Mascitti (mid-1983)[23] and was first used in December 1983. When Mascitti was questioned informally in 1992 about the naming, he indicated that it was given in a tongue-in-cheek spirit. The name comes from C's ++ operator (which increments the value of a variable) and a common naming convention of using "+" to indicate an enhanced computer program.
 During C++'s development period, the language had been referred to as "new C" and "C with Classes"[23][33] before acquiring its final name.
 Throughout C++'s life, its development and evolution has been guided by a set of principles:[22]
 C++ is standardized by an ISO working group known as JTC1/SC22/WG21. So far, it has published six revisions of the C++ standard and is currently working on the next revision, C++23.
 In 1998, the ISO working group standardized C++ for the first time as ISO/IEC 14882:1998, which is informally known as C++98. In 2003, it published a new version of the C++ standard called ISO/IEC 14882:2003, which fixed problems identified in C++98.
 The next major revision of the standard was informally referred to as "C++0x", but it was not released until 2011.[40] C++11 (14882:2011) included many additions to both the core language and the standard library.[37]
 In 2014, C++14 (also known as C++1y) was released as a small extension to C++11, featuring mainly bug fixes and small improvements.[41]  The Draft International Standard ballot procedures completed in mid-August 2014.[42]
 After C++14, a major revision C++17, informally known as C++1z, was completed by the ISO C++ committee in mid July 2017 and was approved and published in December 2017.[43]
 As part of the standardization process, ISO also publishes technical reports and specifications:
 More technical specifications are in development and pending approval, including new set of concurrency extensions.[61]
 The C++ language has two main components: a direct mapping of hardware features provided primarily by the C subset, and zero-overhead abstractions based on those mappings. Stroustrup describes C++ as "a light-weight abstraction programming language [designed] for building and using efficient and elegant abstractions";[15] and "offering both hardware access and abstraction is the basis of C++. Doing it efficiently is what distinguishes it from other languages."[62]
 C++ inherits most of C's syntax. The following is Bjarne Stroustrup's version of the Hello world program that uses the C++ Standard Library stream facility to write a message to standard output:[63][64][note 2]
 As in C, C++ supports four types of memory management: static storage duration objects, thread storage duration objects, automatic storage duration objects, and dynamic storage duration objects.[65]
 Static storage duration objects are created before main() is entered (see exceptions below) and destroyed in reverse order of creation after main() exits. The exact order of creation is not specified by the standard (though there are some rules defined below) to allow implementations some freedom in how to organize their implementation. More formally, objects of this type have a lifespan that "shall last for the duration of the program".[66]
 Static storage duration objects are initialized in two phases. First, "static initialization" is performed, and only after all static initialization is performed, "dynamic initialization" is performed.  In static initialization, all objects are first initialized with zeros; after that, all objects that have a constant initialization phase are initialized with the constant expression (i.e. variables initialized with a literal or constexpr). Though it is not specified in the standard, the static initialization phase can be completed at compile time and saved in the data partition of the executable.  Dynamic initialization involves all object initialization done via a constructor or function call (unless the function is marked with constexpr, in C++11). The dynamic initialization order is defined as the order of declaration within the compilation unit (i.e. the same file). No guarantees are provided about the order of initialization between compilation units.
 Variables of this type are very similar to static storage duration objects. The main difference is the creation time is just prior to thread creation and destruction is done after the thread has been joined.[67]
 The most common variable types in C++ are local variables inside a function or block, and temporary variables.[68] The common feature about automatic variables is that they have a lifetime that is limited to the scope of the variable. They are created and potentially initialized at the point of declaration (see below for details) and destroyed in the reverse order of creation when the scope is left. This is implemented by allocation on the stack.
 Local variables are created as the point of execution passes the declaration point. If the variable has a constructor or initializer this is used to define the initial state of the object. Local variables are destroyed when the local block or function that they are declared in is closed. C++ destructors for local variables are called at the end of the object lifetime, allowing a discipline for automatic resource management termed RAII, which is widely used in C++.
 Member variables are created when the parent object is created. Array members are initialized from 0 to the last member of the array in order. Member variables are destroyed when the parent object is destroyed in the reverse order of creation. i.e. If the parent is an "automatic object" then it will be destroyed when it goes out of scope which triggers the destruction of all its members.
 Temporary variables are created as the result of expression evaluation and are destroyed when the statement containing the expression has been fully evaluated (usually at the ; at the end of a statement).
 These objects have a dynamic lifespan and can be created directly with a call to new and destroyed explicitly with a call to delete.[69] C++ also supports malloc and free, from C, but these are not compatible with new and delete. Use of new returns an address to the allocated memory. The C++ Core Guidelines advise against using new directly for creating dynamic objects in favor of smart pointers through make_unique<T> for single ownership and make_shared<T> for reference-counted multiple ownership,[70] which were introduced in C++11.
 C++ templates enable generic programming. C++ supports function, class, alias, and variable templates. Templates may be parameterized by types, compile-time constants, and other templates. Templates are implemented by instantiation at compile-time. To instantiate a template, compilers substitute specific arguments for a template's parameters to generate a concrete function or class instance. Some substitutions are not possible; these are eliminated by an overload resolution policy described by the phrase "Substitution failure is not an error" (SFINAE). Templates are a powerful tool that can be used for generic programming, template metaprogramming, and code optimization, but this power implies a cost. Template use may increase object code size, because each template instantiation produces a copy of the template code: one for each set of template arguments, however, this is the same or smaller amount of code that would be generated if the code were written by hand.[71] This is in contrast to run-time generics seen in other languages (e.g., Java) where at compile-time the type is erased and a single template body is preserved.
 Templates are different from macros: while both of these compile-time language features enable conditional compilation, templates are not restricted to lexical substitution. Templates are aware of the semantics and type system of their companion language, as well as all compile-time type definitions, and can perform high-level operations including programmatic flow control based on evaluation of strictly type-checked parameters. Macros are capable of conditional control over compilation based on predetermined criteria, but cannot instantiate new types, recurse, or perform type evaluation and in effect are limited to pre-compilation text-substitution and text-inclusion/exclusion. In other words, macros can control compilation flow based on pre-defined symbols but cannot, unlike templates, independently instantiate new symbols. Templates are a tool for static polymorphism (see below) and generic programming.
 In addition, templates are a compile-time mechanism in C++ that is Turing-complete, meaning that any computation expressible by a computer program can be computed, in some form, by a template metaprogram prior to runtime.
 In summary, a template is a compile-time parameterized function or class written without knowledge of the specific arguments used to instantiate it. After instantiation, the resulting code is equivalent to code written specifically for the passed arguments. In this manner, templates provide a way to decouple generic, broadly applicable aspects of functions and classes (encoded in templates) from specific aspects (encoded in template parameters) without sacrificing performance due to abstraction.
 C++ introduces object-oriented programming (OOP) features to C. It offers classes, which provide the four features commonly present in OOP (and some non-OOP) languages: abstraction, encapsulation, inheritance, and polymorphism. One distinguishing feature of C++ classes compared to classes in other programming languages is support for deterministic destructors, which in turn provide support for the Resource Acquisition is Initialization (RAII) concept.
 Encapsulation is the hiding of information to ensure that data structures and operators are used as intended and to make the usage model more obvious to the developer. C++ provides the ability to define classes and functions as its primary encapsulation mechanisms. Within a class, members can be declared as either public, protected, or private to explicitly enforce encapsulation. A public member of the class is accessible to any function. A private member is accessible only to functions that are members of that class and to functions and classes explicitly granted access permission by the class ("friends"). A protected member is accessible to members of classes that inherit from the class in addition to the class itself and any friends.
 The object-oriented principle ensures the encapsulation of all and only the functions that access the internal representation of a type. C++ supports this principle via member functions and friend functions, but it does not enforce it. Programmers can declare parts or all of the representation of a type to be public, and they are allowed to make public entities not part of the representation of a type. Therefore, C++ supports not just object-oriented programming, but other decomposition paradigms such as modular programming.
 It is generally considered good practice to make all data private or protected, and to make public only those functions that are part of a minimal interface for users of the class. This can hide the details of data implementation, allowing the designer to later fundamentally change the implementation without changing the interface in any way.[72][73]
 Inheritance allows one data type to acquire properties of other data types. Inheritance from a base class may be declared as public, protected, or private. This access specifier determines whether unrelated and derived classes can access the inherited public and protected members of the base class. Only public inheritance corresponds to what is usually meant by "inheritance". The other two forms are much less frequently used. If the access specifier is omitted, a "class" inherits privately, while a "struct" inherits publicly. Base classes may be declared as virtual; this is called virtual inheritance. Virtual inheritance ensures that only one instance of a base class exists in the inheritance graph, avoiding some of the ambiguity problems of multiple inheritance.
 Multiple inheritance is a C++ feature allowing a class to be derived from more than one base class; this allows for more elaborate inheritance relationships. For example, a "Flying Cat" class can inherit from both "Cat" and "Flying Mammal". Some other languages, such as C# or Java, accomplish something similar (although more limited) by allowing inheritance of multiple interfaces while restricting the number of base classes to one (interfaces, unlike classes, provide only declarations of member functions, no implementation or member data). An interface as in C# and Java can be defined in C++ as a class containing only pure virtual functions, often known as an abstract base class or "ABC". The member functions of such an abstract base class are normally explicitly defined in the derived class, not inherited implicitly. C++ virtual inheritance exhibits an ambiguity resolution feature called dominance.
 C++ provides more than 35 operators, covering basic arithmetic, bit manipulation, indirection, comparisons, logical operations and others. Almost all operators can be overloaded for user-defined types, with a few notable exceptions such as member access (. and .*) as well as the conditional operator. The rich set of overloadable operators is central to making user-defined types in C++ seem like built-in types.
 Overloadable operators are also an essential part of many advanced C++ programming techniques, such as smart pointers. Overloading an operator does not change the precedence of calculations involving the operator, nor does it change the number of operands that the operator uses (any operand may however be ignored by the operator, though it will be evaluated prior to execution). Overloaded "&&" and "||" operators lose their short-circuit evaluation property.
 Polymorphism enables one common interface for many implementations, and for objects to act differently under different circumstances.
 C++ supports several kinds of static (resolved at compile-time) and dynamic (resolved at run-time) polymorphisms, supported by the language features described above. Compile-time polymorphism does not allow for certain run-time decisions, while runtime polymorphism typically incurs a performance penalty.
 Function overloading allows programs to declare multiple functions having the same name but with different arguments (i.e. ad hoc polymorphism). The functions are distinguished by the number or types of their formal parameters. Thus, the same function name can refer to different functions depending on the context in which it is used. The type returned by the function is not used to distinguish overloaded functions and differing return types would result in a compile-time error message.
 When declaring a function, a programmer can specify for one or more parameters a default value. Doing so allows the parameters with defaults to optionally be omitted when the function is called, in which case the default arguments will be used. When a function is called with fewer arguments than there are declared parameters, explicit arguments are matched to parameters in left-to-right order, with any unmatched parameters at the end of the parameter list being assigned their default arguments. In many cases, specifying default arguments in a single function declaration is preferable to providing overloaded function definitions with different numbers of parameters.
 Templates in C++ provide a sophisticated mechanism for writing generic, polymorphic code (i.e. parametric polymorphism). In particular, through the curiously recurring template pattern, it is possible to implement a form of static polymorphism that closely mimics the syntax for overriding virtual functions. Because C++ templates are type-aware and Turing-complete, they can also be used to let the compiler resolve recursive conditionals and generate substantial programs through template metaprogramming. Contrary to some opinion, template code will not generate a bulk code after compilation with the proper compiler settings.[71]
 Variable pointers and references to a base class type in C++ can also refer to objects of any derived classes of that type. This allows arrays and other kinds of containers to hold pointers to objects of differing types (references cannot be directly held in containers). This enables dynamic (run-time) polymorphism, where the referred objects can behave differently, depending on their (actual, derived) types.
 C++ also provides the dynamic_cast operator, which allows code to safely attempt conversion of an object, via a base reference/pointer, to a more derived type: downcasting. The attempt is necessary as often one does not know which derived type is referenced. (Upcasting, conversion to a more general type, can always be checked/performed at compile-time via static_cast, as ancestral classes are specified in the derived class's interface, visible to all callers.) dynamic_cast relies on run-time type information (RTTI), metadata in the program that enables differentiating types and their relationships. If a dynamic_cast to a pointer fails, the result is the nullptr constant, whereas if the destination is a reference (which cannot be null), the cast throws an exception. Objects known to be of a certain derived type can be cast to that with static_cast, bypassing RTTI and the safe runtime type-checking of dynamic_cast, so this should be used only if the programmer is very confident the cast is, and will always be, valid.
 Ordinarily, when a function in a derived class overrides a function in a base class, the function to call is determined by the type of the object. A given function is overridden when there exists no difference in the number or type of parameters between two or more definitions of that function. Hence, at compile time, it may not be possible to determine the type of the object and therefore the correct function to call, given only a base class pointer; the decision is therefore put off until runtime. This is called dynamic dispatch. Virtual member functions or methods[74] allow the most specific implementation of the function to be called, according to the actual run-time type of the object. In C++ implementations, this is commonly done using virtual function tables. If the object type is known, this may be bypassed by prepending a fully qualified class name before the function call, but in general calls to virtual functions are resolved at run time.
 In addition to standard member functions, operator overloads and destructors can be virtual. An inexact rule based on practical experience states that if any function in the class is virtual, the destructor should be as well. As the type of an object at its creation is known at compile time, constructors, and by extension copy constructors, cannot be virtual. Nonetheless, a situation may arise where a copy of an object needs to be created when a pointer to a derived object is passed as a pointer to a base object. In such a case, a common solution is to create a clone() (or similar) virtual function that creates and returns a copy of the derived class when called.
 A member function can also be made "pure virtual" by appending it with = 0 after the closing parenthesis and before the semicolon. A class containing a pure virtual function is called an abstract class. Objects cannot be created from an abstract class; they can only be derived from. Any derived class inherits the virtual function as pure and must provide a non-pure definition of it (and all other pure virtual functions) before objects of the derived class can be created. A program that attempts to create an object of a class with a pure virtual member function or inherited pure virtual member function is ill-formed.
 C++ provides support for anonymous functions, also known as lambda expressions, with the following form:[75]
 Since C++20, the keyword template is optional for template parameters of lambda expressions:
 If the lambda takes no parameters, and no return type or other specifiers are used, the () can be omitted; that is,
 The return type of a lambda expression can be automatically inferred, if possible; e.g.:
 The [capture] list supports the definition of closures. Such lambda expressions are defined in the standard as syntactic sugar for an unnamed function object.
 Exception handling is used to communicate the existence of a runtime problem or error from where it was detected to where the issue can be handled.[76] It permits this to be done in a uniform manner and separately from the main code, while detecting all errors.[77] Should an error occur, an exception is thrown (raised), which is then caught by the nearest suitable exception handler. The exception causes the current scope to be exited, and also each outer scope (propagation) until a suitable handler is found, calling in turn the destructors of any objects in these exited scopes.[78] At the same time, an exception is presented as an object carrying the data about the detected problem.[79]
 Some C++ style guides, such as Google's,[80] LLVM's,[81] and Qt's,[82] forbid the usage of exceptions.
 The exception-causing code is placed inside a try block. The exceptions are handled in separate catch blocks (the handlers); each try block can have multiple exception handlers, as it is visible in the example below.[83]
 It is also possible to raise exceptions purposefully, using the throw keyword; these exceptions are handled in the usual way. In some cases, exceptions cannot be used due to technical reasons. One such example is a critical component of an embedded system, where every operation must be guaranteed to complete within a specified amount of time. This cannot be determined with exceptions as no tools exist to determine the maximum time required for an exception to be handled.[84]
 Unlike signal handling, in which the handling function is called from the point of failure, exception handling exits the current scope before the catch block is entered, which may be located in the current function or any of the previous function calls currently on the stack.
 C++ has enumeration types that are directly inherited from C's and work mostly like these, except that an enumeration is a real type in C++, giving added compile-time checking. Also (as with structs), the C++ enum keyword is combined with a typedef, so that instead of naming the type enum name, simply name it name. This can be simulated in C using a typedef: typedef enum {Value1, Value2} name;
 C++11 also provides a second kind of enumeration, called a scoped enumeration. These are type-safe: the enumerators are not implicitly converted to an integer type. Among other things, this allows I/O streaming to be defined for the enumeration type. Another feature of scoped enumerations is that the enumerators do not leak, so usage requires prefixing with the name of the enumeration (e.g., Color::Red for the first enumerator in the example below), unless a using enum declaration (introduced in C++20) has been used to bring the enumerators into the current scope. A scoped enumeration is specified by the phrase enum class (or enum struct). For example:
 The underlying type of an enumeration is an implementation-defined integral type that is large enough to hold all enumerated values; it does not have to be the smallest possible type. The underlying type can be specified directly, which allows "forward declarations" of enumerations:
 The C++ standard consists of two parts: the core language and the standard library. C++ programmers expect the latter on every major implementation of C++; it includes aggregate types (vectors, lists, maps, sets, queues, stacks, arrays, tuples), algorithms (find, for_each, binary_search, random_shuffle, etc.), input/output facilities (iostream, for reading from and writing to the console and files), filesystem library, localisation support, smart pointers for automatic memory management, regular expression support, multi-threading library, atomics support (allowing a variable to be read or written to by at most one thread at a time without any external synchronisation), time utilities (measurement, getting current time, etc.), a system for converting error reporting that does not use C++ exceptions into C++ exceptions, a random number generator, and a slightly modified version of the C standard library (to make it comply with the C++ type system).
 A large part of the C++ library is based on the Standard Template Library (STL). Useful tools provided by the STL include containers as the collections of objects (such as vectors and lists), iterators that provide array-like access to containers, and algorithms that perform operations such as searching and sorting.
 Furthermore, (multi)maps (associative arrays) and (multi)sets are provided, all of which export compatible interfaces. Therefore, using templates it is possible to write generic algorithms that work with any container or on any sequence defined by iterators. As in C, the features of the library are accessed by using the #include directive to include a standard header. The C++ Standard Library provides 105 standard headers, of which 27 are deprecated.
 The standard incorporates the STL that was originally designed by Alexander Stepanov, who experimented with generic algorithms and containers for many years. When he started with C++, he finally found a language where it was possible to create generic algorithms (e.g., STL sort) that perform even better than, for example, the C standard library qsort, thanks to C++ features like using inlining and compile-time binding instead of function pointers. The standard does not refer to it as "STL", as it is merely a part of the standard library, but the term is still widely used to distinguish it from the rest of the standard library (input/output streams, internationalization, diagnostics, the C library subset, etc.).[85]
 Most C++ compilers, and all major ones, provide a standards-conforming implementation of the C++ standard library.
 The C++ Core Guidelines[86] are an initiative led by Bjarne Stroustrup, the inventor of C++, and Herb Sutter, the convener and chair of the C++ ISO Working Group, to help programmers write 'Modern C++' by using best practices for the language standards C++11 and newer, and to help developers of compilers and static checking tools to create rules for catching bad programming practices.
 The main aim is to efficiently and consistently write type and resource safe C++.
 The Core Guidelines were announced[87] in the opening keynote at CPPCon 2015.
 The Guidelines are accompanied by the Guideline Support Library (GSL),[88] a header only library of types and functions to implement the Core Guidelines and static checker tools for enforcing Guideline rules.[89]
 To give compiler vendors greater freedom, the C++ standards committee decided not to dictate the implementation of name mangling, exception handling, and other implementation-specific features. The downside of this decision is that object code produced by different compilers is expected to be incompatible. There are, however, attempts to standardize compilers for particular machines or operating systems. For example, the Itanium C++ ABI is processor-independent (despite its name) and is implemented by GCC and Clang.[90]
 C++ is often considered to be a superset of C but this is not strictly true.[91] Most C code can easily be made to compile correctly in C++ but there are a few differences that cause some valid C code to be invalid or behave differently in C++. For example, C allows implicit conversion from void* to other pointer types but C++ does not (for type safety reasons). Also, C++ defines many new keywords, such as new and class, which may be used as identifiers (for example, variable names) in a C program.
 Some incompatibilities have been removed by the 1999 revision of the C standard (C99), which now supports C++ features such as line comments (//) and declarations mixed with code. On the other hand, C99 introduced a number of new features that C++ did not support that were incompatible or redundant in C++, such as variable-length arrays, native complex-number types (however, the std::complex class in the C++ standard library provides similar functionality, although not code-compatible), designated initializers, compound literals, and the restrict keyword.[92] Some of the C99-introduced features were included in the subsequent version of the C++ standard, C++11 (out of those which were not redundant).[93][94][95] However, the C++11 standard introduces new incompatibilities, such as disallowing assignment of a string literal to a character pointer, which remains valid C.
 To intermix C and C++ code, any function declaration or definition that is to be called from/used both in C and C++ must be declared with C linkage by placing it within an extern "C" {/*...*/} block. Such a function may not rely on features depending on name mangling (i.e., function overloading).
 Despite its widespread adoption, some notable programmers have criticized the C++ language, including Linus Torvalds,[96] Richard Stallman,[97] Joshua Bloch, Ken Thompson,[98][99][100] and Donald Knuth.[101][102]
 
One of the most often criticised points of C++ is its perceived complexity as a language, with the criticism that a large number of non-orthogonal features in practice necessitates restricting code to a subset of C++, thus eschewing the readability benefits of common style and idioms. As expressed by Joshua Bloch:   I think C++ was pushed well beyond its complexity threshold, and yet there are a lot of people programming it. But what you do is you force people to subset it. So almost every shop that I know of that uses C++ says, "Yes, we're using C++ but we're not doing multiple-implementation inheritance and we're not using operator overloading." There are just a bunch of features that you're not going to use because the complexity of the resulting code is too high. And I don't think it's good when you have to start doing that. You lose this programmer portability where everyone can read everyone else's code, which I think is such a good thing.  Donald Knuth (1993, commenting on pre-standardized C++), who said of Edsger Dijkstra that "to think of programming in C++" "would make him physically ill":[101][102]   The problem that I have with them today is that... C++ is too complicated. At the moment, it's impossible for me to write portable code that I believe would work on lots of different systems, unless I avoid all exotic features. Whenever the C++ language designers had two competing ideas as to how they should solve some problem, they said "OK, we'll do them both". So the language is too baroque for my taste.  Ken Thompson, who was a colleague of Stroustrup at Bell Labs, gives his assessment:[99][100]   It certainly has its good points. But by and large I think it's a bad language. It does a lot of things half well and it's just a garbage heap of ideas that are mutually exclusive. Everybody I know, whether it's personal or corporate, selects a subset and these subsets are different. So it's not a good language to transport an algorithm—to say, "I wrote it; here, take it." It's way too big, way too complex. And it's obviously built by a committee.
Stroustrup campaigned for years and years and years, way beyond any sort of technical contributions he made to the language, to get it adopted and used. And he sort of ran all the standards committees with a whip and a chair. And he said "no" to no one. He put every feature in that language that ever existed. It wasn't cleanly designed—it was just the union of everything that came along. And I think it suffered drastically from that.  
However Brian Kernighan, also a colleague at Bell Labs, disputes this assessment:[103]  C++ has been enormously influential. ... Lots of people say C++ is too big and too complicated etc. etc. but in fact it is a very powerful language and pretty much everything that is in there is there for a really sound reason: it is not somebody doing random invention, it is actually people trying to solve real world problems. Now a lot of the programs that we take for granted today, that we just use, are C++ programs.  Stroustrup himself comments that C++ semantics are much cleaner than its syntax: "within C++, there is a much smaller and cleaner language struggling to get out."[104]
 Other complaints may include a lack of reflection or garbage collection, long compilation times, perceived feature creep,[105] and verbose error messages, particularly from template metaprogramming.[106]


Source: https://en.wikipedia.org/wiki/Perl_(Programmiersprache)
Content: Other reasons this message may be displayed:


Source: https://en.wikipedia.org/wiki/Java_(Programmiersprache)
Content: Other reasons this message may be displayed:


Source: https://en.wikipedia.org/wiki/Lisp
Content: A lisp is a  speech impairment in which a person misarticulates sibilants ([s], [z], [ts], [dz], [ʃ], [ʒ], [t͡ʃ], [d͡ʒ]).[1] These misarticulations often result in unclear speech.
 Successful treatments have shown that causes are functional rather than physical: that is, most lisps are caused by errors in tongue placement or density of the tongue within the mouth rather than caused by any injury or congenital or acquired deformity to the mouth. The most frequently discussed of these problems is tongue thrust in which the tongue protrudes beyond the front teeth.[3] This protrusion affects speech as well as swallowing and can lead to lisping. Ankyloglossia or tongue tie can also be responsible for lisps in children — however, it is unclear whether these deficiencies are caused by the tongue tie itself or the muscle weakness following the correction of the tongue tie.[4] Overbites and underbites may also contribute to non lingual lisping. Temporary lisps can be caused by dental work, dental appliances such as dentures or retainers or by swollen or bruised tongues.
 Lisps caused by tongue tie can be treated by a dentist or otolaryngologist (ENT) with a lingual frenectomy, or laser incision, which takes less than 10 to 15 minutes to complete.[5][6][7]
 With an interdental lisp, the therapist teaches the student how to keep the tongue behind the two front incisors.[8]
 One popular method of correcting articulation or lisp disorders is to isolate sounds and work on correcting the sound in isolation. The basic sound, or phoneme, is selected as a target for treatment. Typically the position of the sound within a word is considered and targeted. The sound appears in the beginning of the word, middle, or end of the word (initial, medial, or final).
 Take for example, correction of an "S" sound (lisp). Most likely, a speech language pathologist (SLP) would employ exercises to work on "Sssssss."[clarify] Starting practice words would most likely consist of "S-initial" words such as "say, sun, soap, sip, sick, said, sail." According to this protocol, the SLP slowly increases the complexity of tasks (context of pronunciations) as the production of the sound improves. Examples of increased complexity could include saying words in phrases and sentences, saying longer multi syllabic words, or increasing the tempo of pronunciation.
 Using this method, the SLP achieves success with their student by targeting a sound in a phonetically consistent manner. Phonetic consistency means that a target sound is isolated at the smallest possible level (phoneme, phone, or allophone) and that the context of production must be consistent. Consistency is critical, because factors such as the position within the word, grouping with other sounds (vowels or consonants), and the complexity all may affect production.
 Another popular method for treating a lisp is using specially designed devices that go in the mouth to provide a tactile cue of exactly where the tongue should be positioned when saying the "S" sound. This tactile feedback has been shown to correct lisp errors twice as fast as traditional therapy. 
 Using either or both methods, the repetition of consistent contexts allows the student to align all the necessary processes required to properly produce language; language skills (ability to formulate correct sounds in the brain: What sounds do I need to make?), motor planning (voicing and jaw and tongue movements: How do I produce the sound?), and auditory processing (receptive feedback: Was the sound produced correctly? Do I need to correct?).
 A student with an articulation or lisp disorder has a deficiency in one or more of these areas. To correct the deficiency, adjustments have to be made in one or more of these processes. The process to correct it is more often than not, trial and error. With so many factors, however, isolating the variables (the sound) is imperative to getting to the result faster. 
 A phonetically consistent treatment strategy means practicing the same thing over and over. What is practiced is consistent and does not change. The words might change, but the phoneme and its positioning is the same (say, sip, sill, soap, ...). Thus, successful correction of the disorder is found in manipulating or changing the other factors involved with speech production (tongue positioning, cerebral processing, etc.). Once a successful result (speech) is achieved, then consistent practice becomes essential to reinforcing correct productions.
 When the difficult sound is mastered, the student will then learn to say the sound in syllables, then words, then phrases and then sentences. When a student can speak a whole sentence without lisping, attention is then focused on making correct sounds throughout natural conversation. Towards the end of the course of therapy, the student will be taught how to monitor his or her own speech, and how to correct as necessary. Speech therapy can sometimes fix the problem, but in some cases speech therapy fails to work.


Source: https://en.wikipedia.org/wiki/Haskell_(Programmiersprache)
Content: Other reasons this message may be displayed:


Source: https://en.wikipedia.org/wiki/APL_(Programmiersprache)
Content: Other reasons this message may be displayed:


Source: https://en.wikipedia.org/wiki/CLU_(Programmiersprache)
Content: Other reasons this message may be displayed:


Source: https://en.wikipedia.org/wiki/Dylan_(Programmiersprache)
Content: Other reasons this message may be displayed:


Source: https://en.wikipedia.org/wiki/Standard_ML
Content: Standard ML (SML) is a general-purpose, modular, functional programming language with compile-time type checking and type inference. It is popular for writing compilers, for programming language research, and for developing theorem provers.
 Standard ML is a modern dialect of ML, the language used in the Logic for Computable Functions (LCF) theorem-proving project. It is distinctive among widely used languages in that it has a formal specification, given as typing rules and operational semantics in The Definition of Standard ML.[5]
 Standard ML is a functional programming language with some impure features. Programs written in Standard ML consist of expressions in contrast to statements or commands, although some expressions of type unit are only evaluated for their side-effects.
 Like all functional languages, a key feature of Standard ML is the function, which is used for abstraction. The factorial function can be expressed as follows:
 An SML compiler must infer the static type val factorial : int -> int without user-supplied type annotations. It has to deduce that n is only used with integer expressions, and must therefore itself be an integer, and that all terminal expressions are integer expressions.
 The same function can be expressed with clausal function definitions where the if-then-else conditional is replaced with templates of the factorial function evaluated for specific values:
 or iteratively:
 or as a lambda function:
 Here, the keyword val introduces a binding of an identifier to a value, fn introduces an anonymous function, and rec allows the definition to be self-referential.
 The encapsulation of an invariant-preserving tail-recursive tight loop with one or more accumulator parameters within an invariant-free outer function, as seen here, is a common idiom in Standard ML.
 Using a local function, it can be rewritten in a more efficient tail-recursive style:
 A type synonym is defined with the keyword type. Here is a type synonym for points on a plane, and functions computing the distances between two points, and the area of a triangle with the given corners as per Heron's formula. (These definitions will be used in subsequent examples).
 Standard ML provides strong support for algebraic datatypes (ADT). A data type can be thought of as a disjoint union of tuples (or a "sum of products"). They are easy to define and easy to use, largely because of pattern matching, and most Standard ML implementations' pattern-exhaustiveness checking and pattern redundancy checking.
 In object-oriented programming languages, a disjoint union can be expressed as class hierarchies. However, in contrast to class hierarchies, ADTs are closed. Thus, the extensibility of ADTs is orthogonal to the extensibility of class hierarchies. Class hierarchies can be extended with new subclasses which implement the same interface, while the functions of ADTs can be extended for the fixed set of constructors. See expression problem.
 A datatype is defined with the keyword datatype, as in:
 Note that a type synonym cannot be recursive; datatypes are necessary to define recursive constructors. (This is not at issue in this example.)
 Patterns are matched in the order in which they are defined. C programmers can use tagged unions, dispatching on tag values, to do what ML does with datatypes and pattern matching. Nevertheless, while a C program decorated with appropriate checks will, in a sense, be as robust as the corresponding ML program, those checks will of necessity be dynamic; ML's static checks provide strong guarantees about the correctness of the program at compile time.
 Function arguments can be defined as patterns as follows:
 The so-called "clausal form" of function definition, where arguments are defined as patterns, is merely syntactic sugar for a case expression:
 Pattern-exhaustiveness checking will make sure that each constructor of the datatype is matched by at least one pattern. 
 The following pattern is not exhaustive:
 There is no pattern for the Triangle case in the center function. The compiler will issue a warning that the case expression is not exhaustive, and if a Triangle is passed to this function at runtime, exception Match will be raised.
 The pattern in the second clause of the following (meaningless) function is redundant:
 Any value that would match the pattern in the second clause would also match the pattern in the first clause, so the second clause is unreachable. Therefore, this definition as a whole exhibits redundancy, and causes a compile-time warning.
 The following function definition is exhaustive and not redundant:
 If control gets past the first pattern (Circle), we know the shape must be either a Square or a Triangle. In either of those cases, we know the shape has corners, so we can return true without discerning the actual shape.
 Functions can consume functions as arguments:
 Functions can produce functions as return values:
 Functions can also both consume and produce functions:
 The function List.map from the basis library is one of the most commonly used higher-order functions in Standard ML:
 A more efficient implementation with tail-recursive List.foldl:
 Exceptions are raised with the keyword raise and handled with the pattern matching handle construct. The exception system can implement non-local exit; this optimization technique is suitable for functions like the following.
 When exception Zero is raised, control leaves the function List.foldl altogether. Consider the alternative: the value 0 would be returned, it would be multiplied by the next integer in the list, the resulting value (inevitably 0) would be returned, and so on. The raising of the exception allows control to skip over the entire chain of frames and avoid the associated computation. Note the use of the underscore (_) as a wildcard pattern.
 The same optimization can be obtained with a tail call.
 Standard ML's advanced module system allows programs to be decomposed into hierarchically organized structures of logically related type and value definitions. Modules provide not only namespace control but also abstraction, in the sense that they allow the definition of abstract data types. Three main syntactic constructs comprise the module system: signatures, structures and functors.
 A signature is an interface, usually thought of as a type for a structure; it specifies the names of all entities provided by the structure, the arity of each type component, the type of each value component, and the signature of each substructure. The definitions of type components are optional; type components whose definitions are hidden are abstract types.
 For example, the signature for a queue may be:
 This signature describes a module that provides a polymorphic type 'a queue, exception QueueError, and values that define basic operations on queues.
 A structure is a module; it consists of a collection of types, exceptions, values and structures (called substructures) packaged together into a logical unit.
 A queue structure can be implemented as follows:
 This definition declares that structure TwoListQueue implements signature QUEUE. Furthermore, the opaque ascription denoted by :> states that any types which are not defined in the signature (i.e. type 'a queue) should be abstract, meaning that the definition of a queue as a pair of lists is not visible outside the module. The structure implements all of the definitions in the signature.
 The types and values in a structure can be accessed with "dot notation":
 A functor is a function from structures to structures; that is, a functor accepts one or more arguments, which are usually structures of a given signature, and produces a structure as its result. Functors are used to implement generic data structures and algorithms.
 One popular algorithm[6] for breadth-first search of trees makes use of queues. Here is a version of that algorithm parameterized over an abstract queue structure:
 Within functor BFS, the representation of the queue is not visible. More concretely, there is no way to select the first list in the two-list queue, if that is indeed the representation being used. This data abstraction mechanism makes the breadth-first search truly agnostic to the queue's implementation. This is in general desirable; in this case, the queue structure can safely maintain any logical invariants on which its correctness depends behind the bulletproof wall of abstraction.
 Snippets of SML code are most easily studied by entering them into an interactive top-level.
 The following is a "Hello, World!" program:
 Insertion sort for int list (ascending) can be expressed concisely as follows:
 Here, the classic mergesort algorithm is implemented in three functions: split, merge and mergesort. Also note the absence of types, with the exception of the syntax op :: and [] which signify lists. This code will sort lists of any type, so long as a consistent ordering function cmp is defined. Using Hindley–Milner type inference, the types of all variables can be inferred, even complicated types such as that of the function cmp.
 Split
 fun split is implemented with a stateful closure which alternates between true and false, ignoring the input:
 Merge
 Merge uses a local function loop for efficiency. The inner loop is defined in terms of cases: when both lists are non-empty (x :: xs) and when one list is empty ([]).
 This function merges two sorted lists into one sorted list. Note how the accumulator acc is built backwards, then reversed before being returned. This is a common technique, since 'a list is represented as a linked list; this technique requires more clock time, but the asymptotics are not worse.
 Mergesort
 The main function:
 Quicksort can be expressed as follows. fun part is a closure that consumes an order operator op <<.
 Note the relative ease with which a small expression language can be defined and processed:
 Example usage on well-typed and ill-typed expressions:
 The IntInf module provides arbitrary-precision integer arithmetic. Moreover, integer literals may be used as arbitrary-precision integers without the programmer having to do anything.
 The following program implements an arbitrary-precision factorial function:
 Curried functions have many applications, such as eliminating redundant code. For example, a module may require functions of type a -> b, but it is more convenient to write functions of type a * c -> b where there is a fixed relationship between the objects of type a and c. A function of type c -> (a * c -> b) -> a -> b can factor out this commonality. This is an example of the adapter pattern.[citation needed]
 In this example, fun d computes the numerical derivative of a given function f at point x:
 The type of fun d indicates that it maps a "float" onto a function with the type (real -> real) -> real -> real. This allows us to partially apply arguments, known as currying. In this case, function d can be specialised by partially applying it with the argument delta. A good choice for delta when using this algorithm is the cube root of the machine epsilon.[citation needed]
 The inferred type indicates that d' expects a function with the type real -> real as its first argument. We can compute an approximation to the derivative of 



f
(
x
)
=

x

3


−
x
−
1


{\displaystyle f(x)=x^{3}-x-1}

 at 



x
=
3


{\displaystyle x=3}

. The correct answer is 




f
′

(
3
)
=
27
−
1
=
26


{\displaystyle f'(3)=27-1=26}

.
 The Basis Library[7] has been standardized and ships with most implementations. It provides modules for trees, arrays, and other data structures, and input/output and system interfaces.
 For numerical computing, a Matrix module exists (but is currently broken), https://www.cs.cmu.edu/afs/cs/project/pscico/pscico/src/matrix/README.html.
 For graphics, cairo-sml is an open source interface to the Cairo graphics library. For machine learning, a library for graphical models exists.
 Implementations of Standard ML include the following:
 Standard
 Derivative
 Research
 All of these implementations are open-source and freely available. Most are implemented themselves in Standard ML. There are no longer any commercial implementations; Harlequin, now defunct, once produced a commercial IDE and compiler called MLWorks which passed on to Xanalys and was later open-sourced after it was acquired by Ravenbrook Limited on April 26, 2013.
 The IT University of Copenhagen's entire enterprise architecture is implemented in around 100,000 lines of SML, including staff records, payroll, course administration and feedback, student project management, and web-based self-service interfaces.[8]
 The proof assistants HOL4, Isabelle, LEGO, and Twelf are written in Standard ML. It is also used by compiler writers and integrated circuit designers such as ARM.[9]
 About Standard ML
 About successor ML
 Practical
 Academic


Source: https://en.wikipedia.org/wiki/Ruby_(Programmiersprache)
Content: Other reasons this message may be displayed:


Source: https://en.wikipedia.org/wiki/Cython
Content: 
 Cython (/ˈsaɪθɒn/) is a superset of the programming language Python, which allows developers to write Python code (with optional, C-inspired syntax extensions) that yields performance comparable to that of C.[5][6]
 Cython is a compiled language that is typically used to generate CPython extension modules. Annotated Python-like code is compiled to C (also usable from e.g. C++) and then automatically wrapped in interface code, producing extension modules that can be loaded and used by regular Python code using the import statement, but with significantly less computational overhead at run time. Cython also facilitates wrapping independent C or C++ code into python-importable modules.
 Cython is written in Python and C and works on Windows, macOS, and Linux, producing C source files compatible with CPython 2.6, 2.7, and 3.3 and later versions. The Cython source code that Cython compiles (to C) can use both Python 2 and Python 3 syntax, defaulting to Python 2 syntax in Cython 0.x (and Python 3 syntax in Cython 3.x). The default can be overridden (e.g. in source code comment) to Python 3 (or 2) syntax. Since Python 3 syntax has changed in recent versions, Cython may not be up to date with latest addition. Cython has "native support for most of the C++ language" and "compiles almost all existing Python code".[7]
 Cython 3.0.0 was released on 17 July 2023.[8]
 Cython works by producing a standard Python module. However, the behavior differs from standard Python in that the module code, originally written in Python, is translated into C. While the resulting code is fast, it makes many calls into the CPython interpreter and CPython standard libraries to perform actual work. Choosing this arrangement saved considerably on Cython's development time, but modules have a dependency on the Python interpreter and standard library.
 Although most of the code is C-based, a small stub loader written in interpreted Python is usually required (unless the goal is to create a loader written entirely in C, which may involve work with the undocumented internals of CPython). However, this is not a major problem due to the presence of the Python interpreter.[9]
 Cython has a foreign function interface for invoking C/C++ routines and the ability to declare the static type of subroutine parameters and results, local variables, and class attributes.
 A Cython program that implements the same algorithm as a corresponding Python program may consume fewer computing resources such as core memory and processing cycles due to differences between the CPython and Cython execution models. A basic Python program is loaded and executed by the CPython virtual machine, so both the runtime and the program itself consume computing resources. A Cython program is compiled to C code, which is further compiled to machine code, so the virtual machine is used only briefly when the program is loaded.[10][11][12][13]
 Cython employs:
 Performance depends both on what C code is generated by Cython and how that code is compiled by the C compiler.[16]
 Cython is a derivative of the Pyrex language, and supports more features and optimizations than Pyrex.[17][18] Cython was forked from Pyrex in 2007 by developers of the Sage computer algebra package, because they were unhappy with Pyrex's limitations and could not get patches accepted by Pyrex's maintainer Greg Ewing, who envisioned a much smaller scope for his tool than the Sage developers had in mind. They then forked Pyrex as SageX. When they found people were downloading Sage just to get SageX, and developers of other packages (including Stefan Behnel, who maintains the XML library LXML) were also maintaining forks of Pyrex, SageX was split off the Sage project and merged with cython-lxml to become Cython.[19]
 Cython files have a .pyx extension. At its most basic, Cython code looks exactly like Python code. However, whereas standard Python is dynamically typed, in Cython, types can optionally be provided, allowing for improved performance, allowing loops to be converted into C loops where possible. For example:
 A sample hello world program for Cython is more complex than in most languages because it interfaces with the Python C API and setuptools or other PEP517-compliant extension building facilities. At least three files are required for a basic project:
 The following code listings demonstrate the build and launch process:
 These commands build and launch the program:
 A more straightforward way to start with Cython is through command-line IPython (or through in-browser python console called Jupyter notebook):
 which gives a 95 times improvement over the pure-python version. More details on the subject in the official quickstart page.[20]
 Cython is particularly popular among scientific users of Python,[12][21][22] where it has "the perfect audience" according to Python creator Guido van Rossum.[23] Of particular note:
 Cython's domain is not limited to just numerical computing. For example, the lxml XML toolkit is written mostly in Cython, and like its predecessor Pyrex, Cython is used to provide Python bindings for many C and C++ libraries such as the messaging library ZeroMQ.[28] Cython can also be used to develop parallel programs for multi-core processor machines; this feature makes use of the OpenMP library.


Source: https://en.wikipedia.org/wiki/Betriebssystem
Content: Other reasons this message may be displayed:


Source: https://en.wikipedia.org/wiki/Plattformunabh%C3%A4ngigkeit
Content: Other reasons this message may be displayed:


Source: https://en.wikipedia.org/wiki/Lizenz
Content: Lučenec (Slovak pronunciation: [ˈlutʂeɲets] ⓘ; German: Lizenz; Hungarian: Losonc; Yiddish: לאשאנץ; Latin: Lutetia Hungarorum[4]) is a town in the Banská Bystrica Region of south-central Slovakia. Historically, it was part, and in the 18th century the capital, of Nógrád County of the Kingdom of Hungary. In 1920, as a result of the Treaty of Trianon, it became a part of Czechoslovakia. The town has a large synagogue, built in 1924, which served a large Jewish population before World War II. The synagogue underwent renovations in 2016.
 Lučenec is the economic centre of the whole Novohrad region, which includes districts Poltár and Veľký Krtíš.
 Lučenec and its surroundings were inhabited in the Stone Ages. Slavs moved to this area in the 6th and 7th century as the first permanent settlers and the Hungarians joined them in the 10th century.
 The first indirect mention of Lučenec was in 1128, when Lambert built a chapel in honour of Virgin Mary. The first direct mention of the settlement was in 1247 under the name Luchunch, but until the first half of the 15th century it was only a village, and was located off the main trade routes. In 1442, Lučenec was conquered by the Hussites troops under command of John Jiskra of Brandýs and in 1451 the Battle of Lučenec took place near the village between the troops of John Hunyadi and those of Jiskra, where the latter emerged victorious.
 After the fall of the Fiľakovo (Hungarian: Fülek, German: Fülleck) castle in 1554, Lučenec was under the control of the Ottomans and their vassals as part of Budin Eyalet until capturing by Austrians in 1593. It was regained by Ottomans in 1596 and was again part of Filek (Ottoman name for Fiľakovo) sanjak (its centre was in modern-day Rimavská Sobota) in Eğri Eyalet till 1686. The town was burned down many times until the first half of the 19th century, when during the Revolutions of 1848/1849 it was occupied by the Russian imperial troops.[citation needed]
 The town underwent modernization in the 19th and 20th centuries, for example, new industries like brickworks or tanneries were built, telegraph line in 1865, and in 1871 it was connected to the railway connecting Budapest (Slovak: Budapešť) and Žilina. After World War I, Lučenec became part of Czechoslovakia, and, briefly in 1919, part of the Slovak Soviet Republic. In 1938, Lučenec was annexed to Hungary as a result of the First Vienna Award, and this lasted until 1945 when it was returned to Czechoslovakia. Approximately 8.3% of current residents are ethnic Hungarians.[citation needed]
 The Novohrad Museum and Gallery with a collection of over 30,000 artefacts moved to a building in Kubínyiho Square in 1985.[5]
 Lučenec has a Humid continental climate (Köppen: Dfa) with four alternating seasons. There are hot summers and cold winters. There is a high number of sunshine days with a short time of duration of snow cover as well as the cover is relatively low. Near by Lučenec are located several water reservoirs such as Ľadovo, Mýtna, Málinec and most popular Ružiná.
 From 25,902 inhabitants (from census 2021)[7] are:
 In 1910 out of 12,939 inhabitants some 10,634 were Hungarians (82%), 1,675 Slovaks (13%), 428 Germans, 9 Romas, 1 Ruthenian, 12 Croatians, 18 Serbans and 162 others.[citation needed]
 According to the 2001 census, there were 28,332 people living in the town, with majority of them being Slovaks (81.63%), with a minority of Hungarians (13.11%) and with a small percentage of Roma (2.32%), Czechs (0.61%) and others. The religious make-up was: 56.56% Roman Catholics, 21.12% people with no religious affiliation and 14.77% Lutherans.[8]
 Lučenec is divided into these boroughs:
 Lučenec is twinned with:[11]


Source: https://en.wikipedia.org/wiki/Liste_der_IPA-Zeichen
Content: Other reasons this message may be displayed:


Source: https://en.wikipedia.org/wiki/Liste_der_IPA-Zeichen
Content: Other reasons this message may be displayed:


Source: https://en.wikipedia.org/wiki/Liste_der_IPA-Zeichen
Content: Other reasons this message may be displayed:


Source: https://en.wikipedia.org/wiki/Interpreter
Content: Interpreting is a translational activity in which one produces a first and final target-language output on the basis of a one-time exposure to an expression in a source language.
 The most common two modes of interpreting are simultaneous interpreting, which is done at the time of the exposure to the source language, and consecutive interpreting, which is done at breaks to this exposure.
 Interpreting is an ancient human activity which predates the invention of writing.[1] However, the origins of the profession of interpreting date back to less than a century ago.[when?][2]
 Research into the various aspects of the history of interpreting is quite new.[3] For as long as most scholarly interest was given to professional conference interpreting, very little academic work was done on the practice of interpreting in history, and until the 1990s, only a few dozen publications were done on it.[4]
 Considering the amount of interpreting activities that is assumed to have occurred for thousands of years, historical records are limited.[5] Moreover, interpreters and their work have usually not found their way into the history books.[6] One of the reasons for that is the dominance of the written text over the spoken word (in the sense that those who have left written texts are more likely to be recorded by historians).[3][4] Another problem is the tendency to view it as an ordinary support activity which does not require any special attention,[4] and the social status of interpreters, who were sometimes treated unfairly by scribes, chroniclers and historians.[note 1][3]
 Our knowledge of the past of interpreting tends to come from letters, chronicles, biographies, diaries and memoirs, along with a variety of other documents and literary works, many of which (and with few exceptions) were only incidentally or marginally related to interpreting.[6][4]
 Many Indo-European languages have words for interpreting and interpreter.[1] Expressions in Germanic, Scandinavian and Slavic languages denoting an interpreter can be traced back to Akkadian, around 1900 BCE.[1] The Akkadian root targumânu/turgumânu also gave rise to the term dragoman via an etymological sideline from Arabic.[7]
 The English word interpreter, however, is derived from Latin interpres (meaning 'expounder', 'person explaining what is obscure'), whose semantic roots are not clear.[8] Some scholars take the second part of the word to be derived from partes or pretium (meaning 'price', which fits the meaning of a 'middleman', 'intermediary' or 'commercial go-between'), but others have suggested a Sanskrit root.[8]
 In consecutive interpreting (CI), the interpreter starts to interpret after the speaker pauses; thus much more time (perhaps double) is needed. Customarily, such an interpreter will sit or stand near the speaker.[9]
 Consecutive interpretation can be conducted in a pattern of short or long segments according to the interpreter's preference. In short CI, the interpreter relies mostly on memory whereas, in long CI, most interpreters will rely on note-taking. The notes must be clear and legible in order to not waste time on reading them.[10] Consecutive interpreting of whole thoughts, rather than in small pieces, is desirable so that the interpreter has the whole meaning before rendering it in the target language. This affords a truer, more accurate, and more accessible interpretation than where short CI or simultaneous interpretation is used.
 An attempt at consensus about lengths of segments may be reached prior to commencement, depending upon complexity of the subject matter and purpose of the interpretation, though speakers generally face difficulty adjusting to unnatural speech patterns.[citation needed]
 On occasion, document sight translation is required of the interpreter during consecutive interpretation work. Sight translation combines interpretation and translation; the interpreter must render the source-language document to the target-language as if it were written in the target language. Sight translation occurs usually, but not exclusively, in judicial and medical work.
 Consecutive interpretation may be the chosen mode when bilingual listeners are present who wish to hear both the original and interpreted speech or where, as in a court setting, a record must be kept of both.[citation needed]
 When no interpreter is available to interpret directly from source to target, an intermediate interpreter will be inserted in a relay mode, e.g. a Greek source language could be interpreted into English and then from English to another language. This is also commonly known as double-interpretation. Triple-interpretation may even be needed, particularly where rare languages or dialects are involved. Such interpretation can only be effectively conducted using consecutive interpretation.
 Simultaneous interpretation (SI) has the disadvantage that if a person is performing the service the interpreter must do the best they can within the time permitted by the pace of source speech. However they also have the advantages of saving time and not disturbing the natural flow of the speaker.  SI can also be accomplished by software where the program can simultaneously listen to incoming speech and speak the associated interpretation. The most common form is extempore SI, where the interpreter does not know the message until they hear it.
 Simultaneous interpretation using electronic equipment where the interpreter can hear the speaker's voice as well as the interpreter's own voice was introduced at the Nuremberg trials in 1945.[11] The equipment facilitated large numbers of listeners, and interpretation was offered in French, Russian, German and English.[12] The technology arose in the 1920s and 1930s when American businessman Edward Filene and British engineer Alan Gordon Finlay developed simultaneous interpretation equipment with IBM.[13] Yvonne Kapp attended a conference with simultaneous interpretation in 1935 in the Soviet Union.[14] As it proved successful, IBM was able to sell the equipment to the United Nations, where it is now widely used in the United Nations Interpretation Service.
 In the ideal setting for oral language, the interpreter sits in a sound-proof booth and speaks into a microphone, while clearly seeing and hearing the source-language speaker via earphones. The simultaneous interpretation is rendered to the target-language listeners via their earphones.
 Pavel Palazchenko's My Years with Gorbachev and Shevardnadze: The Memoir of a Soviet Interpreter gives a short history of modern interpretation and of the transition from its consecutive to simultaneous forms. He explains that during the nineteenth century interpreters were rarely needed during European diplomatic discussions; these were routinely conducted in French, and all government diplomats were required to be fluent in this language.  Most European government leaders and heads of state could also speak French.[15] Historian Harold Nicolson attributes the growing need for interpretation after World War I to the fact that U.S. President Woodrow Wilson and British Prime Minister David Lloyd George "were no linguists".[16] At the time, the concept and special equipment needed for simultaneous interpretation, later patented by Alan Gordon Finlay, had not been developed, so consecutive interpretation was used.[15]
 Consecutive interpreters, in order be accurate, used a specialized system of note-taking which included symbols abbreviations and acronyms.  Because they waited until the speaker was finished to provide interpretation, the interpreters then had the difficult task of creating from these notes as much as half an hour of free-flowing sentences closely matching the speaker's meaning. Palazchenko cites Anton Velleman [de], Jean Herbert and the Kaminker brothers as skilled interpreters, and notes one unusual case in which André Kaminker interpreted a speech by a French diplomat who spoke for two and a half hours without stopping.[15]
 After World War II, simultaneous interpretation came into use at the Nuremberg trial, and began to be more accepted. Experienced consecutive interpreters asserted that the difficulties of listening and speaking at the same time, adjusting for differences in sentence structure between languages, and interpreting the beginning of a sentence before hearing its end, would produce an inferior result.  As well, these interpreters, who to that point had been prominent speakers, would now be speaking invisibly from booths.[15]
 In 1951, when the United Nations expanded its number of working languages to five (English, French, Russian, Chinese and Spanish), consecutive interpretation became impractical in most cases, and simultaneous interpretation became the most common process for the organization's large meetings.[17] Consecutive interpretation, which provides a more fluent result without the need for specialized equipment, continued to be used for smaller discussions.[15]
 Stemming from the field of computer-assisted translation, the field of computer-assisted interpretation has emerged, with dedicated tools integrating glossaries and automated speech recognition.[18][19]
 Whispered interpretation is known in the trade by the French term chuchotage. To avoid disturbing the participants using the original language, the interpreter speaks to a few people at close proximity with normal voiced delivery at a very low volume, or through electronic equipment without the benefit of a soundproof booth. Typically, no actual whispering is involved as this is difficult to decipher, causes postural fatigue while parties lean in to one another, and straining to be heard at a whisper "can be as bad for your voice as shouting."[20]
 Conference interpreting refers to interpretation at a conference or large meeting, either simultaneously or consecutively. The advent of multi-lingual meetings has reduced the amount of consecutive interpretation in the last 20 years.
 Conference interpretation is divided between two markets: institutional and private. International institutions (EU, UN, EPO, et cetera), which hold multilingual meetings, often favor interpreting several foreign languages into the interpreters' mother tongues. Local private markets tend to have bilingual meetings (the local language plus another), and the interpreters work both into and out of their mother tongues. These markets are not mutually exclusive. The International Association of Conference Interpreters (AIIC) is the only worldwide association of conference interpreters. Founded in 1953, its membership includes more than 2,800 professional conference interpreters, in more than 90 countries.
 Judicial, legal, or court interpreting occurs in courts of justice, administrative tribunals, and wherever a legal proceeding is held (i.e., a police station for an interrogation, a conference room for a deposition, or the locale for taking a sworn statement). Legal interpreting can be the consecutive interpretation of witnesses' testimony, for example, or the simultaneous interpretation of entire proceedings, by electronic means, for one person, or all of the people attending. In a legal context, where ramifications of misinterpretation may be dire, accuracy is paramount. Teams of two or more interpreters, with one actively interpreting and the second monitoring for greater accuracy, may be deployed.
 The right to a competent interpreter for anyone who does not understand the language of the court (especially for the accused in a criminal trial) is usually considered a fundamental rule of justice. Therefore, this right is often guaranteed in national constitutions, declarations of rights, fundamental laws establishing the justice system or by precedents set by the highest courts. However, it is not a constitutionally required procedure (in the United States) that a certified interpreter be present at police interrogation.[21] This has been especially controversial in cases where illegal immigrants with no English skills are accused of crimes.
 In the US, depending upon the regulations and standards adhered to per state and venue, court interpreters usually work alone when interpreting consecutively, or as a team, when interpreting simultaneously. In addition to practical mastery of the source and target languages, thorough knowledge of law and legal and court procedures is required of court interpreters. They are often required to have formal authorization from the state to work in the courts – and then are called certified court interpreters.[note 2] In many jurisdictions, the interpretation is considered an essential part of the evidence. Incompetent interpretation, or simply failure to swear in the interpreter, can lead to a mistrial.
 In escort interpreting, an interpreter accompanies a person or a delegation on a tour, on a visit, or to a business meeting or interview. An interpreter in this role is called an escort interpreter or an escorting interpreter. An escort interpreter's work session may run for days, weeks, or even months, depending on the period of the client's visit. This type of interpreting is often needed in business contexts, during presentations, investor meetings, and business negotiations. As such, an escort interpreter needs to be equipped with some business and financial knowledge in order to best understand and convey messages back and forth.
 Signed language interpreters typically refer to this role as a "designated interpreter."[22] It is not a new practice; since the 1960s, deaf professionals and academics such as Robert Sanderson[23] increasingly sought out and trained specific interpreters to work with on a regular, if not exclusive basis.
 Also known as community interpreting, is the type of interpreting occurring in fields such as legal, health, and federal and local government, social, housing, environmental health, education, and welfare services. In community interpreting, factors exist which determine and affect language and communication production, such as speech's emotional content, hostile or polarized social surroundings, its created stress, the power relationships among participants, and the interpreter's degree of responsibility – in many cases more than extreme; in some cases, even the life of the other person depends upon the interpreter's work.
 Medical interpreting is a subset of public service interpreting, consisting of communication among healthcare personnel and the patient and their family or among Healthcare personnel speaking different languages, facilitated by an interpreter, usually formally educated and qualified to provide such interpretation services. In some situations, medical employees who are multilingual may participate part-time as members of internal language banks.[24] Depending on country/state-specific requirements, the interpreter is often required to have some knowledge of medical terminology, common procedures, the patient interview and exam process. Medical interpreters are often cultural liaisons for people (regardless of language) who are unfamiliar with or uncomfortable in hospital, clinical, or medical settings.
 For example, in China, there is no mandatory certificate for medical interpreters as of 2012. Most interpretation in hospitals in China is done by doctors, who are proficient in both Chinese and English (mostly) in his/her specialty. They interpret more in academic settings than for communications between doctors and patients. When a patient needs English language service in a Chinese hospital, more often than not the patient will be directed to a staff member in the hospital, who is recognized by his/her colleagues as proficient in English. The actual quality of such service for patients or medical interpretation for communications between doctors speaking different languages is unknown by the interpreting community as interpreters who lack Healthcare background rarely receive accreditation for medical interpretation in the medical community. Interpreters working in the Healthcare setting may be considered Allied Health Professionals.
 In the United States, language access is a socioeconomic disparity, and language access to federally-funded health services is required by law. Title VI of the Civil Rights Act of 1964 prohibits discrimination on the basis of race, color, or national origin in any program or activity that receives Federal funds or other Federal financial assistance.[25] Hospital systems and clinics that are funded by federal programs, such as Medicare, are required by this law to take reasonable steps towards ensuring equitable access to health services for limited English proficient patients.
 Interpreters are often used in a military context, carrying out interpretation usually either during active military combat or during noncombat operations. Interpretation is one of the main factors in multi-national and multi-lingual cooperation and military cohesion of the military and civilian populations.
 During inactive military operations, the most common goal of military interpreters is to increase overall cohesion in the military unit, and with the civilian population. One of the primary forces behind the feeling of an occupation is a lack of mutual intelligibility. During the War in Afghanistan, the use of American soldiers that did not speak the languages of Afghanistan, and the primary recruitment from northern Afghanistan, primarily Tajiks, led to a feeling of the United States and Tajik forces as an occupying force.[26] This feeling was most common in majority Pashtun areas of the country, which in turn was one of the main causes of the Taliban's resurgence. If interpreters are not present inside war zones, it becomes extremely common for misunderstandings from the civilian population and a military force to spiral into an open conflict, or to produce animosity and distrust, forming the basis of a conflict or an insurgency.[27]
 Military interpreters are commonly found in Iraq and have been largely effective, particularly in the Kurdish held regions (Kurdistan Regional Government), during the fighting against ISIS. Military interpreters were the primary drivers in cooperation between the coalition and the Iraqi population and military. Likewise managing to produce stability in areas held by the coalition, Kurdish interpreters were known for being a primary aid in this endeavour.[28]
 The fundamental act of interpreting during active combat is extremely stressful and dangerous. It is, however, necessary when different-language battalions are fighting together with no common intermediate language. Misunderstandings in this context are most often fatal, the most common misinterpretations are positioning and attempted break outs. In the chaos of combat, however, it can be very easy to make a mistake in interpreting, particularly with the immense noise and changing locations.[29]
 Military interpreters are also used within single armies instead of multi-lingual cooperation. In this context, a military interpreter is usually a given job in each unit. Common examples include Bosnia, Pakistan, Switzerland, and South Africa. This use of assigning soldiers with different languages to a single battalion helps reinforce a feeling of unity in the military force.[30][31]
 For an historical example, see also Linguistics and translations in the Austro-Hungarian Army.
 A sign language interpreter conveys messages between combinations of spoken and signed languages and manual systems. This may be between deaf signers and hearing nonsigners, or among users of different signed languages and manual systems.[32][33] This may be done in simultaneous or consecutive modes, or as sight translation from printed text.
 Interpreters may be hearing, hard of hearing, or deaf, and work in teams of any combination, depending upon the circumstance or audience. Historically, deaf interpreters or DIs work with DeafBlind people who use either close vision or Protactile signing, deaf people with nonstandard, emerging, or idiolect language varieties, affinity or cultural groups within the Deaf community, minors, immigrants of a different signed language, users of a minority signed language, participants in medical, carceral, or legal matters, and persons with cognitive or intellectual disabilities.[34] DIs may work in relay teams with hearing interpreters, from a teleprompter, or with another DI to access the source language. DIs are commonly the member of the team visible on camera or on stage at televised, recorded, or public events.
 Interpreters can be formally trained in postsecondary programs and receive a certificate, associates, bachelors, masters, or doctoral degree.[35][36] In some circumstances, lay interpreters take an experiential route through churches, families, and social networks. Formal interpreter education practices are largely the product of 20th century developments.[37]
 In the United States, Sign Language interpreters have National and some states have a State level certifications. The Registry of Interpreters for the Deaf (RID), a non-profit organization, is known for its national recognition and certification process. In addition to training requirements and stringent certification testing, RID members must abide by a Code of Professional Conduct, Grievance Process and Continuing Education Requirement. There are many interpreter-training programs in the U.S. The Collegiate Commission on Interpreter Education is the body that accredits Interpreter Preparation Programs. A list of accredited programs can be found on the CCIE web site.[38]
 Some countries have more than one national association due to regional or language differences.[39] National associations can become members of the umbrella organizations, the World Association of Sign Language Interpreters[40] or the European Forum of Sign Language Interpreters (efsli).[41] In Canada, the professional association that recognizes and nationally certifies sign language interpreters is the Association of Visual Language Interpreters of Canada (AVLIC). Under AVLIC holds several affiliate chapters representing a specified region of Canada.[42]
 Sign language interpreters encounter a number of linguistic, environmental, interpersonal and intrapersonal factors that can have an effect on their ability to provide accurate interpretation. Studies have found that most interpreter training programs do not sufficiently prepare students for the highly variable day-to-day stresses that an interpreter must manage, and there is an ongoing conversation in the interpreting field as to how to appropriately prepare students for the challenges of the job. Proposed changes include having a more robust definition of what a qualified interpreter should know, as well as a post-graduate internship structure that would allow new interpreters to work with the benefit of supervision from more experienced interpreters, much like the programs in place in medicine, law enforcement, etc.[43]
 In Israel, Naama Weiss, a board member of Malach, the Organization of the Israeli Sign Language Interpreters,[44] advertised a video which she produced. It was her paraphrase of the video So-Low,[45] and showed her viewpoint upon the Israeli Sign Language interpreters' jobs.[46] A study which was made in Finland found that, in comparison to the foreign language teachers and non-linguistic experts, a high cooperativeness was found to be more characteristic to simultaneous and consecutive interpreters,[47] and Weiss showed it in her video, although she claimed to be comic.[48]
 The World Federation of the Deaf asserts that computer-generated signing avatars "do not surpass the natural quality and skill provided by appropriately trained and qualified interpreters," and approves their application only "for pre-recorded static customer information, for example, in hotels or train stations".[49] The WFD statement concedes to such a project only if "deaf people have been involved in advising," and it does not intend to replace human interpreters. Quality and naturalness of movements are closely critiqued by sign-fluent viewers, particularly those who began signing at a younger age.[50]
 By its very nature, media interpreting has to be conducted in the simultaneous mode. It is provided particularly for live television coverages such as press conferences, live or taped interviews with political figures, musicians, artists, sportsmen or people from the business circle. In this type of interpreting, the interpreter has to sit in a sound-proof booth where ideally he/she can see the speakers on a monitor and the set. All equipment should be checked before recording begins. In particular, satellite connections have to be double-checked to ensure that the interpreter's voice is not sent back and the interpreter gets to hear only one channel at a time. In the case of interviews recorded outside the studio and some current affairs program, the interpreter interprets what they hear on a TV monitor. Background noise can be a serious problem. The interpreter working for the media has to sound as slick and confident as a television presenter.
 Media interpreting has gained more visibility and presence especially after the Gulf War. Television channels have begun to hire staff simultaneous interpreters. The interpreter renders the press conferences, telephone beepers, interviews and similar live coverage for the viewers. It is more stressful than other types of interpreting as the interpreter has to deal with a wide range of technical problems coupled with the control room's hassle and wrangling during live coverage.
 Interpreting services can be delivered in multiple modalities. The most common modality through which interpreting services are provided is on-site interpreting.
 Also called "in-person" and "face-to-face" or "F2F" interpreting, this traditional method requires the interpreter be physically present. With the growth of remote settings, having interpreters on-site remains crucial in high-stakes medical, legal, and diplomatic situations, and with socially, intellectually, or emotionally vulnerable clients.[51]
 Also referred to as "over-the-phone interpreting", "telephonic interpreting", and "tele-interpreting", telephone interpreting enables interpretation via telephone. Telephone interpreting can be used in community settings as well as conference settings. Telephone interpreting may be used in place of on-site interpreting when no on-site interpreter is readily available at the location where services are needed. However, it is more commonly used for situations in which all parties who wish to communicate are already speaking to one another via telephone (e.g. telephone applications for insurance or credit cards, or telephone inquiries from consumers to businesses).
 Interpretation services via Video Remote Interpreting (VRI) or a Video Relay Service (VRS) are useful for spoken language barriers where visual-cultural recognition is relevant, and even more applicable where one of the parties is deaf, hard-of-hearing or speech-impaired (mute). In such cases the direction of interpretation is normally within the same principal language, such as French Sign Language (FSL) to spoken French and Spanish Sign Language (SSL) to spoken Spanish. Multilingual sign language interpreters, who can also interpret as well across principal languages (such as to and from SSL, to and from spoken English), are also available, albeit less frequently. Such activities involve considerable effort on the part of the interpreter, since sign languages are distinct natural languages with their own construction and syntax, different from the aural version of the same principal language.
 With video interpreting, sign language interpreters work remotely with live video and audio feeds, so that the interpreter can see the deaf or mute party, converse with the hearing party and vice versa. Much like telephone interpreting, video interpreting can be used for situations in which no on-site interpreters are available. However, video interpreting cannot be used for situations in which all parties are speaking via telephone alone. VRI and VRS interpretation requires all parties to have the necessary equipment. Some advanced equipment enables interpreters to control the video camera, in order to zoom in and out, and to point the camera toward the party that is signing.
 The majority of professional full-time conference interpreters work for phone interpreting agencies, health care institutions, courts, school systems and international organizations like the United Nations (for the United Nations Interpretation Service), the European Union, or the African Union.
 The world's largest employer of interpreters is currently the European Commission,[52] which employs hundreds of staff and freelance interpreters working into the official languages of the European Union and some others in DG Interpretation. In 2016, Florika Fink-Hooijer was appointed as Director General and the first ever Knowledge Centre on Interpretation was created.[53] She had spoken about the need to "futureproof" services by strengthening the skills of colleagues to work with new technologies.' as well as how Artificial Intelligence may be an (un)desired revolution in linguistic services.[54][55] Subsequently, she drove forward the digitalization of the service by introducing features like automatic speech recognition and other support services to interpreters.[56] During the COVID-19 pandemic, she scaled up multilingual interpretation in hybrid meetings via new digital platforms and technologies, which was a "watershed moment" for the interpretation profession.[57]
 The European Union's other institutions (the European Parliament and the European Court of Justice) have smaller interpreting services.
 The United Nations employs interpreters at almost all its sites throughout the world. Because it has only six official languages, however, it is a smaller employer than the European Union.
 Interpreters may also work as freelance operators in their local, regional and national communities, or may take on contract work under an interpreting business or service. They would typically take on work as described above.
 Militaries often use interpreters to better communicate with the local population. One notable example is the US military during the war in Iraq and Afghanistan.
 There are a number of interpreting and translation associations around the world, including NAATI (National Accreditation Authority for Translators and Interpreters), AIIC (The International Association of Conference Interpreters), CATTI (China Accreditation Test for Translators and Interpreters), CTTIC (Canadian Translators, Terminologists and Interpreters Council), the Institute of Translation & Interpreting, ADICA (Argentinian International Association of Conference and Interpreters) and TAALS (The American Association of Language Specialists).
 No worldwide testing or certification agency exists for all types of interpreters. For conference interpretation, there is the International Association of Conference Interpreters, or AIIC.
 Specific regions, countries, or even cities will have their own certification standards. In many cases, graduates of a certain caliber university program acts as a de facto certification for conference interpretation.
 The most recognized interpretation and translation certificate in P.R.C. is China Accreditation Test for Translation and Interpretation, or CATTI.
It is entrusted by the Ministry of Human Resources and Social Security of P.R.C. It is a translation and interpretation professional qualification accreditation test which is implemented throughout the country according to uniform standards, in order to assess examinees' bilingual translation or interpretation capability.
CATTI was introduced in 2003. In later 2013, translation and interpreting tests of different levels in English, French, Japanese, Russian, German, Spanish and Arabic were held across the nation.
 Those examinees who pass CATTI and obtain translation and interpretation certificates acquire corresponding translation and interpretation professional titles.
 Relevant institutions from Australia, France, Japan, the Republic of Korea, Singapore and other countries as well as Hong Kong Special Administrative Region and Taiwan have established work ties with CATTI.
 In Germany, anyone can become and call themselves an interpreter; access to this profession is not regulated, but court interpreters must be sworn in and prove their qualifications, e.g. through a recognized certificate or professional experience of several years.[58]
 In order to learn and practice the necessary skills, colleges and universities offer studies in Translation and/or Interpretation Studies, primarily to/from English, but there are also Sign Language Interpretation studies.[58]
Admission to higher education, however, is highly restricted.
 Some states offer a State Examination title Staatlich geprüfter Dolmetscher.
Unlike a bachelor's or master's degree, this certificate merely certifies professional skills.
Access to the exam is far easier, but requires proof of the necessary skills.
For that, there are private schools that offer preparatory courses.
Attending these schools is usually sufficient to prove someone's aptitude.[58]
Of course, a university or college degree is accepted, too.
 Furthermore, the State Examination is offered in many more languages, including German Sign Language, yet primarily to/from German.


Source: https://en.wikipedia.org/wiki/H%C3%B6here_Programmiersprache
Content: Other reasons this message may be displayed:


Source: https://en.wikipedia.org/wiki/Blockstruktur
Content: Other reasons this message may be displayed:


Source: https://en.wikipedia.org/wiki/Programmierparadigma
Content: Other reasons this message may be displayed:


Source: https://en.wikipedia.org/wiki/Objektorientierte_Programmierung
Content: Other reasons this message may be displayed:


Source: https://en.wikipedia.org/wiki/Aspektorientierte_Programmierung
Content: Other reasons this message may be displayed:


Source: https://en.wikipedia.org/wiki/Funktionale_Programmierung
Content: Other reasons this message may be displayed:


Source: https://en.wikipedia.org/wiki/Dynamische_Typisierung
Content: Other reasons this message may be displayed:


Source: https://en.wikipedia.org/wiki/Dynamische_Programmiersprache
Content: Other reasons this message may be displayed:


Source: https://en.wikipedia.org/wiki/Skriptsprache
Content: Other reasons this message may be displayed:


Source: https://en.wikipedia.org/wiki/Python_Software_Foundation
Content: The Python Software Foundation (PSF) is an American nonprofit organization devoted to the Python programming language,[3] launched on March 6, 2001. The mission of the foundation is to foster development of the Python community and is responsible for various processes within the Python community, including developing the core Python distribution, managing intellectual rights, developer conferences including the Python Conference (PyCon), and raising funds.
 In 2005, the Python Software Foundation received the Computerworld Horizon Award for "cutting-edge" technology.[4][5]
 The PSF focuses on empowering and supporting people within the Python community with grant programs that support sprints, conferences, meetups, user groups, and Python development.  The PSF runs Python Conference (PyCon) US, the leading Python community conference.  The PSF is the primary point of contact for organizations that wish to work with Python, to support Python, or sponsor Python development. The PSF provides a structure by which work, donations, and sponsorships are coordinated worldwide. The PSF also possesses and protects intellectual property associated with Python and the Python community, such as the word "Python," the two-snakes logo, and the terms "PyLadies" and "PyCon."[6]
 There are five tiers of membership within the PSF. These tiers include:
 Since late 2012, the Python Software Foundation started recommending that all Python conferences create and apply a code of conduct. This is mandatory to any event to be granted funds by the Python Software Foundation.[8]
 
 This article about an organization in the United States is a stub. You can help Wikipedia by expanding it.

Source: https://en.wikipedia.org/wiki/Allzweck-Programmiersprache
Content: Other reasons this message may be displayed:


Source: https://en.wikipedia.org/wiki/Datei:Guido_van_Rossum.jpg
Content: Other reasons this message may be displayed:


Source: https://en.wikipedia.org/wiki/Guido_van_Rossum
Content: 
 Guido van Rossum (Dutch: [ˈɣido vɑn ˈrɔsʏm, -səm]; born 31 January 1956) is a Dutch programmer best known as the creator of the Python programming language, for which he was the "benevolent dictator for life" (BDFL) until he stepped down from the position on 12 July 2018.[4][5] He remained a member of the Python Steering Council through 2019, and withdrew from nominations for the 2020 election.[6]
 Van Rossum was born and raised in the Netherlands, where he received a master's degree in mathematics and computer science from the University of Amsterdam in 1982. He received a bronze medal in 1974 in the International Mathematical Olympiad.[7] He has a brother, Just van Rossum, who is a type designer and programmer who designed the typeface used in the "Python Powered" logo.[8]
 Van Rossum lives in Belmont, California, with his wife, Kim Knapp,[9] and their son.[10][11][12] According to his home page and Dutch naming conventions, the "van" in his name is capitalized when he is referred to by surname alone, but not when using his first and last name together.[13]
 While working at the Centrum Wiskunde & Informatica (CWI), Van Rossum wrote and contributed a glob() routine to BSD Unix in 1986[14][15] and helped develop the ABC programming language. He once stated, "I try to mention ABC's influence because I'm indebted to everything I learned during that project and to the people who worked on it."[16] He also created Grail, an early web browser written in Python, and engaged in discussions about the HTML standard.[17]
 He has worked for various research institutes, including the Centrum Wiskunde & Informatica (CWI) in the Netherlands, the U.S. National Institute of Standards and Technology (NIST), and the Corporation for National Research Initiatives (CNRI). In May 2000, he left CNRI along with three other Python core developers to work for tech startup BeOpen.com, which subsequently collapsed by October of the same year.[18][19] From late 2000 until 2003 he worked for Zope Corporation. In 2003 Van Rossum left Zope for Elemental Security. While there he worked on a custom programming language for the organization.[20]
 From 2005 to December 2012, he worked at Google, where he spent half of his time developing the Python language. 
At Google, Van Rossum developed Mondrian, a web-based code review system written in Python and used within the company. He named the software after the Dutch painter Piet Mondrian.[21] He named another related software project after Gerrit Rietveld, a Dutch designer.[22] On 7 December 2012, Van Rossum left Google.[23]
 In January 2013, Van Rossum started working at the cloud file storage company Dropbox.[24][25]
 In October 2019, Van Rossum left Dropbox and officially retired.[26][27][28]
 On 12 November 2020 Van Rossum announced that he was coming out of retirement to join the Developer Division at Microsoft. He currently holds the title Distinguished Engineer at Microsoft.[29][30][31]
 In December 1989, Van Rossum had been looking for a "'hobby' programming project that would keep [him] occupied during the week around Christmas" as his office was closed when he decided to write an interpreter for a "new scripting language [he] had been thinking about lately: a descendant of ABC that would appeal to Unix/C hackers". He attributes choosing the name "Python" to "being in a slightly irreverent mood (and a big fan of Monty Python's Flying Circus)".[32]
 He has explained that Python's predecessor, ABC, was inspired by SETL, noting that ABC co-developer Lambert Meertens had "spent a year with the SETL group at NYU before coming up with the final ABC design".[33]
 On 12 July 2018, Van Rossum announced that he would be stepping down from the position of BDFL of the Python programming language.[34]
 In 1999, Van Rossum submitted a funding proposal to DARPA called "Computer Programming for Everybody", in which he further defined his goals for Python:
 In 2019, Python became the second most popular language on GitHub, the largest source code management website on the internet, second only to JavaScript.[35] According to a programming language popularity survey[36] it is consistently among the top 10 most mentioned languages in job postings. Furthermore, Python has been among the 10 most popular programming languages every year since 2004 according to the TIOBE Programming Community Index and got the number one spot on the index in October 2021.[37]


Source: https://en.wikipedia.org/wiki/Guido_van_Rossum
Content: 
 Guido van Rossum (Dutch: [ˈɣido vɑn ˈrɔsʏm, -səm]; born 31 January 1956) is a Dutch programmer best known as the creator of the Python programming language, for which he was the "benevolent dictator for life" (BDFL) until he stepped down from the position on 12 July 2018.[4][5] He remained a member of the Python Steering Council through 2019, and withdrew from nominations for the 2020 election.[6]
 Van Rossum was born and raised in the Netherlands, where he received a master's degree in mathematics and computer science from the University of Amsterdam in 1982. He received a bronze medal in 1974 in the International Mathematical Olympiad.[7] He has a brother, Just van Rossum, who is a type designer and programmer who designed the typeface used in the "Python Powered" logo.[8]
 Van Rossum lives in Belmont, California, with his wife, Kim Knapp,[9] and their son.[10][11][12] According to his home page and Dutch naming conventions, the "van" in his name is capitalized when he is referred to by surname alone, but not when using his first and last name together.[13]
 While working at the Centrum Wiskunde & Informatica (CWI), Van Rossum wrote and contributed a glob() routine to BSD Unix in 1986[14][15] and helped develop the ABC programming language. He once stated, "I try to mention ABC's influence because I'm indebted to everything I learned during that project and to the people who worked on it."[16] He also created Grail, an early web browser written in Python, and engaged in discussions about the HTML standard.[17]
 He has worked for various research institutes, including the Centrum Wiskunde & Informatica (CWI) in the Netherlands, the U.S. National Institute of Standards and Technology (NIST), and the Corporation for National Research Initiatives (CNRI). In May 2000, he left CNRI along with three other Python core developers to work for tech startup BeOpen.com, which subsequently collapsed by October of the same year.[18][19] From late 2000 until 2003 he worked for Zope Corporation. In 2003 Van Rossum left Zope for Elemental Security. While there he worked on a custom programming language for the organization.[20]
 From 2005 to December 2012, he worked at Google, where he spent half of his time developing the Python language. 
At Google, Van Rossum developed Mondrian, a web-based code review system written in Python and used within the company. He named the software after the Dutch painter Piet Mondrian.[21] He named another related software project after Gerrit Rietveld, a Dutch designer.[22] On 7 December 2012, Van Rossum left Google.[23]
 In January 2013, Van Rossum started working at the cloud file storage company Dropbox.[24][25]
 In October 2019, Van Rossum left Dropbox and officially retired.[26][27][28]
 On 12 November 2020 Van Rossum announced that he was coming out of retirement to join the Developer Division at Microsoft. He currently holds the title Distinguished Engineer at Microsoft.[29][30][31]
 In December 1989, Van Rossum had been looking for a "'hobby' programming project that would keep [him] occupied during the week around Christmas" as his office was closed when he decided to write an interpreter for a "new scripting language [he] had been thinking about lately: a descendant of ABC that would appeal to Unix/C hackers". He attributes choosing the name "Python" to "being in a slightly irreverent mood (and a big fan of Monty Python's Flying Circus)".[32]
 He has explained that Python's predecessor, ABC, was inspired by SETL, noting that ABC co-developer Lambert Meertens had "spent a year with the SETL group at NYU before coming up with the final ABC design".[33]
 On 12 July 2018, Van Rossum announced that he would be stepping down from the position of BDFL of the Python programming language.[34]
 In 1999, Van Rossum submitted a funding proposal to DARPA called "Computer Programming for Everybody", in which he further defined his goals for Python:
 In 2019, Python became the second most popular language on GitHub, the largest source code management website on the internet, second only to JavaScript.[35] According to a programming language popularity survey[36] it is consistently among the top 10 most mentioned languages in job postings. Furthermore, Python has been among the 10 most popular programming languages every year since 2004 according to the TIOBE Programming Community Index and got the number one spot on the index in October 2021.[37]


Source: https://en.wikipedia.org/wiki/Centrum_Wiskunde_%26_Informatica
Content: 
 The Centrum Wiskunde & Informatica (abbr. CWI; English: "National Research Institute for Mathematics and Computer Science") is a research centre in the field of mathematics and  theoretical computer science. It is part of the institutes organization of the Dutch Research Council (NWO) and is located at the Amsterdam Science Park. This institute is famous as the creation site of the programming language Python. It was a founding member of the European Research Consortium for Informatics and Mathematics (ERCIM).
 The institute was founded in 1946 by Johannes van der Corput, David van Dantzig, Jurjen Koksma, Hendrik Anthony Kramers, Marcel Minnaert and Jan Arnoldus Schouten. It was originally called Mathematical Centre (in Dutch: Mathematisch Centrum). One early mission was to develop mathematical prediction models to assist large Dutch engineering projects, such as the  Delta Works. During this early period, the Mathematics Institute also helped with designing the wings of the Fokker F27 Friendship airplane, voted in 2006 as the most beautiful Dutch design of the 20th century.[1][2]
 The computer science component developed soon after. Adriaan van Wijngaarden, considered the founder of computer science (or informatica) in the Netherlands, was the director of the institute for almost 20 years. Edsger Dijkstra did most of his early influential work on algorithms and formal methods at CWI. The first Dutch computers, the Electrologica X1 and Electrologica X8, were both designed at the centre, and Electrologica was created as a spinoff to manufacture the machines.
 In 1983, the name of the institute was changed to Centrum Wiskunde & Informatica (CWI) to reflect a governmental push for emphasizing computer science research in the Netherlands.[3]
 The institute is known for its work in fields such as operations research, software engineering, information processing, and mathematical applications in life sciences and logistics.
More recent examples of research results from CWI include the development of scheduling algorithms for the Dutch railway system (the Nederlandse Spoorwegen, one of the busiest rail networks in the world) and the development of the Python programming language by Guido van Rossum. Python has played an important role in the development of the Google search platform from the beginning, and it continues to do so as the system grows and evolves.[4] 
Many information retrieval techniques used by packages such as SPSS were initially developed by Data Distilleries, a CWI spinoff.[5][6]
 Work at the institute was recognized by national or international research awards, such as the Lanchester Prize (awarded yearly by INFORMS), the Gödel Prize (awarded by ACM SIGACT) and the Spinoza Prize. Most of its senior researchers hold part-time professorships at other Dutch universities, with the institute producing over 170 full professors during the course of its history. Several CWI researchers have been recognized as members of the Royal Netherlands Academy of Arts and Sciences, the Academia Europaea, or as knights in the Order of the Netherlands Lion.[7]
 In February 2017, CWI in association with Google announced a successful collision attack on SHA 1 encryption algorithm.[8]
 CWI was an early user of the Internet in Europe, in the form of a TCP/IP connection to NSFNET. Piet Beertema at CWI established one of the first two connections outside the United States to the NSFNET (shortly after France's INRIA)[9][10][11] for EUnet on 17 November 1988. The first Dutch country code top-level domain issued was cwi.nl.[12][13][14] When this domain cwi.nl was registered, on 1 May 1986, .nl effectively became the first active ccTLD outside the United States.[15] For the first ten years CWI, or rather Beertema, managed the .nl administration, until in 1996 this task was transferred to its spin-off SIDN.[12]
 The Amsterdam Internet Exchange (one of the largest Internet Exchanges in the world, in terms of both members and throughput traffic) is located at the neighbouring SARA (an early CWI spin-off) and Nikhef institutes. The World Wide Web Consortium (W3C) office for the Benelux countries is located at CWI.[16]
 CWI has demonstrated a continuing effort to put the work of its researchers at the disposal of society, mainly by collaborating with commercial companies and creating spin-off businesses. In 2000 CWI established "CWI Incubator BV", a dedicated company with the aim to generate high tech spin-off companies.[17] Some of the CWI spinoffs include:[18]
 52°21′23″N 4°57′07″E﻿ / ﻿52.35639°N 4.95194°E﻿ / 52.35639; 4.95194


Source: https://en.wikipedia.org/wiki/Amsterdam
Content: 
 Amsterdam (/ˈæmstərdæm/ AM-stər-dam, UK also /ˌæmstərˈdæm/ AM-stər-DAM,[10][11] Dutch: [ˌɑmstərˈdɑm] ⓘ; literally, "The Dam on the River Amstel") is the capital[a] and most populated city of the Netherlands. It has a population of 921,402[12] within the city proper, 1,457,018 in the urban area[7] and 2,480,394 in the metropolitan area.[13] Located in the Dutch province of North Holland,[14][15] Amsterdam is colloquially referred to as the "Venice of the North", for its large number of canals, now a UNESCO World Heritage Site.[16]
 Amsterdam was founded at the mouth of the Amstel River that was dammed to control flooding.[17] Originally a small fishing village in the 12th century, Amsterdam became a major world port during the Dutch Golden Age of the 17th century, when the Netherlands was an economic powerhouse. Amsterdam was the leading centre for finance and trade, as well as a hub of secular art production.[18] In the 19th and 20th centuries, the city expanded and new neighborhoods and suburbs were built. The city has a long tradition of openness, liberalism, and tolerance.[19] Cycling is key to the city's modern character, and there are numerous biking paths and lanes spread throughout.[20][21]
 Amsterdam's main attractions include its historic canals; the Rijksmuseum, the state museum with Dutch Golden Age art; the Van Gogh Museum; the Dam Square, where the Royal Palace of Amsterdam and former city hall are located; the Amsterdam Museum; Stedelijk Museum, with modern art; the Concertgebouw concert hall; the Anne Frank House; the Scheepvaartmuseum, the Natura Artis Magistra; Hortus Botanicus, NEMO, the red-light district and cannabis coffee shops. The city is known for its nightlife and festival activity; with several nightclubs among the world's most famous. Its artistic heritage, canals and narrow canal houses with gabled façades; well-preserved legacies of the city's 17th-century Golden Age, have attracted millions of visitors annually.
 The Amsterdam Stock Exchange, founded in 1602, is considered the oldest "modern" securities market stock exchange in the world. As the commercial capital of the Netherlands and one of the top financial centres in Europe, Amsterdam is considered an alpha world city. The city is the cultural capital of the Netherlands.[22] Many large Dutch institutions have their headquarters in the city.[23] Many of the world's largest companies are based here or have established their European headquarters in the city, such as technology companies Uber, Netflix and Tesla.[24] In 2022, Amsterdam was ranked the ninth-best city to live in by the Economist Intelligence Unit[25] and 12th on quality of living for environment and infrastructure by Mercer.[26] The city was ranked 4th place globally as top tech hub in 2019.[27] The Port of Amsterdam is the fifth largest in Europe.[28] The KLM hub and Amsterdam's main airport, Schiphol, is the busiest airport in the Netherlands, third in Europe, and 11th in the world.[29] The Dutch capital is one of the most multicultural cities in the world, with about 180 nationalities represented.[30] Immigration and ethnic segregation in Amsterdam is a current issue.[31]
 Amsterdam's notable residents throughout its history include painters Rembrandt and Vincent van Gogh, 17th-century philosophers Baruch Spinoza, John Locke, René Descartes, and the Holocaust victim and diarist Anne Frank.
 Due to its geographical location in what used to be wet peatland, the founding of Amsterdam is later than other urban centres in the Low Countries. However, around the area of what later became Amsterdam, farmers settled as early as three millennia ago. They lived along the prehistoric IJ river and upstream of its tributary Amstel. The prehistoric IJ was a shallow and quiet stream in peatland behind beach ridges. This secluded area was able to grow into an important local settlement centre, especially in the late Bronze Age, the Iron Age and the Roman Age. Neolithic and Roman artefacts have also been found in the prehistoric Amstel bedding under Amsterdam's Damrak and Rokin, such as shards of Bell Beaker culture pottery (2200-2000 BC) and a granite grinding stone (2700-2750 BC),[32][33] but the location of these artefacts around the river banks of the Amstel probably point to a presence of a modest semi-permanent or seasonal settlement. Until water issues were controlled, a permanent settlement would not have been possible, since the river mouth and the banks of the Amstel in this period in time were too wet for permanent habitation.[34][35]
 The origins of Amsterdam are linked to the development of a dam on the Amstel River called Amestelle, meaning 'watery area', from Aa(m) 'river' + stelle 'site at a shoreline', 'river bank'.[36] In this area, land reclamation started as early as the late 10th century.[37] Amestelle was located along a side arm of the IJ. This side arm took the name from the eponymous land: Amstel. Amestelle was inhabited by farmers, who lived more inland and more upstream, where the land was not as wet as at the banks of the downstream river mouth. These farmers were starting the reclamation around upstream Ouderkerk aan de Amstel, and later at the other side of the river at Amstelveen. The Van Amstel family, known in documents by this name since 1019,[36] held the stewardship in this northwestern nook of the ecclesiastical district of the bishop of Utrecht. The family later served also under the count of Holland.
 A major turning point in the development of the Amstel river mouth was the All Saint's Flood of 1170. In an extremely short period of time, the shallow river IJ turned into a wide estuary, which from then on offered the Amstel an open connection to the Zuiderzee, IJssel and waterways further afield. This made the water flow of the Amstel more active, so excess water could be drained better. With drier banks, the downstream Amstel mouth became attractive for permanent habitation. Moreover, the river had grown from an insignificant peat stream into a junction of international waterways.[38] A settlement was built here immediately after the landscape change of 1170. Right from the start of its foundation it focused on traffic, production and trade; not on farming, as opposed to how communities had lived further upstream for the past 200 years and northward for thousands of years.[39] The construction of a dam at the mouth of the Amstel, eponymously named Dam, is historically estimated to have occurred between 1264 and 1275. The settlement first appeared in a document from 1275, concerning a road toll granted by the count of Holland Floris V to the residents apud Amestelledamme 'at the dam in the Amstel' or 'at the dam of Amstelland'.[40] This allowed the inhabitants of the village to travel freely through the County of Holland, paying no tolls at bridges, locks and dams.[41] This was a move in a years-long struggle for power in the area between the count of Holland and the Amstel family who governed the area on behalf of the bishop of Utrecht.[42] By 1327, the name had developed into Aemsterdam.[43][44]
 The bishop of Utrecht granted Amsterdam zone rights in either 1300 or 1306.[45] The Mirakel van Amsterdam [nl] in 1345 rendered the city an important place of pilgrimage. During the heyday of the Stille Omgang, which became the expression of the pilgrimage after the Protestant Reformation,[46][47] up to 90,000 pilgrims came to Amsterdam.
 From the 14th century on, Amsterdam flourished, largely from trade with the Hanseatic League. From the 15th century on the city established an independent trade route with the Baltic Sea in grain and timber, cutting out the Hanseatic League as middlemen. The city became the staple market of Europe for bulk cargo. This was made possible due to innovations in the herring fishery, from which Amsterdam reaped great wealth.[11] Herring had demand in markets all around Europe. Inventions of on-board gibbing and the haringbuis in 1415, made longer voyages feasible, and hence enabled Dutch fishermen to follow the herring shoals far from the coasts, giving them a monopoly in the industry.
 The herring industry relied on international trade cooperation and large initial investments in ships, which needed many highly skilled and unskilled workers cooperating, which required the import of the necessary raw materials to turn an unfinished product into a marketable one, which required merchants to then sell it throughout the continent and book-keepers and accountants to divide the profit. In short, the herring industry was setting up the foundations for what would later become the transcontinental trade system and the Dutch Golden Age, with Amsterdam at its centre,[12] hence the saying "Amsterdam is built on Herring bones".[17]
 The Low Countries were part of the Hapsburg inheritance and came under the Spanish monarchy in the early sixteenth century. The Dutch rebelled against Philip II of Spain, who led a defense of Catholicism during the Protestant Reformation. The main reasons for the uprising were the imposition of new taxes, the tenth penny, and the religious persecution of Protestants by the newly introduced Inquisition. The revolt escalated into the Eighty Years' War, which ultimately led to Dutch independence.[48] Strongly pushed by Dutch Revolt leader William the Silent, the Dutch Republic became known for its relative religious tolerance. Jews from the Iberian Peninsula, Protestant Huguenots from France, prosperous merchants and printers from Flanders, and economic and religious refugees from the Spanish-controlled parts of the Low Countries found safety in Amsterdam. The influx of Flemish printers and the city's intellectual tolerance made Amsterdam a centre for the European free press.[49]
 During the 17th century, Amsterdam experienced what is considered its Golden Age, during which it became the wealthiest city in the Western world.[51] Ships sailed from Amsterdam to the Baltic Sea, the Caribbean, North America, and Africa, as well as present-day Indonesia, India, Sri Lanka, and Brazil, forming the basis of a worldwide trading network. Amsterdam's merchants had the largest share in both the Dutch East India Company (VOC) and the Dutch West India Company. These companies acquired overseas possessions that later became Dutch colonies.
 Amsterdam was Europe's most important hub for the shipment of goods and was the leading financial centre of the Western world.[52] In 1602, the Amsterdam office of the Dutch East India Company became the world's first stock exchange by trading in its own shares.[53] The Bank of Amsterdam started operations in 1609, acting as a full-service bank for Dutch merchant bankers and as a reserve bank.
 From the 17th century onwards, Amsterdam also became involved in the Atlantic slave trade. The city was a major destination port for Dutch slave ships participating in the triangular trade, which lasted until the United Netherlands abolished the Netherlands' involvement in the trade in 1814 at the request of the British government. Amsterdam was also a member of the Society of Suriname, an organisation founded to oversee the management of the Dutch colony of Surinam, which was economically dependent on slave plantations. On 1 July 2021, the mayor of Amsterdam, Femke Halsema, apologised for the city's involvement in the slave trade.[54][55]
 Amsterdam's prosperity declined during the 18th and early 19th centuries. The wars of the Dutch Republic with England (latterly, Great Britain) and France took their toll on the city. During the Napoleonic Wars, Amsterdam's significance reached its lowest point, with Holland being absorbed into the French Empire. However, the later establishment of the United Kingdom of the Netherlands in 1815 marked a turning point.
 The end of the 19th century is sometimes called Amsterdam's second Golden Age.[56] New museums, a railway station, and the Concertgebouw were built; At the same time, the Industrial Revolution reached the city. The Amsterdam–Rhine Canal was dug to give Amsterdam a direct connection to the Rhine, and the North Sea Canal was dug to give the port a shorter connection to the North Sea. Both projects dramatically improved commerce with the rest of Europe and the world. In 1906, Joseph Conrad gave a brief description of Amsterdam as seen from the seaside, in The Mirror of the Sea.
 Shortly before the First World War, the city started to expand again, and new suburbs were built. Even though the Netherlands remained neutral in this war, Amsterdam suffered a food shortage, and heating fuel became scarce. The shortages sparked riots in which several people were killed. These riots are known as the Aardappeloproer (Potato rebellion). People started looting stores and warehouses in order to get supplies, mainly food.[57]
 On 1 January 1921, after a flood in 1916, the depleted municipalities of Durgerdam, Holysloot, Zunderdorp and Schellingwoude, all lying north of Amsterdam, were, at their own request, annexed to the city.[58][59] Between the wars, the city continued to expand, most notably to the west of the Jordaan district in the Frederik Hendrikbuurt and surrounding neighbourhoods.
 Nazi Germany invaded the Netherlands on 10 May 1940 and took control of the country. Some Amsterdam citizens sheltered Jews, thereby exposing themselves and their families to a high risk of being imprisoned or sent to concentration camps. More than 100,000 Dutch Jews were deported to Nazi concentration camps, of whom some 60,000 lived in Amsterdam. In response, the Dutch Communist Party organized the February strike attended by 300,000 people to protest against the raids. The most famous deportee was the young Jewish girl Anne Frank, who died in the Bergen-Belsen concentration camp.[60] At the end of the Second World War, communication with the rest of the country broke down, and food and fuel became scarce. Many citizens traveled to the countryside to forage. Dogs, cats, raw sugar beets, and tulip bulbs—cooked to a pulp—were consumed to stay alive.[61] Many trees in Amsterdam were cut down for fuel, and wood was taken from the houses, apartments and other buildings of deported Jews. The city was finally liberated by Canadian forces on 5 May 1945, shortly before the end of the war in Europe.
 Many new suburbs, such as Osdorp, Slotervaart, Slotermeer and Geuzenveld, were built in the years after the Second World War.[62] These suburbs contained many public parks and wide-open spaces, and the new buildings provided improved housing conditions with larger and brighter rooms, gardens, and balconies. Because of the war and other events of the 20th century, almost the entire city centre had fallen into disrepair. As society was changing,[clarification needed] politicians and other influential figures made plans to redesign large parts of it. There was an increasing demand for office buildings, and also for new roads, as the automobile became available to most people.[63] A metro started operating in 1977 between the new suburb of Bijlmermeer in the city's Zuidoost (southeast) exclave and the centre of Amsterdam. Further plans were to build a new highway above the metro to connect Amsterdam Centraal and the city centre with other parts of the city.
 The required large-scale demolitions began in Amsterdam's former Jewish neighborhood. Smaller streets, such as the Jodenbreestraat and Weesperstraat, were widened and almost all houses and buildings were demolished. At the peak of the demolition, the Nieuwmarktrellen (Nieuwmarkt Riots) broke out;[64] the rioters expressed their fury about the demolition caused by the restructuring of the city.
 As a result, the demolition was stopped and the highway into the city's centre was never fully built; only the metro was completed. Only a few streets remained widened. The new city hall was built on the almost completely demolished Waterlooplein. Meanwhile, large private organizations, such as Stadsherstel Amsterdam, were founded to restore the entire city centre. Although the success of this struggle is visible today, efforts for further restoration are still ongoing.[63] The entire city centre has reattained its former splendour and, as a whole, is now a protected area. Many of its buildings have become monuments, and in July 2010 the Grachtengordel (the three concentric canals: Herengracht, Keizersgracht, and Prinsengracht) was added to the UNESCO World Heritage List.[65]
 In the 21st century, the Amsterdam city centre has attracted large numbers of tourists: between 2012 and 2015, the annual number of visitors rose from 10 to 17 million. Real estate prices have surged, and local shops are making way for tourist-oriented ones, making the centre unaffordable for the city's inhabitants.[69] These developments have evoked comparisons with Venice, a city thought to be overwhelmed by the tourist influx.[70]
 Construction of a new metro line connecting the part of the city north of the IJ to its southern part was started in 2003. The project was controversial because its cost had exceeded its budget by a factor of three by 2008,[71] because of fears of damage to buildings in the centre, and because construction had to be halted and restarted multiple times.[72] The new metro line was completed in 2018.[73]
 Since 2014, renewed focus has been given to urban regeneration and renewal, especially in areas directly bordering the city centre, such as Frederik Hendrikbuurt. This urban renewal and expansion of the traditional centre of the city—with the construction on artificial islands of the new eastern IJburg neighbourhood—is part of the Structural Vision Amsterdam 2040 initiative.[74][75]
 Amsterdam is located in the Western Netherlands, in the province of North Holland, the capital of which is not Amsterdam, but rather Haarlem. The river Amstel ends in the city centre and connects to a large number of canals that eventually terminate in the IJ. Amsterdam's elevation is about −2 m (−6.6 ft) below sea level.[76] The surrounding land is flat as it is formed of large polders. An artificial forest, Amsterdamse Bos, is in the southwest. Amsterdam is connected to the North Sea through the long North Sea Canal.
 Amsterdam is intensely urbanised, as is the Amsterdam metropolitan area surrounding the city. Comprising 219.4 km2 (84.7 sq mi) of land, the city proper has 4,457 inhabitants per km2 and 2,275 houses per km2.[77] Parks and nature reserves make up 12% of Amsterdam's land area.[78]
 Amsterdam has more than 100 km (60 mi) of canals, most of which are navigable by boat. The city's three main canals are the Prinsengracht, the Herengracht and the Keizersgracht.
 In the Middle Ages, Amsterdam was surrounded by a moat, called the Singel, which now forms the innermost ring in the city, and gives the city centre a horseshoe shape. The city is also served by a seaport. It has been compared with Venice, due to its division into about 90 islands, which are linked by more than 1,200 bridges.[79]
 Amsterdam has an oceanic climate (Köppen: Cfb)[80] strongly influenced by its proximity to the North Sea to the west, with prevailing westerly winds.
 Amsterdam, as well as most of the North Holland province, lies in USDA Hardiness zone 8b. Frosts mainly occur during spells of easterly or northeasterly winds from the inner European continent. Even then, because Amsterdam is surrounded on three sides by large bodies of water, as well as having a significant heat-island effect, nights rarely fall below −5 °C (23 °F), while it could easily be −12 °C (10 °F) in Hilversum, 25 km (16 mi) southeast.
 Summers are moderately warm with a number of hot and humid days with occasional rain every month. The average daily high in August is 22.1 °C (72 °F), and 30 °C (86 °F) or higher is only measured on average on 2.5 days, placing Amsterdam in AHS Heat Zone 2. The record extremes range from −19.7 °C (−3.5 °F) to 36.3 °C (97.3 °F).[81][82][unreliable source?] 
Days with more than 1 mm (0.04 in) of precipitation are common, on average 133 days per year.
 Amsterdam's average annual precipitation is 838 mm (33 in).[83] A large part of this precipitation falls as light rain or brief showers. Cloudy and damp days are common during the cooler months of October through March.
 In 1300, Amsterdam's population was around 1,000 people.[88] While many towns in Holland experienced population decline during the 15th and 16th centuries, Amsterdam's population grew,[89] mainly due to the rise of the profitable Baltic maritime trade especially in grain after the Burgundian victory in the Dutch–Hanseatic War in 1441.[90] The population of Amsterdam was only modest compared to the towns and cities of Flanders and Brabant, which comprised the most urbanized area of the Low Countries.[91]
 This changed when, during the Dutch Revolt, many people from the Southern Netherlands fled to the North, especially after Antwerp fell to Spanish forces in 1585. Jews from Spain, Portugal and Eastern Europe similarly settled in Amsterdam, as did Germans and Scandinavians.[89] In thirty years, Amsterdam's population more than doubled between 1585 and 1610.[92] By 1600, its population was around 50,000.[88] During the 1660s, Amsterdam's population reached 200,000.[93] The city's growth levelled off and the population stabilized around 240,000 for most of the 18th century.[94]
 In 1750, Amsterdam was the fourth largest city in Western Europe, behind London (676,000), Paris (560,000) and Naples (324,000).[95] This was all the more remarkable as Amsterdam was neither the capital city nor the seat of government of the Dutch Republic, which itself was a much smaller state than Great Britain, France or the Ottoman Empire. In contrast to those other metropolises, Amsterdam was also surrounded by large towns such as Leiden (about 67,000), Rotterdam (45,000), Haarlem (38,000) and Utrecht (30,000).[96]
 The city's population declined in the early 19th century,[97] dipping under 200,000 in 1820.[98] By the second half of the 19th century, industrialization spurred renewed growth.[99] Amsterdam's population hit an all-time high of 872,000 in 1959,[100] before declining in the following decades due to government-sponsored suburbanisation to so-called groeikernen (growth centres) such as Purmerend and Almere.[101][102][103] Between 1970 and 1980, Amsterdam experienced a sharp population decline, peaking at a net loss of 25,000 people in 1973.[103] By 1985 the city had only 675,570 residents.[104] This was soon followed by reurbanization and gentrification,[105][103] leading to renewed population growth in the 2010s. Also in the 2010s, much of Amsterdam's population growth was due to immigration to the city.[106]
 In the 16th and 17th century, non-Dutch immigrants to Amsterdam were mostly Protestant Huguenots and Flemings, Sephardic Jews, and Westphalians. Huguenots came after the Edict of Fontainebleau in 1685, while the Flemish Protestants came during the Eighty Years' War against Catholic Spain. The Westphalians came to Amsterdam mostly for economic reasons; their influx continued through the 18th and 19th centuries.[citation needed] Before the Second World War, 10% of the city population was Jewish. Just twenty percent of them survived the Holocaust.[107]
 The first mass immigration in the 20th century was by people from Indonesia, who came to Amsterdam after the independence of the Dutch East Indies in the 1940s and 1950s. In the 1960s guest workers from Turkey, Morocco, Italy, and Spain emigrated to Amsterdam. After the independence of Suriname in 1975, a large wave of Surinamese settled in Amsterdam, mostly in the Bijlmer area. Other immigrants, including refugees asylum seekers and undocumented immigrants, came from Europe, the Americas, Asia and Africa. In the 1970s and 1980s, many 'old' Amsterdammers moved to 'new' cities like Almere and Purmerend, prompted by the third Land-use planning bill of the Dutch Government. This bill promoted suburbanization and arranged for new developments in so-called "groeikernen", literally cores of growth. Young professionals and artists moved into neighborhoods De Pijp and the Jordaan abandoned by these Amsterdammers. The non-Western immigrants settled mostly in the social housing projects in Amsterdam-West and the Bijlmer. Today, people of non-Western origin make up approximately one-fifth of the population of Amsterdam, and more than 30% of the city's children.[108][109][110] A slight majority of the residents of Amsterdam have at least one parent who was born outside the country. However, a much larger majority has at least one parent who was born inside the country (intercultural marriages are common in the city). Thus, while the demographics are changing, the city still has an ethnic Dutch majority. Only one in three inhabitants under 15 is an autochthon, or a person who has two parents of Dutch origin.[111] Segregation along ethnic lines is clearly visible, with people of non-Western origin, considered a separate group by Statistics Netherlands, concentrating in specific neighborhoods especially in Nieuw-West, Zeeburg, Bijlmer and in certain areas of Amsterdam-Noord.[112][113]
 In 2000, Christians formed the largest religious group in the city (28% of the population). The next largest religion was Islam (8%), most of whose followers were Sunni.[115][116] In 2015, Christians formed the largest religious group in the city (28% of the population). The next largest religion was Islam (7.1%), most of whose followers were Sunni.[114]
 Religion in Amsterdam (2015)[117]
 In 1578, the largely Catholic city of Amsterdam joined the revolt against Spanish rule,[118] late in comparison to other major northern Dutch cities.[119] Catholic priests were driven out of the city.[118] Following the Dutch takeover, all churches were converted to Protestant worship.[120] Calvinism was declared the main religion.[119] It was forbidden to openly profess Roman Catholicism and the Catholic hierarchy was prohibited until the mid-19th century. This led to the establishment of clandestine churches, covert religious buildings hidden in pre-existing buildings. Catholics, some Jews and dissenting Protestants worshipped in such buildings.[121] A large influx of foreigners of many religions came to 17th-century Amsterdam, in particular Sefardic Jews from Spain and Portugal,[122][123] Huguenots from France,[124] Lutherans, Mennonites, as well as Protestants from across the Netherlands.[125] This led to the establishment of many non-Dutch-speaking churches.[citation needed] In 1603, the Jewish received permission to practice their religion in the city. In 1639, the first synagogue was consecrated.[126] The Jews came to call the town 'Jerusalem of the West'.[127]
 As they became established in the city, other Christian denominations used converted Catholic chapels to conduct their own services. The oldest English-language church congregation in the world outside the United Kingdom is found at the Begijnhof.[citation needed][128] Regular services there are still offered in English under the auspices of the Church of Scotland.[129] Being Calvinists, the Huguenots soon integrated into the Dutch Reformed Church, though often retaining their own congregations. Some, commonly referred to by the moniker 'Walloon', are recognizable today as they offer occasional services in French.[citation needed]
 In the second half of the 17th century, Amsterdam experienced an influx of Ashkenazim, Jews from Central and Eastern Europe. Jews often fled the pogroms in those areas. The first Ashkenazis who arrived in Amsterdam were refugees from the Khmelnytsky Uprising occurring in Ukraine and the Thirty Years' War, which devastated much of Central Europe. They not only founded their own synagogues, but had a strong influence on the 'Amsterdam dialect' adding a large Yiddish local vocabulary.[130] Despite an absence of an official Jewish ghetto, most Jews preferred to live in the eastern part, which used to be the centre of medieval Amsterdam. The main street of this Jewish neighbourhood was Jodenbreestraat. The neighbourhood comprised the Waterlooplein and the Nieuwmarkt.[130][131] Buildings in this neighbourhood fell into disrepair after the Second World War[132] a large section of the neighbourhood was demolished during the construction of the metro system. This led to riots, and as a result the original plans for large-scale reconstruction were abandoned by the government.[133][134] The neighbourhood was rebuilt with smaller-scale residence buildings on the basis of its original layout.[135]
 Catholic churches in Amsterdam have been constructed since the restoration of the episcopal hierarchy in 1853.[136] One of the principal architects behind the city's Catholic churches, Cuypers, was also responsible for the Amsterdam Centraal station and the Rijksmuseum.[137][138]
 In 1924, the Catholic Church hosted the International Eucharistic Congress in Amsterdam;[139] numerous Catholic prelates visited the city, where festivities were held in churches and stadiums.[140] Catholic processions on the public streets, however, were still forbidden under law at the time.[141] Only in the 20th century was Amsterdam's relation to Catholicism normalised,[142] but despite its far larger population size, the episcopal see of the city was placed in the provincial town of Haarlem.[143]
 Historically, Amsterdam has been predominantly Christian. In 1900 Christians formed the largest religious group in the city (70% of the population), Dutch Reformed Church formed 45% of the city population, and the Catholic Church formed 25% of the city population.[144] In recent times, religious demographics in Amsterdam have been changed by immigration from former colonies. Hinduism has been introduced from the Hindu diaspora from Suriname[145] and several distinct branches of Islam have been brought from various parts of the world.[146] Islam is now the largest non-Christian religion in Amsterdam.[117] The large community of Ghanaian immigrants have established African churches,[147] often in parking garages in the Bijlmer area.[148]
 Amsterdam experienced an influx of religions and cultures after the Second World War. With 180 different nationalities,[149] Amsterdam is home to one of the widest varieties of nationalities of any city in the world.[150] The proportion of the population of immigrant origin in the city proper is about 50%[151] and 88% of the population are Dutch citizens.[152]
 Amsterdam has been one of the municipalities in the Netherlands which provided immigrants with extensive and free Dutch-language courses, which have benefited many immigrants.[153]
 Inhabitants by origin
 Amsterdam fans out south from the Amsterdam Centraal station and Damrak, the main street off the station. The oldest area of the town is known as De Wallen (English: "The Quays"). It lies to the east of Damrak and contains the city's famous red-light district. To the south of De Wallen is the old Jewish quarter of Waterlooplein.
 The medieval and colonial age canals of Amsterdam, known as grachten, embraces the heart of the city where homes have interesting gables. Beyond the Grachtengordel are the former working-class areas of Jordaan and de Pijp. The Museumplein with the city's major museums, the Vondelpark, a 19th-century park named after the Dutch writer Joost van den Vondel, as well as the Plantage neighbourhood, with the zoo, are also located outside the Grachtengordel.
 Several parts of the city and the surrounding urban area are polders. This can be recognised by the suffix -meer which means lake, as in Aalsmeer, Bijlmermeer, Haarlemmermeer and Watergraafsmeer.
 The Amsterdam canal system is the result of conscious city planning.[155] In the early 17th century, when immigration was at a peak, a comprehensive plan was developed that was based on four concentric half-circles of canals with their ends emerging at the IJ bay. Known as the Grachtengordel, three of the canals were mostly for residential development: the Herengracht (where "Heren" refers to Heren Regeerders van de stad Amsterdam, ruling lords of Amsterdam, whilst gracht means canal, so that the name can be roughly translated as "Canal of the Lords"), Keizersgracht (Emperor's Canal) and Prinsengracht (Prince's Canal).[156] The fourth and outermost canal is the Singelgracht, which is often not mentioned on maps because it is a collective name for all canals in the outer ring. The Singelgracht should not be confused with the oldest and innermost canal, the Singel.
 The canals served for defense, water management and transport. The defenses took the form of a moat and earthen dikes, with gates at transit points, but otherwise no masonry superstructures.[157] The original plans have been lost, so historians, such as Ed Taverne, need to speculate on the original intentions: it is thought that the considerations of the layout were purely practical and defensive rather than ornamental.[158]
 Construction started in 1613 and proceeded from west to east, across the breadth of the layout, like a gigantic windshield wiper as the historian Geert Mak calls it – and not from the centre outwards, as a popular myth has it. The canal construction in the southern sector was completed by 1656. Subsequently, the construction of residential buildings proceeded slowly. The eastern part of the concentric canal plan, covering the area between the Amstel river and the IJ bay, has never been implemented. In the following centuries, the land was used for parks, senior citizens' homes, theatres, other public facilities, and waterways without much planning.[159] Over the years, several canals have been filled in, becoming streets or squares, such as the Nieuwezijds Voorburgwal and the Spui.[160]
 After the development of Amsterdam's canals in the 17th century, the city did not grow beyond its borders for two centuries. During the 19th century, Samuel Sarphati devised a plan based on the grandeur of Paris and London at that time. The plan envisaged the construction of new houses, public buildings and streets just outside the Grachtengordel. The main aim of the plan, however, was to improve public health. Although the plan did not expand the city, it did produce some of the largest public buildings to date, like the Paleis voor Volksvlijt.[161][162][163]
 Following Sarphati, civil engineers Jacobus van Niftrik and Jan Kalff designed an entire ring of 19th-century neighbourhoods surrounding the city's centre, with the city preserving the ownership of all land outside the 17th-century limit, thus firmly controlling development.[164] Most of these neighbourhoods became home to the working class.[165]
 In response to overcrowding, two plans were designed at the beginning of the 20th century which were very different from anything Amsterdam had ever seen before: Plan Zuid (designed by the architect Berlage) and West. These plans involved the development of new neighbourhoods consisting of housing blocks for all social classes.[166][167]
 After the Second World War, large new neighbourhoods were built in the western, southeastern, and northern parts of the city. These new neighbourhoods were built to relieve the city's shortage of living space and give people affordable houses with modern conveniences. The neighbourhoods consisted mainly of large housing blocks located among green spaces, connected to wide roads, making the neighbourhoods easily accessible by motor car. The western suburbs which were built in that period are collectively called the Westelijke Tuinsteden. The area to the southeast of the city built during the same period is known as the Bijlmer.[168][169]
 Amsterdam has a rich architectural history. The oldest building in Amsterdam is the Oude Kerk (English: Old Church), at the heart of the Wallen, consecrated in 1306.[170] The oldest wooden building is Het Houten Huys[171] at the Begijnhof. It was constructed around 1425 and is one of only two existing wooden buildings. It is also one of the few examples of Gothic architecture in Amsterdam. The oldest stone building of the Netherlands, The Moriaan is built in 's-Hertogenbosch.
 In the 16th century, wooden buildings were razed and replaced with brick ones. During this period, many buildings were constructed in the architectural style of the Renaissance. Buildings of this period are very recognisable with their stepped gable façades, which is the common Dutch Renaissance style. Amsterdam quickly developed its own Renaissance architecture. These buildings were built according to the principles of the architect Hendrick de Keyser.[172] One of the most striking buildings designed by Hendrick de Keyser is the Westerkerk. In the 17th century baroque architecture became very popular, as it was elsewhere in Europe. This roughly coincided with Amsterdam's Golden Age. The leading architects of this style in Amsterdam were Jacob van Campen, Philips Vingboons and Daniel Stalpaert.[173]
 Philip Vingboons designed splendid merchants' houses throughout the city. A famous building in baroque style in Amsterdam is the Royal Palace on Dam Square. Throughout the 18th century, Amsterdam was heavily influenced by French culture. This is reflected in the architecture of that period. Around 1815, architects broke with the baroque style and started building in different neo-styles.[174] Most Gothic style buildings date from that era and are therefore said to be built in a neo-gothic style. At the end of the 19th century, the Jugendstil or Art Nouveau style became popular and many new buildings were constructed in this architectural style. Since Amsterdam expanded rapidly during this period, new buildings adjacent to the city centre were also built in this style. The houses in the vicinity of the Museum Square in Amsterdam Oud-Zuid are an example of Jugendstil. The last style that was popular in Amsterdam before the modern era was Art Deco. Amsterdam had its own version of the style, which was called the Amsterdamse School. Whole districts were built in this style, such as the Rivierenbuurt.[175] A notable feature of the façades of buildings designed in Amsterdamse School is that they are highly decorated and ornate, with oddly shaped windows and doors.
 The old city centre is the focal point of all the architectural styles before the end of the 19th century. Jugendstil and Georgian are mostly found outside the city centre in the neighbourhoods built in the early 20th century, although there are also some striking examples of these styles in the city centre. Most historic buildings in the city centre and nearby are houses, such as the famous merchants' houses lining the canals.
 Amsterdam has many parks, open spaces, and squares throughout the city. The Vondelpark, the largest park in the city, is located in the Oud-Zuid neighbourhood and is named after the 17th-century Amsterdam author Joost van den Vondel. Yearly, the park has around 10 million visitors. In the park is an open-air theatre, a playground and several horeca facilities. In the Zuid borough, is the Beatrixpark, named after Queen Beatrix. Between Amsterdam and Amstelveen is the Amsterdamse Bos ("Amsterdam Forest"), the largest recreational area in Amsterdam. Annually, almost 4.5 million people visit the park, which has a size of 1,000 hectares (2,500 acres) and is approximately three times the size of Central Park.[176] The Amstelpark in the Zuid borough houses the Rieker windmill, which dates to 1636. Other parks include the Sarphatipark in the De Pijp neighbourhood, the Oosterpark in the Oost borough and the Westerpark in the Westerpark neighbourhood. The city has three beaches: Nemo Beach, Citybeach "Het stenen hoofd" (Silodam) and Blijburg, all located in the Centrum borough.
 The city has many open squares (plein in Dutch). The namesake of the city as the site of the original dam, Dam Square, is the main city square and has the Royal Palace and National Monument. Museumplein hosts various museums, including the Rijksmuseum, Van Gogh Museum, and Stedelijk Museum. Other squares include Rembrandtplein, Muntplein, Nieuwmarkt, Leidseplein, Spui and Waterlooplein. Also, near to Amsterdam is the Nekkeveld estate conservation project.
 Amsterdam is the financial and business capital of the Netherlands.[177]
According to the 2007 European Cities Monitor (ECM) – an annual location survey of Europe's leading companies carried out by global real estate consultant Cushman & Wakefield – Amsterdam is one of the top European cities in which to locate an international business, ranking fifth in the survey.[178] with the survey determining London, Paris, Frankfurt and Barcelona as the four European cities surpassing Amsterdam in this regard.
 A substantial number of large corporations and banks' headquarters are located in the Amsterdam area, including: AkzoNobel, Heineken International, ING Group, ABN AMRO, TomTom, Delta Lloyd Group, Booking.com and Philips. Although many small offices remain along the historic canals, centrally based companies have increasingly relocated outside Amsterdam's city centre. Consequently, the Zuidas (English: South Axis) has become the new financial and legal hub of Amsterdam,[179] with the country's five largest law firms and several subsidiaries of large consulting firms, such as Boston Consulting Group and Accenture, as well as the World Trade Centre (Amsterdam) located in the Zuidas district. In addition to the Zuidas, there are three smaller financial districts in Amsterdam:
 The adjoining municipality of Amstelveen is the location of KPMG International's global headquarters. Other non-Dutch companies have chosen to settle in communities surrounding Amsterdam since they allow freehold property ownership, whereas Amsterdam retains ground rent.
 The Amsterdam Stock Exchange (AEX), now part of Euronext, is the world's oldest stock exchange and, due to Brexit, has overtaken LSE as the largest bourse in Europe.[183] It is near Dam Square in the city centre.
 The Port of Amsterdam is the fourth-largest port in Europe, the 38th largest port in the world and the second-largest port in the Netherlands by metric tons of cargo. In 2014, the Port of Amsterdam had a cargo throughput of 97,4 million tons of cargo, which was mostly bulk cargo. Amsterdam has the biggest cruise port in the Netherlands with more than 150 cruise ships every year. In 2019, the new lock in IJmuiden opened; since then, the port has been able to grow to 125 million tonnes in capacity.
 Amsterdam is one of the most popular tourist destinations in Europe, receiving more than 5.34 million international visitors annually; this is excluding the 16 million day-trippers visiting the city every year.[184] The number of visitors has been growing steadily over the past decade. This can be attributed to an increasing number of European visitors. Two-thirds of the hotels are located in the city's centre.[185] Hotels with four or five stars contribute 42% of the total beds available and 41% of the overnight stays in Amsterdam. The room occupation rate was 85% in 2017, up from 78% in 2006.[186][187] The majority of tourists (74%) originate from Europe. The largest group of non-European visitors come from the United States, accounting for 14% of the total.[187] Certain years have a theme in Amsterdam to attract extra tourists. For example, the year 2006 was designated "Rembrandt 400", to celebrate the 400th birthday of Rembrandt van Rijn. Some hotels offer special arrangements or activities during these years. The average number of guests per year staying at the four campsites around the city range from 12,000 to 65,000.[187]
 In 2023, the city began running a campaign to dissuade British men between the ages of 18 and 35 from coming to the city as tourists. The ad shows young men being handcuffed by police and is part of a new campaign to clean up the city's reputation.[188] On 25 May 2023, in a bid to crackdown on wild tourist behaviour, the city banned weed smoking in public areas in and around the red light district.[189]
 
 De Wallen, also known as Walletjes or Rosse Buurt, is a designated area for legalised prostitution and is Amsterdam's largest and best-known red-light district. This neighbourhood has become a famous attraction for tourists. It consists of a network of canals, streets, and alleys containing several hundred small, one-room apartments rented by sex workers who offer their services from behind a window or glass door, typically illuminated with red lights. In recent years, the city government has been closing and repurposing the famous red-light district windows in an effort to clean up the area and reduce the amount of party and sex tourism.
 Shops in Amsterdam range from large high-end department stores such as De Bijenkorf founded in 1870 to small speciality shops. Amsterdam's high-end shops are found in the streets P.C. Hooftstraat[191] and Cornelis Schuytstraat, which are located in the vicinity of the Vondelpark. One of Amsterdam's busiest high streets is the narrow, medieval Kalverstraat in the heart of the city. Other shopping areas include the Negen Straatjes and Haarlemmerdijk and Haarlemmerstraat. Negen Straatjes are nine narrow streets within the Grachtengordel, the concentric canal system of Amsterdam. The Negen Straatjes differ from other shopping districts with the presence of a large diversity of privately owned shops. The Haarlemmerstraat and Haarlemmerdijk were voted best shopping street in the Netherlands in 2011. These streets have as the Negen Straatjes a large diversity of privately owned shops. However, as the Negen Straatjes are dominated by fashion stores, the Haarlemmerstraat and Haarlemmerdijk offer a wide variety of stores, just to name some specialities: candy and other food-related stores, lingerie, sneakers, wedding clothing, interior shops, books, Italian deli's, racing and mountain bikes, skatewear, etc.[original research?]
 The city also features a large number of open-air markets such as the Albert Cuyp Market, Westerstraat-markt, Ten Katemarkt, and Dappermarkt. Some of these markets are held daily, like the Albert Cuypmarkt and the Dappermarkt. Others, like the Westerstraatmarkt, are held every week.[original research?]
 Several fashion brands and designers are based in Amsterdam. Fashion designers include Iris van Herpen,[192] Mart Visser, Viktor & Rolf, Marlies Dekkers and Frans Molenaar. Fashion models like Yfke Sturm, Doutzen Kroes and Kim Noorda started their careers in Amsterdam. Amsterdam has its garment centre in the World Fashion Center. Fashion photographers Inez van Lamsweerde and Vinoodh Matadin were born in Amsterdam.[193]
 During the later part of the 16th century, Amsterdam's Rederijkerskamer (Chamber of rhetoric) organised contests between different Chambers in the reading of poetry and drama.[194][195] In 1637, Schouwburg, the first theatre in Amsterdam was built, opening on 3 January 1638.[196] The first ballet performances in the Netherlands were given in Schouwburg in 1642 with the Ballet of the Five Senses.[197][198] In the 18th century, French theatre became popular. While Amsterdam was under the influence of German music in the 19th century there were few national opera productions; the Hollandse Opera of Amsterdam was built in 1888 for the specific purpose of promoting Dutch opera.[199] In the 19th century, popular culture was centred on the Nes area in Amsterdam (mainly vaudeville and music-hall).[citation needed] An improved metronome was invented in 1812 by Dietrich Nikolaus Winkel.[200] The Rijksmuseum (1885) and Stedelijk Museum (1895) were built and opened.[201][202] In 1888, the Concertgebouworkest orchestra was established.[203] With the 20th century came cinema, radio and television.[citation needed] Though most studios are located in Hilversum and Aalsmeer, Amsterdam's influence on programming is very strong. Many people who work in the television industry live in Amsterdam. Also, the headquarters of the Dutch SBS Broadcasting Group is located in Amsterdam.[204]
 The most important museums of Amsterdam are located on the Museumplein (Museum Square), located at the southwestern side of the Rijksmuseum. It was created in the last quarter of the 19th century on the grounds of the former World's fair. The northeastern part of the square is bordered by the large Rijksmuseum. In front of the Rijksmuseum on the square itself is a long, rectangular pond. This is transformed into an ice rink in winter.[205] The northwestern part of the square is bordered by the Van Gogh Museum, House of Bols Cocktail & Genever Experience and Coster Diamonds. The southwestern border of the Museum Square is the Van Baerlestraat, which is a major thoroughfare in this part of Amsterdam. The Concertgebouw is located across this street from the square. To the southeast of the square are several large houses, one of which contains the American consulate. A parking garage can be found underneath the square, as well as a supermarket. The Museumplein is covered almost entirely with a lawn, except for the northeastern part of the square which is covered with gravel. The current appearance of the square was realised in 1999, when the square was remodelled. The square itself is the most prominent site in Amsterdam for festivals and outdoor concerts, especially in the summer. Plans were made in 2008 to remodel the square again because many inhabitants of Amsterdam are not happy with its current appearance.[206]
 The Rijksmuseum possesses the largest and most important collection of classical Dutch art.[207]
It opened in 1885. Its collection consists of nearly one million objects.[208] The artist most associated with Amsterdam is Rembrandt, whose work, and the work of his pupils, is displayed in the Rijksmuseum. Rembrandt's masterpiece The Night Watch is one of the top pieces of art of the museum. It also houses paintings from artists like Bartholomeus van der Helst, Johannes Vermeer, Frans Hals, Ferdinand Bol, Albert Cuyp, Jacob van Ruisdael and Paulus Potter. Aside from paintings, the collection consists of a large variety of decorative art. This ranges from Delftware to giant doll-houses from the 17th century. The architect of the gothic revival building was P.J.H. Cuypers. The museum underwent a 10-year, 375 million euro renovation starting in 2003. The full collection was reopened to the public on 13 April 2013 and the Rijksmuseum has remained the most visited museum in Amsterdam with 2.2 million visitors in 2016 and 2.16 million in 2017.[209]
 Van Gogh lived in Amsterdam for a short while and there is a museum dedicated to his work. The museum is housed in one of the few modern buildings in this area of Amsterdam. The building was designed by Gerrit Rietveld. This building is where the permanent collection is displayed. A new building was added to the museum in 1999. This building, known as the performance wing, was designed by Japanese architect Kisho Kurokawa. Its purpose is to house temporary exhibitions of the museum.[210][211] Some of Van Gogh's most famous paintings, like The Potato Eaters and Sunflowers, are in the collection.[212] The Van Gogh museum is the second most visited museum in Amsterdam, not far behind the Rijksmuseum in terms of the number of visits, being approximately 2.1 million in 2016,[213] for example.
 Next to the Van Gogh museum stands the Stedelijk Museum. This is Amsterdam's most important museum of modern art. The museum is as old as the square it borders and was opened in 1895. The permanent collection consists of works of art from artists like Piet Mondrian, Karel Appel, and Kazimir Malevich. After renovations lasting several years, the museum opened in September 2012 with a new composite extension that has been called 'The Bathtub' due to its resemblance to one.
 Amsterdam contains many other museums throughout the city. They range from small museums such as the Verzetsmuseum (Resistance Museum), the Anne Frank House, and the Rembrandt House Museum, to the very large, like the Tropenmuseum (Museum of the Tropics), Amsterdam Museum (formerly known as Amsterdam Historical Museum), H'ART Museum and the Joods Historisch Museum (Jewish Historical Museum). The modern-styled Nemo is dedicated to child-friendly science exhibitions.
 Amsterdam's musical culture includes a large collection of songs that treat the city nostalgically and lovingly. The 1949 song "Aan de Amsterdamse grachten" ("On the canals of Amsterdam") was performed and recorded by many artists, including John Kraaijkamp Sr.; the best-known version is probably that by Wim Sonneveld (1962). In the 1950s Johnny Jordaan rose to fame with "Geef mij maar Amsterdam" ("I prefer Amsterdam"), which praises the city above all others (explicitly Paris); Jordaan sang especially about his own neighbourhood, the Jordaan ("Bij ons in de Jordaan"). Colleagues and contemporaries of Johnny include Tante Leen and Manke Nelis. Another notable Amsterdam song is "Amsterdam" by Jacques Brel (1964).[214] A 2011 poll by Amsterdam newspaper Het Parool that Trio Bier's "Oude Wolf" was voted "Amsterdams lijflied".[215] Notable Amsterdam bands from the modern era include the Osdorp Posse and The Ex.
 AFAS Live (formerly known as the Heineken Music Hall) is a concert hall located near the Johan Cruyff Arena (known as the Amsterdam Arena until 2018). Its main purpose is to serve as a podium for pop concerts for big audiences. Many famous international artists have performed there. Two other notable venues, Paradiso and the Melkweg are located near the Leidseplein. Both focus on broad programming, ranging from indie rock to hip hop, R&B, and other popular genres. Other more subcultural music venues are OCCII, OT301, De Nieuwe Anita, Winston Kingdom, and Zaal 100. Jazz has a strong following in Amsterdam, with the Bimhuis being the premier venue. In 2012, Ziggo Dome was opened, also near Amsterdam Arena, a state-of-the-art indoor music arena.
 AFAS Live is also host to many electronic dance music festivals, alongside many other venues. Armin van Buuren and Tiesto, some of the world's leading Trance DJ's hail from the Netherlands and frequently perform in Amsterdam. Each year in October, the city hosts the Amsterdam Dance Event (ADE) which is one of the leading electronic music conferences and one of the biggest club festivals for electronic music in the world, attracting over 350,000 visitors each year.[216] Another popular dance festival is 5daysoff, which takes place in the venues Paradiso and Melkweg. In the summertime, there are several big outdoor dance parties in or nearby Amsterdam, such as Awakenings, Dance Valley, Mystery Land, Loveland, A Day at the Park, Welcome to the Future, and Valtifest.
 Amsterdam has a world-class symphony orchestra, the Royal Concertgebouw Orchestra. Their home is the Concertgebouw, which is across the Van Baerlestraat from the Museum Square. It is considered by critics to be a concert hall with some of the best acoustics in the world. The building contains three halls, Grote Zaal, Kleine Zaal, and Spiegelzaal. Some nine hundred concerts and other events per year take place in the Concertgebouw, for a public of over 700,000, making it one of the most-visited concert halls in the world.[217] The opera house of Amsterdam is located adjacent to the city hall. Therefore, the two buildings combined are often called the Stopera, (a word originally coined by protesters against it very construction: Stop the Opera[-house]). This huge modern complex, opened in 1986, lies in the former Jewish neighbourhood at Waterlooplein next to the river Amstel. The Stopera is the home base of Dutch National Opera, Dutch National Ballet and the Holland Symfonia. Muziekgebouw aan 't IJ is a concert hall, which is located in the IJ near the central station. Its concerts perform mostly modern classical music. Located adjacent to it, is the Bimhuis, a concert hall for improvised and Jazz music.
 Amsterdam has three main theatre buildings.
 The Stadsschouwburg at the Leidseplein is the home base of Toneelgroep Amsterdam. The current building dates from 1894. Most plays are performed in the Grote Zaal (Great Hall). The normal program of events encompasses all sorts of theatrical forms. In 2009, the new hall of the Stadsschouwburg Amsterdam, Toneelgroep Amsterdam and Melkweg opened, and the renovation of the front end of the theatre was ready.
 The Dutch National Opera and Ballet (formerly known as Het Muziektheater), dating from 1986, is the principal opera house and home to Dutch National Opera and Dutch National Ballet. Royal Theatre Carré was built as a permanent circus theatre in 1887 and is currently mainly used for musicals, cabaret performances, and pop concerts.
 The recently re-opened DeLaMar Theater houses more commercial plays and musicals. A new theatre has also moved into the Amsterdam scene in 2014, joining other established venues: Theater Amsterdam is located in the west part of Amsterdam, on the Danzigerkade. It is housed in a modern building with a panoramic view over the harbour. The theatre is the first-ever purpose-built venue to showcase a single play entitled ANNE, the play based on Anne Frank's life.
 On the east side of town, there is a small theatre in a converted bathhouse, the Badhuistheater. The theatre often has English programming.
 The Netherlands has a tradition of cabaret or kleinkunst, which combines music, storytelling, commentary, theatre and comedy. Cabaret dates back to the 1930s and artists like Wim Kan, Wim Sonneveld and Toon Hermans were pioneers of this form of art in the Netherlands. In Amsterdam is the Kleinkunstacademie (English: Cabaret Academy) and Nederlied Kleinkunstkoor (English: Cabaret Choir). Contemporary popular artists are Youp van 't Hek, Freek de Jonge, Herman Finkers, Hans Teeuwen, Theo Maassen, Herman van Veen, Najib Amhali, Raoul Heertje, Jörgen Raymann, Brigitte Kaandorp and Comedytrain. The English spoken comedy scene was established with the founding of Boom Chicago in 1993. They have their own theatre at Leidseplein.
 Amsterdam is famous for its vibrant and diverse nightlife. Amsterdam has many cafés (bars). They range from large and modern to small and cosy. The typical Bruine Kroeg (brown café) breathe a more old fashioned atmosphere with dimmed lights, candles, and somewhat older clientele. These brown cafés mostly offer a wide range of local and international artisanal beers. Most cafés have terraces in summertime. A common sight on the Leidseplein during summer is a square full of terraces packed with people drinking beer or wine. Many restaurants can be found in Amsterdam as well. Since Amsterdam is a multicultural city, a lot of different ethnic restaurants can be found. Restaurants range from being rather luxurious and expensive to being ordinary and affordable. Amsterdam also possesses many discothèques. The two main nightlife areas for tourists are the Leidseplein and the Rembrandtplein. The Paradiso, Melkweg and Sugar Factory are cultural centres, which turn into discothèques on some nights. Examples of discothèques near the Rembrandtplein are the Escape, Air, John Doe and Club Abe. Also noteworthy are Panama, Hotel Arena (East), TrouwAmsterdam and Studio 80. In recent years '24-hour' clubs opened their doors, most notably Radion De School, Shelter and Marktkantine. Bimhuis located near the Central Station, with its rich programming hosting the best in the field is considered one of the best jazz clubs in the world. The Reguliersdwarsstraat is the main street for the LGBT community and nightlife.
 In 2008, there were 140 festivals and events in Amsterdam.[218] During the same year, Amsterdam was designated as the World Book Capital for one year by UNESCO.[219]
 Famous festivals and events in Amsterdam include: Koningsdag (which was named Koninginnedag until the crowning of King Willem-Alexander in 2013) (King's Day – Queen's Day); the Holland Festival for the performing arts; the yearly Prinsengrachtconcert (classical concerto on the Prinsen canal) in August; the 'Stille Omgang' (a silent Roman Catholic evening procession held every March); Amsterdam Gay Pride; The Cannabis Cup; and the Uitmarkt. On Koningsdag—that is held each year on 27 April—hundreds of thousands of people travel to Amsterdam to celebrate with the city's residents. The entire city becomes overcrowded with people buying products from the freemarket, or visiting one of the many music concerts.
 The yearly Holland Festival attracts international artists and visitors from all over Europe. Amsterdam Gay Pride is a yearly local LGBT parade of boats in Amsterdam's canals, held on the first Saturday in August.[220] The annual Uitmarkt is a three-day cultural event at the start of the cultural season in late August. It offers previews of many different artists, such as musicians and poets, who perform on podia.[221]
 Amsterdam is home of the Eredivisie football club AFC Ajax. The stadium Johan Cruyff Arena is the home of Ajax. It is located in the south-east of the city next to the new Amsterdam Bijlmer ArenA railway station. Before moving to their current location in 1996, Ajax played their regular matches in the now demolished De Meer Stadion in the eastern part of the city[222] or in the Olympic Stadium.
In 1928, Amsterdam hosted the Summer Olympics. The Olympic Stadium built for the occasion has been completely restored and is now used for cultural and sporting events, such as the Amsterdam Marathon.[223] In 1920, Amsterdam assisted in hosting some of the sailing events for the Summer Olympics held in neighbouring Antwerp, Belgium by hosting events at Buiten IJ.
 The city holds the Dam to Dam Run, a 16 km (10 mi) race from Amsterdam to Zaandam, as well as the Amsterdam Marathon. The ice hockey team Amstel Tijgers play in the Jaap Eden ice rink. The team competes in the Dutch ice hockey premier league. Speed skating championships have been held on the 400-meter lane of this ice rink.
 Amsterdam holds two American football franchises: the Amsterdam Crusaders and the Amsterdam Panthers. The Amsterdam Pirates baseball team competes in the Dutch Major League. There are three field hockey teams: Amsterdam, Pinoké and Hurley, who play their matches around the Wagener Stadium in the nearby city of Amstelveen. The basketball team MyGuide Amsterdam competes in the Dutch premier division and play their games in the Sporthallen Zuid.[224]
 There is one rugby club in Amsterdam, which also hosts sports training classes such as RTC (Rugby Talenten Centrum or Rugby Talent Centre) and the National Rugby stadium.
 Since 1999, the city of Amsterdam honours the best sportsmen and women at the Amsterdam Sports Awards. Boxer Raymond Joval and field hockey midfielder Carole Thate were the first to receive the awards, in 1999.
 Amsterdam hosted the World Gymnaestrada in 1991 and will do so again in 2023.[225]
 The city of Amsterdam is a municipality under the Dutch Municipalities Act. It is governed by a directly elected municipal council, a municipal executive board and a mayor. Since 1981, the municipality of Amsterdam has gradually been divided into semi-autonomous boroughs, called stadsdelen or 'districts'. Over time, a total of 15 boroughs were created. In May 2010, under a major reform, the number of Amsterdam boroughs was reduced to eight: Amsterdam-Centrum covering the city centre including the canal belt, Amsterdam-Noord consisting of the neighbourhoods north of the IJ lake, Amsterdam-Oost in the east, Amsterdam-Zuid in the south, Amsterdam-West in the west, Amsterdam Nieuw-West in the far west, Amsterdam Zuidoost in the southeast, and Westpoort covering the Port of Amsterdam area.[226]
 As with all Dutch municipalities, Amsterdam is governed by a directly elected municipal council, a municipal executive board and a government appointed[227] mayor (burgemeester). The mayor is a member of the municipal executive board, but also has individual responsibilities in maintaining public order. On 27 June 2018, Femke Halsema (former member of House of Representatives for GroenLinks from 1998 to 2011) was appointed as the first woman to be Mayor of Amsterdam by the King's Commissioner of North Holland for a six-year term after being nominated by the Amsterdam municipal council and began serving a six-year term on 12 July 2018. She replaces Eberhard van der Laan (Labour Party) who was the Mayor of Amsterdam from 2010 until his death in October 2017. After the 2014 municipal council elections, a governing majority of D66, VVD and SP was formed – the first coalition without the Labour Party since World War II.[228] Next to the Mayor, the municipal executive board consists of eight wethouders ('alderpersons') appointed by the municipal council: four D66 alderpersons, two VVD alderpersons and two SP alderpersons.[229]
 On 18 September 2017, it was announced by Eberhard van der Laan in an open letter to Amsterdam citizens that Kajsa Ollongren would take up his office as acting Mayor of Amsterdam with immediate effect due to ill health.[230] Ollongren was succeeded as acting Mayor by Eric van der Burg on 26 October 2017 and by Jozias van Aartsen on 4 December 2017.
 Unlike most other Dutch municipalities, Amsterdam is subdivided into eight boroughs, called stadsdelen or 'districts', and the urban area of Weesp, a system that was implemented gradually in the 1980s to improve local governance. The boroughs are responsible for many activities that had previously been run by the central city. In 2010, the number of Amsterdam boroughs reached fifteen. Fourteen of those had their own district council (deelraad), elected by a popular vote. The fifteenth, Westpoort, covers the harbour of Amsterdam and had very few residents. Therefore, it was governed by the central municipal council.
 Under the borough system, municipal decisions are made at borough level, except for those affairs pertaining to the whole city such as major infrastructure projects, which are the jurisdiction of the central municipal authorities. In 2010, the borough system was restructured, in which many smaller boroughs merged into larger boroughs. In 2014, under a reform of the Dutch Municipalities Act, the Amsterdam boroughs lost much of their autonomous status, as their district councils were abolished.
 The municipal council of Amsterdam voted to maintain the borough system by replacing the district councils with smaller, but still directly elected district committees (bestuurscommissies). Under a municipal ordinance, the new district committees were granted responsibilities through delegation of regulatory and executive powers by the central municipal council.
 "Amsterdam" is usually understood to refer to the municipality of Amsterdam. Colloquially, some areas within the municipality, such as the town of Durgerdam, may not be considered part of Amsterdam.
 Statistics Netherlands uses three other definitions of Amsterdam: metropolitan agglomeration Amsterdam (Grootstedelijke Agglomeratie Amsterdam, not to be confused with Grootstedelijk Gebied Amsterdam, a synonym of Groot Amsterdam), Greater Amsterdam (Groot Amsterdam, a COROP region) and the urban region Amsterdam (Stadsgewest Amsterdam).[114] The Amsterdam Department for Research and Statistics uses a fourth conurbation, namely the Stadsregio Amsterdam ('City Region of Amsterdam'). The city region is similar to Greater Amsterdam but includes the municipalities of Zaanstad and Wormerland. It excludes Graft-De Rijp.
 The smallest of these areas is the municipality of Amsterdam with a population of about 870,000 in 2021.[231] The larger conurbation had a population of over one million. It includes the municipalities of Zaanstad, Wormerland, Oostzaan, Diemen and Amstelveen only, as well as the municipality of Amsterdam. Greater Amsterdam includes 15 municipalities, and had a population of 1,400,000 in 2021.[231] Though much larger in area, the population of this area is only slightly larger, because the definition excludes the relatively populous municipality of Zaanstad. The largest area by population, the Amsterdam Metropolitan Area (Dutch: Metropoolregio Amsterdam), has a population of 2,33 million.[232] It includes for instance Zaanstad, Wormerland, Muiden, Abcoude, Haarlem, Almere and Lelystad but excludes Graft-De Rijp. Amsterdam is part of the conglomerate metropolitan area Randstad, with a total population of 6,659,300 inhabitants.[233]
 Of these various metropolitan area configurations, only the Stadsregio Amsterdam (City Region of Amsterdam) has a formal governmental status. Its responsibilities include regional spatial planning and the metropolitan public transport concessions.[234]
 Under the Dutch Constitution, Amsterdam is the capital of the Netherlands. Since the 1983 constitutional revision, the constitution mentions "Amsterdam" and "capital" in chapter 2, article 32: The king's confirmation by oath and his coronation take place in "the capital Amsterdam" ("de hoofdstad Amsterdam").[235] Previous versions of the constitution only mentioned "the city of Amsterdam" ("de stad Amsterdam").[236] For a royal investiture, therefore, the States General of the Netherlands (the Dutch Parliament) meets for a ceremonial joint session in Amsterdam. The ceremony traditionally takes place at the Nieuwe Kerk on Dam Square, immediately after the former monarch has signed the act of abdication at the nearby Royal Palace of Amsterdam. Normally, however, the Parliament sits in The Hague, the city which has historically been the seat of the Dutch government, the Dutch monarchy, and the Dutch supreme court. Foreign embassies are also located in The Hague.
 The coat of arms of Amsterdam is composed of several historical elements. First and centre are three St Andrew's crosses, aligned in a vertical band on the city's shield (although Amsterdam's patron saint was Saint Nicholas). These St Andrew's crosses can also be found on the city shields of neighbours Amstelveen and Ouder-Amstel. This part of the coat of arms is the basis of the flag of Amsterdam, flown by the city government, but also as civil ensign for ships registered in Amsterdam. Second is the Imperial Crown of Austria. In 1489, out of gratitude for services and loans, Maximilian I awarded Amsterdam the right to adorn its coat of arms with the king's crown. Then, in 1508, this was replaced with Maximilian's imperial crown when he was crowned Holy Roman Emperor. In the early years of the 17th century, Maximilian's crown in Amsterdam's coat of arms was again replaced, this time with the crown of Emperor Rudolph II, a crown that became the Imperial Crown of Austria. The lions date from the late 16th century, when city and province became part of the Republic of the Seven United Netherlands. Last came the city's official motto: Heldhaftig, Vastberaden, Barmhartig ("Heroic, Determined, Merciful"), bestowed on the city in 1947 by Queen Wilhelmina, in recognition of the city's bravery during the Second World War.
 Currently, there are sixteen tram routes and five metro routes. All are operated by municipal public transport operator Gemeentelijk Vervoerbedrijf (GVB), which also runs the city bus network.
 Four fare-free GVB ferries carry pedestrians and cyclists across the IJ lake to the borough of Amsterdam-Noord, and two fare-charging ferries run east and west along the harbour. There are also privately operated water taxis, a water bus, a boat sharing operation, electric rental boats and canal cruises, that transport people along Amsterdam's waterways.
 Regional buses, and some suburban buses, are operated by Connexxion and EBS. International coach services are provided by Eurolines from Amsterdam Amstel railway station, IDBUS from Amsterdam Sloterdijk railway station, and Megabus from the Zuiderzeeweg in the east of the city.
 In order to facilitate easier transport to the centre of Amsterdam, the city has various P+R Locations where people can park their car at an affordable price and transfer to one of the numerous public transport lines.[237]
 Amsterdam was intended in 1932 to be the hub, a kind of Kilometre Zero, of the highway system of the Netherlands,[238] with freeways numbered One to Eight planned to originate from the city.[238] The outbreak of the Second World War and shifting priorities led to the current situation, where only roads A1, A2, and A4 originate from Amsterdam according to the original plan. The A3 to Rotterdam was cancelled in 1970 in order to conserve the Groene Hart. Road A8, leading north to Zaandam and the A10 Ringroad were opened between 1968 and 1974.[239] Besides the A1, A2, A4 and A8, several freeways, such as the A7 and A6, carry traffic mainly bound for Amsterdam.
 The A10 ringroad surrounding the city connects Amsterdam with the Dutch national network of freeways. Interchanges on the A10 allow cars to enter the city by transferring to one of the 18 city roads, numbered S101 through to S118. These city roads are regional roads without grade separation, and sometimes without a central reservation. Most are accessible by cyclists. The S100 Centrumring is a smaller ringroad circumnavigating the city's centre.
 In the city centre, driving a car is discouraged. Parking fees are expensive, and many streets are closed to cars or are one-way.[240] The local government sponsors carsharing and carpooling initiatives such as Autodelen and Meerijden.nu.[241] The local government has also started removing parking spaces in the city, with the goal of removing 10,000 spaces (roughly 1,500 per year) by 2025.[242]
 Amsterdam is served by ten stations of the Nederlandse Spoorwegen (Dutch Railways).[243] Five are intercity stops: Sloterdijk, Zuid, Amstel, Bijlmer ArenA and Amsterdam Centraal. The stations for local services are: Lelylaan, RAI, Holendrecht, Muiderpoort and Science Park. Amsterdam Centraal is also an international railway station. From the station there are regular services to destinations such as Austria, Belarus, Belgium, Czech Republic, Denmark, France, Germany, Hungary, Poland, Russia, Switzerland and the United Kingdom. Among these trains are international trains of the Nederlandse Spoorwegen (Amsterdam-Berlin), the Eurostar (Amsterdam-Brussels-London), Thalys (Amsterdam-Brussels-Paris/Lille), and Intercity-Express (Amsterdam–Cologne–Frankfurt).[244][245][246]
 Amsterdam Airport Schiphol is less than 20 minutes by train from Amsterdam Centraal station and is served by domestic and international intercity trains, such as Thalys, Eurostar and Intercity Brussel. Schiphol is the largest airport in the Netherlands, the third-largest in Europe, and the 14th-largest in the world in terms of passengers. It handles over 68 million passengers per year and is the home base of four airlines, KLM, Transavia, Martinair and Arkefly.[247] As of 2014[update], Schiphol was the fifth busiest airport in the world measured by international passenger numbers.[248] This airport is 4 meters below sea level.[249] Although Schiphol is internationally known as Amsterdam Schiphol Airport it actually lies in the neighbouring municipality of Haarlemmermeer, southwest of the city.
 Amsterdam is one of the most bicycle-friendly large cities in the world and is a centre of bicycle culture with good facilities for cyclists such as bike paths and bike racks, and several guarded bike storage garages (fietsenstalling) which can be used.
 According to the most recent figures published by the Central Bureau of Statistics (CBS), in 2015 the 442,693 households (850,000 residents) in Amsterdam together owned 847,000 bicycles – 1.91 bicycles per household.[250] Theft is widespread—in 2011, about 83,000 bicycles were stolen in Amsterdam.[251] Bicycles are used by all socio-economic groups because of their convenience, Amsterdam's small size, the 400 km (249 mi) of bike paths,[252] the flat terrain, and the inconvenience of driving an automobile.[253]
 Amsterdam has two universities: the University of Amsterdam (Universiteit van Amsterdam, UvA), and the Vrije Universiteit Amsterdam (VU). Other institutions for higher education include an art school – Gerrit Rietveld Academie, a university of applied sciences – the Hogeschool van Amsterdam, and the Amsterdamse Hogeschool voor de Kunsten. Amsterdam's International Institute of Social History is one of the world's largest documentary and research institutions concerning social history, and especially the history of the labour movement. Amsterdam's Hortus Botanicus, founded in the early 17th century, is one of the oldest botanical gardens in the world,[254] with many old and rare specimens, among them the coffee plant that served as the parent for the entire coffee culture in Central and South America.[255]
 There are over 200 primary schools in Amsterdam.[256] Some of these primary schools base their teachings on particular pedagogic theories like the various Montessori schools. The biggest Montessori high school in Amsterdam is the Montessori Lyceum Amsterdam. Many schools, however, are based on religion. This used to be primarily Roman Catholicism and various Protestant denominations, but with the influx of Muslim immigrants, there has been a rise in the number of Islamic schools. Jewish schools can be found in the southern suburbs of Amsterdam.
 Amsterdam is noted for having five independent grammar schools (Dutch: gymnasia), the Vossius Gymnasium, Barlaeus Gymnasium, St. Ignatius Gymnasium, Het 4e Gymnasium and the Cygnus Gymnasium where a classical curriculum including Latin and classical Greek is taught. Though believed until recently by many to be an anachronistic and elitist concept that would soon die out, the gymnasia have recently experienced a revival, leading to the formation of a fourth and fifth grammar school in which the three aforementioned schools participate. Most secondary schools in Amsterdam offer a variety of different levels of education in the same school. The city also has various colleges ranging from art and design to politics and economics which are mostly also available for students coming from other countries. 
 Schools for foreign nationals in Amsterdam include the Amsterdam International Community School, British School of Amsterdam, Albert Einstein International School Amsterdam, Lycée Vincent van Gogh La Haye-Amsterdam primary campus (French school), International School of Amsterdam, and the Japanese School of Amsterdam.
 Amsterdam is a prominent centre for national and international media. Some locally based newspapers include Het Parool, a national daily paper; De Telegraaf, the largest Dutch daily newspaper; the daily newspapers Trouw, de Volkskrant and NRC; De Groene Amsterdammer, a weekly newspaper; the free newspapers Metro and The Holland Times (printed in English).
 Amsterdam is home to the second-largest Dutch commercial TV group SBS Broadcasting Group, consisting of TV-stations SBS 6, Net 5 and Veronica. However, Amsterdam is not considered 'the media city of the Netherlands'. The town of Hilversum, 30 km (19 mi) south-east of Amsterdam, has been crowned with this unofficial title. Hilversum is the principal centre for radio and television broadcasting in the Netherlands. Radio Netherlands, heard worldwide via shortwave radio since the 1920s, is also based there. Hilversum is home to an extensive complex of audio and television studios belonging to the national broadcast production company NOS, as well as to the studios and offices of all the Dutch public broadcasting organisations and many commercial TV production companies.
 In 2012, the music video of Far East Movement, 'Live My Life', was filmed in various parts of Amsterdam.
 Also, several movies were filmed in Amsterdam, such as James Bond's Diamonds Are Forever, Ocean's Twelve, Girl with a Pearl Earring and The Hitman's Bodyguard. Amsterdam is also featured in John Green's book The Fault in Our Stars, which has been made into a film as well that partly takes place in Amsterdam.
 From the late 1960s onwards many buildings in Amsterdam have been squatted both for housing and for using as social centres.[257] A number of these squats have legalised and become well known, such as OCCII, OT301, Paradiso and Vrankrijk.


Source: https://en.wikipedia.org/wiki/ABC_(Programmiersprache)
Content: Other reasons this message may be displayed:


Source: https://en.wikipedia.org/wiki/Betriebssystem
Content: Other reasons this message may be displayed:


Source: https://en.wikipedia.org/wiki/Amoeba_(Betriebssystem)
Content: Other reasons this message may be displayed:


Source: https://en.wikipedia.org/wiki/Pythons
Content: 
 The Pythonidae, commonly known as pythons, are a family of nonvenomous snakes found in Africa, Asia, and Australia. Among its members are some of the largest snakes in the world. Ten genera and 39 species are currently recognized. Being naturally non-venomous, pythons must constrict their prey to suffocate it prior to consumption. Pythons will typically strike at and bite their prey of choice to gain hold of it; they then must use physical strength to constrict their prey, by coiling their muscular bodies around the animal, effectively suffocating it before swallowing whole. This is in stark contrast to venomous snakes such as the rattlesnake, for example, which delivers a swift, venomous bite but releases, waiting as the prey succumbs to envenomation before being consumed.  Collectively, the pythons are well-documented and -studied as constrictors, much like other non-venomous snakes, including the boas and even kingsnakes of the New World.[2]
 Pythons are found in regions like sub-Saharan Africa, Southeast Asia, and Australia, with an invasive population of Burmese pythons in the Everglades National Park, Florida. They are ambush predators that primarily kill prey by constriction, causing cardiac arrest. Pythons are oviparous, laying eggs that females incubate until they hatch. While many species are available in the exotic pet trade, caution is needed with larger species due to potential danger. The taxonomy of pythons has evolved, and they are now more closely related to sunbeam snakes and the Mexican burrowing python.
 Pythons are poached for their meat and skin, leading to a billion-dollar global trade. They can carry diseases, such as salmonella and leptospirosis, which can be transmitted to humans. Pythons are also used in African traditional medicine to treat ailments like rheumatism and mental illnesses. Their body parts, including blood and organs, are believed to have various healing properties. In some African cultures, pythons have significant roles in folklore and mythology, often symbolizing strength or having sacred status.
 Pythons are found in sub-Saharan Africa, Nepal, India, Sri Lanka, Bangladesh, Southeast Asia, southeastern Pakistan, southern China, the Philippines and Australia.[1]
 In the United States, an introduced population of Burmese pythons (Python bivittatus) has existed as an invasive species in Everglades National Park since the late 1990s. As of January 2023, estimates place the Floridian Burmese python population at around half a million. Local bounties are awarded and scientists study dead Burmese pythons to better understand breeding cycles and trends associated with rapid population explosion. The pythons readily prey on native North American fauna in Florida, including (but not limited to) American alligators, birds, bobcats, American bullfrogs, opossums, raccoons, river otters, white-tailed deer, and occasionally domestic pets and livestock. They are also known to prey on other invasive and introduced animals to Florida, such as the green iguana and nutria (coypu), though not at a rate as to lower their numbers rapidly or effectively.[3]
 Many species have been hunted aggressively, which has greatly reduced the population of some, such as the Indian python (Python molurus) and the Ball python (Python regius).
 Most members of this family are ambush predators, in that they typically remain motionless in a camouflaged position, and then strike suddenly at passing prey. Attacks on humans, although known to occur, are extremely rare.[4][5]
 Pythons use their sharp, backward-curving teeth, four rows in the upper jaw, two in the lower, to grasp prey which is then killed by constriction; after an animal has been grasped to restrain it, the python quickly wraps a number of coils around it. Death occurs primarily by cardiac arrest.[6][7]
 Larger specimens usually eat animals about the size of a domestic cat, but larger food items are known; some large Asian species have been known to take down adult deer, and the Central African rock python, Python sebae, has been known to eat antelope. In 2017, there was a recorded case of a human devoured by a python in Sulawesi, Indonesia.[8] All prey is swallowed whole, and may take several days or even weeks to fully digest.
 Even the larger species, such as the reticulated python, Malayopython reticulatus, do not crush their prey to death; in fact, prey is not even noticeably deformed before it is swallowed.[citation needed] The speed with which the coils are applied is impressive and the force they exert may be significant, but death is caused by cardiac arrest.[citation needed]
 Pythons are oviparous. This sets them apart from the family Boidae (boas), most of which bear live young (ovoviviparous). After they lay their eggs, females typically incubate them until they hatch. This is achieved by causing the muscles to "shiver", which raises the temperature of the body to a certain degree, and thus that of the eggs. Keeping the eggs at a constant temperature is essential for healthy embryo development. During the incubation period, females do not eat and leave only to bask to raise their body temperature.
 Most species in this family are available in the exotic pet trade. However, caution must be exercised with the larger species, as they can be dangerous; rare cases of large specimens killing their owners have been documented.[9][10]
 Obsolete classification schemes—such as that of Boulenger (1890)—place pythons in Pythoninae, a subfamily of the boa family, Boidae.[1] However, despite a superficial resemblance to boas, pythons are more closely related to the sunbeam snakes (Xenopeltis) and the Mexican python (Loxocemus).[11][12]
 Poaching of pythons is a lucrative business with the global python skin trade being an estimated US$1 billion as of 2012.[14] Pythons are poached for their meat, mostly consumed locally as bushmeat and their skin, which is sent to Europe and North America for manufacture of accessories like bags, belts and shoes.[15] The demand for poaching is increased because python farming is very expensive.[16]
 In Cameroon bushmeat markets, the Central African rock python is commonly sold for meat and is very expensive at US$175.[15] The poaching of the pythons is illegal in Cameroon under their wildlife law, but there is little to no enforcement. In Kenya, there has been an increase in snake farms to address the demand for snakeskin internationally, but there are health concerns for the workers, and danger due to poachers coming to the farms to hunt the snakes.[17]
 While pythons are not venomous, they do carry a host of potential health issues for humans. Pythons are disease vectors for multiple illnesses, including Salmonella, Chlamydia, Leptospirosis, Aeromoniasis, Campylobacteriosis, and Zygomycosis. These diseases may be transmitted to humans through excreted waste, open wounds, and contaminated water.[18][19] A 2013 study found that Reptile-Associated Salmonella (RAS) is most common in young children who had been in contact with invasive pythons, with symptoms including "sepsis, meningitis, and bone and joint infection".[20]
 Pythons are also integrated into some aspects of African health and belief use, often with the added risk of contacting zoonotic diseases. Python bodies and blood are used for African traditional medicines and other belief uses as well, one in-depth study of all animals used by the Yorubas of Nigeria for traditional medicine found that the African Python is used to cure rheumatism, snake poison, appeasing witches, and accident prevention.[21]
 Python habitats, diets, and invasion into new areas also impact human health and prosperity. A University of Florida Institute of Food and Agriculture Sciences study found that the Burmese python, as an invasive species, enters new habitats and eats an increasing number of mammals, leaving limited species for mosquitoes to bite, forcing them to bite disease-carrying hispid cotton rats and then infect humans with the Everglades virus, a dangerous infection that is carried by very few animals.[22] While direct human-python interactions can be potentially dangerous, the risk of zoonotic diseases is always a concern, whether considering medical and belief use in Nigeria or when addressing invasive species impacts in Florida. In 2022, a woman who lived near a lake area in south-eastern New South Wales state, Australia, is found to be infested with the Ophidascaris robertsi roundworm which is common in carpet pythons - non-venomous snakes found across much of Australia.[23]
 Python skin has traditionally been used as the attire of choice for medicine men and healers.[24] Typically, South African Zulu traditional healers will use python skin in ceremonial regalia.[24] Pythons are viewed by the Zulu tradition to be a sign of power. This is likely why the skin is worn by traditional healers. Healers are seen as all-powerful since they have a wealth of knowledge, as well as accessibility to the ancestors.[24]
 Typically, species are attributed to healing various ailments based on their likeliness to a specific bodily attribute. For example, in many cultures, the python is seen as a strong and powerful creature. As a result, pythons are often prescribed as a method of increasing strength.[24] It is very common for the body fat of pythons to be used to treat a large variation of issues such as joint pain, rheumatic pain, toothache and eye sight.[25] Additionally, python fat has been used to treat those suffering from mental illnesses like psychosis.[26] Their calm nature is thought to be of use to treat combative patients. The fat of the python is rubbed onto the body part that is in pain. To improve mental illnesses, it is often rubbed on the temple.[25] The existence of evidence for genuine anti inflammatory and anti-microbial properties of the refined 'snake oil' ironic with respect to the expression "snake oil salesman".[27]
 Python blood plays another important role in traditional medicine. Many believe that python blood prevents the accumulation of fatty acids, triglycerides and lipids from reaching critically high levels.[25] Additionally, their blood has been used as a source of iron for people who are anemic, which helps reduce fatigue.[25][The sources were not specific on the way this blood is administered; however, due to the use of snake blood in traditional treatments in other parts of the world for similar causes, it is likely that the patient drinks the blood in order to feel the effects.[28]
 The Sukuma tribe of Tanzania have been known to use python feces in order to treat back pain. The feces are frequently mixed with a little water, placed on the back, and left for two to three days.[29]
 In Nigeria, the gallbladder and liver of a python are used to treat poison or bites from other snakes.[21] The python head has been used to "appease witches". Many traditional African cultures believe that they can be cursed by witches. In order to reverse spells and bad luck, traditional doctors will prescribe python heads.[21]
 In northwestern Ghana, people see pythons as a savior and have taboos to prevent the snake from being harmed or eaten. Their folklore states that this is because a python once helped them flee from their enemies by transforming into a log to allow them to cross a river.[30]
 In Botswana, San ritual practices surrounding pythons date back 70,000 years. In San mythology the python is a sacred creature that is highly respected.[31] They believe that mankind was made by a python that moved in between hills to create stream beds.
 In Benin, Vodun practitioners believe that pythons symbolize strength and the spirit of Dagbe ["to do good" in Yoruba]. Annually, people sacrifice animals and proclaim their sins to pythons that are kept inside temples.[32]
 


Source: https://en.wikipedia.org/wiki/Monty_Python
Content: 
 Monty Python (also collectively known as the Pythons)[2][3] were a British comedy troupe formed in 1969 consisting of Graham Chapman, John Cleese, Terry Gilliam, Eric Idle, Terry Jones, and Michael Palin. The group came to prominence for the sketch comedy series Monty Python's Flying Circus, which aired on the BBC from 1969 to 1974. Their work then developed into a larger collection that included live shows, films, albums, books, and musicals; their influence on comedy has been compared to the Beatles' influence on music.[4][5][6] Their sketch show has been called "an important moment in the evolution of television comedy".[7]
 Monty Python's Flying Circus was loosely structured as a sketch show, but its innovative stream-of-consciousness approach and Gilliam's animation skills pushed the boundaries of what was acceptable in style and content.[8][9] A self-contained comedy unit, the Pythons had creative control which allowed them to experiment with form and content, discarding rules of television comedy.[10] They followed their television work by making  the films Monty Python and the Holy Grail (1975), Life of Brian (1979), and The Meaning of Life (1983). Their influence on British comedy has been apparent for years, while it has coloured the work of the early editions of Saturday Night Live through to absurdist trends in television comedy.
 At the 41st British Academy Film Awards in 1988, Monty Python received the BAFTA Award for Outstanding British Contribution to Cinema. In 1998, they were awarded the AFI Star Award by the American Film Institute. Holy Grail and Life of Brian are frequently ranked on lists of the greatest comedy films. A 2005 poll asked more than 300 comedians, comedy writers, producers, and directors to name the greatest comedians of all time, and half of Monty Python's members made the top 50.[11][12]
 Jones and Palin met at Oxford University, where they performed together with the Oxford Revue. Chapman and Cleese met at Cambridge University. Idle was also at Cambridge, but started a year after Chapman and Cleese. Cleese met Gilliam in New York City while on tour with the Cambridge University Footlights revue Cambridge Circus (originally entitled A Clump of Plinths). Chapman, Cleese, and Idle were members of the Footlights, which at that time also included the future Goodies (Tim Brooke-Taylor, Bill Oddie, and Graeme Garden), and Jonathan Lynn (co-writer of Yes Minister and Yes, Prime Minister).[13] During Idle's presidency of the club, feminist writer Germaine Greer and broadcaster Clive James were members. Recordings of Footlights' revues (called "Smokers") at Pembroke College include sketches and performances by Cleese and Idle, which, along with tapes of Idle's performances in some of the drama society's theatrical productions, are kept in the archives of the Pembroke Players.[14]
 The six Python members appeared in or wrote these shows before Flying Circus:
 The BBC's satirical television show The Frost Report, broadcast from March 1966 to December 1967, is credited as first uniting the British Pythons and providing an environment in which they could develop their particular styles.[15]
 Following the success of Do Not Adjust Your Set, broadcast on ITV from December 1967 to May 1969, Thames Television offered Gilliam, Idle, Jones, and Palin their own late-night adult comedy series together. At the same time, Chapman and Cleese were offered a show by the BBC, which had been impressed by their work on The Frost Report and At Last the 1948 Show. Cleese was reluctant to do a two-man show for various reasons, including Chapman's supposedly difficult and erratic personality. Cleese had fond memories of working with Palin on How to Irritate People and invited him to join the team. With no studio available at Thames until summer 1970 for the late-night show, Palin agreed to join Cleese and Chapman, and suggested the involvement of his writing partner Jones and colleague Idle—who in turn wanted Gilliam to provide animations for the projected series. Much has been made of the fact that the Monty Python troupe is the result of Cleese's desire to work with Palin and the chance circumstances that brought the other four members into the fold.[16]
 By contrast, according to John Cleese's autobiography, the origins of Monty Python lay in the admiration that writing partners Cleese and Chapman had for the new type of comedy being done on Do Not Adjust Your Set; as a result, a meeting was initiated by Cleese between Chapman, Idle, Jones, Palin, and himself at which it was agreed to pool their writing and performing efforts and jointly seek production sponsorship.[17] According to their official website, the group was born from a Kashmir tandoori restaurant in Hampstead on 11 May 1969, following a taping of Do Not Adjust Your Set which Cleese and Chapman attended.[18] It was the first time all six got together, reportedly going back to Cleese's apartment on nearby Basil Street afterwards to continue discussions.[19]
 According to show director Ian MacNaughton, the first discussion over the idea for the show, Monty Python's Flying Circus, was a result of BBC's comedy advisor, Barry Took, bringing the Pythons along with John Howard Davies (director of the first four episodes) and MacNaughton together into one conference room at the BBC Television Centre.[22] The Pythons had a definite idea about what they wanted to do with the series. They were admirers of the work of Peter Cook, Alan Bennett, Jonathan Miller, and Dudley Moore on Beyond the Fringe—seminal to the British "satire boom"—and had worked on Frost, which was similar in style.[23]
 "The 1960s satire boom opened up the way for a fresh, inventive generation of young comedy writer-performers to flourish on TV and to take comedy in a new and exciting direction."
 —BBC profile for Monty Python's Flying Circus.[24] They enjoyed Cook and Moore's sketch show Not Only... But Also. One problem the Pythons perceived with these programmes was that though the body of the sketch would be strong, the writers would often struggle to then find a punchline funny enough to end on, and this would detract from the overall sketch quality. They decided that they would simply not bother to "cap" their sketches in the traditional manner, and early episodes of the Flying Circus series make great play of this abandonment of the punchline (one scene has Cleese turn to Idle, as the sketch descends into chaos, and remark that "This is the silliest sketch I've ever been in"—they all resolve not to carry on and simply walk off the set).[25] However, as they began assembling material for the show, the Pythons watched one of their collective heroes, Spike Milligan, whom they had admired on The Goon Show (a show the Pythons regard as their biggest influence, which also featured Peter Sellers, whom Cleese called "the greatest voice man of all time") recording his groundbreaking BBC series Q... (1969).[26][27] Not only was Q... more irreverent and anarchic than any previous television comedy, but Milligan also would often "give up" on sketches halfway through and wander off set (often muttering "Did I write this?"). It was clear that their new series would now seem less original, and Jones in particular became determined the Pythons should innovate. Michael Palin recalls "Terry Jones and I adored the Q... shows...[Milligan] was the first writer to play with the conventions of television."[28]
 After much debate, Jones remembered an animation Gilliam had created for Do Not Adjust Your Set called "Beware of the Elephants", which had intrigued him with its stream-of-consciousness style. Jones felt it would be a good concept to apply to the series: allowing sketches to blend into one another. Palin had been equally fascinated by another of Gilliam's efforts, entitled "Christmas Cards", and agreed that it represented "a way of doing things differently". Since Cleese, Chapman, and Idle were less concerned with the overall flow of the programme, Jones, Palin, and Gilliam became largely responsible for the presentation style of the Flying Circus series, in which disparate sketches are linked to give each episode the appearance of a single stream-of-consciousness (often using a Gilliam animation to move from the closing image of one sketch to the opening scene of another).[29] The BBC states, "Gilliam's unique animation style became crucial, segueing seamlessly between any two completely unrelated ideas and making the stream-of-consciousness work."[30]
 Writing started at 9 am and finished at 5 pm. Typically, Cleese and Chapman worked as one pair isolated from the others, as did Jones and Palin, while Idle wrote alone. After a few days, they would join with Gilliam, critique their scripts, and exchange ideas. Their approach to writing was democratic. If the majority found an idea humorous, it was included in the show. The casting of roles for the sketches was a similarly unselfish process, since each member viewed himself primarily as a "writer", rather than an actor eager for screen time. When the themes for sketches were chosen, Gilliam had a free hand in bridging them with animations, using a camera, scissors, and airbrush.[29]
 While the show was a collaborative process, different factions within Python were responsible for elements of the team's humour. In general, the work of the Oxford-educated members (Jones and Palin) was more visual, and more fanciful conceptually (e.g., the arrival of the Spanish Inquisition in a suburban front room), while the Cambridge graduates' sketches tended to be more verbal and more aggressive (for example, Cleese and Chapman's many "confrontation" sketches, where one character intimidates or hurls abuse, or Idle's characters with bizarre verbal quirks, such as "The Man Who Speaks In Anagrams"). Cleese confirmed that "most of the sketches with heavy abuse were Graham's and mine, anything that started with a slow pan across countryside and impressive music was Mike and Terry's, and anything that got utterly involved with words and disappeared up any personal orifice was Eric's".[31] Gilliam's animations ranged from the whimsical to the savage (the cartoon format allowing him to create some astonishingly violent scenes without fear of censorship).[29]
 Several names for the show were considered before Monty Python's Flying Circus was settled upon. Some were Owl Stretching Time, The Toad Elevating Moment, A Horse, a Spoon and a Basin, Vaseline Review, and Bun, Wackett, Buzzard, Stubble and Boot. Reportedly, these names were considered for the show because the group members found it funny that the show name would have nothing to do with the actual content of the series.[22] Flying Circus stuck when the BBC explained it had printed that name in its schedules and was not prepared to amend it.[32] Gwen Dibley's Flying Circus was named after a woman Palin had read about in the newspaper, thinking it would be amusing if she were to discover she had her own TV show. Baron Von Took's Flying Circus was considered as an affectionate tribute to Barry Took, the man who had brought them together.[22][33] Arthur Megapode's Flying Circus was suggested, then discarded. The name Baron Von Took's Flying Circus had the form of Baron Manfred von Richthofen's Flying Circus of WWI fame, and the new group was forming in a time when the Royal Guardsmen's 1966 song "Snoopy vs. the Red Baron" had peaked. The term 'flying circus' was also another name for the popular entertainment of the 1920s known as barnstorming, where multiple performers collaborated with their stunts to perform a combined set of acts.[34]
 Differing, somewhat confusing accounts are given of the origins of the Python name, although the members agree that its only "significance" was that they thought it sounded funny. In the 1998 documentary Live at Aspen during the US Comedy Arts Festival, where the troupe was awarded the AFI Star Award by the American Film Institute, the group implied that "Monty" was selected (Eric Idle's idea) as a gently mocking tribute to Field Marshal Lord Montgomery, a British general of World War II; requiring a "slippery-sounding" surname, they settled on "Python". On other occasions, Idle has claimed that the name "Monty" was that of a popular and rotund fellow who drank in his local pub; people would often walk in and ask the barman, "Has Monty been in yet?", forcing the name to become stuck in his mind. The name Monty Python was later described by the BBC as being "envisaged by the team as the perfect name for a sleazy entertainment agent".[24]
 Flying Circus popularised innovative formal techniques, such as the cold open, in which an episode began without the traditional opening titles or announcements.[35] An example of this is the "It's" man: Palin, outfitted in Robinson Crusoe garb, making a tortuous journey across various terrains, before finally approaching the camera to state, "It's ...", to be then followed by the title sequence and theme music. On several occasions, the cold open lasted until mid-show, after which the regular opening titles ran. Occasionally, the Pythons tricked viewers by rolling the closing credits halfway through the show, usually continuing the joke by fading to the familiar globe logo used for BBC continuity, over which Cleese would parody the clipped tones of a BBC announcer.[36] On one occasion, the credits ran directly after the opening titles. On the subversive nature of the show (and their subsequent films), Cleese states "anti-authoritarianism was deeply ingrained in Python".[26]
 "Our first rule was: no punchlines. [Some sketches] start brilliant, great acting, really funny sketch, but the punchline is just not as good as the rest of the sketch, so it kills the entire thing. That's why we eliminated them."
 —Terry Gilliam in 2007.[37] Because of their dislike of finishing with punchlines, they experimented with ending the sketches by cutting abruptly to another scene or animation, walking offstage, addressing the camera (breaking the fourth wall), or introducing a totally unrelated event or character. A classic example of this approach was the use of Chapman's "anti-silliness" character of "the Colonel", who walked into several sketches and ordered them to be stopped because things were becoming "far too silly".[38]
 Another favourite way of ending sketches was to drop a cartoonish "16-ton weight" prop on one of the characters when the sketch seemed to be losing momentum, or a knight in full armour (played by Terry Gilliam) would wander on-set and hit characters over the head with a rubber chicken,[39] before cutting to the next scene. Yet another way of changing scenes was when John Cleese, usually outfitted in a dinner suit, would come in as a radio commentator and, in a rather pompous manner, make the formal and determined announcement "And now for something completely different.", which later became the title of the first Monty Python film.[40]
 The Python theme music is the Band of the Grenadier Guards' rendition of John Philip Sousa's "The Liberty Bell" which was first published in 1893.[41] Under the Berne Convention's "country of origin" concept, the composition was subject to United States copyright law which states that any work first published prior to 1924 was in the public domain, owing to copyright expiration.[42] This enabled Gilliam to co-opt the march for the series without having to make any royalty payments.[43]
 The use of Gilliam's surreal, collage stop motion animations was another innovative intertextual element of the Python style.[29] Many of the images Gilliam used were lifted from famous works of art, and from Victorian illustrations and engravings. The giant foot which crushes the show's title at the end of the opening credits is in fact the foot of Cupid, cut from a reproduction of the Renaissance masterpiece Venus, Cupid, Folly and Time by Bronzino. This foot, and Gilliam's style in general, are visual trademarks of the programme.[29]
 The Pythons used the British tradition of cross-dressing comedy by donning frocks and makeup and playing female roles themselves while speaking in falsetto.[45] Jones specialised in playing the working-class housewife, or "ratbag old women" as termed by the BBC.[30] Palin and Idle generally played the role more posh, with Idle playing more feminine women.[30] Cleese played female roles more sparsely, while Chapman was frequently paired with Jones as a ratbag woman or with Idle portraying middle-class women commenting upon TV. Generally speaking, female roles were played by women only when the scene specifically required that the character be sexually attractive (although sometimes they used Idle for this). The troupe later turned to Carol Cleveland—often described as the unofficial seventh member—who co-starred in numerous episodes after 1970.[46] In some episodes, and later in the stoning scene in Monty Python's Life of Brian, they took the idea one step further by playing women who impersonated men.[47]
 Many sketches are well-known and widely quoted. "Dead Parrot sketch", "The Lumberjack Song", "Spam" (which led to the coining of the term email spam),[48] "Nudge Nudge", "The Spanish Inquisition", "Upper Class Twit of the Year", "Cheese Shop", "The Ministry of Silly Walks", "Argument Clinic", "The Funniest Joke in the World" (a sketch referenced in Google Translate), and Four Yorkshiremen sketch" are just a few examples.[49][50] Most of the show's sketches satirise areas of public life, such as: Dead Parrot (poor customer service), Silly Walks (bureaucratic inefficiency), Spam (ubiquity of Spam post World War II), and Four Yorkshiremen (nostalgic conversations).[51][52][53] Featuring regularly in skits, Gumbys (characters of limited intelligence and vocabulary) were part of the Pythons' satirical view of television of the 1970s which condescendingly encouraged more involvement from the "man on the street".[54]
 The Canadian Broadcasting Corporation (CBC) added Monty Python's Flying Circus to its national September 1970 fall line-up.[55] They aired the 13 episodes of series 1, which had first run on the BBC the previous autumn (October 1969 to January 1970), as well as the first six episodes of series 2 only a few weeks after they first appeared on the BBC (September to November 1970).[55] The CBC dropped the show when it returned to regular programming after the Christmas 1970 break, choosing to not place the remaining seven episodes of series 2 on the January 1971 CBC schedule.[55] Within a week, the CBC received hundreds of calls complaining of the cancellation, and more than 100 people staged a demonstration at the CBC's Montreal studios. The show eventually returned, becoming a fixture on the network during the first half of the 1970s.[55]
 Sketches from Monty Python's Flying Circus were introduced to American audiences in August 1972, with the release of the Python film And Now for Something Completely Different, featuring sketches from series 1 and 2 of the television show. This 1972 release met with limited box office success.[56]
 The ability to show Monty Python's Flying Circus under the American NTSC standard had been made possible by the commercial actions of American television producer Greg Garrison. Garrison produced the NBC series The Dean Martin Comedy World, which ran during the summer of 1974.[57] The concept was to show clips from comedy shows produced in other countries, including tape of the Python sketches "Bicycle Repairman" and "The Dull Life of a Stockbroker".[58]
 "When [Monty Python] hit the airwaves, it really was quite shocking but it was shocking in a good way. It set you up right and opened up a whole new form of comedy. 'Pythonesque."
 — Ron Devillier, PBS programming director.[59] Through the efforts of Python's American manager Nancy Lewis, during the summer of 1974, Ron Devillier, the programming director for nonprofit PBS television station KERA in Dallas, Texas, started airing episodes of Monty Python's Flying Circus.[60][61] Ratings shot through the roof, prompting other PBS affiliates to pick up the show.[59] Devillier states, "We got the Nielsens in and started looking at the Saturday ratings. The first night, it was a 6 rating. We couldn't believe it. We didn't know what a 6 looked like. The next week, it was a 7 and it may have taken a month but it stayed there and we started getting 8s, 9s and 10s."[59] There was also cross-promotion from FM radio stations across the US, whose airing of tracks from the Python LPs had already introduced American audiences to this bizarre brand of comedy.[62] The popularity on PBS resulted in the 1974 re-release of the 1972 ...Completely Different film, with much greater box office success.[56] The success of the show was captured by a March 1975 article headline in The New York Times, "Monty Python's Flying Circus Is Barnstorming Here".[63] Asked what challenges were left, now that they had made TV shows, films, written books, and produced records, Chapman responded, "Well, actually world supremacy would be very nice", before Idle cautioned, "Yes, but that sort of thing has got to be done properly".[63]
 In 1975 ABC broadcast two 90-minute Monty Python specials, each with three shows, but cut out a total of 24 minutes from each, in part to make time for commercials, and in part to avoid upsetting their audience.  As the judge observed in Gilliam v. American Broadcasting Companies, Inc., where Monty Python sued for damages caused by broadcast of the mutilated version, "According to the network, appellants should have anticipated that most of the excised material contained scatological references inappropriate for American television and that these scenes would be replaced with commercials, which presumably are more palatable to the American public."  Monty Python won the case.[64]
 With the popularity of Python throughout the rest of the 1970s and through most of the 1980s, PBS stations looked at other British comedies, leading to UK shows such as Are You Being Served? gaining a US audience, and leading, over time, to many PBS stations having a "British Comedy Night" which airs many popular UK comedies.[65]
 In 1976, Monty Python became the top rated show in Japan. The popularity of the show in the Netherlands saw the town of Spijkenisse near Rotterdam open a 'silly walks' road crossing in 2018. Believed to be a world first, the official sign asks pedestrians to cross the road in a comical manner.[66]
 Having considered the possibility at the end of the second series, Cleese left the Flying Circus at the end of the third. He later explained that he felt he no longer had anything fresh to offer the show, and claimed that only two Cleese- and Chapman-penned sketches in the third series ("Dennis Moore" and the "Cheese Shop") were truly original, and that the others were bits and pieces from previous work cobbled together in slightly different contexts.[16] He was also finding Chapman, who was at that point in the full throes of alcoholism, difficult to work with. According to an interview with Idle, "It was on an Air Canada flight on the way to Toronto, when John (Cleese) turned to all of us and said 'I want out.' Why? I don't know. He gets bored more easily than the rest of us. He's a difficult man, not easy to be friendly with. He's so funny because he never wanted to be liked. That gives him a certain fascinating, arrogant freedom."[67] In 2012, Jones spoke on Cleese's work in the third series and subsequent departure, "He was good at it, when he did it he was professional, but he'd rather not have done it. The others all loved it, but he got more and more pissed off about having to come out and do filming, and the one that really swung it, in my view, was when we had to do the day on the Newhaven lifeboat."[68]
 The rest of the group carried on for one more "half" season before calling a halt to the programme in 1974. While the first three seasons contained 13 episodes each, the fourth ended after just six.[69] The name Monty Python's Flying Circus appears in the opening animation for season four, but in the end credits, the show is listed as simply Monty Python.[69] Although Cleese left the show, he was credited as a writer for three of the six episodes, largely concentrated in the "Michael Ellis" episode, which had begun life as one of the many drafts of the "Holy Grail" motion picture.  When a new direction for "Grail" was decided upon, the subplot of Arthur and his knights wandering around a strange department store in modern times was lifted out and recycled as the aforementioned TV episode. Songwriter Neil Innes contributed to some sketches, including "Appeal on Behalf of Very Rich People".[70]
 The Pythons' first feature film was directed by Ian MacNaughton, reprising his role from the television series. It consisted of sketches from the first two seasons of the Flying Circus, reshot on a low budget (and often slightly edited) for cinema release. Material selected for the film includes: "Dead Parrot", "The Lumberjack Song", "Upper Class Twit of the Year", "Hell's Grannies", "Self-Defence Class", "How Not to Be Seen", and "Nudge Nudge".[71] Financed by Playboy's UK executive Victor Lownes, it was intended as a way of breaking Monty Python into America, and although it was ultimately unsuccessful in this,[72] the film did good business in the UK, and later in the US on the "Midnight movie" circuit after their breakthrough television and film success, this being in the era before home video would make the original material much more accessible. The group did not consider the film a success.
 In 1974, between production on the third and fourth series, the group decided to embark on their first "proper" feature film, containing entirely new material. Monty Python and the Holy Grail was based on Arthurian legend and was directed by Jones and Gilliam. Again, the latter also contributed linking animations (and put together the opening credits). Along with the rest of the Pythons, Jones and Gilliam performed several roles in the film, but Chapman took the lead as King Arthur. Cleese returned to the group for the film, feeling that they were once again breaking new ground. Holy Grail was filmed on location, in picturesque rural areas of Scotland, with a budget of only £229,000; the money was raised in part with investments from rock groups such as Pink Floyd, Jethro Tull, and Led Zeppelin, as well as UK music industry entrepreneur Tony Stratton Smith (founder and owner of the Charisma Records label, for which the Pythons recorded their comedy albums).[73]
 The backers of the film wanted to cut the famous Black Knight scene (a Sam Peckinpah send-up in which the Black Knight loses his limbs in a duel), but it was eventually kept in the movie.[74] "Tis but a scratch" and "It's just a flesh wound…" are often quoted.[75] Holy Grail was selected as the second-best comedy of all time in the ABC special Best in Film: The Greatest Movies of Our Time. and viewers in a Channel 4 poll placed it sixth.[76]
 Following the success of Holy Grail, reporters asked for the title of the next Python film, though the team had not even begun to consider a third one. Eventually, Idle flippantly replied "Jesus Christ – Lust for Glory", which became the group's stock answer to such questions.[77] However, they soon began to seriously consider a film lampooning the New Testament era in the same way Holy Grail had lampooned Arthurian legend. Despite sharing a distrust of organised religion, they agreed not to mock Jesus or his teachings directly. They also mentioned that they could not think of anything legitimate to make fun of about him.[78] Instead, they decided to write a satire on credulity and hypocrisy among the followers of someone [Brian] who had been mistaken for the "Messiah", but who had no desire to be followed as such.[79] Terry Jones adds it was a satire on those who for the next 2,000 years "couldn't agree on what Jesus was saying about peace and love".[79]
 "We are three wise men."
"Well, what are you doing creeping around a cow shed at two o'clock in the morning? That doesn't sound very wise to me."
 —Early scene from Life of Brian.[75] The focus therefore shifted to a separate individual, Brian Cohen, born at the same time, and in a neighbouring stable. When Jesus appears in the film (first, as a baby in the stable, and then later on the Mount, speaking the Beatitudes), he is played straight (by actor Kenneth Colley) and portrayed with respect. The comedy begins when members of the crowd mishear his statements of peace, love, and tolerance ("I think he said, 'Blessed are the cheesemakers'").[78]
 Directing duties were handled solely by Jones, having amicably agreed with Gilliam that Jones' approach to film-making was better suited for Python's general performing style. Holy Grail's production had often been stilted by their differences behind the camera. Gilliam again contributed two animated sequences (one being the opening credits) and took charge of set design. The film was shot on location in Tunisia, the finances being provided this time by The Beatles' George Harrison, who together with Denis O'Brien formed the production company Hand-Made Films for the movie.[80] Harrison had a cameo role as the "owner of the Mount".[80]
 Despite its subject matter attracting controversy, particularly upon its initial release, it has (together with its predecessor) been ranked among the greatest comedy films.[81][82] In 2006 it was ranked first on a Channel 4 list of the 50 Greatest Comedy Films.[76] Empire magazine called it "an unrivalled satire on religion".[83] In 2013, Richard Burridge, a theologian decorated by Pope Francis, called Life of Brian an "extraordinary tribute to the life and work and teaching of Jesus—that they couldn't actually blaspheme or make a joke out of it. They did a great satire on closed minds and people who follow blindly. Then you have them splitting into factions...it is a wonderful satire on the way that Jesus's own teaching has been used to persecute others. They were satirising fundamentalism and persecution of others and at the same time saying the one person who rises above all this was Jesus".[78]
 Monty Python performed four consecutive dates at the Hollywood Bowl in Los Angeles in September 1980 during preparations for Meaning of Life. The performances were filmed and released in the concert film, Monty Python Live at the Hollywood Bowl (directed by Terry Hughes), with the Pythons performing sketches from the television series in front of an audience.[84] The released film also incorporated footage from the German television specials (the inclusion of which gives Ian MacNaughton his first on-screen credit for Python since the end of Flying Circus) and live performances of several songs from the troupe's then-current Monty Python's Contractual Obligation Album.[85] Monty Python's 4-night stint as headliners at the Hollywood Bowl set a record for a comedy act at the venue that has since been equalled by Dave Chappelle in May 2022.[86]
 The Pythons' final film returned to something structurally closer to the style of Flying Circus. A series of sketches loosely follows the ages of man from birth to death. Directed again by Jones solo, The Meaning of Life is embellished with some of the group's most bizarre and disturbing moments, as well as various elaborate musical numbers, which include "Galaxy Song" (performed by Idle) and "Every Sperm Is Sacred" (performed by Palin and Jones).[88] The film is by far their darkest work, containing a great deal of black humour, garnished by some spectacular violence (including an operation to remove a liver from a living patient without anaesthetic and the morbidly obese Mr. Creosote exploding over several restaurant patrons after finally giving in to the smooth maître d' telling him to eat a mint – "It's only a wafer-thin mint...").[75] At the time of its release, the Pythons confessed their aim was to offend "absolutely everyone", adding "It is guaranteed to offend".[89]
 The Liver Donor scene (in which a paramedic appears at the door of a living man to take his liver) is a satire on bureaucracy, a common Python trope.[88] Besides the opening credits and the fish sequence, Gilliam, by now an established live-action director, no longer wanted to produce any linking cartoons, offering instead to direct one sketch, "The Crimson Permanent Assurance". Under his helm, though, the segment grew so ambitious and tangential that it was cut from the movie and used as a supporting feature in its own right. (Television screenings also use it as a prologue.) This was the last project on which all six Pythons collaborated, except for the 1989 compilation Parrot Sketch Not Included, where they are all seen sitting in a closet for four seconds. This was the last time Chapman appeared on screen with the Pythons.[71]
 Although not as acclaimed as its two predecessors (Holy Grail and Life of Brian), The Meaning of Life was still well received critically and was screened at the 1983 Cannes Film Festival where it won the Grand Prix.[90]
 Members of Python contributed their services to charitable endeavours and causes—sometimes as an ensemble, at other times as individuals. The cause that has been the most frequent and consistent beneficiary has been the human rights work of Amnesty International. Between 1976 and 1981, the troupe or its members appeared in four major fund-raisers for Amnesty—known collectively as the Secret Policeman's Ball shows—which were turned into multiple films, TV shows, videos, record albums, and books. The brainchild of John Cleese, these benefit shows in London and their many spin-offs raised considerable sums of money for Amnesty, raised public and media awareness of the human rights cause, and influenced many other members of the entertainment community (especially rock musicians) to become involved in political and social issues.[91][92] Among the many musicians who have publicly attributed their activism—and the organisation of their own benefit events—to the inspiration of the work in this field of Monty Python are Bob Geldof (organiser of Live Aid), U2, Pete Townshend, and Sting.[91][93] Bono told Rolling Stone in 1986, "I saw The Secret Policeman's Ball and it became a part of me. It sowed a seed..."[93] Sting states, "before [the Ball] I did not know about Amnesty, I did not know about its work, I did not know about torture in the world."[94] On the impact of the Ball on Geldof, Sting states, "he took the 'Ball' and ran with it."[92]
 Ball co-founder Cleese and Jones had an involvement (as performer, writer or director) in all four Amnesty benefit shows, Palin in three, Chapman in two, and Gilliam in one. Idle did not participate in the Amnesty shows. Notwithstanding Idle's lack of participation, the other five members (together with "Associate Pythons" Carol Cleveland and Neil Innes) all appeared together in the first Secret Policeman's Ball benefit—the 1976 A Poke in the Eye held at Her Majesty's Theatre in London's West End—where they performed several Python sketches. In this first show, they were collectively billed as Monty Python. Peter Cook deputised for the absent Idle in a courtroom sketch.[91] In the next three shows, the participating Python members performed many Python sketches, but were billed under their individual names rather than under the collective Python banner. The second show featured newcomer Rowan Atkinson and Scottish comedian Billy Connolly.[95] The Secret Policeman's Ball were the first stage shows in the UK to present comedic performers (such as Monty Python and Rowan Atkinson) in the same setting and shows as their contemporaries in rock music (which included Eric Clapton, Sting and Phil Collins).[95] After a six-year break, Amnesty resumed producing Secret Policeman's Ball benefit shows which were held at the London Palladium in 1987 (sometimes with, and sometimes without, variants of the title) and by 2006 had presented a total of twelve shows. Since 1987 the Balls featured newer generations of British comedic performers, such as Stephen Fry, Hugh Laurie, and puppets from the satirical TV show Spitting Image, with many attributing their participation in the show to their desire to emulate the Python's pioneering work for Amnesty. Cleese and Palin made a brief cameo appearance in the 1989 Amnesty show; apart from that, the Pythons have not appeared in shows after the first four.[96]
 Each member has pursued various film, television, and stage projects since the break-up of the group, but often continued to work with one another. Many of these collaborations were very successful, most notably A Fish Called Wanda (1988), written by Cleese, in which he starred along with Palin. The pair also appeared in Time Bandits (1981), a film directed by Gilliam, who wrote it together with Palin. Gilliam directed Jabberwocky (1977), and also directed and co-wrote Brazil (1985), which featured Palin, The Adventures of Baron Munchausen (1988), which featured Idle; he followed these with writing and directing an additional six (as of 2021) films.
 HandMade Films, the film studio that George Harrison co-founded to produce Life of Brian, contributed to British cinema in the 1980s, producing classics of the period including The Long Good Friday and Mona Lisa (both starring Bob Hoskins), Time Bandits, and Withnail and I, with the studio also launching Terry Gilliam's directorial career.[10] Yellowbeard (1983) was co-written by Chapman and starred Chapman, Idle, and Cleese, as well as many other English comedians including Peter Cook, Spike Milligan, and Marty Feldman.[98]
 Palin and Jones wrote the comedic TV series Ripping Yarns (1976–79), starring Palin. Jones also appeared in the pilot episode and Cleese appeared in a nonspeaking part in the episode "Golden Gordon". Jones' film Erik the Viking also has Cleese playing a small part. In 1996 Terry Jones wrote and directed an adaptation of Kenneth Grahame's novel The Wind in the Willows. It featured four members of Monty Python: Jones as Mr. Toad, Idle as Ratty, Cleese as Mr. Toad's lawyer, and Palin as the Sun. Gilliam was considered for the voice of the river. The film included Steve Coogan who played Mole.[99]
 Cleese has the most prolific solo career, appearing in dozens of films, several TV shows or series (including Cheers, 3rd Rock from the Sun, Q's assistant in the James Bond movies, and Will & Grace), many direct-to-video productions, some video games and a number of commercials.[100] His BBC sitcom Fawlty Towers (written by and starring Cleese together with his wife Connie Booth) is the only comedy series to rank higher than the Flying Circus on the BFI TV 100's list, topping the whole poll.[101][102] Cleese's character, Basil Fawlty, was ranked second (to Homer Simpson) on Channel 4's 2001 list of the 100 Greatest TV Characters.[103]
 Idle enjoyed critical success with Rutland Weekend Television in the mid-1970s, out of which came the Beatles parody the Rutles (responsible for the cult mockumentary All You Need Is Cash), and as an actor in Nuns on the Run (1990) with Robbie Coltrane. In 1976 Idle directed music videos for George Harrison songs "This Song" and "Crackerbox Palace", the latter of which also featured cameo appearances from Neil Innes and John Cleese. Idle has had success with Python songs: "Always Look on the Bright Side of Life" went to no. 3 in the UK singles chart in 1991.[104] The song had been revived by Simon Mayo on BBC Radio 1, and was consequently released as a single that year. The theatrical phenomenon of the Python musical Spamalot has made Idle the most financially successful of the troupe after Python. Written by Idle (and featuring a pre-recorded cameo of Cleese as the voice of God), it has proved to be an enormous hit on Broadway, London's West End and Las Vegas.[105] This was followed by Not the Messiah, which revises The Life of Brian as an oratorio. For the work's 2007 premiere at the Luminato festival in Toronto (which commissioned the work), Idle himself sang the "baritone-ish" part.
 Since The Meaning of Life, their last project as a team, the Pythons have often been the subject of reunion rumours.[105] In 1988 Monty Python won the BAFTA Award for Outstanding British Contribution To Cinema, with four of the six Pythons (Jones, Palin, Gilliam and Chapman) collecting the award.[106] The final appearance of all six together occurred during the 1989 Parrot Sketch Not Included – 20 Years of Monty Python TV special.[99][107] The death of Chapman in October 1989 put an end to the speculation of any further reunions. However, there were several occasions after 1989 when the remaining five members gathered together for appearances — albeit not formal reunions. In 1996 Jones, Idle, Cleese, and Palin were featured in a film adaptation of The Wind in the Willows, which was later renamed Mr. Toad's Wild Ride.[79] In 1997 Palin and Cleese rolled out a new version of the "Dead Parrot sketch" for Saturday Night Live.[99]
 Monty Python were the inaugural recipients of the Empire Inspiration Award in 1997. Palin, Jones and Gilliam received the award on stage in London from Elton John while Cleese and Idle appeared via satellite from Los Angeles.[108] In 1998 during the US Comedy Arts Festival, where the troupe were awarded the AFI Star Award by the American Film Institute, the five remaining members, along with what was purported to be Chapman's ashes, were reunited on stage for the first time in 18 years.[109] The occasion was in the form of an interview called Monty Python Live at Aspen, (hosted by Robert Klein, with an appearance by Eddie Izzard) in which the team looked back at some of their work and performed a few new sketches. On 9 October 1999, to commemorate 30 years since the first Flying Circus television broadcast, BBC2 devoted an evening to Python programmes, including a documentary charting the history of the team, interspersed with new sketches by the Monty Python team filmed especially for the event.[99]
 The surviving Pythons had agreed in principle to perform a live tour of America in 1999. Several shows were to be linked with Q&A meetings in various cities. Although all had said yes, Palin later changed his mind, much to the annoyance of Idle, who had begun work organising the tour. This led to Idle refusing to take part in the new material shot for the BBC anniversary evening. In 2002, four of the surviving members, bar Cleese, performed "The Lumberjack Song" and "Sit on My Face" for George Harrison's memorial concert. The reunion also included regular supporting contributors Neil Innes and Carol Cleveland, with a special appearance from Tom Hanks.[110] In an interview to publicise the DVD release of The Meaning of Life, Cleese said a further reunion was unlikely. "It is absolutely impossible to get even a majority of us together in a room, and I'm not joking," Cleese said. He said that the problem was one of busyness rather than one of bad feelings.[111] A sketch appears on the same DVD spoofing the impossibility of a full reunion, bringing the members "together" in a deliberately unconvincing fashion with modern bluescreen/greenscreen techniques.
 Idle responded to queries about a Python reunion by adapting a line used by George Harrison in response to queries about a possible Beatles reunion. When asked in November 1989 about such a possibility, Harrison responded: "As far as I'm concerned, there won't be a Beatles reunion as long as John Lennon remains dead."[112] Idle's version of this was that he expected to see a proper Python reunion, "just as soon as Graham Chapman comes back from the dead", but added, "we're talking to his agent about terms."[113]
 The Pythons Autobiography by The Pythons (2003), compiled from interviews with the surviving members, reveals that a series of disputes in 1998, over a possible sequel to Holy Grail that had been conceived by Idle, may have resulted in the group's split. Cleese's feeling was that The Meaning of Life had been personally difficult and ultimately mediocre, and did not wish to be involved in another Python project for a variety of reasons (not least amongst them was the absence of Chapman, whose straight man-like central roles in the Grail and Brian films had been considered to be an essential anchoring performance). The book also reveals that Cleese saw Chapman as his "greatest sounding board. If Graham thought something was funny, then it almost certainly was funny. You cannot believe how invaluable that is.'[114] Ultimately it was Cleese who ended the possibility of another Python movie.[115]
 A full, if nonperforming, reunion of the surviving Python members appeared at the March 2005 premiere of Idle's musical Spamalot. Based on Monty Python and the Holy Grail, it also spoofs popular musicals, including those of Andrew Lloyd Webber.[116] It opened in Chicago and has since played in New York on Broadway, London, and numerous other major cities across the world. In 2004, Spamalot was nominated for 14 Tony Awards and won three: Best Musical, Best Direction of a Musical for Mike Nichols, and Best Performance by a Featured Actress in a Musical for Sara Ramirez, who played the Lady of the Lake, a character specially added for the musical.  The original Broadway cast included Tim Curry as King Arthur, Michael McGrath as Patsy, David Hyde Pierce as Sir Robin, Hank Azaria as Sir Lancelot and other roles (e.g., the French Taunter, Knight of Ni, and Tim the Enchanter), Christopher Sieber as Sir Galahad and other roles (e.g., the Black Knight and Prince Herbert's Father).[117] Cleese played the voice of God, a role played in the film by Chapman.[118]
 Owing in part to the success of Spamalot, PBS announced on 13 July 2005 that it would begin to re-air the entire run of Monty Python's Flying Circus and new one-hour specials focusing on each member of the group, called Monty Python's Personal Best.[119] Each episode was written and produced by the individual being honoured, with the five remaining Pythons collaborating on Chapman's programme, the only one of the editions to take on a serious tone with its new material.[120]
 In 2009, to commemorate the 40th anniversary of the first episode of Monty Python's Flying Circus, a six-part documentary entitled Monty Python: Almost the Truth (Lawyers Cut) was released, featuring interviews with the surviving members of the team, as well as archive interviews with Graham Chapman and numerous excerpts from the television series and films.[121] Each episode opens with a different re-recording of the theme song from Life of Brian, with Iron Maiden vocalist and Python fan Bruce Dickinson performing the sixth.[122]
 Also in commemoration of the 40th anniversary, Idle, Palin, Jones, and Gilliam appeared in a production of Not the Messiah at the Royal Albert Hall. The European premiere was held on 23 October 2009.[123] An official 40th anniversary Monty Python reunion event took place in New York City on 15 October 2009, where the team received a Special Award from the British Academy of Film and Television Arts.[124]
 In June 2011, it was announced that A Liar's Autobiography: The Untrue Story of Monty Python's Graham Chapman, an animated 3D movie based on the memoir of Graham Chapman, was in the making. The memoir A Liar's Autobiography was published in 1980 and details Chapman's journey through medical school, alcoholism, acknowledgement of his gay identity, and the tolls of surreal comedy. Asked what was true in a deliberately fanciful account by Chapman of his life, Terry Jones joked: "Nothing ... it's all a downright, absolute, blackguardly lie." The film uses Chapman's own voice—from a reading of his autobiography shortly before he died of cancer—and entertainment channel Epix announced the film's release in early 2012 in both 2D and 3D formats. Produced and directed by London-based Bill Jones, Ben Timlett, and Jeff Simpson, the new film has 15 animation companies working on chapters that will range from three to 12 minutes in length, each in a different style. John Cleese recorded dialogue which was matched with Chapman's voice. Michael Palin voiced Chapman's father and Terry Jones voiced his mother. Terry Gilliam voiced Graham's psychiatrist.  They all play various other roles. Among the original Python group, only Eric Idle was not involved.[125]
 On 26 January 2012, Terry Jones announced that the five surviving Pythons would reunite in a sci-fi comedy film called Absolutely Anything.[126] The film would combine computer-generated imagery and live action. It would be directed by Jones based on a script by Jones and Gavin Scott, and in addition to the Python members it would also star Simon Pegg, Kate Beckinsale and Robin Williams (in his final film role).[127] The plot revolves around a teacher who discovers aliens (voiced by the Pythons) have given him magical powers to do "absolutely anything".[128] Eric Idle responded via Twitter that he would not, in fact, be participating,[129] although he was later added to the cast.[130]
 In 2013, the Pythons lost a legal case to Mark Forstater, the film producer of Monty Python and the Holy Grail, over royalties for the derivative work Spamalot. They owed a combined £800,000 in legal fees and back royalties to Forstater. They proposed a reunion show to pay their legal bill.[131]
 On 19 November 2013, a new reunion was reported, following months of "secret talks".[132] The original plan was for a live, one-off stage show at the O2 Arena in London on 1 July 2014, with "some of Monty Python's greatest hits, with modern, topical, Pythonesque twists" according to a press release.[133][134][135] The tickets for this show went on sale in November 2013 and sold out in just 43 seconds.[136] Nine additional shows were added, all of them at the O2, the last on 20 July.  They have said that their reunion was inspired by South Park creators Trey Parker and Matt Stone, who are massive Monty Python fans.[137]
 Mick Jagger and Charlie Watts featured in a promotional video for the shows: "Who wants to see that again, really? It's a bunch of wrinkly old men trying to relive their youth and make a load of money—the best one died years ago!"[51] Michael Palin stated that the final reunion show on 20 July 2014 would be the last time that the troupe would perform together. It was screened to 2,000 cinemas around the world.[138] Prior to the final night, Idle stated, "It is a world event and that's really quite exciting. It means we're actually going to say goodbye publicly on one show. Nobody ever has the chance to do that. The Beatles didn't get a last good night."[139] The last show was broadcast in the UK on Gold TV and internationally in cinemas by Fathom Events through a Dish Network satellite link.[140]
 Graham Chapman was originally a medical student, joining the Footlights at Cambridge. He completed his medical training and was legally entitled to practise as a doctor. Chapman is best remembered for the lead roles in Holy Grail, as King Arthur, and Life of Brian, as Brian Cohen. He died of metastatic throat cancer on 4 October 1989. At Chapman's memorial service, Cleese delivered an irreverent eulogy that included all the euphemisms for being dead from the "Dead Parrot" sketch, which they had written; and was also the first person to say "fuck" at a British memorial service. Chapman's comedic fictional memoir, A Liar's Autobiography: Volume VI, was adapted into an animated 3D film in 2012.[141]
 John Cleese is the oldest Python. He met his future Python writing partner, Chapman, in Cambridge. Outside of Python, he is best known for setting up the Video Arts group and for the sitcom Fawlty Towers (co-written with Connie Booth, whom Cleese met during work on Python and to whom he was married for a decade). In Fawlty Towers Cleese starred as hotel owner Basil Fawlty, and received the 1980 British Academy Television Award for Best Entertainment Performance.[142] Cleese has also co-authored several books on psychology and wrote the screenplay for the award-winning A Fish Called Wanda, in which he starred with Michael Palin, and was nominated for the Academy Award for Best Original Screenplay.[143]
 Terry Gilliam, an American by birth, is the only member of the troupe of non-British origin.[144] He started off as an animator and strip cartoonist for Harvey Kurtzman's Help! magazine, one issue of which featured Cleese. Moving from the US to England, he animated features for Do Not Adjust Your Set and was then asked by its makers to join them on their next project: Monty Python's Flying Circus. He co-directed Monty Python and the Holy Grail and directed short segments of other Python films (for instance "The Crimson Permanent Assurance", the short film that appears before The Meaning of Life).[145]
 When Monty Python was first formed, two writing partnerships were already in place: Cleese and Chapman, as well as Jones and Palin. That left two in their own corners: Gilliam, operating solo due to the nature of his work, and Eric Idle. Regular themes in Idle's contributions were elaborate wordplay and musical numbers. After Flying Circus, he hosted Saturday Night Live four times in the first five seasons. Idle's initially successful solo career faltered in the 1990s with the failures of his 1993 film Splitting Heirs (written, produced by, and starring him) and 1998's An Alan Smithee Film: Burn Hollywood Burn (in which he starred). He revived his career by returning to the source of his worldwide fame, adapting Monty Python material for other media. Idle wrote the Tony Award-winning musical Spamalot, based on Holy Grail. Following the success of the musical he wrote Not the Messiah, an oratorio derived from the Life of Brian.[146] Representing Monty Python, Idle featured in a one-hour symphony of British Music when he performed at the London 2012 Olympic Games closing ceremony.[147]
 Terry Jones has been described by other members of the team as the "heart" of the operation. Jones had a lead role in maintaining the group's unity and creative independence. Python biographer George Perry has commented that should "[you] speak to him on subjects as diverse as fossil fuels, or Rupert Bear, or mercenaries in the Middle Ages or Modern China ... in a moment you will find yourself hopelessly out of your depth, floored by his knowledge."[148] Many others agree that Jones is characterised by his irrepressible, good-natured enthusiasm. However, Jones' passion often led to prolonged arguments with other group members—in particular Cleese—with Jones often unwilling to back down. Since his major contributions were largely behind the scenes (direction, writing), and he often deferred to the other members of the group as an actor, Jones' importance to Python was often under-rated. However, he does have the legacy of delivering possibly the most famous line in all of Python, as Brian's mother Mandy in Life of Brian, "He's not the Messiah, he's a very naughty boy!", a line voted the funniest in film history on two occasions.[149][150] Jones died on 21 January 2020 from complications of dementia.[151]
 Sir Michael Palin attended Oxford, where he met his Python writing partner Jones. The two also wrote the series Ripping Yarns together. Palin and Jones originally wrote face-to-face, but soon found it was more productive to write apart and then come together to review what the other had written. Therefore, Jones and Palin's sketches tended to be more focused than that of the others, taking one bizarre situation, sticking to it, and building on it. After Flying Circus, Palin hosted Saturday Night Live four times in the first 10 seasons. His comedy output began to decrease in amount following the increasing success of his travel documentaries for the BBC. Palin released a book of diaries from the Python years entitled Michael Palin Diaries 1969–1979, published in 2007. Palin was awarded a knighthood in the 2019 New Year Honours, which was announced by Buckingham Palace in December 2018.[152]
 Several people have been accorded unofficial "associate Python" status over the years. Occasionally such people have been referred to as the 'seventh Python', in a style reminiscent of George Martin (or other associates of the Beatles) being dubbed "the Fifth Beatle".  The two collaborators with the most meaningful and plentiful contributions have been Neil Innes and Carol Cleveland. Both were present and presented as Associate Pythons at the official Monty Python 25th-anniversary celebrations held in Los Angeles in July 1994.[153]
 Neil Innes is the only non-Python besides Douglas Adams to be credited with writing material for Flying Circus. He appeared in sketches and the Python films, as well as performing some of his songs in Monty Python Live at the Hollywood Bowl. He was also a regular stand-in for absent team members on the rare occasions when they recreated sketches. For example, he took the place of Cleese at the Concert for George.[154] Gilliam once noted that if anyone qualified for the title of the seventh Python, it would be Innes. He was one of the creative talents in the off-beat Bonzo Dog Band. He would later portray Ron Nasty of the Rutles and write all of the Rutles' compositions for All You Need Is Cash (1978), a mockumentary film co-directed by Idle. By 2005, a falling out had occurred between Idle and Innes over additional Rutles projects, the results being Innes' critically acclaimed Rutles "reunion" album The Rutles: Archaeology and Idle's straight-to-DVD The Rutles 2: Can't Buy Me Lunch, each undertaken without the other's participation. According to an interview with Idle in the Chicago Tribune in May 2005, his attitude is that Innes and he go back "too far. And no further."[155] Innes died of a heart attack on 29 December 2019 near Toulouse, where he had lived for several years.[156]
 Carol Cleveland was the most important female performer in the Monty Python ensemble, commonly referred to as "the female Python". She was originally hired by producer/director John Howard Davies for just the first five episodes of the Flying Circus. The Pythons then pushed to make Cleveland a permanent recurring performer after producer/director Ian MacNaughton brought in several other actresses who were not as good as she was.[157] Cleveland went on to appear in about two-thirds of the episodes, as well as in all of the Python films, and in most of their stage shows, as well.[158] According to Time, her most recognisable film roles are playing Zoot and Dingo, two maidens in the Castle Anthrax in Holy Grail.[158]
 Cleese's first wife, Connie Booth, appeared as various characters in all four series of Flying Circus. Her most significant role was the "best girl" of the eponymous Lumberjack in "The Lumberjack Song", though this role was sometimes played by Carol Cleveland. Booth appeared in a total of six sketches and also played one-off characters in Python feature films And Now for Something Completely Different and Monty Python and the Holy Grail.[159]
 Douglas Adams was "discovered" by Chapman when a version of Footlights Revue (a 1974 BBC2 television show featuring some of Adams' early work) was performed live in London's West End. In Cleese's absence from the final TV series, the two formed a brief writing partnership, with Adams earning a writing credit in one episode for a sketch called "Patient Abuse". In the sketch—a satire on mind-boggling bureaucracy—a man who had been stabbed by a nurse arrives at his doctor's office bleeding profusely from the stomach, when the doctor makes him fill in numerous senseless forms before he can administer treatment.[160] He also had two cameo appearances in this season. Firstly, in the episode "The Light Entertainment War", Adams shows up in a surgeon's mask (as Dr. Emile Koning, according to the on-screen captions), pulling on gloves, while Palin narrates a sketch that introduces one person after another, and never actually gets started. Secondly, at the beginning of "Mr. Neutron", Adams is dressed in a "pepperpot" outfit and loads a missile onto a cart being driven by Terry Jones, who is calling out for scrap metal ("Any old iron ..."). Adams and Chapman also subsequently attempted a few non-Python projects, including Out of the Trees.[161] He also contributed to a sketch on the soundtrack album for Monty Python and the Holy Grail.
 Other than Carol Cleveland, the only other non-Python to make a significant number of appearances in the Flying Circus was Ian Davidson. He appeared in the first two series of the show, and played over 10 roles. While Davidson is primarily known as a scriptwriter, it is not known if he had any contribution toward the writing of the sketches, as he is only credited as a performer. In total, Davidson is credited as appearing in eight episodes of the show, which is more than any other male actor who was not a Python. Despite this, Davidson did not appear in any Python-related media subsequent to series 2, though footage of him was shown on the documentary Python Night – 30 Years of Monty Python.[162]
 Stand-up comedian Eddie Izzard, a devoted fan of the group, has occasionally stood in for absent members. When the BBC held a "Python Night" in 1999 to celebrate 30 years of the first broadcast of Flying Circus, the Pythons recorded some new material with Izzard standing in for Idle, who had declined to partake in person (he taped a solo contribution from the US). Izzard hosted The Life of Python (1999), a history of the group that was part of Python Night and appeared with them at a festival/tribute in Aspen, Colorado, in 1998 (released on DVD as Live at Aspen). Izzard has said that Monty Python was a significant influence on her style of comedy and Cleese has referred to her as "the lost Python".[163]
 Series director of Flying Circus, Ian MacNaughton, is also regularly associated with the group and made a few on-screen appearances in the show and in the film And Now for Something Completely Different. Apart from Neil Innes, others to contribute musically included Fred Tomlinson and the Fred Tomlinson Singers.[164] They made appearances in songs such as "The Lumberjack Song" as a backup choir. Other contributors and performers for the Pythons included John Howard Davies, John Hughman, Lyn Ashley, Bob Raymond, John Young, Rita Davies, Stanley Mason, Maureen Flanagan, and David Ballantyne.[165][166]
 By the time of Monty Python's 25th anniversary, in 1994, the point was already being made that "the five surviving members had with the passing years begun to occupy an institutional position in the edifice of British social culture that they had once had so much fun trying to demolish".[167] A similar point is made in a 2006 book on the relationship between Python and philosophy: "It is remarkable, after all, not only that the utterly bizarre Monty Python's Flying Circus was sponsored by the BBC in the first place, but that Monty Python itself grew into an institution of enormous cultural influence."[168]
 Ron Devillier, the PBS programming director who put Monty Python's Flying Circus on US television, states, "they brought through a kind of phony baloney surface ethic that we all lived under and shot right through it and split it in half. If you really let it happen, you could laugh at yourself. All the things that they were doing were really funny like playing on our sensibilities and making fun of them in a very funny way, pointing out how pompous we can be and making fun of themselves at the same time."[59] Danny Gallagher of the Dallas Observer writes, "Monty Python has also been good to American comedy. If America's television viewing public had never seen "The Lumberjack Song" or "The Dead Parrot" sketch, we might still be holding up The Sonny & Cher Comedy Hour as a supreme example of cutting-edge TV comedy."[59]
 A self-contained comedy unit responsible for both writing and performing their work, Monty Python's influence on comedy has been compared to the Beatles' influence on music.[4][5][6] Author Neil Gaiman writes, "A strange combination of individuals gave us Python. And you needed those people, just in the same way that with the Beatles you had four talented people, but together you had the Beatles. And I think that's so incredibly true when it comes to Python."[50]
 "Everything I've ever done can be distilled to at least one Python sketch. If comedy had a periodic element table, Python would have more than one atom on it."
 —Mike Myers.[169] Monty Python have been named as being influential to the comedy stylings of a great many people including: Sacha Baron Cohen,[170] David Cross,[171] Rowan Atkinson,[172] Seth MacFarlane,[173] Seth Meyers,[174] Trey Parker,[175] Matt Stone,[176] Vic and Bob,[177] Mike Myers,[72] Russell Brand,[178] Robin Williams,[179] Jerry Seinfeld,[180] Eddie Izzard,[181] and "Weird Al" Yankovic.[182] Matt Groening, creator of The Simpsons, was influenced by Python's "high velocity sense of the absurd and not stopping to explain yourself", and pays tribute through a couch gag used in seasons five and six.[183] Appearing on Monty Python's Best Bits (Mostly), Jim Carrey—who refers to Monty Python as the "Super Justice League of comedy"—recalled the effect on him of Ernest Scribbler (played by Palin) laughing himself to death in "The Funniest Joke in the World" sketch.[184] Nick Park, creator of Wallace and Gromit, was inspired by Gilliam's animation in Monty Python "to be a bit wacky and off the wall."[185] Simon Pegg, co-writer of the Three Flavours Cornetto trilogy of British comedy films (from Shaun of the Dead to The World's End), stated his "love of comedy was hugely informed by Monty Python."[186] Jerry Seinfeld told Parade, "Monty Python was a gigantic influence on me. They were just about silly, funny things that meant nothing, and that's the stuff I love. There's a wonderful childlike freedom in those kinds of things."[180] Monty Python's Flying Circus served as an inspiration for voice actor Rob Paulsen in voicing Pinky from the animated television series Animaniacs and Pinky and the Brain, giving the character "a goofy whack job" of a British accent.[187][188]
 Comedian John Oliver states, "Writing about the importance of Monty Python is basically pointless. Citing them as an influence is almost redundant. It's assumed. This strange group of wildly talented, appropriately disrespectful, hugely imaginative and massively inspirational idiots changed what comedy could be for their generation and for those that followed."[189] On how Python's freeform style influenced sketch comedy, Tina Fey of the US television show Saturday Night Live states, "Sketch endings are overrated. Their key was to do something as long as it was funny and then just stop and do something else."[169] Stephen Merchant, co-creator of The Office with Ricky Gervais, stated, "I don't remember where I got this grand idea that I could somehow be John Cleese. That was my overriding passion from my mid-teens. Cleese had grown up in Weston-Super-Mare, not far from Bristol where I grew up, and he was tall and he was very funny and very British and it's almost like I thought 'well if they want tall people from the west country I can do that.'"[190]
 Among the more visible cultural influences of Monty Python is the inclusion of terms either directly from, or derived from, Monty Python, into the lexicon of the English language.
 The Japanese anime series Girls und Panzer featured the special episode "Survival War!", which referenced the "Spam" sketch,[198] but the word "spam" was censored to avoid legal issue with the Pythons.
 Beyond a dictionary definition, Python terms have entered the lexicon in other ways.
 On St George's Day, 23 April 2007, the cast and creators of Spamalot gathered in Trafalgar Square under the tutelage of the two Terrys (Jones and Gilliam) to set a new record for the world's largest coconut orchestra. They led 5,567 people "clip-clopping" in time to the Python classic, "Always Look on the Bright Side of Life", for the Guinness World Records attempt.[210]
 On 5 October 2019, to mark the 50th anniversary of Monty Python's first show, the "first official Monty Python Guinness world record attempt" tried to break the record for "the largest gathering of people dressed as Gumbys."[211] A recurring character on the show, a Gumby wears a handkerchief on their head, has spectacles, braces, a knitted tank top, and wellington boots. The shirt sleeves and trouser legs are always rolled up, exposing their socks and knees. Dimwitted, their most famous catchphrases are "My brain hurts!" and repeated shouts of "Hello!" and "Sorry!".[212]
 Five Monty Python productions were released as theatrical films:
 Books by Monty Python
 Script books
 Compilations
 Books about Monty Python by Pythons
 Other books about Monty Python


Source: https://en.wikipedia.org/wiki/Monty_Python%E2%80%99s_Flying_Circus
Content: 
 Monty Python's Flying Circus (also known as simply Monty Python) is a British surreal sketch comedy series created by and starring Graham Chapman, John Cleese, Eric Idle, Terry Jones, Michael Palin, and Terry Gilliam, who became known collectively as "Monty Python", or the "Pythons". The first episode was recorded at the BBC on 7 September 1969 and premiered on 5 October on BBC1, with 45 episodes airing over four series from 1969 to 1974, plus two episodes for German TV. A feature film adaptation of several sketches, And Now for Something Completely Different, was released in 1971.
 The series stands out for its use of absurd situations, mixed with risqué and innuendo-laden humour, sight gags, and observational sketches without punchlines. Live-action segments were broken up with animations by Gilliam, often merging with the live action to form segues. The overall format used for the series followed and elaborated upon the style used by Spike Milligan in his groundbreaking series Q..., rather than the traditional sketch show format. The Pythons play the majority of the series's characters themselves, along with supporting cast members including Carol Cleveland (referred to by the team as the unofficial "Seventh Python"), Connie Booth (Cleese's first wife), series producer Ian MacNaughton, Ian Davidson, musician Neil Innes, and Fred Tomlinson and the Fred Tomlinson Singers for musical numbers.[1][2]
 The programme came about as the six Pythons, having met each other through university and in various radio and television programmes in the 1960s, sought to make a new sketch comedy show unlike anything else on British television at the time. Much of the humour in the series' various episodes and sketches targets the idiosyncrasies of British life, especially that of professionals, as well as aspects of politics. Their comedy is often pointedly intellectual, with numerous erudite references to philosophers and literary figures and their works. The team intended their humour to be impossible to categorise, and succeeded so completely that the adjective "Pythonesque" was invented to define it and, later, similar material. However, their humour was not always seen as appropriate for television by the BBC, leading to some censorship during the third series. Cleese left the show following that series, and the remaining Pythons completed a final, shortened fourth series before ending the show.
 The show became very popular in the United Kingdom, and after initially failing to draw an audience in the United States, gained American popularity after PBS member stations began airing it in 1974. The programme's success on both sides of the Atlantic led to the Pythons going on live tours and creating three additional films, while the individual Pythons flourished in solo careers. Monty Python's Flying Circus has become an influential work on comedy as well as in popular culture. The programming language Python was named by Guido van Rossum after the show, and the word spam, for junk email, took its name from a word used in a Monty Python sketch.
 Monty Python's Flying Circus is a sketch comedy show, though it does not adhere to any regular format. The sketches include live-action skits performed by Graham Chapman, John Cleese, Eric Idle, Terry Jones, Michael Palin, and Terry Gilliam, along with animations created by Gilliam, frequently used as linking devices or interstitial between skits. During the first three series, Cleese would be dressed in a tuxedo and introduce the show with the phrase "And Now for Something Completely Different." Afterwards, a long-haired man (called the It's man) played by Michael Palin would run all the way to the camera and say "It's.." which would start the show proper. The show's introductory theme, which varied with each series, was also based on Gilliam's animations and was accompanied by a rendition of "The Liberty Bell" march by John Philip Sousa, as performed by the Band of the Grenadier Guards. The march was first published in 1893;[3] Gilliam chose it as the show's theme because it had fallen into the public domain under the terms of the Berne Convention and United States copyright law, and could thus be used without royalty payments.[4]
 The title Monty Python's Flying Circus was partly the result of the group's reputation at the BBC. Michael Mills, the BBC's Head of Comedy, wanted their name to include the word "circus" because the BBC referred to the six members wandering around the building as a circus, in particular, "Baron Von Took's Circus", after Barry Took, who had brought them to the BBC.[5] The group added "flying" to make it sound less like an actual circus and more like something from World War I. The group was coming up with their name at a time when the 1966 The Royal Guardsmen song Snoopy vs. the Red Baron had been at a peak. Freiherr Manfred von Richthofen, the World War I German flying ace known as The Red Baron, commanded the Jagdgeschwader 1 squadron of planes known as "The Flying Circus".
 The words "Monty Python" were added because they claimed it sounded like a really bad theatrical agent, the sort of person who would have brought them together, with John Cleese suggesting "Python" as something slimy and slithery, and Eric Idle suggesting "Monty".[6] They later explained that the name Monty "made us laugh because Monty to us means Lord Montgomery, our great general of the Second World War".[7]
 The BBC had rejected some other names put forward by the group, including Whither Canada?, The Nose Show, Ow! It's Colin Plint!, A Horse, a Spoon and a Basin, The Toad Elevating Moment and Owl Stretching Time.[6] Several of these titles were later used for individual episodes.
 Compared with many other sketch comedy shows, Flying Circus had fewer recurring characters, many of whom were involved only in titles and linking sequences. Continuity for many of these recurring characters was frequently non-existent from sketch to sketch, with sometimes even the most basic information (such as a character's name) being changed from one appearance to the next.
 The most frequently returning characters on the show include:
 Other characters appearing multiple times include:
 Other returning characters include a married couple, often mentioned but never seen, Ann Haydon-Jones and her husband Pip. In "Election Night Special", Pip has lost a political seat to Engelbert Humperdinck. Several recurring characters are played by different Pythons. Both Palin and Chapman played the insanely violent Police Constable Pan Am. Both Jones and Palin portrayed police sergeant Harry 'Snapper' Organs of Q division. Various historical figures were played by a different cast member in each appearance, such as Mozart (Cleese, then Palin), or Queen Victoria (Jones, then Palin, then all five Pythons in Series 4).
 Some of the Pythons' real-life targets recurred more frequently than others. Reginald Maudling, a contemporary Conservative politician, was singled out for perhaps the most consistent ridicule.[10] Then-Secretary of State for Education and Science, and (well after the programme had ended) Prime Minister Margaret Thatcher, was occasionally mentioned, in particular referring to Thatcher's brain as being in her shin received a hearty laugh from the studio audience[citation needed]. Then-US President Richard Nixon was also frequently mocked, as was Conservative party leader Edward Heath, prime minister for much of the series' run. The British police were also a favourite target, often acting bizarrely, stupidly, or abusing their authority, frequently in drag.
 There were a total of 45 episodes of Monty Python's Flying Circus made across four series.
 Two episodes were produced in German for WDR (Westdeutscher Rundfunk), both titled Monty Python's Fliegender Zirkus, the literal German translation of the English title. While visiting the UK in the early 1970s, German entertainer and TV producer Alfred Biolek caught notice of the Pythons. Excited by their innovative, absurd sketches, he invited them to Germany in 1971 and 1972 to write and act in two special German episodes.
 The first episode, advertised as Monty Python's Fliegender Zirkus: Blödeln für Deutschland ("Monty Python's Flying Circus: Clowning Around for Germany"), was produced in 1971 and performed in German. The second episode, advertised as Monty Python's Fliegender Zirkus: Blödeln auf die feine englische Art ("Monty Python's Flying Circus: Clowning Around in the Distinguished English Way"), produced in 1972, was recorded in English and dubbed into German for its broadcast in Germany. The original English recording was transmitted by the BBC in October 1973.
 Prior to the show, the six main cast members had met each other as part of various comedy shows: Jones and Palin were members of The Oxford Revue, while Chapman, Cleese, and Idle were members of Cambridge University's Footlights, and while on tour in the United States, met Gilliam. In various capacities, the six worked on a number of different British radio and television comedy shows from 1964 to 1969 as both writers and on-screen roles. The six began to collaborate on ideas together, blending elements of their previous shows, to devise the premise of a new comedy show which presented a number of skits with minimal common elements, as if it were comedy presented by a stream of consciousness. This was aided through the use of Gilliam's animations to help transition skits from one to the next.[11]
 Although there were few recurring characters, and the six cast members played many diverse roles, each perfected some character traits.
 Graham Chapman often portrayed straight-laced men, of any age or class, frequently authority figures such as military officers, policemen or doctors. His characters could, at any moment, engage in "Pythonesque" maniacal behaviour and then return to their former sobriety.[12] He was also skilled in abuse, which he brusquely delivered in such sketches as "Argument Clinic" and "Flying Lessons". He adopted a dignified demeanour as the leading "straight man" in the Python feature films Holy Grail (King Arthur) and Life of Brian (the title character).[13]
 John Cleese played ridiculous authority figures. Gilliam claims that Cleese is the funniest of the Pythons in drag, as he barely needs to be dressed up to look hilarious, with his square chin and 6' 5" (196 cm) frame (see the "Mr. and Mrs. Git" sketch).[citation needed] Cleese also played intimidating maniacs, such as an instructor in the "Self-Defence Against Fresh Fruit" sketch. His character Mr. Praline, the put-upon consumer, featured in some of the most popular sketches, most famously in "Dead Parrot".[14] One star turn that proved most memorable among Python fans was "The Ministry of Silly Walks", where he worked for the eponymous government department. The sketch displays the notably tall and loose-limbed Cleese's physicality in a variety of silly walks. Despite its popularity, particularly among American fans, Cleese himself particularly disliked the sketch, feeling that many of the laughs it generated were cheap and that no balance was provided by what could have been the true satirical centrepoint.[citation needed] Another of his trademarks is his over-the-top delivery of abuse, particularly his screaming "You bastard!"
 Cleese often played foreigners with ridiculous accents, especially Frenchmen, most of the time with Palin. Sometimes this extended to the use of actual French or German (such as "The Funniest Joke in the World", "Mr. Hilter", or "La Marche Futile" at the end of "The Ministry of Silly Walks"), but still with a very heavy accent (or impossible to understand, as for example Hilter's speech).
 Many Python sketches were linked together by the cut-out animations of Terry Gilliam, including the opening titles featuring the iconic giant foot that became a symbol of all that was 'Pythonesque'.[15] Gilliam's unique visual style was characterised by sudden, dramatic movements and deliberate mismatches of scale, set in surrealist landscapes populated by engravings of large buildings with elaborate architecture, grotesque Victorian gadgets, machinery, and people cut from old Sears Roebuck catalogues. Gilliam added airbrush illustrations and many familiar pieces of art. All of these elements were combined in incongruous ways to obtain new and humorous meanings.
 The surreal nature of the series allowed Gilliam's animation to go off on bizarre, imaginative tangents, features that were impossible to produce live-action at the time. Some running gags derived from these animations were a giant hedgehog named Spiny Norman who appeared over the tops of buildings shouting, "Dinsdale!", further petrifying the paranoid Dinsdale Piranha; and The Foot of Cupid, the giant foot that suddenly squashed things. The latter was appropriated from the figure of Cupid in the Agnolo Bronzino painting Venus, Cupid, Folly and Time[16] and appeared in the opening credits of every series to crush the show's title when it appeared on-screen.
 Notable Gilliam sequences for the show include Conrad Poohs and his Dancing Teeth, the rampage of the cancerous black spot, The Killer Cars and a giant cat that stomps its way through London, destroying everything in its path.
 Initially only hired to be the animator of the series, Gilliam was not thought of (even by himself) as an on-screen performer at first, being American and not very good at the deep and sometimes exaggerated English accent of his fellows. The others felt they owed him something and so he sometimes appeared before the camera, usually in the parts that no one else wanted to play, generally because they required a lot of make-up or involved uncomfortable costumes.[17] The most recurrent of these was The-Knight-Who-Hits-People-With-A-Chicken, a knight in armour who would walk on-set and hit another character on the head with a plucked chicken either to end a sketch or when they said something really corny. Some of Gilliam's other on-screen portrayals included:
 Gilliam soon became distinguished as the go-to member for the most obscenely grotesque characters. This carried over into the Holy Grail film, where Gilliam played King Arthur's hunchbacked page 'Patsy' and the bridgekeeper at the Bridge of Death as well as the 'deaf and mad' jailer in Life of Brian. It has also been claimed that he was originally asked by Terry Jones to play Mr. Creosote in The Meaning of Life, but turned it down.
 Eric Idle is known for his roles as a cheeky, suggestive playboy ("Nudge Nudge"), a variety of pretentious television presenters (such as his over-the-top portrayal of Philip Jenkinson in the segments connecting the "Cheese Shop" and "Salad Days" sketches), a crafty, slick salesman ("Door-to-Door Joke Salesman", "Encyclopedia Salesman") and the merchant who loves to haggle in Monty Python's Life of Brian. He is acknowledged as 'the master of the one-liner' by the other Pythons, along with his ability to deliver extensive, sometimes maniacal monologues with barely a breath, such as in "The Money Programme".[18] He is also considered the best singer/songwriter in the group; for example, he played guitar in several sketches and wrote and performed "Always Look on the Bright Side of Life" from The Life of Brian.[19] Unlike Jones, he often played female characters in a more straightforward way, only altering his voice slightly, as opposed to the falsetto shrieking used by the others. Several times, Idle appeared as upper-class, middle-aged women, such as Rita Fairbanks ("Reenactment of the Battle of Pearl Harbor") and the sexually-repressed Protestant wife in the "Every Sperm is Sacred" sketch, in The Meaning of Life.
 Because he was not from an already-established writing partnership prior to Python, Idle wrote his sketches alone.[20]
 Although all of the Pythons played women, Terry Jones is renowned by the rest to be 'the best Rat-Bag woman in the business'.[citation needed] His portrayal of a middle-aged housewife was louder, shriller, and more dishevelled than that of any of the other Pythons. Examples of this are the "Dead Bishop" sketch, his role as Brian's mother Mandy in Life of Brian, Mrs Linda S-C-U-M in "Mr Neutron" and the café proprietor in "Spam". Also recurring was the upper-class reserved men, in "Nudge, Nudge" and the "It's a Man's Life" sketch, and incompetent authority figures (Harry "Snapper" Organs). He also played the iconic Nude Organist that introduced all of series three. Generally, he deferred to the others as a performer, but proved himself behind the scenes, where he would eventually end up pulling most of the strings.[citation needed] Jones also portrayed the tobacconist in the "Hungarian translation sketch" and the enormously fat and bucket-vomiting Mr. Creosote in Meaning of Life.
 Michael Palin was regarded by the other members of the troupe as the one with the widest range, equally adept as a straight man or wildly over the top character.[citation needed] He portrayed many working-class northerners, often portrayed in a disgusting light: "The Funniest Joke in the World" sketch and the "Every Sperm Is Sacred" segment of Monty Python's The Meaning of Life. In contrast, Palin also played weak-willed, put-upon men such as the husband in the "Marriage Guidance Counsellor" sketch, the boring accountant in the "Vocational Guidance Counsellor" sketch, and the hapless client in the "Argument Clinic". He was equally at home as the indefatigable Cardinal Ximinez of Spain in "The Spanish Inquisition" sketch. Another high-energy character that Palin portrays is the slick TV show host, constantly smacking his lips together and generally being over-enthusiastic ("Blackmail" sketch). In one sketch, he plays the role with an underlying hint of self-revulsion, where he wipes his oily palms on his jacket, makes a disgusted face, then continues. One of his most famous creations[citation needed] was the shopkeeper who attempts to sell useless goods by very weak attempts at being sly and crafty, which are invariably spotted by the customer (often played by Cleese), as in the "Dead Parrot" and "Cheese Shop" sketches. Palin is also well known for his leading role in "The Lumberjack Song".
 Palin also often plays heavy-accented foreigners, mostly French ("La marche futile") or German ("Hitler in Minehead"), usually alongside Cleese. In one of the last episodes, he delivers a full speech, first in English, then in French, then in heavily accented German.
 Of all the Pythons, Palin played the fewest female roles.[citation needed] Among his portrayals of women are Queen Victoria in the "Michael Ellis" episode, Debbie Katzenberg the American in Monty Python's The Meaning of Life, a rural idiot's wife in the "Idiot in rural society" sketch, and an implausible English housewife who is married to Jean-Paul Sartre.
 The first five episodes of the series were produced by John Howard Davies, with Davies serving as studio director, and Ian MacNaughton acting as location director. From the sixth episode onwards, MacNaughton became the producer and sole director on the series. Other regular team members included Hazel Pethig (costumes), Madelaine Gaffney (makeup) and John Horton (video effects designer). Maggie Weston, who worked on both makeup and design, married Gilliam in 1973 and they remain together. The series was primarily filmed in London studios and nearby locations, although location shooting to take in beaches and villages included filming in Somerset, Norwich and the island of Jersey.
 Pre-production of the series had started by April 1969. Documents from the BBC showed that the viability of the show had been threatened around this time when Cleese reminded the BBC that he was still under contract from David Frost's David Paradine Productions, who wanted to co-produce the show. The BBC memos indicated the potential of holding off the show until 1971, when Cleese's contract with Paradine expired, but ultimately the situation was resolved, though the details of these negotiations have been lost.[21]
 The first episode aired on the BBC on Sunday, 5 October 1969, at 10:50 p.m.[21] The BBC had to reassure some of its workers (who were considering going on strike and who thought the show was replacing a late-night, religious/devotional programme) by asserting that it was using the alternative programming to give clergymen time off on their busiest day.[21] The first episode did not fare well in terms of audience, capturing only about 3% of the total UK population, roughly 1.5 million, compared to Dad's Army that had 22% on the Thursday of that same week. In addition to the lowest audience figures for shows during that week, the first episode has had the lowest Appreciation Index for any of the BBC's light entertainment programmes.[22][21] While public reception improved over the course of the first series, certain BBC executives had already conceived a dislike for the show, with some BBC documents describing the show as "disgusting and nihilistic".[22] Some within the BBC had been more upbeat on how the first series had turned out and had congratulated the group accordingly, but a more general dislike for the show had already made an impact on the troupe, with Cleese announcing that he would be unlikely to continue to participate after the making of the second series.[22] Separately, the BBC had to re-edit several of the first series' episodes to remove the personal address and phone number for David Frost that the troupe had included in some sketches.[23]
 The second series, while more popular than the first, further strained relations between the troupe and the BBC. Two of the sketches from the series finale "Royal Episode 13" were called out by BBC executives in a December 1970 meeting: "The Queen Will be Watching" in which the troupe mocks the UK national anthem, and the "Undertakers sketch" which took a comedic turn on how to dispose of the body of a loved one.[22][23] The BBC executives criticised producer MacNaughton for not alerting them to the content prior to airing.[23] According to Palin, via his published diary, the BBC started to censor the programme within the third series following this.[23]
 Cleese remained for the third series but left afterwards. Cleese cited that he was no longer interested in the show, believing most of the material was rehashes of prior skits.[24] He also found it more difficult to work with Chapman, who was struggling with alcoholism.[25] The remaining Pythons, however, went on to produce a shortened fourth series, of which only six episodes were made prior to their decision to end the show prematurely, the final episode being broadcast on 5 December 1974.
 The first cut that the BBC forced on the show was the removal of David Frost's phone number from re-airings of the second episode of the first season, "Sex and Violence", in the sketch "The Mouse Problem". The Pythons had slipped in a real contact number for David Frost to the initial airing, which resulted in numerous viewers bothering him.
 Some material originally recorded went missing later, such as the use of the word "masturbating" in the "Summarize Proust" sketch (which was muted during the first airing, and later cut out entirely) or "What a silly bunt" in the Travel Agent sketch (which featured a character [Idle] who has a speech impediment that makes him pronounce "C"s as "B"s),[26] which was cut before the sketch ever went to air. However, when this sketch was included in the album Monty Python's Previous Record and the Live at the Hollywood Bowl film, the line remained intact. Both sketches were included in the Danish DR K re-airing of all episodes ("Episode 31", aired 1 November 2018, 6:50 pm).[27]
 Some sketches were deleted in their entirety and later recovered. One such sketch is the "Party Political Broadcast (Choreographed)", where a Conservative Party spokesman (Cleese) delivers a party political broadcast before getting up and dancing, being coached by a choreographer (Idle), and being joined by a chorus of spokesmen dancing behind him. The camera passes two Labour Party spokesmen practising ballet, and an animation featuring Edward Heath in a tutu. Once deemed lost, a home-recorded tape of this sketch, captured from a broadcast from Buffalo, New York PBS outlet WNED-TV, turned up on YouTube in 2008.[28] Another high-quality recording of this sketch, broadcast on WTTW in Chicago, has also turned up on YouTube.[29] The Buffalo version can be seen as an extra on the new Region 2/4 eight-disc The Complete Monty Python's Flying Circus DVD set.[citation needed] The Region 1 DVD of Before The Flying Circus, which is included in The Complete Monty Python's Flying Circus Collector's Edition Megaset and Monty Python: The Other British Invasion, also contains the Buffalo version as an extra.[30]
 Another lost sketch is the "Satan" animation following the "Crackpot Religion" piece and the "Cartoon Religion Ltd" animation, and preceding the "How Not To Be Seen" sketch: this had been edited out of the official tape. Six frames of the animation can be seen at the end of the episode, wherein that particular episode is repeated in fast-forward. A black and white 16 mm film print has since turned up (found by a private film collector in the US) showing the animation in its entirety.
 At least two references to cancer were censored, both during the second series. In the sixth episode ("It's A Living" or "School Prizes"), Carol Cleveland's narration of a Gilliam cartoon suddenly has a male voice dub 'gangrene' over the word cancer (although this word was used unedited when the animation appeared in the movie And Now for Something Completely Different; the 2006 special Terry Gilliam's Personal Best uses this audio to restore the censored line). Another reference was removed from the sketch "Conquistador Coffee Campaign", in the eleventh episode "How Not to Be Seen", although a reference to leprosy remained intact. This line has also been recovered from the same 16 mm film print as the above-mentioned "Satan" animation.
 A sketch from Episode 7 of Series 2 (subtitled 'The Attila the Hun Show') featured a parody of Michael Miles, the 1960s TV game show host (played by Cleese), and was introduced as 'Spot The Braincell'. This sketch was deleted shortly afterwards from a repeat broadcast as a mark of respect following Miles' death in February 1971. Also, the controversial "Undertaker" sketch from Episode 13 of the same series was removed by the BBC after negative reviewer response. Both of these sketches have been restored to the official tapes, although the only source for the Undertaker sketch was an NTSC copy of the episode, duplicated before the cut had been made.
 Animation in episode 9 of series 3 was cut out following the initial broadcast. The animation was a parody of a German commercial, and the original owners complained about the music use, so the BBC simply removed part of the animation, and replaced the music with a song from a Python album. Terry Gilliam later complained about the cut, thinking it was because producer Ian McNaughton "just didn't get what it was and he cut it. That was a big mistake."[31]
 Music copyright issues have resulted in at least two cuts. In episode 209, Graham Chapman as a Pepperpot sings "The Girl from Ipanema", but some versions use "Jeanie with the Light Brown Hair", which is public domain. In the bus conductor sketch in season 3 episode 4, a brief parody of "Tonight" from West Side Story was removed. Though it was later determined that this version never even aired on BBC at all, instead was first seen in the American broadcasts.[32] There have also been reports of substituting different performances of classical music in some uses, presumably because of performance royalties.
 A Region 2 DVD release of Series 1–4 was released by Sony Pictures Home Entertainment in 2007. This included certain things which had been cut from the US A&E releases, including the "masturbation" line, but failed to reinstate most of the long-lost sketches and edits. A Blu-ray release of the series featuring every episode restored to its original uncut broadcast length was released by Network for the show's 50th anniversary in 2019.[33]
 Rediscovered sketch Ursula Hitler, once deemed impossible to find, was re-released with the 50th anniversary sets in 2019. Also some of the extra American broadcast material, for instance the original parody of "Tonight" from West Side Story in the bus conductor sketch from season 3 episode 4, were included as deleted scenes.[34]
 At the time of the original broadcasting of Monty Python in the United Kingdom, the BBC used Time-Life Television to distribute its shows in the United States. For Monty Python, Time-Life had been concerned that the show was "too British" in its humour to reach American audiences, and did not opt to bring the programme across.[35] However, the show became a fixture on the Canadian Broadcasting Corporation beginning in the fall of 1970, and hence was also seen in some American markets.[36]
 The Python's first film, And Now for Something Completely Different, a selection of skits from the show released in the UK in 1971 and in the United States in 1972, was not a hit in the USA.[35] During their first North American tour in 1973, the Pythons performed twice on US television, firstly on The Tonight Show, hosted by Joey Bishop, and then on The Midnight Special. The group spoke of how badly the first appearance went down with the audience; Idle described The Tonight Show performance: "We did thirty minutes [thirty minutes' worth of material] in fifteen minutes to no laughs whatsoever. We ran out onto the green grass in Burbank and we lay down and laughed for 15 minutes because it was the funniest thing ever. In America they didn't know what on earth we were talking about."[37]
 Despite the poor reception on their live appearances on American television, the Pythons' American manager, Nancy Lewis, began to push the show herself into the States. In 1974, the PBS member station KERA in Dallas was the first television station in the United States to broadcast episodes of Monty Python's Flying Circus, and is often credited with introducing the programme to American audiences.[38] Many other PBS stations acquired the show, and by 1975, it was often the most popular show on these stations.[35] And Now for Something Completely Different was re-released to American theaters in 1974 and had a much better box office take that time. That would also set the stage for the Pythons' next film, Monty Python and the Holy Grail, released near simultaneously in the UK and the United States in April 1975, to also perform well in American theaters.[37][39] The popularity of Monty Python's Flying Circus helped to open the door for other British television series to make their way into the United States via PBS and its member stations.[40] One notable American fan of Monty Python was singer Elvis Presley. Billy Smith, Presley's cousin noted that during the last few months of Elvis' life in 1977, when Elvis was addicted to prescription drugs and mainly confined to his bedroom at his mansion Graceland, Elvis would sit at his room and chat with Smith for hours about various topics including among other things, Presley's favourite Monty Python sketches.[41]
 With the rise in American popularity, the ABC network acquired rights to show select episodes of Monty Python's Flying Circus in their Wide World of Entertainment showcase in mid 1975. However, ABC re-edited the episodes, thus losing the continuity and flow intended in the originals. When ABC refused to stop treating the series in this way, the Pythons took them to court. Initially the court ruled that their artistic rights had indeed been violated, but it refused to stop the ABC broadcasts. However, on appeal the team gained control over all subsequent US broadcasts of its programmes.[42][35] The case also led to their gaining the master tapes of the series from the BBC, once their original contracts ended at the end of 1980.
 The show also aired on MTV in 1988.[43] Monty Python was part of a two-hour comedy block on Sunday nights that also included another BBC series, The Young Ones.
 In April 2006, Monty Python's Flying Circus returned to non-cable American television directly through PBS. In connection with this, PBS commissioned Monty Python's Personal Best, a six-episode series featuring each Python's favourite sketches, plus a tribute to Chapman, who died in 1989. BBC America has aired the series on a sporadic basis since the mid-2000s, in an extended 40-minute time slot in order to include commercials. IFC acquired the rights to the show in 2009, though not exclusive, as BBC America still airs occasional episodes of the show. IFC also presented a six-part documentary Monty Python: Almost the Truth (The Lawyers Cut), produced by Terry Jones's son Bill.
 The members of Monty Python embarked on a series of stage shows during and after the television series. These mostly consisted of sketches from the series, though they also revived material which predated it. One such sketch was the Four Yorkshiremen sketch, written by Cleese and Chapman with Marty Feldman and Tim Brooke-Taylor, and originally performed for At Last the 1948 Show; the sketch subsequently became part of the live Python repertoire. The shows also included songs from collaborator Neil Innes.
 Recordings of four of these stage shows have subsequently appeared as separate works:
 Graham Chapman and Michael Palin also performed on stage at the Knebworth Festival in 1975 with Pink Floyd.[44]
 In 2005, a troupe of actors headed by Rémy Renoux translated and "adapted" a stage version of Monty Python's Flying Circus into French. Usually the original actors defended their material very closely, but given in this case the "adaptation" and also the translation into French (with subtitles), the group supported this production. The adapted material largely adhered to the original text, primarily deviating when it came to ending a sketch, something the Python members themselves changed many times over the course of their stage performances.[45][46]
Language differences also occur in the lyrics of several songs. For example, "Sit on My Face" (which translated into French would be "Asseyez-vous sur mon visage") becomes "cum in my mouth".[47]
 After the broadcast of the first episode, British newspapers printed brief reviews of the new program. Reviewers had mixed opinions. One wrote that the show was "absurd and frivolous", and that it did not "offer anything very new or exciting".[48] Another described the show as "enjoyably Goonish", saying that not all of the material was "scintillating" but that "there was enough packed into the 30 minutes to raise a few laughs."[49] The Reading Evening Post's columnist was more enthusiastic, calling the show "much-needed comedy" and noting that "The real laughs, for me, came from the crazy cartoon and photo-montage work".
[50]
 As the series continued, reviews became more positive. After the third episode, the Guardian's television columnist described the show as "undoubtedly the high spot of a lot of viewers' weekend", saying the humour was "whacky rather than satiric."[51] A week later, the Observer's reviewer gave the series a "strong recommendation", saying "The material, despite a tendency to prolong a good idea beyond its natural length, is of a high standard, but what lifts the show out of an honourable rut is its extraordinary use of animated cartoons."[52] However this positive view was by no means unanimous. An Evening Standard reviewer complained that "last week it almost crushed my enthusiasm and loyalty forever by transmitting a number of dismal skits that were little more than broad, obvious slapstick."[53]
 Recorded in 1998 as Monty Python Live at Aspen, the group received the American Film Institute Star Award.
 Monty Python's Flying Circus placed fifth on a list of the BFI TV 100, drawn up by the British Film Institute in 2000, and voted for by industry professionals.
 In a list of the 50 Greatest British Sketches released by Channel 4 in 2005, five Monty Python sketches made the list:[54]
 In 2004[55] and 2007, Monty Python's Flying Circus was ranked #5 and #6 on TV Guide's Top Cult Shows Ever.[56]
 Time magazine included the show on its 2007 list of the "100 Best TV Shows of All Time".[57]
 In 2013, the programme was ranked #58 on TV Guide's list of the 60 Best Series of All Time,[58] while the Writers Guild of America ranked it #79 – along with Upstairs, Downstairs, Star Trek: The Next Generation and Alfred Hitchcock Presents – on their list of the 101 Best Written TV Series.[59]
 Douglas Adams, creator of The Hitchhiker's Guide to the Galaxy and co-writer of the "Patient Abuse" sketch, once said "I loved Monty Python's Flying Circus. For years I wanted to be John Cleese, I was most disappointed when I found out the job had been taken."[60]
 Lorne Michaels counts the show as a major influence on his Saturday Night Live sketches.[61] Cleese and Palin re-enacted the Dead Parrot sketch on SNL in 1997.
 The show was a major influence on the Danish cult sketch show Casper & Mandrilaftalen (1999)[62] and Cleese starred in its 50th episode.[63][64]
 In computing, the term spam and the name of the Python programming language[65] are both derived from the series.
 Notes
 Bibliography


Source: https://en.wikipedia.org/wiki/Assoziation_(Psychologie)
Content: Other reasons this message may be displayed:


Source: https://en.wikipedia.org/wiki/Cobra_(Programmiersprache)
Content: Other reasons this message may be displayed:


Source: https://en.wikipedia.org/wiki/Garbage_Collection
Content: Waste collection is a part of the process of waste management. It is the transfer of solid waste from the point of use and disposal to the point of treatment or landfill. Waste collection also includes the curbside collection of recyclable materials that technically are not waste, as part of a municipal landfill diversion program.
 Household waste in economically developed countries will generally be left in waste containers or recycling bins prior to collection by a waste collector using a waste collection vehicle. Waste collection barges are used in some towns, for example in Venice, Italy.
 However, in many developing countries, such as Mexico[1] and Egypt,[2] waste left in bins or bags at the side of the road will not be removed unless residents interact with the waste collectors.
 Mexico City residents must haul their trash to a waste collection vehicle which makes frequent stops around each neighborhood. The waste collectors will indicate their readiness by ringing a distinctive bell and possibly shouting. Residents line up and hand their trash container to the waste collector. A tip may be expected in some neighborhoods.[1] Private contracted waste collectors may circulate in the same neighborhoods as many as five times per day, pushing a cart with a waste container, ringing a bell and shouting to announce their presence. These private contractors are not paid a salary, and survive only on the tips they receive.[1] Later, they meet up with a waste collection vehicle to deposit their accumulated waste.
 The waste collection vehicle will often take the waste to a transfer station where it will be loaded up into a larger vehicle and sent to either a landfill or alternative waste treatment facility.
 Waste collection considerations of waste during different types of waste and size of bins, positioning of the bins, and how often bins are to be serviced. Overfilled bins result in rubbish falling out while being tipped. Hazardous rubbish like empty petrol cans can cause fires igniting other trash when the truck compactor is operating. Bins may be locked or stored in secure areas to avoid having non-paying parties placing rubbish in the bin.[3] The cost of old waste is also a concern in collection of waste across the globe.[4][5]
 If waste collection is not carried out properly, it can lead to environmental pollution. This includes the breeding of animals and insects, and can eventually lead to the spread of disease. Usually waste is burned, however this leads to a bigger problem when it comes to air pollution. This can eventually create health issues to people in the surrounding areas.[6]
 According to USNews,
 The Bureau of Labor Statistics predicts that employment in this industry will increase 16.2 percent, adding 21,600 new jobs, by 2022. The main drivers of this growth include a rise in population, individual income and more people choosing to recycle.[7]

Source: https://en.wikipedia.org/wiki/Unicode
Content: 
 Unicode, formally The Unicode Standard,[note 1] is a text encoding standard maintained by the Unicode Consortium designed to support the use of text written in all of the world's major writing systems. Version 15.1 of the standard[A] defines 149813 characters[3] and 161 scripts used in various ordinary, literary, academic, and technical contexts.
 Many common characters, including numerals, punctuation, and other symbols, are unified within the standard and are not treated as specific to any given writing system. Unicode encodes thousands of emoji, with the continued development thereof conducted by the Consortium as a part of the standard.[4] Moreover, the widespread adoption of Unicode was in large part responsible for the initial popularization of emoji outside of Japan. Unicode is ultimately capable of encoding more than 1.1 million characters.
 Unicode has largely supplanted the previous environment of myriad incompatible character sets, each used within different locales and on different computer architectures. Unicode is used to encode the vast majority of text on the Internet, including most web pages, and relevant Unicode support has become a common consideration in contemporary software development.
 The Unicode character repertoire is synchronized with ISO/IEC 10646, each being code-for-code identical with one another. However, The Unicode Standard is more than just a repertoire within which characters are assigned. To aid developers and designers, the standard also provides charts and reference data, as well as annexes explaining concepts germane to various scripts, providing guidance for their implementation. Topics covered by these annexes include character normalization, character composition and decomposition, collation, and directionality.[5]
 Unicode text is processed and stored as binary data using one of several encodings, which define how to translate the standard's abstracted codes for characters into sequences of bytes. The Unicode Standard itself defines three encodings: UTF-8, UTF-16, and UTF-32, though several others exist. Of these, UTF-8 is the most widely used by a large margin, in part due to its backwards-compatibility with ASCII.
 Unicode was originally designed with the intent of transcending limitations present in all text encodings designed up to that point: each encoding was relied upon for use in its own context, but with no particular expectation of compatibility with any other. Indeed, any two encodings chosen were often totally unworkable when used together, with text encoded in one interpreted as garbage characters by the other. Most encodings had only been designed to facilitate interoperation between a handful of scripts—often primarily between a given script and Latin characters—not between a large number of scripts, and not with all of the scripts supported being treated in a consistent manner.
 The philosophy that underpins Unicode seeks to encode the underlying characters—graphemes and grapheme-like units—rather than graphical distinctions considered mere variant glyphs thereof, that are instead best handled by the typeface, through the use of markup, or by some other means. In particularly complex cases, such as the treatment of orthographical variants in Han characters, there is considerable disagreement regarding which differences justify their own encodings, and which are only graphical variants of other characters.
 At the most abstract level, Unicode assigns a unique number called a code point to each character. Many issues of visual representation—including size, shape, and style—are intended to be up to the discretion of the software actually rendering the text, such as a web browser or word processor. However, partially with the intent of encouraging rapid adoption, the simplicity of this original model has become somewhat more elaborate over time, and various pragmatic concessions have been made over the course of the standard's development.
 The first 256 code points mirror the ISO/IEC 8859-1 standard, with the intent of trivializing the conversion of text already written in Western European scripts. To preserve the distinctions made by different legacy encodings, therefore allowing for conversion between them and Unicode without any loss of information, many characters nearly identical to others, in both appearance and intended function, were given distinct code points. For example, the Halfwidth and Fullwidth Forms block encompasses a full semantic duplicate of the Latin alphabet, because legacy CJK encodings contained both "fullwidth" (matching the width of CJK characters) and "halfwidth" (matching ordinary Latin script) characters.
 The Unicode Bulldog Award is given to people deemed to be influential in Unicode's development, with recipients including Tatsuo Kobayashi, Thomas Milo, Roozbeh Pournader, Ken Lunde, and Michael Everson.[6]
 The origins of Unicode can be traced back to the 1980s, to a group of individuals with connections to Xerox's Character Code Standard (XCCS).[7] In 1987, Xerox employee Joe Becker, along with Apple employees Lee Collins and Mark Davis, started investigating the practicalities of creating a universal character set.[8] With additional input from Peter Fenwick and Dave Opstad,[7] Becker published a draft proposal for an "international/multilingual text character encoding system in August 1988, tentatively called Unicode". He explained that "the name 'Unicode' is intended to suggest a unique, unified, universal encoding".[7]
 In this document, entitled Unicode 88, Becker outlined a scheme using 16-bit characters:[7]
 Unicode is intended to address the need for a workable, reliable world text encoding. Unicode could be roughly described as "wide-body ASCII" that has been stretched to 16 bits to encompass the characters of all the world's living languages. In a properly engineered design, 16 bits per character are more than sufficient for this purpose.
 This design decision was made based on the assumption that only scripts and characters in 'modern' use would require encoding:[7]
 Unicode gives higher priority to ensuring utility for the future than to preserving past antiquities. Unicode aims in the first instance at the characters published in the modern text (e.g. in the union of all newspapers and magazines printed in the world in 1988), whose number is undoubtedly far below 214 = 16,384. Beyond those modern-use characters, all others may be defined to be obsolete or rare; these are better candidates for private-use registration than for congesting the public list of generally useful Unicode.
 In early 1989, the Unicode working group expanded to include Ken Whistler and Mike Kernaghan of Metaphor, Karen Smith-Yoshimura and Joan Aliprand of Research Libraries Group, and Glenn Wright of Sun Microsystems. In 1990, Michel Suignard and Asmus Freytag of Microsoft and NeXT's Rick McGowan had also joined the group. By the end of 1990, most of the work of remapping existing standards had been completed, and a final review draft of Unicode was ready.
 The Unicode Consortium was incorporated in California on 3 January 1991,[9] and the first volume of The Unicode Standard was published that October. The second volume, now adding Han ideographs, was published in June 1992.
 In 1996, a surrogate character mechanism was implemented in Unicode 2.0, so that Unicode was no longer restricted to 16 bits. This increased the Unicode codespace to over a million code points, which allowed for the encoding of many historic scripts, such as Egyptian hieroglyphs, and thousands of rarely used or obsolete characters that had not been anticipated for inclusion in the standard. Among these characters are various rarely used CJK characters—many mainly being used in proper names, making them far more necessary for a universal encoding than the original Unicode architecture envisioned.[10]
 Version 1.0 of Microsoft's TrueType specification, published in 1992, used the name 'Apple Unicode' instead of 'Unicode' for the Platform ID in the naming table.
 The Unicode Consortium is a nonprofit organization that coordinates Unicode's development. Full members include most of the main computer software and hardware companies (and few others) with any interest in text-processing standards, including Adobe, Apple, Google, IBM, Meta (previously as Facebook), Microsoft, Netflix, and SAP.[11]
 Over the years several countries or government agencies have been members of the Unicode Consortium. Presently only the Ministry of Endowments and Religious Affairs (Oman) is a full member with voting rights.[11]
 The Consortium has the ambitious goal of eventually replacing existing character encoding schemes with Unicode and its standard Unicode Transformation Format (UTF) schemes, as many of the existing schemes are limited in size and scope and are incompatible with multilingual environments.
 Unicode currently covers most major writing systems in use today.[12][better source needed]
 As of 2024[update], a total of 161 scripts[13] are included in the latest version of Unicode (covering alphabets, abugidas and syllabaries), although there are still scripts that are not yet encoded, particularly those mainly used in historical, liturgical, and academic contexts. Further additions of characters to the already encoded scripts, as well as symbols, in particular for mathematics and music (in the form of notes and rhythmic symbols), also occur.
 The Unicode Roadmap Committee (Michael Everson, Rick McGowan, Ken Whistler, V.S. Umamaheswaran)[14] maintain the list of scripts that are candidates or potential candidates for encoding and their tentative code block assignments on the Unicode Roadmap[15] page of the Unicode Consortium website. For some scripts on the Roadmap, such as Jurchen and Khitan large script, encoding proposals have been made and they are working their way through the approval process. For other scripts, such as Mayan (besides numbers) and Rongorongo, no proposal has yet been made, and they await agreement on character repertoire and other details from the user communities involved.
 Some modern invented scripts which have not yet been included in Unicode (e.g., Tengwar) or which do not qualify for inclusion in Unicode due to lack of real-world use (e.g., Klingon) are listed in the ConScript Unicode Registry, along with unofficial but widely used Private Use Areas code assignments.
 There is also a Medieval Unicode Font Initiative focused on special Latin medieval characters. Part of these proposals has been already included in Unicode.
 The Script Encoding Initiative,[16] a project run by Deborah Anderson at the University of California, Berkeley was founded in 2002 with the goal of funding proposals for scripts not yet encoded in the standard. The project has become a major source of proposed additions to the standard in recent years.[17]
 The Unicode Consortium together with the ISO have developed a shared repertoire following the initial publication of The Unicode Standard: Unicode and the ISO's Universal Coded Character Set (UCS) use identical character names and code points. However, the Unicode versions do differ from their ISO equivalents in two significant ways.
 While the UCS is a simple character map, Unicode specifies the rules, algorithms, and properties necessary to achieve interoperability between different platforms and languages. Thus, The Unicode Standard includes more information, covering in-depth topics such as bitwise encoding, collation, and rendering. It also provides a comprehensive catalog of character properties, including those needed for supporting bidirectional text, as well as visual charts and reference data sets to aid implementers. Previously, The Unicode Standard was sold as a print volume containing the complete core specification, standard annexes,[note 2] and code charts. However, version 5.0, published in 2006, was the last version printed this way. Starting with version 5.2, only the core specification, published as a print-on-demand paperback, may be purchased.[18] The full text, on the other hand, is published as a free PDF on the Unicode website.
 A practical reason for this publication method highlights the second significant difference between the UCS and Unicode—the frequency with which updated versions are released and new characters added. The Unicode Standard has regularly released annual expanded versions, occasionally with more than one version released in a calendar year and with rare cases where the scheduled release had to be postponed. For instance, in April 2020, a month after version 13.0 was published, the Unicode Consortium announced they had changed the intended release date for version 14.0, pushing it back six months to September 2021 due to the COVID-19 pandemic.
 Unicode 15.1, the latest version, was released on 12 September 2023. It is a minor version update to version 15.0—released on 13 September 2022—which added a total of 4,489 new characters, including two new scripts, an extension to the CJK Unified Ideographs block, and multiple additions to existing blocks. 33 new emoji were added, such as the "wireless" (network) symbol and additional colored hearts.[19][20]
 Thus far, the following versions of The Unicode Standard have been published. Update versions, which do not include any changes to character repertoire, are signified by the third number (e.g., "version 4.0.1") and are omitted in the table below.[21]
 [b]
 [d]
 [e]
 [f]
 [g]
 [h]
 [52]
 The Unicode Consortium normally releases a new version of The Unicode Standard once a year, or occasionally twice a year. Version 16.0, the next major version, is scheduled to be published in 2024, and is projected to include six new scripts (Todhri, Sunuwar, Gurung Khema, Kirat Rai, Garay, and Ol Onal), additional Burmese numerals for Shan and Mon alphabets, additional symbols for legacy computing, and at least six new emoji.[56][57]
 The Unicode Standard defines a codespace:[58] a sequence of integers called code points[59] covering the interval 



[
0
,
17
×

2

16


)


{\displaystyle [0,17\times 2^{16})}

, notated according to the standard as U+0000–U+10FFFF.[60] The codespace is a systematic, architecture-independent representation of The Unicode Standard; actual text is processed as binary data via one of several Unicode encodings, such as UTF-8.[a]
 In this normative notation, the two-character prefix U+ always precedes a written code point,[61] and the code points themselves are written as hexadecimal numbers. At least four hexadecimal digits are always written, with leading zeros prepended as needed. For example, the code point U+00F7 ÷ DIVISION SIGN is padded with two leading zeros, but U+13254 𓉔 EGYPTIAN HIEROGLYPH O004 () is not padded.[62]
 There are a total of (216 − 211 + 220 =) 1112064 valid code points within the codespace. (This number arises from the limitations of the UTF-16 character encoding, which can encode the 216 code points in the range U+0000 through U+FFFF except for the 211 code points in the range U+D800 through U+DFFF, which are used as surrogate pairs to encode the 220 code points in the range U+10000 through U+10FFFF.)
 The Unicode codespace is divided into 17 planes, numbered 0 to 16. Plane 0 is the Basic Multilingual Plane (BMP), and contains the most commonly used characters. All code points in the BMP are accessed as a single code unit in UTF-16 encoding and can be encoded in one, two or three bytes in UTF-8. Code points in planes 1 through 16 (the supplementary planes) are accessed as surrogate pairs in UTF-16 and encoded in four bytes in UTF-8.
 Within each plane, characters are allocated within named blocks of related characters. The size of a block is always a multiple of 16, and is often a multiple of 128, but is otherwise arbitrary. Characters required for a given script may be spread out over several different, potentially disjunct blocks within the codespace.
 Each code point is assigned a classification, listed as the code point's General Category property. Here, at the uppermost level code points are categorized as one of Letter, Mark, Number, Punctuation, Symbol, Separator, or Other. Under each category, each code point is then further subcategorized. In most cases, other properties must be used to adequately describe all the characteristics of any given code point.
 The 1024 points in the range U+D800–U+DBFF are known as high-surrogate code points, and code points in the range U+DC00–U+DFFF (1024 code points) are known as low-surrogate code points. A high-surrogate code point followed by a low-surrogate code point forms a surrogate pair in UTF-16 in order to represent code points greater than U+FFFF. In principle, these code points cannot otherwise be used, though in practice this rule is often ignored, especially when not using UTF-16.
 A small set of code points are guaranteed never to be assigned to characters, although third-parties may make independent use of them at their discretion. There are 66 of these noncharacters: U+FDD0–U+FDEF and the last two code points in each of the 17 planes (e.g. U+FFFE, U+FFFF, U+1FFFE, U+1FFFF, ..., U+10FFFE, U+10FFFF). The set of noncharacters is stable, and no new noncharacters will ever be defined.[63] Like surrogates, the rule that these cannot be used is often ignored, although the operation of the byte order mark assumes that U+FFFE will never be the first code point in a text. The exclusion of surrogates and noncharacters leaves 1111998 code points available for use.
 Private-use code points are considered to be assigned, but they intentionally have no interpretation specified by The Unicode Standard[64] such that any interchange of such code points requires an independent agreement between the sender and receiver as to their interpretation. There are three private-use areas in the Unicode codespace:
 Graphic characters are those defined by The Unicode Standard to have particular semantics, either having a visible glyph shape or representing a visible space. As of Unicode 15.1, there are 149641 graphic characters.
 Format characters are characters that do not have a visible appearance but may have an effect on the appearance or behavior of neighboring characters. For example, U+200C  ZERO WIDTH NON-JOINER and U+200D  ZERO WIDTH JOINER may be used to change the default shaping behavior of adjacent characters (e.g. to inhibit ligatures or request ligature formation). There are 172 format characters in Unicode 15.1.
 65 code points, the ranges U+0000–U+001F and U+007F–U+009F, are reserved as control codes, corresponding to the C0 and C1 control codes as defined in ISO/IEC 6429. U+0089 LINE TABULATION, U+008A LINE FEED, and U+000D CARRIAGE RETURN are widely used in texts using Unicode. In a phenomenon known as mojibake, the C1 code points are improperly decoded according to the Windows-1252 codepage, previously widely used in Western European contexts.
 Together, graphic, format, control code, and private use characters are collectively referred to as assigned characters. Reserved code points are those code points that are valid and available for use, but have not yet been assigned. As of Unicode 15.1, there are 824652 reserved code points.
 The set of graphic and format characters defined by Unicode does not correspond directly to the repertoire of abstract characters representable under Unicode. Unicode encodes characters by associating an abstract character with a particular code point.[65] However, not all abstract characters are encoded as a single Unicode character, and some abstract characters may be represented in Unicode by a sequence of two or more characters. For example, a Latin small letter "i" with an ogonek, a dot above, and an acute accent, which is required in Lithuanian, is represented by the character sequence U+012F; U+0307; U+0301. Unicode maintains a list of uniquely named character sequences for abstract characters that are not directly encoded in Unicode.[66]
 All assigned characters have a unique and immutable name by which they are identified. This immutability has been guaranteed since version 2.0 of The Unicode Standard by its Name Stability policy.[63] In cases where a name is seriously defective and misleading, or has a serious typographical error, a formal alias may be defined that applications are encouraged to use in place of the official character name. For example, U+A015 ꀕ YI SYLLABLE WU has the formal alias YI SYLLABLE ITERATION MARK, and U+FE18 ︘ PRESENTATION FORM FOR VERTICAL RIGHT WHITE LENTICULAR BRAKCET (sic) has the formal alias PRESENTATION FORM FOR VERTICAL RIGHT WHITE LENTICULAR BRACKET.[67]
 Unicode includes a mechanism for modifying characters that greatly extends the supported repertoire of glyphs. This covers the use of combining diacritical marks that may be added after the base character by the user. Multiple combining diacritics may be simultaneously applied to the same character. Unicode also contains precomposed versions of most letter/diacritic combinations in normal use. These make the conversion to and from legacy encodings simpler, and allow applications to use Unicode as an internal text format without having to implement combining characters. For example, é can be represented in Unicode as U+0065 e LATIN SMALL LETTER E followed by U+0301 ◌́ COMBINING ACUTE ACCENT), and equivalently as the precomposed character U+00E9 é LATIN SMALL LETTER E WITH ACUTE. Thus, users often have multiple equivalent ways of encoding the same character. The mechanism of canonical equivalence within The Unicode Standard ensures the practical interchangeability of these equivalent encodings.
 An example of this arises with the Korean alphabet Hangul: Unicode provides a mechanism for composing Hangul syllables from their individual Hangul Jamo subcomponents. However, it also provides 11172 combinations of precomposed syllables made from the most common jamo.
 CJK characters presently only have codes for uncomposable radicals and precomposed forms. Most Han characters have either been intentionally composed from, or reconstructed as compositions of, simpler orthographic elements called radicals, so in principle Unicode could have enabled their composition as it did with Hangul. While this could have greatly reduced the number of required code points, as well as allowing the algorithmic synthesis of many arbitrary new characters, the complexities of character etymologies and the post-hoc nature of radical systems add immense complexity to the proposal. Indeed, attempts to design CJK encodings on the basis of composing radicals have been met with difficulties resulting from the reality that Chinese characters do not decompose as simply or as regularly as Hangul does.
 The CJK Radicals Supplement block is assigned to the range U+2E80–U+2EFF, and the Kangxi radicals are assigned to U+2F00–U+2FDF. The Ideographic Description Sequences block covers the range U+2FF0–U+2FFB, but The Unicode Standard warns against using its characters as an alternate representation for characters encoded elsewhere:
 This process is different from a formal encoding of an ideograph. There is no canonical description of unencoded ideographs; there is no semantic assigned to described ideographs; there is no equivalence defined for described ideographs. Conceptually, ideographic descriptions are more akin to the English phrase "an 'e' with an acute accent on it" than to the character sequence <U+0065, U+0301>. Many scripts, including Arabic and Devanāgarī, have special orthographic rules that require certain combinations of letterforms to be combined into special ligature forms. The rules governing ligature formation can be quite complex, requiring special script-shaping technologies such as ACE (Arabic Calligraphic Engine by DecoType in the 1980s and used to generate all the Arabic examples in the printed editions of The Unicode Standard), which became the proof of concept for OpenType (by Adobe and Microsoft), Graphite (by SIL International), or AAT (by Apple).
 Instructions are also embedded in fonts to tell the operating system how to properly output different character sequences. A simple solution to the placement of combining marks or diacritics is assigning the marks a width of zero and placing the glyph itself to the left or right of the left sidebearing (depending on the direction of the script they are intended to be used with). A mark handled this way will appear over whatever character precedes it, but will not adjust its position relative to the width or height of the base glyph; it may be visually awkward and it may overlap some glyphs. Real stacking is impossible but can be approximated in limited cases (for example, Thai top-combining vowels and tone marks can just be at different heights to start with). Generally, this approach is only effective in monospaced fonts but may be used as a fallback rendering method when more complex methods fail.
 Several subsets of Unicode are standardized: Microsoft Windows since Windows NT 4.0 supports WGL-4 with 657 characters, which is considered to support all contemporary European languages using the Latin, Greek, or Cyrillic script. Other standardized subsets of Unicode include the Multilingual European Subsets:[69] MES-1 (Latin scripts only, 335 characters), MES-2 (Latin, Greek, and Cyrillic 1062 characters)[70] and MES-3A & MES-3B (two larger subsets, not shown here). MES-2 includes every character in MES-1 and WGL-4.
 The standard DIN 91379[71] specifies a subset of Unicode letters, special characters, and sequences of letters and diacritic signs to allow the correct representation of names and to simplify data exchange in Europe. This standard supports all of the official languages of all European Union countries, as well as the German minority languages and the official languages of Iceland, Liechtenstein, Norway, and Switzerland. To allow the transliteration of names in other writing systems to the Latin script according to the relevant ISO standards, all necessary combinations of base letters and diacritic signs are provided.
 Rendering software that cannot process a Unicode character appropriately often displays it as an open rectangle, or as U+FFFD to indicate the position of the unrecognized character. Some systems have made attempts to provide more information about such characters. Apple's Last Resort font will display a substitute glyph indicating the Unicode range of the character, and the SIL International's Unicode fallback font will display a box showing the hexadecimal scalar value of the character.
 Several mechanisms have been specified for storing a series of code points as a series of bytes.
 Unicode defines two mapping methods: the Unicode Transformation Format (UTF) encodings, and the Universal Coded Character Set (UCS) encodings. An encoding maps (possibly a subset of) the range of Unicode code points to sequences of values in some fixed-size range, termed code units. All UTF encodings map code points to a unique sequence of bytes.[72] The numbers in the names of the encodings indicate the number of bits per code unit (for UTF encodings) or the number of bytes per code unit (for UCS encodings and UTF-1). UTF-8 and UTF-16 are the most commonly used encodings. UCS-2 is an obsolete subset of UTF-16; UCS-4 and UTF-32 are functionally equivalent.
 UTF encodings include:
 UTF-8 uses one to four bytes per code point and, being compact for Latin scripts and ASCII-compatible, provides the de facto standard encoding for the interchange of Unicode text. It is used by FreeBSD and most recent Linux distributions as a direct replacement for legacy encodings in general text handling.
 The UCS-2 and UTF-16 encodings specify the Unicode byte order mark (BOM) for use at the beginnings of text files, which may be used for byte-order detection (or byte endianness detection). The BOM, encoded as U+FEFF  BYTE ORDER MARK, has the important property of unambiguity on byte reorder, regardless of the Unicode encoding used; U+FFFE (the result of byte-swapping U+FEFF) does not equate to a legal character, and U+FEFF in places other than the beginning of text conveys the zero-width non-break space (a character with no appearance and no effect other than preventing the formation of ligatures).
 The same character converted to UTF-8 becomes the byte sequence EF BB BF. The Unicode Standard allows the BOM "can serve as a signature for UTF-8 encoded text where the character set is unmarked".[73] Some software developers have adopted it for other encodings, including UTF-8, in an attempt to distinguish UTF-8 from local 8-bit code pages. However RFC 3629, the UTF-8 standard, recommends that byte order marks be forbidden in protocols using UTF-8, but discusses the cases where this may not be possible. In addition, the large restriction on possible patterns in UTF-8 (for instance there cannot be any lone bytes with the high bit set) means that it should be possible to distinguish UTF-8 from other character encodings without relying on the BOM.
 In UTF-32 and UCS-4, one 32-bit code unit serves as a fairly direct representation of any character's code point (although the endianness, which varies across different platforms, affects how the code unit manifests as a byte sequence). In the other encodings, each code point may be represented by a variable number of code units. UTF-32 is widely used as an internal representation of text in programs (as opposed to stored or transmitted text), since every Unix operating system that uses the gcc compilers to generate software uses it as the standard "wide character" encoding. Some programming languages, such as Seed7, use UTF-32 as an internal representation for strings and characters. Recent versions of the Python programming language (beginning with 2.2) may also be configured to use UTF-32 as the representation for Unicode strings, effectively disseminating such encoding in high-level coded software.
 Punycode, another encoding form, enables the encoding of Unicode strings into the limited character set supported by the ASCII-based Domain Name System (DNS). The encoding is used as part of IDNA, which is a system enabling the use of Internationalized Domain Names in all scripts that are supported by Unicode. Earlier and now historical proposals include UTF-5 and UTF-6.
 GB18030 is another encoding form for Unicode, from the Standardization Administration of China. It is the official character set of the People's Republic of China (PRC). BOCU-1 and SCSU are Unicode compression schemes. The April Fools' Day RFC of 2005 specified two parody UTF encodings, UTF-9 and UTF-18.
 Unicode, in the form of UTF-8, has been the most common encoding for the World Wide Web since 2008.[74] It has near-universal adoption, and much of the non-UTF-8 content is found in other Unicode encodings, e.g. UTF-16. As of 2024[update], UTF-8 accounts for on average 97.8% of all web pages (and 987 of the top 1,000 highest-ranked web pages).[75] Although many pages only use ASCII characters to display content, UTF-8 was designed with 8-bit ASCII as a subset and almost no websites now declare their encoding to only be ASCII instead of UTF-8.[76] Over a third of the languages tracked have 100% UTF-8 use.
 All internet protocols maintained by Internet Engineering Task Force, e.g. FTP,[77] have required support for UTF-8 since the publication of RFC 2277 in 1998, which specified that all IETF protocols "MUST be able to use the UTF-8 charset".[78]
 Unicode has become the dominant scheme for the internal processing and storage of text. Although a great deal of text is still stored in legacy encodings, Unicode is used almost exclusively for building new information processing systems. Early adopters tended to use UCS-2 (the fixed-length two-byte obsolete precursor to UTF-16) and later moved to UTF-16 (the variable-length current standard), as this was the least disruptive way to add support for non-BMP characters. The best known such system is Windows NT (and its descendants, 2000, XP, Vista, 7, 8, 10, and 11), which uses UTF-16 as the sole internal character encoding. The Java and .NET bytecode environments, macOS, and KDE also use it for internal representation. Partial support for Unicode can be installed on Windows 9x through the Microsoft Layer for Unicode.
 UTF-8 (originally developed for Plan 9)[79] has become the main storage encoding on most Unix-like operating systems (though others are also used by some libraries) because it is a relatively easy replacement for traditional extended ASCII character sets. UTF-8 is also the most common Unicode encoding used in HTML documents on the World Wide Web.
 Multilingual text-rendering engines which use Unicode include Uniscribe and DirectWrite for Microsoft Windows, ATSUI and Core Text for macOS, and Pango for GTK+ and the GNOME desktop.
 Because keyboard layouts cannot have simple key combinations for all characters, several operating systems provide alternative input methods that allow access to the entire repertoire.
 ISO/IEC 14755,[80] which standardises methods for entering Unicode characters from their code points, specifies several methods. There is the Basic method, where a beginning sequence is followed by the hexadecimal representation of the code point and the ending sequence. There is also a screen-selection entry method specified, where the characters are listed in a table on a screen, such as with a character map program.
 Online tools for finding the code point for a known character include Unicode Lookup[81] by Jonathan Hedley and Shapecatcher[82] by Benjamin Milde. In Unicode Lookup, one enters a search key (e.g. "fractions"), and a list of corresponding characters with their code points is returned. In Shapecatcher, based on Shape context, one draws the character in a box and a list of characters approximating the drawing, with their code points, is returned.
 MIME defines two different mechanisms for encoding non-ASCII characters in email, depending on whether the characters are in email headers (such as the "Subject:"), or in the text body of the message; in both cases, the original character set is identified as well as a transfer encoding. For email transmission of Unicode, the UTF-8 character set and the Base64 or the Quoted-printable transfer encoding are recommended, depending on whether much of the message consists of ASCII characters. The details of the two different mechanisms are specified in the MIME standards and generally are hidden from users of email software.
 The IETF has defined[83][84] a framework for internationalized email using UTF-8, and has updated[85][86][87][88] several protocols in accordance with that framework.
 The adoption of Unicode in email has been very slow.[citation needed] Some East Asian text is still encoded in encodings such as ISO-2022, and some devices, such as mobile phones[citation needed], still cannot correctly handle Unicode data. Support has been improving, however. Many major free mail providers such as Yahoo! Mail, Gmail, and Outlook.com support it.
 All W3C recommendations have used Unicode as their document character set since HTML 4.0. Web browsers have supported Unicode, especially UTF-8, for many years. There used to be display problems resulting primarily from font related issues; e.g. v6 and older of Microsoft Internet Explorer did not render many code points unless explicitly told to use a font that contains them.[89]
 Although syntax rules may affect the order in which characters are allowed to appear, XML (including XHTML) documents, by definition,[90] comprise characters from most of the Unicode code points, with the exception of:
 HTML characters manifest either directly as bytes according to the document's encoding, if the encoding supports them, or users may write them as numeric character references based on the character's Unicode code point. For example, the references &#916;, &#1049;, &#1511;, &#1605;, &#3671;, &#12354;, &#21494;, &#33865;, and &#47568; (or the same numeric values expressed in hexadecimal, with &#x as the prefix) should display on all browsers as Δ, Й, ק ,م, ๗, あ, 叶, 葉, and 말.
 When specifying URIs, for example as URLs in HTTP requests, non-ASCII characters must be percent-encoded.
 Unicode is not in principle concerned with fonts per se, seeing them as implementation choices.[91] Any given character may have many allographs, from the more common bold, italic and base letterforms to complex decorative styles. A font is "Unicode compliant" if the glyphs in the font can be accessed using code points defined in The Unicode Standard.[92] The standard does not specify a minimum number of characters that must be included in the font; some fonts have quite a small repertoire.
 Free and retail fonts based on Unicode are widely available, since TrueType and OpenType support Unicode (and Web Open Font Format (WOFF and WOFF2) is based on those). These font formats map Unicode code points to glyphs, but OpenType and TrueType font files are restricted to 65,535 glyphs. Collection files provide a "gap mode" mechanism for overcoming this limit in a single font file. (Each font within the collection still has the 65,535 limit, however.) A TrueType Collection file would typically have a file extension of ".ttc".
 Thousands of fonts exist on the market, but fewer than a dozen fonts—sometimes described as "pan-Unicode" fonts—attempt to support the majority of Unicode's character repertoire. Instead, Unicode-based fonts typically focus on supporting only basic ASCII and particular scripts or sets of characters or symbols. Several reasons justify this approach: applications and documents rarely need to render characters from more than one or two writing systems; fonts tend to demand resources in computing environments; and operating systems and applications show increasing intelligence in regard to obtaining glyph information from separate font files as needed, i.e., font substitution. Furthermore, designing a consistent set of rendering instructions for tens of thousands of glyphs constitutes a monumental task; such a venture passes the point of diminishing returns for most typefaces.
 Unicode partially addresses the newline problem that occurs when trying to read a text file on different platforms. Unicode defines a large number of characters that conforming applications should recognize as line terminators.
 In terms of the newline, Unicode introduced U+2028  LINE SEPARATOR and U+2029  PARAGRAPH SEPARATOR. This was an attempt to provide a Unicode solution to encoding paragraphs and lines semantically, potentially replacing all of the various platform solutions. In doing so, Unicode does provide a way around the historical platform-dependent solutions. Nonetheless, few if any Unicode solutions have adopted these Unicode line and paragraph separators as the sole canonical line ending characters. However, a common approach to solving this issue is through newline normalization. This is achieved with the Cocoa text system in Mac OS X and also with W3C XML and HTML recommendations. In this approach, every possible newline character is converted internally to a common newline (which one does not really matter since it is an internal operation just for rendering). In other words, the text system can correctly treat the character as a newline, regardless of the input's actual encoding.
 The Ideographic Research Group (IRG) is tasked with advising the Consortium and ISO regarding Han unification, or Unihan, especially the further addition of CJK unified and compatibility ideographs to the repertoire. The IRG is composed of experts from each region that has historically used Chinese characters. However, despite the deliberation within the committee, Han unification has consistently been one of the most contested aspects of The Unicode Standard since the genesis of the project.[93]
 Existing character set standards such as the Japanese JIS X 0208 (encoded by Shift JIS) defined unification criteria, meaning rules for determining when a variant Chinese character is to be considered a handwriting/font difference (and thus unified), versus a spelling difference (to be encoded separately). Unicode's character model for CJK characters was based on the unification criteria used by JIS X 0208, as well as those developed by the Association for a Common Chinese Code in China.[94] Due to the standard's principle of encoding semantic instead of stylistic variants, Unicode has received criticism for not assigning code points to certain rare and archaic kanji variants, possibly complicating processing of ancient and uncommon Japanese names. Since it places particular emphasis on Chinese, Japanese and Korean sharing many characters in common, Han unification is also sometimes perceived as treating the three as the same thing.[95]
 Less-frequently-used alternative encodings exist, often predating Unicode, with character models differing from this paradigm, aimed at preserving the various stylistic differences between regional and/or nonstandard character forms. One example is the TRON Code favored by some users for handling historical Japanese text, though not widely adopted among the Japanese public. Another is the CCCII encoding adopted by library systems in Hong Kong, Taiwan and the United States. These have their own drawbacks in general use, leading to the Big5 encoding (introduced in 1984, four years after CCCII) having become more common than CCCII outside of library systems.[96] Although work at Apple based on Research Libraries Group's CJK Thesaurus, which was used to maintain the EACC variant of CCCII, was one of the direct predecessors of Unicode's Unihan set, Unicode adopted the JIS-style unification model.[94]
 The earliest version of Unicode had a repertoire of fewer than 21,000 Han characters, largely limited to those in relatively common modern usage. As of version 15.1, the standard now encodes more than 97,000 Han characters, and work is continuing to add thousands more—largely historical and dialectal variant characters used throughout the Sinosphere.
 Modern typefaces provide a means to address some of the practical issues in depicting unified Han characters with various regional graphical representations. The 'locl' OpenType table allows a renderer to select a different glyph for each code point based on the text locale.[97] The Unicode variation sequences can also provide in-text annotations for a desired glyph selection; this requires registration of the specific variant in the Ideographic Variation Database.
 If the appropriate glyphs for characters in the same script differ only in the italic, Unicode has generally unified them, as can be seen in the comparison among a set of seven characters' italic glyphs as typically appearing in Russian, traditional Bulgarian, Macedonian, and Serbian texts at right, meaning that the differences are displayed through smart font technology or manually changing fonts. The same OpenType 'locl' technique is used.[98]
 Unicode was designed to provide code-point-by-code-point round-trip format conversion to and from any preexisting character encodings, so that text files in older character sets can be converted to Unicode and then back and get back the same file, without employing context-dependent interpretation. That has meant that inconsistent legacy architectures, such as combining diacritics and precomposed characters, both exist in Unicode, giving more than one method of representing some text. This is most pronounced in the three different encoding forms for Korean Hangul. Since version 3.0, any precomposed characters that can be represented by a combined sequence of already existing characters can no longer be added to the standard to preserve interoperability between software using different versions of Unicode.
 Injective mappings must be provided between characters in existing legacy character sets and characters in Unicode to facilitate conversion to Unicode and allow interoperability with legacy software. Lack of consistency in various mappings between earlier Japanese encodings such as Shift-JIS or EUC-JP and Unicode led to round-trip format conversion mismatches, particularly the mapping of the character JIS X 0208 '～' (1-33, WAVE DASH), heavily used in legacy database data, to either U+FF5E ～ FULLWIDTH TILDE (in Microsoft Windows) or U+301C 〜 WAVE DASH (other vendors).[99]
 Some Japanese computer programmers objected to Unicode because it requires them to separate the use of U+005C \ REVERSE SOLIDUS (backslash) and U+00A5 ¥ YEN SIGN, which was mapped to 0x5C in JIS X 0201, and a lot of legacy code exists with this usage.[100] (This encoding also replaces tilde '~' 0x7E with macron '¯', now 0xAF.) The separation of these characters exists in ISO 8859-1, from long before Unicode.
 Indic scripts such as Tamil and Devanagari are each allocated only 128 code points, matching the ISCII standard. The correct rendering of Unicode Indic text requires transforming the stored logical order characters into visual order and the forming of ligatures (also known as conjuncts) out of components. Some local scholars argued in favor of assignments of Unicode code points to these ligatures, going against the practice for other writing systems, though Unicode contains some Arabic and other ligatures for backward compatibility purposes only.[101][102][103] Encoding of any new ligatures in Unicode will not happen, in part, because the set of ligatures is font-dependent, and Unicode is an encoding independent of font variations. The same kind of issue arose for the Tibetan script in 2003 when the Standardization Administration of China proposed encoding 956 precomposed Tibetan syllables,[104] but these were rejected for encoding by the relevant ISO committee (ISO/IEC JTC 1/SC 2).[105]
 Thai alphabet support has been criticized for its ordering of Thai characters. The vowels เ, แ, โ, ใ, ไ that are written to the left of the preceding consonant are in visual order instead of phonetic order, unlike the Unicode representations of other Indic scripts. This complication is due to Unicode inheriting the Thai Industrial Standard 620, which worked in the same way, and was the way in which Thai had always been written on keyboards. This ordering problem complicates the Unicode collation process slightly, requiring table lookups to reorder Thai characters for collation.[95] Even if Unicode had adopted encoding according to spoken order, it would still be problematic to collate words in dictionary order. E.g., the word แสดง [sa dɛːŋ] "perform" starts with a consonant cluster "สด" (with an inherent vowel for the consonant "ส"), the vowel แ-, in spoken order would come after the ด, but in a dictionary, the word is collated as it is written, with the vowel following the ส.
 Characters with diacritical marks can generally be represented either as a single precomposed character or as a decomposed sequence of a base letter plus one or more non-spacing marks. For example, ḗ (precomposed e with macron and acute above) and ḗ (e followed by the combining macron above and combining acute above) should be rendered identically, both appearing as an e with a macron (◌̄) and acute accent (◌́), but in practice, their appearance may vary depending upon what rendering engine and fonts are being used to display the characters. Similarly, underdots, as needed in the romanization of Indic, will often be placed incorrectly.[citation needed]. Unicode characters that map to precomposed glyphs can be used in many cases, thus avoiding the problem, but where no precomposed character has been encoded, the problem can often be solved by using a specialist Unicode font such as Charis SIL that uses Graphite, OpenType ('gsub'), or AAT technologies for advanced rendering features.
 The Unicode Standard has imposed rules intended to guarantee stability.[106] Depending on the strictness of a rule, a change can be prohibited or allowed. For example, a "name" given to a code point cannot and will not change. But a "script" property is more flexible, by Unicode's own rules. In version 2.0, Unicode changed many code point "names" from version 1. At the same moment, Unicode stated that, thenceforth, an assigned name to a code point would never change. This implies that when mistakes are published, these mistakes cannot be corrected, even if they are trivial (as happened in one instance with the spelling BRAKCET for BRACKET in a character name). In 2006 a list of anomalies in character names was first published, and, as of June 2021, there were 104 characters with identified issues,[107] for example:
 While Unicode defines the script designator (name) to be "Phags_Pa", in that script's character names, a hyphen is added: U+A840 ꡀ PHAGS-PA LETTER KA.[110][111] This, however, is not an anomaly, but the rule: hyphens are replaced by underscores in script designators.[110]
 Unicode has a large number of homoglyphs, many of which look very similar or identical to ASCII letters. Substitution of these can make an identifier or URL that looks correct, but directs to a different location than expected.[112] Additionally, homoglyphs can also be used for manipulating the output of natural language processing (NLP) systems.[113] Mitigation requires disallowing these characters, displaying them differently, or requiring that they resolve to the same identifier;[114] all of this is complicated due to the huge and constantly changing set of characters.[115][116]
 A security advisory was released in 2021 by two researchers, one from the University of Cambridge and the other from the University of Edinburgh, in which they assert that the BiDi marks can be used to make large sections of code do something different from what they appear to do. The problem was named "Trojan Source".[117] In response, code editors started highlighting marks to indicate forced text-direction changes.[118]


Source: https://en.wikipedia.org/wiki/Schl%C3%BCsselwort_(Programmierung)
Content: Other reasons this message may be displayed:


Source: https://en.wikipedia.org/wiki/ABC_(Programmiersprache)
Content: Other reasons this message may be displayed:


Source: https://en.wikipedia.org/wiki/C_(Programmiersprache)
Content: Other reasons this message may be displayed:


Source: https://en.wikipedia.org/wiki/Plug-in
Content: Plug-in, plug in or plugin may refer to:


Source: https://en.wikipedia.org/wiki/Blender_(Software)
Content: Other reasons this message may be displayed:


Source: https://en.wikipedia.org/wiki/Cinema_4D
Content: Cinema 4D is a 3D software suite developed by the German company Maxon.
 As of R21, only a single version of Cinema 4D is available. It replaces all previous variants, including BodyPaint 3D, and includes all features of the past 'Studio' variant. With R21, all binaries were unified. There is no technical difference between commercial, educational, or demo versions. The difference is now only in licensing.
2014 saw the release of Cinema 4D Lite, which came packaged with Adobe After Effects Creative Cloud 2014. "Lite" acts as an introductory version, with many features withheld. This is part of a partnership between the two companies, where a Maxon-produced plug-in, called Cineware, allows any variant to create a seamless workflow with After Effects. The "Lite" variant is dependent on After Effects CC, needing the latter application running to launch, and is only sold as a package component included with AE CS through Adobe.
 Initially, Cinema 4D was developed for Amiga computers in the early 1990s, and the first three versions of the program were available exclusively for that platform. With v4, however, Maxon began to develop the application for Windows and Macintosh computers as well, citing the wish to reach a wider audience and the growing instability of the Amiga market following Commodore's bankruptcy. It was also released for BeOS.[2]
 On Linux, Cinema 4D is available as a commandline rendering version.
 From R12 to R20, Cinema 4D was available in four variants. A core Cinema 4D 'Prime' application, a 'Broadcast' version with additional motion-graphics features, 'Visualize,' which adds functions for architectural design and 'Studio,' which includes all modules.
 From Release 8 until Release 11.5, Cinema 4D had a modular approach to the application, with the ability to expand upon the core application with various modules.  This ended with Release 12, though the functionality of these modules remains in the different flavors of Cinema 4D (Prime, Broadcast, Visualize, Studio)
 The old modules were:
 A number of films and related works have been modeled and rendered in Cinema 4D, including:[3]
 Cinebench is a cross-platform test suite which tests a computer's hardware capabilities. It can be used as a test for Cinema 4D's 3D modeling, animation, motion graphic and rendering performance on multiple CPU cores. The program "target[s] a certain niche and [is] better suited for high-end desktop and workstation platforms".[13]
 Cinebench is commonly used to demonstrate hardware capabilities at tech shows to show a CPU performance,[14] especially by Tech YouTubers and review sites.[15][16]


Source: https://en.wikipedia.org/wiki/GIMP
Content: 
 GNU Image Manipulation Program, commonly known by its acronym GIMP (/ɡɪmp/ GHIMP), is a free and open-source raster graphics editor[4] used for image manipulation (retouching) and image editing, free-form drawing, transcoding between different image file formats, and more specialized tasks. It is extensible by means of plugins, and scriptable. It is not designed to be used for drawing, though some artists and creators have used it in this way.[5]
 GIMP is released under the GPL-3.0-or-later license and is available for Linux, macOS, and Microsoft Windows.[6]
 In 1995, Spencer Kimball and Peter Mattis began developing GIMP—originally named General Image Manipulation Program—as a semester-long project at the University of California, Berkeley for the eXperimental Computing Facility.[7] The acronym was coined first, with the letter G being added to -IMP as a reference to "the gimp" in the scene from the 1994 film Pulp Fiction.[8]
 1996 was the initial public release of GIMP (0.54).[9][10] The editor was quickly adopted and a community of contributors formed. The community began developing tutorials and artwork and sharing better work-flows and techniques.[11]
 In the following year, Kimball and Mattis met with Richard Stallman of the GNU Project while he visited UC Berkeley and asked if they could change General in the application's name to GNU (the name of the operating system created by Stallman), and Stallman approved.[12] The application subsequently formed part of the GNU software collection.[13]
 The first release supported Unix systems, such as Linux, SGI IRIX and HP-UX.[7][14] Since then, GIMP has been ported to other operating systems, including Microsoft Windows (1997, GIMP 1.1)[14] and macOS.
 A GUI toolkit called GTK (at the time known as the GIMP ToolKit) was developed to facilitate the development of GIMP. The development of the GIMP ToolKit has been attributed to Peter Mattis becoming disenchanted with the Motif toolkit GIMP originally used. Motif was used up until GIMP 0.60.[10][15]
 
In recent versions (since the GIMP 2.9 build), the removal of the Lanczos image scaling algorithm, which had been used by GIMP and other image editing programs for many years, in favor of pushing forward the new NoHalo and LoHalo algorithms developed by Nicolas Robidoux, caused some controversy among GIMP users, with some users standing by the change but others expressing their dissatisfaction about it, due to mixed quality results in some image scaling scenarios, leading some users to keep using the older 2.8 version of GIMP simply because it's the last build with Lanczos support, and a few users giving up on using the application altogether as a result. To this day, several users hope to see a future version of GIMP with the Lanczos algorithm added back as an option for image resampling.[16][17][18] GIMP's mascot is called Wilber and was created in GIMP by Tuomas Kuosmanen, known as tigert, on 25 September 1997. Wilber received additional accessories from other GIMP developers, which can be found in the Wilber Construction Kit, included in the GIMP source code as /docs/Wilber_Construction_Kit.xcf.gz.[19]
 GIMP is primarily developed by volunteers as a free and open source software project associated with both the GNU and GNOME projects. Development takes place in a public git source code repository,[20] on public mailing lists and in public chat channels on the GIMPNET IRC network.[21]
 New features are held in public separate source code branches and merged into the main (or development) branch when the GIMP team is sure they won't damage existing functions.[20] Sometimes this means that features that appear complete do not get merged or take months or years before they become available in GIMP.
 GIMP itself is released as source code. After a source code release, installers and packages are made for different operating systems by parties who might not be in contact with the maintainers of GIMP.
 The version number used in GIMP is expressed in a major-minor-micro format, with each number carrying a specific meaning: the first (major) number is incremented only for major developments (and is currently 2). The second (minor) number is incremented with each release of new features, with odd numbers reserved for in-progress development versions and even numbers assigned to stable releases; the third (micro) number is incremented before and after each release (resulting in even numbers for releases, and odd numbers for development snapshots) with any bug fixes subsequently applied and released for a stable version.
 Previously, GIMP applied for several positions in the Google Summer of Code (GSoC).[22][23] From 2006 to 2009 there have been nine GSoC projects that have been listed as successful,[22] although not all successful projects have been merged into GIMP immediately. The healing brush and perspective clone tools and Ruby bindings were created as part of the 2006 GSoC and can be used in version 2.8.0 of GIMP, although there were three other projects that were completed and are later available in a stable version of GIMP; those projects being Vector Layers (end 2008 in 2.8 and master),[24] and a JPEG 2000 plug-in (mid 2009 in 2.8 and master).[25] Several of the GSoC projects were completed in 2008, but have been merged into a stable GIMP release later in 2009 to 2014 for Version 2.8.xx and 2.10.x. Some of them needed some more code work for the master tree.
 Second public Development 2.9-Version was 2.9.4 with many deep improvements after initial Public Version 2.9.2.[26][27] Third Public 2.9-Development version is Version 2.9.6.[28] One of the new features is removing the 4 GB size limit of XCF file.[29][30] Increase of possible threads to 64 is also an important point for modern parallel execution in actual AMD Ryzen and Intel Xeon processors. Version 2.9.8 included many bug fixes and improvements in gradients and clips.[31] Improvements in performance and optimization beyond bug hunting were the development targets for 2.10.0.[32] MacOS Beta is available with Version 2.10.4.[33]
 The next stable version in the roadmap is 3.0 with a GTK3 port.[34] 2.99-Series is the development Series to 3.0. Jehan Pages, the lead developer and maintainer of GIMP, stated that GIMP 3.0's release is tentative for May 2024 and plans to announce the release at the next Libre Graphics Meeting conference.[35]
 GIMP developers meet during the annual Libre Graphics Meeting.[36] Interaction designers from OpenUsability have also contributed to GIMP.[37]
 Some blocker bugs: 6 (as of November 2023[update]) on road to 3.0.[52]
 The current version of GIMP works with numerous operating systems, including Linux, macOS and Windows. Many Linux distributions, such as Fedora Linux[56] and Debian.,[57][58] include GIMP as a part of their desktop operating systems.
 GIMP began to host its own downloads after discontinuing use of SourceForge in 2013.[59] The website later repossessed GIMP's dormant account and hosted advertising-laden versions of GIMP for Windows.[60]
 Lifewire reviewed GIMP favorably in March 2019, writing that "[f]or those who have never experienced Photoshop, GIMP is simply a very powerful image manipulation program," and "[i]f you're willing to invest some time learning it, it can be a very good graphics tool."[61]
 GIMP's fitness for use in professional environments is regularly reviewed; it is often compared to and suggested as a possible replacement for Adobe Photoshop.[62][63]
 GIMP 2.6 was used to create nearly all of the art in Lucas the Game, an independent video game by developer Timothy Courtney. Courtney started development of Lucas the Game in early 2014, and the video game was published in July 2015 for PC and Mac. Courtney explains GIMP is a powerful tool, fully capable of large professional projects, such as video games.[64]
 The single-window mode introduced in GIMP 2.8 was reviewed in 2012 by Ryan Paul of Ars Technica, who noted that it made the user experience feel "more streamlined and less cluttered".[65] Michael Burns, writing for Macworld in 2014, described the single-window interface of GIMP 2.8.10 as a "big improvement".[66]
 In his review of GIMP for ExtremeTech in October 2013, David Cardinal noted that GIMP's reputation of being hard to use and lacking features has "changed dramatically over the last couple years", and that it was "no longer a crippled alternative to Photoshop". He described GIMP's scripting as one of its strengths, but also remarked that some of Photoshop's features –  such as Text, 3D commands, Adjustment Layers and History –  are either less powerful or missing in GIMP. Cardinal favorably described the UFRaw converter for raw images used with GIMP, noting that it still "requires some patience to figure out how to use those more advanced capabilities". Cardinal stated that GIMP is "easy enough to try" despite not having as well developed documentation and help system as those for Photoshop, concluding that it "has become a worthy alternative to Photoshop for anyone on a budget who doesn't need all of Photoshop's vast feature set".[67]
 The user interface has been criticized for being "hard to use".[68]
 Tools used to perform image editing can be accessed via the toolbox, through menus and dialogue windows. They include filters and brushes, as well as transformation, selection, layer and masking tools. GIMP's developers have asserted that it has, or at least aspire to it having, similar functionality to Photoshop, but has a different user interface.[69] Also, as of 2024 and version 2.10, a fundamental and essential difference between GIMP, on one hand, and major commercial software like Photoshop and Serif Affinity Photo, on the other, is that very few of GIMP's editing operations occur as non-destructive edits, unlike the main commercial software.
 There are several ways of selecting colors, including palettes, color choosers and using an eyedropper tool to select a color on the canvas. The built-in color choosers include RGB/HSV/LAB/LCH selector or scales, water-color selector, CMYK selector and a color-wheel selector. Colors can also be selected using hexadecimal color codes, as used in HTML color selection. GIMP has native support for indexed color and RGB color spaces; other color spaces are supported using decomposition, where each channel of the new color space becomes a black-and-white image. CMYK, LAB and HSV (hue, saturation, value) are supported this way.[70][71] Color blending can be achieved using the Blend tool, by applying a gradient to the surface of an image and using GIMP's color modes. Gradients are also integrated into tools such as the brush tool, when the user paints this way the output color slowly changes. There are a number of default gradients included with GIMP; a user can also create custom gradients with tools provided. Gradient plug-ins are also available.
 GIMP selection tools include a rectangular and circular selection tool, free select tool, and fuzzy select tool (also known as magic wand). More advanced selection tools include the select by color tool for selecting contiguous regions of color—and the scissors select tool, which creates selections semi-automatically between areas of highly contrasting colors. GIMP also supports a quick mask mode where a user can use a brush to paint the area of a selection. Visibly this looks like a red colored overlay being added or removed. The foreground select tool is an implementation of Simple interactive object extraction (SIOX), a method used to perform the extraction of foreground elements, such as a person or a tree in focus. The Paths Tool allows a user to create vectors (also known as Bézier curves). Users can use paths to create complex selections, including around natural curves. They can paint (or "stroke") the paths with brushes, patterns, or various line styles. Users can name and save paths for reuse.
 There are many tools that can be used for editing images in GIMP. The more common tools include a paint brush, pencil, airbrush, eraser and ink tools used to create new or blended pixels. The Bucket Fill tool can be used to fill a selection with a color or pattern. The Blend tool can be used to fill a selection with a color gradient. These color transitions can be applied to large regions or smaller custom path selections.
 GIMP also provides "smart" tools that use a more complex algorithm to do things that otherwise would be time-consuming or impossible. These include:
 An image being edited in GIMP can consist of many layers in a stack. The user manual suggests that "A good way to visualize a GIMP image is as a stack of transparencies," where in GIMP terminology, each level (analogous to a transparency) is called a layer.[72] Each layer in an image is made up of several channels. In an RGB image, there are normally 3 or 4 channels, each consisting of a red, green and blue channel. Color sublayers look like slightly different gray images, but when put together they make a complete image. The fourth channel that may be part of a layer is the alpha channel (or layer mask). This channel measures opacity where a whole or part of an image can be completely visible, partially visible or invisible. Each layer has a layer mode that can be set to change the colors in the image.[73]
 Text layers can be created using the text tool, allowing a user to write on an image. Text layers can be transformed in several ways, such as converting them to a path or selection.[74][75]
 GIMP has approximately 150 standard effects and filters, including Drop Shadow, Blur, Motion Blur and Noise.
 GIMP operations can be automated with scripting languages. The Script-Fu is a Scheme-based language implemented using a TinyScheme interpreter built into GIMP.[76] GIMP can also be scripted in Perl,[77][78] Python (Python-Fu),[79][80] or Tcl, using interpreters external to GIMP.[81] New features can be added to GIMP not only by changing program code (GIMP core), but also by creating plug-ins. These are external programs that are executed and controlled by the main GIMP program.[82][83] MathMap is an example of a plug-in written in C.[citation needed]
 There is support for several methods of sharpening and blurring images, including the blur and sharpen tool. The unsharp mask tool is used to sharpen an image selectively – it sharpens only those areas of an image that are sufficiently detailed. The Unsharp Mask tool is considered to give more targeted results for photographs than a normal sharpening filter.[84][85] The Selective Gaussian Blur tool works in a similar way, except it blurs areas of an image with little detail.
 GIMP-ML is an extension for machine learning with 15 filters.[86]
 The Generic Graphics Library (GEGL) was first introduced as part of GIMP on the 2.6 release of GIMP. This initial introduction does not yet exploit all of the capabilities of GEGL; as of the 2.6 release, GIMP can use GEGL to perform high bit-depth color operations; because of this, less information is lost when performing color operations.[87] When GEGL is fully integrated, GIMP will have a higher color bit depth and better non-destructive work-flow. GIMP 2.8.xx supports only 8-bit color, which is much lower than digital cameras, e.g., produce (12-bit or higher). Full support for high bit depth is included with GIMP 2.10. OpenCL enables hardware acceleration for some operations.[88]
 CTX is a new rasterizer for vector graphics in GIMP 3.0. Some simple objects, like lines and circles, can be reduced to vector objects.[89][90]
 GIMP supports importing and exporting with a large number of different file formats.[91] GIMP's native format XCF is designed to store all information GIMP can contain about an image; XCF is named after the eXperimental Computing Facility where GIMP was authored. Import and export capability can be extended to additional file formats by means of plug-ins. XCF file size is extended to more than 4 GB since 2.9.6 and new stable tree 2.10.x.[citation needed]
 Because of the free and open-source nature of GIMP, several forks, variants and derivatives of the computer program have been created to fit the needs of their creators. While GIMP is cross-platform, variants of GIMP may not be. These variants are neither hosted nor linked on the GIMP site. The GIMP site does not host GIMP builds for Windows or Unix-like operating systems either, although it does include a link to a Windows build.
 GIMP's functionality can be extended with plugins. Notable ones include:


Source: https://en.wikipedia.org/wiki/Maya_(Software)
Content: Other reasons this message may be displayed:


Source: https://en.wikipedia.org/wiki/Apache_OpenOffice
Content: 
 Apache OpenOffice (AOO) is an open-source office productivity software suite. It is one of the successor projects of OpenOffice.org and the designated successor of IBM Lotus Symphony.[6] It was a close cousin of LibreOffice, Collabora Online and NeoOffice in 2014. It contains a word processor (Writer), a spreadsheet (Calc), a presentation application (Impress), a drawing application (Draw), a formula editor (Math), and a database management application (Base).[7]
 Apache OpenOffice's default file format is the OpenDocument Format (ODF), an ISO/IEC standard. It can also read and write a wide variety of other file formats, with particular attention to those from Microsoft Office –  although, unlike LibreOffice, it cannot save documents in Microsoft's post-2007 Office Open XML formats, but only import them.[8]
 Apache OpenOffice is developed for Linux, macOS and Windows, with ports to other operating systems. It is distributed under the Apache-2.0 license.[5] The first release was version 3.4.0, on 8 May 2012.[1] The most recent significant feature release was version 4.1, which was made available in 2014. The project has continued to release minor updates that fix bugs, update dictionaries and sometimes include feature enhancements. The most recent maintenance release was 4.1.15 on December 22, 2023.[9]
 Difficulties maintaining a sufficient number of contributors to keep the project viable have persisted for several years. In January 2015 the project reported a lack of active developers and code contributions.[10] There have been continual problems providing timely fixes to security vulnerabilities since 2015.[11][12][13][14] Downloads of the software peaked in 2013 with an average of just under 148,000 per day, compared to about 50,000 in 2019 and 2020.[15]
 After acquiring Sun Microsystems in January 2010, Oracle Corporation continued developing OpenOffice.org and StarOffice, which it renamed Oracle Open Office. In September 2010, the majority[16][17] of outside OpenOffice.org developers left the project[18][19] due to concerns over Sun's, and then Oracle's, management of the project,[20][21] to form The Document Foundation (TDF). TDF released the fork LibreOffice in January 2011,[22] which most Linux distributions soon moved to,[23][24][25][26] including Oracle Linux in 2012.[27][28][29]
 In April 2011, Oracle stopped development of OpenOffice.org[30] and laid off the remaining development team.[31] Its reasons for doing so were not disclosed; some speculate that it was due to the loss of mindshare with much of the community moving to LibreOffice[32] while others suggest it was a commercial decision.[33] In June 2011 Oracle contributed the OpenOffice.org trademarks[34] and source code to the Apache Software Foundation, which Apache re-licensed under the Apache License.[35] IBM, to whom Oracle had contractual obligations concerning the code, appears to have preferred that OpenOffice.org be spun out to the Apache Software Foundation above other options or being abandoned by Oracle.[36][37] Additionally, in March 2012, in the context of donating IBM Lotus Symphony to the Apache OpenOffice project, IBM expressed a preference for permissive licenses, such as the Apache license, over copyleft license.[38] The developer pool for the Apache project was seeded by IBM employees,[39] who, from project inception through to 2015, did the majority of the development.[40][41][42][43][44][45]
 The project was accepted to the Apache Incubator on 13 June 2011,[46] the Oracle code drop was imported on 29 August 2011,[47] Apache OpenOffice 3.4 was released 8 May 2012[1] and Apache OpenOffice graduated as a top-level Apache project on 18 October 2012.[48][49][50]
 IBM donated the Lotus Symphony codebase to the Apache Software Foundation in 2012, and Symphony was deprecated in favour of Apache OpenOffice.[44] Many features and bug fixes, including a reworked sidebar, were merged.[51]  The IAccessible2 screen reader support from Symphony was ported and included in the AOO 4.1 release[6] (April 2014), although its first appearance in an open source software release was as part of LibreOffice 4.2 in January 2014.[52] IBM ceased official participation by the release of AOO 4.1.1.[53]
 In September 2016, OpenOffice's project management committee chair Dennis Hamilton began a discussion of possibly discontinuing the project, after the Apache board had put them on monthly reporting due to the project's ongoing problems handling security issues.[54][55][56]
 By December 2011, the project was being called Apache OpenOffice.org (Incubating);[57] in 2012, the project chose the name Apache OpenOffice,[58] a name used in the 3.4 press release.[1]
 Apache OpenOffice includes OpenSymbol, DejaVu,[59] the Gentium fonts, and the Apache-licensed ChromeOS fonts Arimo (sans serif), Tinos (serif) and Cousine (monospace).[60][61]
 Apache OpenOffice includes OpenOffice Basic, a programming language similar to Microsoft Visual Basic for Applications (VBA). Apache OpenOffice has some Microsoft VBA macro support. OpenOffice Basic is available in Writer, Calc, Draw, Impress and Base.
 Apache OpenOffice obtains its handling of file formats from OpenOffice.org, excluding some which were supported only by copyleft libraries,[60] such as WordPerfect support. There is no definitive list of what formats the program supports other than the program's behaviour.[62]  Notable claimed improvements in file format handling in 4.0 include improved interoperability with Microsoft's 2007 format Office Open XML (DOCX, XLSX, PPTX)[63] — although it cannot write OOXML, only read it to some degree.[8]
 Apache OpenOffice does not bundle a Java virtual machine with the installer, as OpenOffice.org did,[64] although the suite still requires Java for "full functionality".[65]
 Apache OpenOffice 4.1.0 was released for x86 and X86-64 versions of Microsoft Windows XP or later, Linux (32-bit and 64-bit), and Mac OS X 10.4 "Tiger" or later.[66]
 Other operating systems are supported by community ports; completed ports for 3.4.1 included various other Linux platforms, FreeBSD, OS/2 and derivatives like ArcaOS, Solaris SPARC,[67] and ports of 3.4.0 for Mac OS X v10.4–v10.5 PowerPC[68] and Solaris x86.[69]
 Apache OpenOffice does not "release early, release often"; it eschews time-based release schedules, releasing only "when it is ready".[70]
 Apache OpenOffice has lost its initial developer participation. During March 2014 –  March 2015 it had only sixteen developers; the top four (by changesets) were IBM employees,[40] and IBM had ceased official participation by the release of 4.1.1.[53]
 In January 2015, the project reported that it was struggling to attract new volunteers because of a lack of mentoring and was badly in need of contributions from experienced developers.[10] Industry analysts noted the project's inactivity, describing it as "all but stalled"[53] and "dying" and noting its inability to maintain OpenOffice infrastructure[71] or security.[11] Red Hat developer Christian Schaller sent an open letter to the Apache Software Foundation in August 2015 asking them to direct Apache OpenOffice users towards LibreOffice "for the sake of open source and free software",[72] which was widely covered[73][74][75][76][77] and echoed[78][79][80][81] by others.
 The project produced two minor updates in 2017, although there was concern about the potential bugginess of the first of these releases. Patricia Shanahan, the release manager for the previous year's update, noted: "I don't like the idea of changes going out to millions of users having only been seriously examined by one programmer — even if I'm that programmer." Brett Porter, then Apache Software Foundation chairman, asked if the project should "discourage downloads".[82] The next update, released in November 2018, included fixes for regressions introduced in previous releases.[83]
 The Register published an article in October 2018 entitled "Apache OpenOffice, the Schrodinger's app: No one knows if it's dead or alive, no one really wants to look inside", which found there were 141 code committers at the time of publication, compared to 140 in 2014; this was a change from the sustained growth experienced prior to 2014. The article concluded: "Reports of AOO's death appear to have been greatly exaggerated; the project just looks that way because it's moving slowly."[84][85]
 Between October 2014 and July 2015 the project had no release manager.[86] During this period, in April 2015, a known remote code execution security vulnerability in Apache OpenOffice 4.1.1 was announced (CVE-2015-1774), but the project did not have the developers available to release the software fix. Instead, the Apache project published a workaround for users, leaving the vulnerability in the download.[11] Former PMC chair Andrea Pescetti volunteered as release manager in July 2015[87] and version 4.1.2 was released in October 2015.[88]
 It was revealed in October 2016 that 4.1.2 had been distributed with a known security hole (CVE-2016-1513) for nearly a year as the project had not had the development resources to fix it.[89]
 4.1.3 was known to have security issues[90] since at least January 2017, but fixes to them were delayed by an absent release manager for 4.1.4.[91] The Apache Software Foundation January 2017 Board minutes were edited after publication to remove mention of the security issue, which Jim Jagielski of the ASF board claimed would be fixed by May 2017.[13] Fixes were finally released in October 2017.[92] Further unfixed problems showed up in February 2019, with The Register unable to get a response from the developers, although the existing proof-of-concept exploit doesn't work with OpenOffice out-of-the-box.[14]
 Version 4.1.11 was released in October 2021 with a fix for a remote code execution security vulnerability (CVE-2021-33035) that was publicly revealed the previous month.[93] The project had been notified in early May 2021.[94] The security hole had been fixed in LibreOffice since 2014.[95]
 Oracle had improved Draw (adding SVG), Writer (adding ODF 1.2) and Calc in the OpenOffice.org 3.4 beta release (12 April 2011),[110] though it cancelled the project only a few days later.[30]
 Apache OpenOffice 3.4 was released on 8 May 2012.[1][111] It differed from the thirteen-month-older OpenOffice.org 3.4 beta mainly in license-related details.[112] Notably, the project removed both code and fonts which were under licenses unacceptable to Apache.[60][113] Language support was considerably reduced, to 15 languages[1] from 121 in OpenOffice.org 3.3.[114] Java, required for the database application, was no longer bundled with the software.[64] 3.4.1, released 23 August 2012, added five languages back,[96] with a further eight added 30 January 2013.[115]
 Version 4.0 was released 23 July 2013.[116] Features include merging the Symphony code drop, reimplementing the sidebar-style interface from Symphony, improved install, MS Office interoperability enhancements, and performance improvements.[117][118] 4.0.1 added nine new languages.[97]
 Version 4.1 was released in April 2014. Various features lined up for 4.1 include comments on text ranges, IAccessible2, in-place editing of input fields, interactive cropping, importing pictures from files and other improvements.[98] 4.1.1 (released 14 August 2014) fixed critical issues in 4.1.[99] 4.1.2 (released in October 2015)[87] was a bugfix release, with improvements in packaging[10] and removal of the HWP file format support associated with the vulnerability CVE-2015-1774.[119] 4.1.3 (September 2016) had updates to the existing language dictionaries, enhanced build tools for AOO developers, a bug fix for databases on macOS, and a security fix for vulnerability CVE-2016-1513.[120] 4.1.4 contained security fixes.[90] Version 4.1.5 was released in December 2017, containing bug fixes.[101][121]
 As a result of harmful downloads being offered by scammers, the project strongly recommends all downloads be made via its official download page,[122] which is managed off-site by SourceForge. SourceForge reported 30 million downloads for the Apache OpenOffice 3.4 series by January 2013, making it one of SourceForge's top downloads;[123] the project claimed 50 million downloads of Apache OpenOffice 3.4.x as of 15 May 2013, slightly over one year after the release of 3.4.0 (8 May 2012),[124] 85,083,221 downloads of all versions by 1 January 2014,[125] 100 million by April 2014,[126] 130 million by the end of 2014[10] and 200 million by November 2016.[127]
 As of May 2012 (the first million downloads), 87% of downloads via SourceForge were for Windows, 11% for Mac OS X and 2% for Linux;[23] statistics for the first 50 million downloads remained consistent, at 88% Windows, 10% Mac OS X, and 2% Linux.[128]
 Apache OpenOffice is available in the FreeBSD ports tree.[129]
 Derivatives include AndrOpen Office,[130][131] a port for Android, and Office 700 for iOS, both ported by Akikazu Yoshikawa.[132]
 LibreOffice also used some changes from Apache OpenOffice.[133] In 2013, 4.5% of new commits in LibreOffice 4.1 came from Apache contributors;[134] in 2016, only 11 commits from Apache OpenOffice were merged into LibreOffice, representing 0.07% of LibreOffice's commits for the period. LibreOffice earlier rebased its LGPL-3.0-or-later codebase on the Apache OpenOffice 3.4 source code (though it used MPL-2.0, not the Apache-2.0) to allow wider (but still copyleft) licensing under MPL-2.0 and LGPL-3.0-or-later.[135]
 Older versions of NeoOffice included stability fixes from Apache OpenOffice,[136] though NeoOffice 2017 and later versions are based on LibreOffice 4.4 which was released mid-2014.[137]


Source: https://en.wikipedia.org/wiki/LibreOffice
Content: 
 LibreOffice (/ˈliːbrə/)[10] is a free and open-source office productivity software suite, a project of The Document Foundation (TDF). It was forked in 2010 from OpenOffice.org, an open-sourced version of the earlier StarOffice. The LibreOffice suite consists of programs for word processing, creating and editing spreadsheets, slideshows, diagrams, and drawings, working with databases, and composing mathematical formulae. It is available in 115 languages.[8] TDF does not provide support for LibreOffice, but enterprise-focused editions are available from companies in the ecosystem.[11]
 LibreOffice uses the OpenDocument standard as its native file format but supports formats of most other major office suites, including Microsoft Office, through a variety of import and export filters.
 LibreOffice is available for a variety of computing platforms, with official support for Microsoft Windows, macOS and Linux[12] and community builds for many other platforms. Ecosystem partner Collabora uses LibreOffice upstream code and provides apps for Android, iOS, iPadOS and ChromeOS. LibreOffice is the default office suite of the most popular Linux distributions.[13][14][15][16]
 LibreOffice Online is an online office suite that includes the applications Writer, Calc, and Impress and provides an upstream for projects such as commercial Collabora Online.
 It is the most actively developed free and open-source office suite, with approximately 50 times the development activity of Apache OpenOffice, the other major descendant of OpenOffice.org, in 2015.[17]
 The project was announced, and a beta was released on September 28, 2010. In the nine months between January 2011 (the first stable release) and October 2011, LibreOffice was downloaded about 7.5 million times.[18] In 2015, the project claimed 120 million unique downloading addresses from May 2011 to May 2015, excluding Linux distributions, with 55 million of those being from May 2014 to May 2015.[19] The Document Foundation estimates that there are 200 million active LibreOffice users worldwide. Approximately 25% are students and 10% are Linux users; the latter usually find LibreOffice part of their preferred distribution.[20]
 LibreOffice is cross platform software. The Document Foundation developers target Microsoft Windows (IA-32 and x86-64), Linux (IA-32, x86-64, and ARM) and macOS (x86-64 and ARM).[28][29] There are community ports for FreeBSD,[30] NetBSD,[31] OpenBSD and Mac OS X 10.5 PowerPC[32] receive support from contributors to those projects, respectively.[33][34][35] LibreOffice is also installable on OpenIndiana via SFE.[36]
 Historically, predecessors of LibreOffice, dating back to StarOffice 3, have run on Solaris with SPARC CPUs that Sun Microsystems (and later Oracle) made. Unofficial ports of LibreOffice, whose versions are now obsolete, have supported SPARC. Current unofficial ports of LibreOffice 5.2.5 run only on Intel-compatible hardware, up to Solaris 11.
 In 2011, developers announced plans to port LibreOffice both to Android and to iOS.[37] A beta version of a document viewer for Android 4.0 or newer was released in January 2015;[38] in May 2015, LibreOffice Viewer for Android was released with basic editing capabilities.[39] In February 2020, Collabora released its first officially supported version of LibreOffice (branded as Collabora Office) for Android and iOS.[40] In July 2020, Collabora shipped an app, branded Collabora Office, for ChromeOS, as used on the popular Chromebook line of notebook computers as well as other form factors of computers.
 The LibreOffice Impress Remote application for various mobile operating systems allows for remote control of LibreOffice Impress presentations.
 In June 2023, Red Hat announced that it will no longer support LibreOffice on future editions of Red Hat Enterprise Linux in order to focus on Wayland support and other priorities towards workstation users. LibreOffice will still be available via distribution-neutral Flatpak.[41] Starting with LibreOffice 7.6 on Fedora 39, packaging and maintenance of LibreOffice on Fedora Linux will be managed by the Fedora LibreOffice Special Interest Group instead of Red Hat.[42][43]
 LibreOffice Online is the online office suite edition of LibreOffice. It allows for the use of LibreOffice through a web browser by using the canvas element of HTML5. Development was announced at the first LibreOffice Conference in October 2011, and is ongoing.[46] The Document Foundation, IceWarp, and Collabora announced a collaboration to work on its implementation.[47][48] A version of the software was shown in a September 2015 conference,[49] and the UK Crown Commercial Service announced an interest in using the software.[50][51] On 15 December 2015, Collabora, in partnership with ownCloud, released a technical preview of LibreOffice Online branded as Collabora Online Development Edition (CODE).[52] In July 2016 the enterprise version Collabora Online 1.0 was released.[53] The same month, Nextcloud and Collabora partnered to bring CODE to Nextcloud users.[54][55] By October 2016, Collabora had released nine updates to CODE.[56] The first source code release of LibreOffice Online was done with LibreOffice version 5.3 in February 2017.[57][58] In June 2019, CIB software GmbH officially announced its contributions to LibreOffice Online and "LibreOffice Online powered by CIB".[59]
 In October 2020 Collabora announced the move of its work on Collabora Online from The Document Foundation infrastructure to GitHub.[60]
 A detailed 60-page report in June 2015 compared the progress of the LibreOffice project with the related project Apache OpenOffice. It showed that "OpenOffice received about 10% of the improvements LibreOffice did in the period of time studied."[61]
 As its native file format to save documents for all of its applications, LibreOffice uses the Open Document Format for Office Applications (ODF), or OpenDocument, an international standard developed jointly by the International Organization for Standardization (ISO) and the International Electrotechnical Commission (IEC). LibreOffice also supports the file formats of most other major office suites, including Microsoft Office, through a variety of import and export filters.[62][21]
 LibreOffice can use the GStreamer multimedia framework in Linux to render multimedia content such as videos in Impress and other programs.
 Visually, LibreOffice used the large "Tango style" icons that are used for the application shortcuts, quick launch icons, icons for associated files and for the icons found on the toolbar of the LibreOffice programs in the past,[84][85] and used on the toolbars and menus by default. They were later replaced by multiple icon themes to adapt the look and feel of specific desktop environment, such as Colibre for Windows, and Elementary for GNOME.[86]
 LibreOffice also ships with a modified theme which looks native on GTK-based Linux distributions. It also renders fonts via Cairo on Linux distributions; this means that text in LibreOffice is rendered the same as the rest of the Linux desktop.[87]
 With version 6.2, LibreOffice includes a ribbon-style GUI, called Notebookbar, including three different views.[88]
This feature has formerly been included as an experimental feature in LibreOffice 6 (experimental features must be enabled from LibreOffice settings to make the option available in the View menu).[89]
 LibreOffice has a feature similar to WordArt called Fontwork.[90]
 LibreOffice uses HarfBuzz for complex text layout, it was first introduced in 4.1 for Linux and 5.3 for Windows and macOS.[91][83] Fonts with OpenType, Apple Advanced Typography or SIL Graphite features can be switched by either a syntax in the Font Name input box[92] or the Font Features dialog from the Character dialog.
 LibreOffice supports a "hybrid PDF" format, a file in Portable Document Format (PDF) which can be read by any program supporting PDF, but also contains the source document in ODF format, editable in LibreOffice by dragging and dropping.[93]
 The LibreOffice project uses a dual LGPLv3 (or later) / MPL 2.0 license for new contributions to allow the license to be upgraded.[94] Since the core of the OpenOffice.org codebase was donated to the Apache Software Foundation, there is an ongoing effort to get all the code rebased to ease future license updates. At the same time, there were complaints that IBM had not in fact released the Lotus Symphony code as open source, despite having claimed to. It was reported that some LibreOffice developers wanted to incorporate some code parts and bug fixes which IBM already fixed in their OpenOffice fork.[95]
 LibreOffice supports third-party extensions.[96] As of July 2017[update], the LibreOffice Extension Repository lists more than 320 extensions.[97] Another list is maintained by the Apache Software Foundation[98] and another one by the Free Software Foundation.[99] Extensions and scripts for LibreOffice can be written in C++, Java, CLI, Python, and LibreOffice Basic. Interpreters for the latter two are bundled with most LibreOffice installers, so no additional installation is needed. The application programming interface for LibreOffice is called "UNO" and is extensively documented.[100]
 LibreOffice Basic is a programming language similar to Microsoft Visual Basic for Applications (VBA) but based on StarOffice Basic. It is available in Writer, Calc and Base. It is used to write small programs known as "macros", with each macro performing a different task, such as counting the words in a paragraph.[101]
 Members of the OpenOffice.org community who were not Sun Microsystems employees had wanted a more egalitarian form for the OpenOffice.org project for many years; Sun had stated in the original OpenOffice.org announcement in 2000 that the project would eventually be run by a neutral foundation[102] and put forward a more detailed proposal in 2001.[103]
 Ximian and then Novell had maintained the ooo-build patch set, a project led by Michael Meeks, to make the build easier on Linux and due to the difficulty of getting contributions accepted upstream by Sun, even from corporate partners. It tracked the main line of development and was not intended to constitute a fork.[104] It was also the standard build mechanism for OpenOffice.org in most Linux distributions[105] and was contributed to by said distributions.[106]
 In 2007, ooo-build was made available by Novell as a software package called Go-oo (ooo-build had used the go-oo.org domain name as early as 2005[107]), which included many features not included in upstream OpenOffice.org. Go-oo also encouraged outside contributions, with rules similar to those later adopted for LibreOffice.[108]
 Sun's contributions to OpenOffice.org had been declining for some time.[109] They remained reluctant to accept contributions[110] and contributors were upset at Sun releasing OpenOffice.org code to IBM for IBM Lotus Symphony under a proprietary contract, rather than under an open source licence.[111]
 Sun was purchased by Oracle Corporation in early 2010. OpenOffice.org community members were concerned by Oracle's behaviour towards open source software, specifically the Java lawsuit against Google[112] and Oracle's withdrawal of developers,[113] and lack of activity on or visible commitment to OpenOffice.org, as had been noted by industry observers;[114] as Meeks put it in early September 2010, "The news from the Oracle OpenOffice conference was that there was no news."[115] Discussion of a fork started soon after.[116]
 On 28 September 2010, The Document Foundation was announced as the host of LibreOffice, a new derivative of OpenOffice.org. The Document Foundation's initial announcement stated their concerns that Oracle would either discontinue OpenOffice.org, or place restrictions on it as an open source project, as it had on Sun's OpenSolaris.[117][118][119][120]
 LibreOffice 3.3 beta used the ooo-build build infrastructure and the OpenOffice.org 3.3 beta code from Oracle, then adding selected patches from Go-oo.[121] Go-oo was discontinued in favour of LibreOffice. Since the office suite that was branded "OpenOffice.org" in most Linux distributions was in fact Go-oo, most moved immediately to LibreOffice.[122]
 Oracle was invited to become a member of The Document Foundation. However, Oracle demanded that all members of the OpenOffice.org Community Council involved with The Document Foundation step down from the OOo Community Council, claiming a conflict of interest.[123]
 The name "LibreOffice" was picked after researching trademark databases and social media, as well as after checks were made to see if it could be used for URLs in various countries.[124] Oracle rejected requests to donate the OpenOffice.org brand to the project.[125]
 LibreOffice was initially named BrOffice in Brazil. OpenOffice.org had been distributed as BrOffice.org by the BrOffice Centre of Excellence for Free Software because of a trademark issue.[126]
 Oracle announced in April 2011 that it was ending its development of OpenOffice.org and would lay off the majority of its paid developers.[127] In June 2011, Oracle announced that it would donate the OpenOffice.org code and trademark to the Apache Software Foundation,[128] where the project was accepted for a project incubation process within the foundation, thus becoming Apache OpenOffice. In an interview with LWN in May 2011, Ubuntu founder Mark Shuttleworth blamed The Document Foundation for destroying OpenOffice.org because it did not license its code under Oracle's Contributor License Agreement.[129] In opposition to Shuttleworth's view, the former Sun executive Simon Phipps argued in the interview for the same online magazine, that the lay-off was an inevitable business decision by Oracle, not impacted by existence of LibreOffice.[130]
 In March 2015, an LWN.net comparison of LibreOffice with its cousin project Apache OpenOffice concluded that "LibreOffice has won the battle for developer participation".[131]
 This was the last version to support the Windows 2000 operating system.
 New features include:[67]
 New features include:[70]
Writer:
 Calc:
 Math:
 Draw:
 Core and filters:
 Writer:
 Calc:
 Impress:
 Math:
 Core and filters:
 Writer:
 Calc:
 Impress:
 Filters:
 GUI:
 Writer:
 Calc:
 Impress:
 Draw:
 Base:
 Text Layout:
 Filters:
 GUI:
 Online:
 Experimental features:
 Writer:
 Calc:
 Impress & Draw:
 Core:
 Filters:
 Online:
 This was the last version to support the Windows XP and Vista operating system.[161]
 Writer:
 Calc:
 Impress:
 Core and filters:
 GUI:
 Writer:
 Calc:
 Draw:
 Base:
 Core / General:
 Writer:
 Calc:
 Impress and Draw:
 Base:
 BASIC:
 Core / General:
 Filters:
 GUI:
 Writer:
 Calc:
 Impress and Draw:
 Base:
 Chart:
 Math:
 Core / General:
 Filters:
 GUI:
 This version removed support for Firefox personas.
 Writer:
 Calc:
 Impress & Draw
 Base:
 BASIC:
 Core / General:
 GUI:
 Writer:
 Calc:
 Impress & Draw
 Base:
 Math:
 Core / General:
 Filters:
 GUI:
 Writer:
 Calc:
 Impress & Draw:
 General:
 Macro:
 This is the first version added experimental support for Windows ARM64 platform.
 Writer:
 Calc:
 Impress & Draw:
 Math:
 General:
 This is the first version to provide experimental support for Apple Silicon-based ARM Macs.
 Writer:
 Calc:
 Impress & Draw:
 Core / General:
 Writer:
 Calc:
 Impress & Draw:
 Core / General:
 Filters:
 GUI:
 Writer:
 Calc:
 Impress & Draw:
 Core / General:
 Writer:
 Calc:
 Impress & Draw:
 Base:
 Core / General:
 Filters:
 This is the last version support FTP protocol
 Writer:
 Calc:
 Impress:
 Draw:
 Math:
 Core / General:
 GUI:
 Localization:
 In late 2017 The Document Foundation held a competition for the new mascot of LibreOffice. The mascot was to be used primarily by the community, and was not intended to supersede existing logos for the project. Over 300 concepts were submitted before the first evaluation phase.[185]
 The mascot contest was cancelled soon after new submissions stopped being accepted. The Document Foundation cited their lack of clear rules and arguments among community members as their reasoning for cancelling the contest.[186]
 Since March 2014 and version 4.2.2, two different major "released" versions of LibreOffice are available at any time in addition to development versions (numbered release candidates and dated nightly builds).[187] The versions are designated to signal their appropriateness for differing user requirements.[188] Releases are designated by three numbers separated by dots. The first two numbers represent the major version (branch) number, and the final number indicates the bugfix releases made in that series. LibreOffice designates the two release versions as:
 Since January 2024 and version 24.2.0, LibreOffice use calendar-based release numbering scheme.[179]
 LibreOffice uses a time-based release schedule for predictability, rather than a "when it's ready" schedule. New major versions are released around every six months, in January or February and July or August of each year. The initial intention was to release in March and September, to align with the schedule of other free software projects.[189]
Minor bugfix versions of the "fresh" and "still" release branches are released frequently.
 Commercially supported distributions for LibreOffice with service-level agreements are available via partners such as Collabora (marketed as Collabora Office and Collabora Online), CIB (marketed as CIB Office on the Microsoft Store), and Red Hat.[190][191] The three vendors are major corporate contributors to the LibreOffice project.[191]
 As of version 7.1, the open source release of LibreOffice is officially branded as "LibreOffice Community", in order to emphasize that the releases are intended primarily for personal individual use, and are "not targeted at enterprises, and not optimized for their support needs". The Document Foundation states that usage of the community versions in such settings "has had a two-fold negative consequence for the project: a poor use of volunteers' time, as they have to spend their time to solve problems for business that provide nothing in return to the community, and a net loss for ecosystem companies."[191]
 The figure shows the worldwide number of LibreOffice users from 2011 to 2018 in millions. References are in the text.
 2011: The Document Foundation estimated in September 2011, that there were 10 million users worldwide who had obtained LibreOffice via downloads or CD-ROMs. Over 90% of those were on Windows, with another 5% on OS X. LibreOffice is the default office suite for most Linux distributions, and is installed when the operating system is installed or updated. Based on International Data Corporation reckonings for new or updated Linux installations in 2011, The Document Foundation estimated a subtotal of 15 million Linux users. This gave a total estimated user base of 25 million users in 2011.[192] In 2011, the Document Foundation set a target of 200 million users worldwide before the end of 2020.[192]
 2013: In September 2013, after two years, the estimated number of LibreOffice users was 75 million.[193] A million new unique IP addresses check for downloads each week.[194]
 2015: In 2015, LibreOffice was used by 100 million users and 18 governments.[195]
 2016: In August 2016, the number of LibreOffice users was estimated at 120 million.[196]
 2018: The Document Foundation estimated in 2018 that there are 200 million active LibreOffice users worldwide. About 25% of them are students and 10% Linux users (who often automatically receive LibreOffice through their distribution).[197] In comparison, Microsoft Office was used in 2018 by 1.2 billion users.[198]
 LibreOffice has seen various major deployments since its inception:
 2003–2010
 2011
 2012
 2013
 2014
 2015
 2016
 2017
 2018
 2019
 2020
 2021
 Starting in 2011, The Document Foundation has organized the annual LibreOffice Conference, as follows:


Source: https://en.wikipedia.org/wiki/PyMOL
Content: 
 PyMOL is an open source but proprietary[3] molecular visualization system created by Warren Lyford DeLano. It was commercialized initially by DeLano Scientific LLC, which was a private software company dedicated to creating useful tools that become universally accessible to scientific and educational communities. It is currently commercialized by Schrödinger, Inc. As the original software license was a permissive licence, they were able to remove it; new versions are no longer released under the Python license, but under a custom license (granting broad use, redistribution, and modification rights, but assigning copyright to any version to Schrodinger, LLC.),[3] and some of the source code is no longer released.[4] PyMOL can produce high-quality 3D images of small molecules and biological macromolecules, such as proteins. According to the original author, by 2009, almost a quarter of all published images of 3D protein structures in the scientific literature were made using PyMOL.[citation needed]
 PyMOL is one of the few mostly open-source model visualization tools available for use in structural biology. The Py part of the software's name refers to the program having been written in the programming language Python.
 PyMOL uses OpenGL Extension Wrangler Library (GLEW) and FreeGLUT, and can solve Poisson–Boltzmann equations using the Adaptive Poisson Boltzmann Solver.[5] PyMOL used Tk for the GUI widgets and had native Aqua binaries for macOS through Schrödinger, which were replaced with a PyQt user interface on all platforms with the release of version 2.0.[6]
 Early versions of PyMol were released under the Python License. On 1 August 2006, DeLano Scientific adopted a controlled-access download system for precompiled PyMOL builds (including betas) distributed by the company. Access to these executables is now limited to registered users who are paying customers; educational builds are available free to students and teachers. However, most of the current source code continues to be available for free, as are older precompiled builds. While the build systems for other platforms are open, the Windows API (WinAPI, Win32) build system is not, although unofficial Windows binaries are available online.[7] Anyone can either compile an executable from the Python-licensed source code or pay for a subscription to support services to obtain access to precompiled executables.
 On 8 January 2010, Schrödinger, Inc. reached an agreement to acquire PyMOL. The firm assumed development, maintenance, support, and sales of PyMOL, including all then-valid subscriptions. They also continue to actively support the PyMOL open-source community. In 2017, Schrödinger revamped the distribution system to unify the user interface under Qt and the package management under Anaconda, and released it as PyMol v2.[6] This version restricts some new functionalities and adds a watermark to the visualization if used unlicensed beyond the 30-day trial period; the overall license policy is similar to the DeLano system. The source code remains mostly available, this time under a BSD-like license.[8] As with the previous distribution, unofficial Windows binaries in the wheel format are available,[7] and indeed Linux distributions continue to provide their own builds of the open-source code.
 PyMOL applies ball-coloring by element.


Source: https://en.wikipedia.org/wiki/SPSS
Content: SPSS Statistics is a statistical software suite developed by IBM for data management, advanced analytics, multivariate analysis, business intelligence, and criminal investigation. Long produced by SPSS Inc., it was acquired by IBM in 2009. Versions of the software released since 2015 have the brand name IBM SPSS Statistics.
 The software name originally stood for Statistical Package for the Social Sciences (SPSS),[4] reflecting the original market, then later changed to Statistical Product and Service Solutions.[5][6]
 SPSS is a widely used program for statistical analysis in social science.[7] It is also used by market researchers, health researchers, survey companies, government, education researchers, marketing organizations, data miners,[8] and others. The original SPSS manual (Nie, Bent & Hull, 1970)[9] has been described as one of "sociology's most influential books" for allowing ordinary researchers to do their own statistical analysis.[10] In addition to statistical analysis, data management (case selection, file reshaping and creating derived data) and data documentation (a metadata dictionary is stored in the datafile) are features of the base software.
 The many features of SPSS Statistics are accessible via pull-down menus or can be programmed with a proprietary 4GL command syntax language. Command syntax programming has the benefits of reproducible output, simplifying repetitive tasks, and handling complex data manipulations and analyses. Additionally, some complex applications can only be programmed in syntax and are not accessible through the menu structure. The pull-down menu interface also generates command syntax: this can be displayed in the output, although the default settings have to be changed to make the syntax visible to the user. They can also be pasted into a syntax file using the "paste" button present in each menu. Programs can be run interactively or unattended, using the supplied Production Job Facility.
 A "macro" language can be used to write command language subroutines. A Python programmability extension can access the information in the data dictionary and data and dynamically build command syntax programs. This extension, introduced in SPSS 14, replaced the less functional SAX Basic "scripts" for most purposes, although SaxBasic remains available. In addition, the Python extension allows SPSS to run any of the statistics in the free software package R. From version 14 onwards, SPSS can be driven externally by a Python or a VB.NET program using supplied "plug-ins". (From version 20 onwards, these two scripting facilities, as well as many scripts, are included on the installation media and are normally installed by default.)
 SPSS Statistics places constraints on internal file structure, data types, data processing, and matching files, which together considerably simplify programming. SPSS datasets have a two-dimensional table structure, where the rows typically represent cases (such as individuals or households) and the columns represent measurements (such as age, sex, or household income). Only two data types are defined: numeric and text (or "string"). All data processing occurs sequentially case-by-case through the file (dataset). Files can be matched one-to-one and one-to-many, but not many-to-many. In addition to that cases-by-variables structure and processing, there is a separate Matrix session where one can process data as matrices using matrix and linear algebra operations.
 The graphical user interface has two views which can be toggled. The 'Data View' shows a spreadsheet view of the cases (rows) and variables (columns). Unlike spreadsheets, the data cells can only contain numbers or text, and formulas cannot be stored in these cells. The 'Variable View' displays the metadata dictionary, where each row represents a variable and shows the variable name, variable label, value label(s), print width, measurement type, and a variety of other characteristics. Cells in both views can be manually edited, defining the file structure and allowing data entry without using command syntax. This may be sufficient for small datasets. Larger datasets such as statistical surveys are more often created in data entry software, or entered during computer-assisted personal interviewing, by scanning and using optical character recognition and optical mark recognition software, or by direct capture from online questionnaires. These datasets are then read into SPSS.
 SPSS Statistics can read and write data from ASCII text files (including hierarchical files), other statistics packages, spreadsheets and databases. It can also read and write to external relational database tables via ODBC and SQL.
 Statistical output is to a proprietary file format (*.spv file, supporting pivot tables) for which, in addition to the in-package viewer, a stand-alone reader can be downloaded. The proprietary output can be exported to text or Microsoft Word, PDF, Excel, and other formats. Alternatively, output can be captured as data (using the OMS command), as text, tab-delimited text, PDF, XLS, HTML, XML, SPSS dataset or a variety of graphic image formats (JPEG, PNG, BMP and EMF).
 Several variants of SPSS Statistics exist. SPSS Statistics Gradpacks are highly discounted versions sold only to students.[11] SPSS Statistics Server is a version of the software with a client/server architecture.  Add-on packages can enhance the base software with additional features (examples include complex samples, which can adjust for clustered and stratified samples, and custom tables, which can create publication-ready tables). SPSS Statistics is available under either an annual or a monthly subscription license.
 Version 25 of SPSS Statistics launched on August 8, 2017. This added new and advanced statistics, such as random effects solution results (GENLINMIXED), robust standard errors (GLM/UNIANOVA), and profile plots with error bars within the Advanced Statistics and Custom Tables add-on. V25 also includes new Bayesian statistics capabilities, a method of statistical inference, and publication ready charts, such as powerful new charting capabilities, including new default templates and the ability to share with Microsoft Office applications.[12]
 SPSS was released in its first version in 1968 as the Statistical Package for the Social Sciences (SPSS) after being developed by Norman H. Nie, Dale H. Bent, and C. Hadlai Hull. Those principals incorporated as SPSS Inc. in 1975. Early versions of SPSS Statistics were written in Fortran and designed for batch processing on mainframes, including for example IBM and ICL versions, originally using punched cards for data and program input. A processing run read a command file of SPSS commands and either a raw input file of fixed-format data with a single record type, or a 'getfile' of data saved by a previous run. To save precious computer time an 'edit' run could be done to check command syntax without analysing the data. From version 10 (SPSS-X) in 1983, data files could contain multiple record types.
 Prior to SPSS 16.0, different versions of SPSS were available for Windows, Mac OS X and Unix.
 SPSS Statistics version 13.0 for Mac OS X was not compatible with Intel-based Macintosh computers, due to the Rosetta emulation software causing errors in calculations. SPSS Statistics 15.0 for Windows needed a downloadable hotfix to be installed in order to be compatible with Windows Vista.
 From version 16.0, the same version runs under Windows, Mac, and Linux. The graphical user interface is written in Java. The Mac OS version is provided as a Universal binary, making it fully compatible with both PowerPC and Intel-based Mac hardware.
 SPSS Inc announced on July 28, 2009, that it was being acquired by IBM for US$1.2 billion.[19] Because of a dispute about ownership of the name "SPSS", between 2009 and 2010, the product was referred to as PASW (Predictive Analytics SoftWare).[20] As of January 2010, it became "SPSS: An IBM Company". Complete transfer of business to IBM was done by October 1, 2010. By that date, SPSS: An IBM Company ceased to exist. IBM SPSS is now fully integrated into the IBM Corporation, and is one of the brands under IBM Software Group's Business Analytics Portfolio, together with IBM Algorithmics, IBM Cognos and IBM OpenPages.
 Companion software in the "IBM SPSS" family are used for data mining and text analytics (IBM SPSS Modeler), realtime credit scoring services (IBM SPSS Collaboration and Deployment Services), and structural equation modeling (IBM SPSS Amos).
 SPSS Data Collection and SPSS Dimensions were sold in 2015 to UNICOM Systems, Inc., a division of UNICOM Global, and merged into the integrated software suite UNICOM Intelligence (survey design, survey deployment, data collection, data management and reporting).[21][22][23]
 IDA (Interactive Data Analysis)[24] was a software package that originated at what was formerly the National Opinion Research Center (NORC), at the University of Chicago. Initially offered on the HP-2000,[25] somewhat later, under the ownership of SPSS, it was also available on DEC's DECSYSTEM-20.[26] Regression analysis was one of IDA's strong points.[25]
 SCSS was a software product intended for online use of IBM mainframes.
[27]
 Although the "C" was for "conversational", it also represented a distinction regarding how the data was stored: it used a column-oriented rather than a row-oriented (internal) database.[citation needed]
 This gave good interactive response time for the SPSS Conversational Statistical System (SCSS), whose strong point, as with SPSS, was Cross-tabulation.[28]
 In October, 2020 IBM announced the start of an Early Access Program for the "New SPSS Statistics", codenamed Project NX.[29][30] It contains "many of your favorite SPSS capabilities presented in a new easy to use interface, with integrated guidance, multiple tabs, improved graphs and much more".
 In December, 2021, IBM opened up the Early Access Program for the next generation of SPSS Statistics for more users and shared more visuals about it.[31][32]


Source: https://en.wikipedia.org/wiki/QGIS
Content: QGIS is a geographic information system (GIS) software that is free and open-source.[2] QGIS supports Windows, macOS, and Linux.[3] It supports viewing, editing, printing, and analysis of geospatial data in a range of data formats. QGIS was previously also known as Quantum GIS.
 QGIS functions as geographic information system (GIS) software, allowing users to analyze and edit spatial information, in addition to composing and exporting graphical maps.[2] QGIS supports raster, vector, mesh, and point cloud layers.[4] Vector data is stored as either point, line, or polygon features. Multiple formats of raster images are supported, and the software can georeference images.
 QGIS supports shapefiles, personal geodatabases, dxf, MapInfo, PostGIS, and other industry-standard formats.[5] Web services, including Web Map Service and Web Feature Service, are also supported to allow use of data from external sources.[6]
 QGIS integrates with other open-source GIS packages, including PostGIS, GRASS GIS, SAGA GIS, and MapServer.[6] Plugins written in Python or C++ extend QGIS's capabilities. Plugins can geocode using the Google Geocoding API, perform geoprocessing functions similar to those of the standard tools found in ArcGIS, and interface with PostgreSQL/PostGIS, SpatiaLite and MySQL databases.
 QGIS is built on top of, and standard installs include, broadly-used open-source GIS format and projection conversion libraries GDAL and proj.
 Gary Sherman began development of Quantum GIS in early 2002, and it became an incubator project of the Open Source Geospatial Foundation in 2007.[7] Version 1.0 was released in January 2009.[8]
 In 2013, along with release of version 2.0 the name was officially changed from Quantum GIS to QGIS to avoid confusion as both names had been used in parallel.[9])
 Written mainly in C++, QGIS makes extensive use of the Qt library.[6] In addition to Qt, required dependencies of QGIS include GEOS and SQLite. GDAL, GRASS GIS, PostGIS, and PostgreSQL are also recommended, as they provide access to additional data formats.[10]
 As of 2017[update], QGIS is available for multiple operating systems including Mac OS X, Linux, Unix, and Microsoft Windows.[11] There are several third-party apps that allow use of QGIS on mobile devices such as QField (Android, iOS and Windows), Mergin Maps (Android, iOS and Windows) and IntraMaps Roam (Windows).[11]
 QGIS can also be used as a graphical user interface to GRASS. QGIS has a small install footprint on the host file system compared to commercial GISs and generally requires less RAM and processing power; hence it can be used on older hardware or running simultaneously with other applications where CPU power may be limited.[citation needed]
 QGIS is maintained by volunteer developers who regularly release updates and bug fixes. As of 2012[update], developers have translated QGIS into 48 languages and the application is used internationally in academic and professional environments. Several companies offer support and feature development services.[12]
 QGIS enables users to visualize their data using maps, charts, and diagrams while customizing the presentation with a variety of symbology choices. The capabilities for geographical analysis provided by QGIS, including as buffer construction, spatial querying, and geoprocessing. For more complex geographical analysis, users can additionally make use of plugins and algorithms. QGIS also makes it simple to share and publish geospatial data as maps, online services, or print maps in a variety of file formats, such as shapefiles, GeoTIFFs, and KML files.
 In order to prepare printed map with QGIS, Print Layout is used. It can be used for adding multiple map views, labels, legends, etc.
 As a free software application under GNU GPLv2, QGIS can be freely modified to perform different or more specialized tasks. Two examples are the QGIS Browser and QGIS Server applications, which use the same code for data access and rendering, but present different front-end interfaces.[13]
 Many public and private organizations have adopted QGIS, including:
 "LTR" indicates a Long Term Release.  Detailed changelogs are available for releases 2.0 and later.[18]


Source: https://en.wikipedia.org/wiki/KiCad
Content: 
 KiCad (/ˈkiːˌkæd/ KEE-kad[7]) is a free software suite for electronic design automation (EDA). It facilitates the design and simulation of electronic hardware for PCB manufacturing. It features an integrated environment for schematic capture, PCB layout, manufacturing file viewing, ngspice-provided SPICE simulation, and engineering calculation. Tools exist within the package to create bill of materials, artwork, Gerber files, and 3D models of the PCB and its components.
 KiCad was created in 1992 by Jean-Pierre Charras while working at IUT de Grenoble.[8] The name came from the first letters in the name of a company of Jean-Pierre's friend in combination with the term CAD.[9] KiCad originally was a collection of electronics programs intended to be used in conjunction with each other. The main tools were EESchema, PCBnew, a Gerber viewer, and calculator.
 With the price of professionally made printed circuit boards rapidly dropping, hobbyists electronic design became much more popular. As a result, KiCad started gaining significant traction and a larger developer base.
 In 2013 the CERN BE-CO-HT section started contributing resources towards KiCad to help foster open hardware development by helping improve KiCad to be on par with commercial EDA tools.[10] From 2013 until approximately 2018 CERN provided two developers part time to help improve KiCad. Much of the work provided by CERN involved massive refactoring of the code base to give KiCad a better structure to grow and adapt.[11] Help is also provided by organizing donations and fundraisers to help pay for additional contract developers for KiCad, along with sponsoring KiCad's web infrastructure. Well over 1400 hours of developer time has been provided by CERN.[12]
 A major milestone was hit in December 2015 starting with KiCad 4.0.0, the first KiCad release adopting a point release versioning scheme. This was also the first release featuring the more advanced tools implemented by CERN developers.
 KiCad joined the Linux Foundation in November 2019.[13]
 Additionally two lead developers formed a services corporation in 2019 to help provide additional paid development support for KiCad.[14]
 KiCad uses an integrated environment for all of the stages of the design process: Schematic capture, PCB layout, Gerber file generation/visualization, and library editing.
 KiCad is a cross-platform program, written in C++ with wxWidgets to run on FreeBSD, Linux, Microsoft Windows and Mac OS X. Many component libraries are available, and users can add custom components. The custom components can be available on a per-project basis or installed for use in any project. There are also tools to help with importing components from other EDA applications, for instance EAGLE. There are also third party libraries available for KiCad, including SnapEDA,[15] and the Digi-Key KiCad Library.[16] Configuration files are in well documented plain text, which helps with interfacing version control systems, as well as with automated component generation scripts.
 Multiple languages are supported, such as Bulgarian, Catalan, Chinese, Czech, Dutch, English, Finnish, French, German, Greek, Hungarian, Italian, Japanese, Korean, Lithuanian, Polish, Portuguese, Russian, Slovak, Slovene, Spanish, and Swedish.
 The KiCad schematic editor has features including hierarchical schematic sheets, custom symbol creation, electrical rules check (ERC) and integrated ngspice circuit simulation. Schematic symbols are very loosely coupled to circuit board footprints to encourage reuse of footprints and symbols (e.g. a single 0805 footprint can be used for capacitors, resistors, inductors, etc).
 Internally KiCad's PCB editor supports up to 32 copper layers and 32 technical layers. Dimensions are stored with nanometer precision in signed 32-bit integers making the theoretical maximum PCB dimension 231 nm, or approximately 2.14 meters.
 There are a variety of tools available while doing layout through both built in functions and external plugins. Some advanced built in functionality includes a push and shove router, differential and single ended trace length tuning, net hilighting and individual layer dimming, and a highly customizable design rule checking (DRC).
 A number of external tools have been developed following the addition of python scripting integration in to KiCad. A number of plugins exist such as a high quality silkscreen label generators, BOM and assembly viewers, panelization plugins, along with many other plugins.
 A 3D PCB viewing function is based on STEP and VRML models, and the board model can be exported for CAD integration.
 On 12 March 2015 Olimex Ltd,[17] a provider of development tools and embedded device programmers, announced that they have switched from EAGLE to KiCad as their primary EDA tool.[18]


Source: https://en.wikipedia.org/wiki/Programmierparadigma
Content: Other reasons this message may be displayed:


Source: https://en.wikipedia.org/wiki/Strukturierte_Programmierung
Content: Other reasons this message may be displayed:


Source: https://en.wikipedia.org/wiki/Referenzz%C3%A4hlung
Content: Other reasons this message may be displayed:


Source: https://en.wikipedia.org/wiki/Datentyp
Content: Other reasons this message may be displayed:


Source: https://en.wikipedia.org/wiki/Typsicherheit
Content: Other reasons this message may be displayed:


Source: https://en.wikipedia.org/wiki/C%2B%2B
Content: 
 C++ (/ˈsiː plʌs plʌs/, pronounced "C plus plus" and sometimes abbreviated as CPP) is a high-level, general-purpose programming language created by Danish computer scientist Bjarne Stroustrup. First released in 1985 as an extension of the C programming language, it has since expanded significantly over time; as of 1997[update], C++ has object-oriented, generic, and functional features, in addition to facilities for low-level memory manipulation for making things like microcomputers or to make operating systems like Linux or Windows. It is almost always implemented as a compiled language, and many vendors provide C++ compilers, including the Free Software Foundation, LLVM, Microsoft, Intel, Embarcadero, Oracle, and IBM.[14]
 C++ was designed with systems programming and embedded, resource-constrained software and large systems in mind, with performance, efficiency, and flexibility of use as its design highlights.[15] C++ has also been found useful in many other contexts, with key strengths being software infrastructure and resource-constrained applications,[15] including desktop applications, video games, servers (e.g., e-commerce, web search, or databases), and performance-critical applications (e.g., telephone switches or space probes).[16]
 C++ is standardized by the International Organization for Standardization (ISO), with the latest standard version ratified and published by ISO in December 2020 as ISO/IEC 14882:2020 (informally known as C++20).[17] The C++ programming language was initially standardized in 1998 as ISO/IEC 14882:1998, which was then amended by the C++03, C++11, C++14, and C++17 standards. The current C++20 standard supersedes these with new features and an enlarged standard library. Before the initial standardization in 1998, C++ was developed by Stroustrup at Bell Labs since 1979 as an extension of the C language; he wanted an efficient and flexible language similar to C that also provided high-level features for program organization.[18] Since 2012, C++ has been on a three-year release schedule[19] with C++23 as the next planned standard.[20]
 In 1979, Bjarne Stroustrup, a Danish computer scientist, began work on "C with Classes", the predecessor to C++.[21] The motivation for creating a new language originated from Stroustrup's experience in programming for his PhD thesis. Stroustrup found that Simula had features that were very helpful for large software development, but the language was too slow for practical use, while BCPL was fast but too low-level to be suitable for large software development. When Stroustrup started working in AT&T Bell Labs, he had the problem of analyzing the UNIX kernel with respect to distributed computing. Remembering his PhD experience, Stroustrup set out to enhance the C language with Simula-like features.[22] C was chosen because it was general-purpose, fast, portable, and widely used. In addition to C and Simula's influences, other languages influenced this new language, including ALGOL 68, Ada, CLU, and ML.[citation needed]
 Initially, Stroustrup's "C with Classes" added features to the C compiler, Cpre, including classes, derived classes, strong typing, inlining, and default arguments.[23]
 In 1982, Stroustrup started to develop a successor to C with Classes, which he named "C++" (++ being the increment operator in C) after going through several other names. New features were added, including virtual functions, function name and operator overloading, references, constants, type-safe free-store memory allocation (new/delete), improved type checking, and BCPL-style single-line comments with two forward slashes (//). Furthermore, Stroustrup developed a new, standalone compiler for C++, Cfront.
 In 1984, Stroustrup implemented the first stream input/output library. The idea of providing an output operator rather than a named output function was suggested by Doug McIlroy[2] (who had previously suggested Unix pipes).
 In 1985, the first edition of The C++ Programming Language was released, which became the definitive reference for the language, as there was not yet an official standard.[24] The first commercial implementation of C++ was released in October of the same year.[21]
 In 1989, C++ 2.0 was released, followed by the updated second edition of The C++ Programming Language in 1991.[25] New features in 2.0 included multiple inheritance, abstract classes, static member functions, const member functions, and protected members. In 1990, The Annotated C++ Reference Manual was published. This work became the basis for the future standard. Later feature additions included templates, exceptions, namespaces, new casts, and a Boolean type.
 In 1998, C++98 was released, standardizing the language, and a minor update (C++03) was released in 2003.
 After C++98, C++ evolved relatively slowly until, in 2011, the C++11 standard was released, adding numerous new features, enlarging the standard library further, and providing more facilities to C++ programmers. After a minor C++14 update released in December 2014, various new additions were introduced in C++17.[26] After becoming finalized in February 2020,[27] a draft of the C++20 standard was approved on 4 September 2020, and officially published on 15 December 2020.[28][29]
 On January 3, 2018, Stroustrup was announced as the 2018 winner of the Charles Stark Draper Prize for Engineering, "for conceptualizing and developing the C++ programming language".[30]
 As of December 2022[update], C++ ranked third on the TIOBE index, surpassing Java for the first time in the history of the index. It ranks third, after Python and C.[31]
 According to Stroustrup, "the name signifies the evolutionary nature of the changes from C."[32] This name is credited to Rick Mascitti (mid-1983)[23] and was first used in December 1983. When Mascitti was questioned informally in 1992 about the naming, he indicated that it was given in a tongue-in-cheek spirit. The name comes from C's ++ operator (which increments the value of a variable) and a common naming convention of using "+" to indicate an enhanced computer program.
 During C++'s development period, the language had been referred to as "new C" and "C with Classes"[23][33] before acquiring its final name.
 Throughout C++'s life, its development and evolution has been guided by a set of principles:[22]
 C++ is standardized by an ISO working group known as JTC1/SC22/WG21. So far, it has published six revisions of the C++ standard and is currently working on the next revision, C++23.
 In 1998, the ISO working group standardized C++ for the first time as ISO/IEC 14882:1998, which is informally known as C++98. In 2003, it published a new version of the C++ standard called ISO/IEC 14882:2003, which fixed problems identified in C++98.
 The next major revision of the standard was informally referred to as "C++0x", but it was not released until 2011.[40] C++11 (14882:2011) included many additions to both the core language and the standard library.[37]
 In 2014, C++14 (also known as C++1y) was released as a small extension to C++11, featuring mainly bug fixes and small improvements.[41]  The Draft International Standard ballot procedures completed in mid-August 2014.[42]
 After C++14, a major revision C++17, informally known as C++1z, was completed by the ISO C++ committee in mid July 2017 and was approved and published in December 2017.[43]
 As part of the standardization process, ISO also publishes technical reports and specifications:
 More technical specifications are in development and pending approval, including new set of concurrency extensions.[61]
 The C++ language has two main components: a direct mapping of hardware features provided primarily by the C subset, and zero-overhead abstractions based on those mappings. Stroustrup describes C++ as "a light-weight abstraction programming language [designed] for building and using efficient and elegant abstractions";[15] and "offering both hardware access and abstraction is the basis of C++. Doing it efficiently is what distinguishes it from other languages."[62]
 C++ inherits most of C's syntax. The following is Bjarne Stroustrup's version of the Hello world program that uses the C++ Standard Library stream facility to write a message to standard output:[63][64][note 2]
 As in C, C++ supports four types of memory management: static storage duration objects, thread storage duration objects, automatic storage duration objects, and dynamic storage duration objects.[65]
 Static storage duration objects are created before main() is entered (see exceptions below) and destroyed in reverse order of creation after main() exits. The exact order of creation is not specified by the standard (though there are some rules defined below) to allow implementations some freedom in how to organize their implementation. More formally, objects of this type have a lifespan that "shall last for the duration of the program".[66]
 Static storage duration objects are initialized in two phases. First, "static initialization" is performed, and only after all static initialization is performed, "dynamic initialization" is performed.  In static initialization, all objects are first initialized with zeros; after that, all objects that have a constant initialization phase are initialized with the constant expression (i.e. variables initialized with a literal or constexpr). Though it is not specified in the standard, the static initialization phase can be completed at compile time and saved in the data partition of the executable.  Dynamic initialization involves all object initialization done via a constructor or function call (unless the function is marked with constexpr, in C++11). The dynamic initialization order is defined as the order of declaration within the compilation unit (i.e. the same file). No guarantees are provided about the order of initialization between compilation units.
 Variables of this type are very similar to static storage duration objects. The main difference is the creation time is just prior to thread creation and destruction is done after the thread has been joined.[67]
 The most common variable types in C++ are local variables inside a function or block, and temporary variables.[68] The common feature about automatic variables is that they have a lifetime that is limited to the scope of the variable. They are created and potentially initialized at the point of declaration (see below for details) and destroyed in the reverse order of creation when the scope is left. This is implemented by allocation on the stack.
 Local variables are created as the point of execution passes the declaration point. If the variable has a constructor or initializer this is used to define the initial state of the object. Local variables are destroyed when the local block or function that they are declared in is closed. C++ destructors for local variables are called at the end of the object lifetime, allowing a discipline for automatic resource management termed RAII, which is widely used in C++.
 Member variables are created when the parent object is created. Array members are initialized from 0 to the last member of the array in order. Member variables are destroyed when the parent object is destroyed in the reverse order of creation. i.e. If the parent is an "automatic object" then it will be destroyed when it goes out of scope which triggers the destruction of all its members.
 Temporary variables are created as the result of expression evaluation and are destroyed when the statement containing the expression has been fully evaluated (usually at the ; at the end of a statement).
 These objects have a dynamic lifespan and can be created directly with a call to new and destroyed explicitly with a call to delete.[69] C++ also supports malloc and free, from C, but these are not compatible with new and delete. Use of new returns an address to the allocated memory. The C++ Core Guidelines advise against using new directly for creating dynamic objects in favor of smart pointers through make_unique<T> for single ownership and make_shared<T> for reference-counted multiple ownership,[70] which were introduced in C++11.
 C++ templates enable generic programming. C++ supports function, class, alias, and variable templates. Templates may be parameterized by types, compile-time constants, and other templates. Templates are implemented by instantiation at compile-time. To instantiate a template, compilers substitute specific arguments for a template's parameters to generate a concrete function or class instance. Some substitutions are not possible; these are eliminated by an overload resolution policy described by the phrase "Substitution failure is not an error" (SFINAE). Templates are a powerful tool that can be used for generic programming, template metaprogramming, and code optimization, but this power implies a cost. Template use may increase object code size, because each template instantiation produces a copy of the template code: one for each set of template arguments, however, this is the same or smaller amount of code that would be generated if the code were written by hand.[71] This is in contrast to run-time generics seen in other languages (e.g., Java) where at compile-time the type is erased and a single template body is preserved.
 Templates are different from macros: while both of these compile-time language features enable conditional compilation, templates are not restricted to lexical substitution. Templates are aware of the semantics and type system of their companion language, as well as all compile-time type definitions, and can perform high-level operations including programmatic flow control based on evaluation of strictly type-checked parameters. Macros are capable of conditional control over compilation based on predetermined criteria, but cannot instantiate new types, recurse, or perform type evaluation and in effect are limited to pre-compilation text-substitution and text-inclusion/exclusion. In other words, macros can control compilation flow based on pre-defined symbols but cannot, unlike templates, independently instantiate new symbols. Templates are a tool for static polymorphism (see below) and generic programming.
 In addition, templates are a compile-time mechanism in C++ that is Turing-complete, meaning that any computation expressible by a computer program can be computed, in some form, by a template metaprogram prior to runtime.
 In summary, a template is a compile-time parameterized function or class written without knowledge of the specific arguments used to instantiate it. After instantiation, the resulting code is equivalent to code written specifically for the passed arguments. In this manner, templates provide a way to decouple generic, broadly applicable aspects of functions and classes (encoded in templates) from specific aspects (encoded in template parameters) without sacrificing performance due to abstraction.
 C++ introduces object-oriented programming (OOP) features to C. It offers classes, which provide the four features commonly present in OOP (and some non-OOP) languages: abstraction, encapsulation, inheritance, and polymorphism. One distinguishing feature of C++ classes compared to classes in other programming languages is support for deterministic destructors, which in turn provide support for the Resource Acquisition is Initialization (RAII) concept.
 Encapsulation is the hiding of information to ensure that data structures and operators are used as intended and to make the usage model more obvious to the developer. C++ provides the ability to define classes and functions as its primary encapsulation mechanisms. Within a class, members can be declared as either public, protected, or private to explicitly enforce encapsulation. A public member of the class is accessible to any function. A private member is accessible only to functions that are members of that class and to functions and classes explicitly granted access permission by the class ("friends"). A protected member is accessible to members of classes that inherit from the class in addition to the class itself and any friends.
 The object-oriented principle ensures the encapsulation of all and only the functions that access the internal representation of a type. C++ supports this principle via member functions and friend functions, but it does not enforce it. Programmers can declare parts or all of the representation of a type to be public, and they are allowed to make public entities not part of the representation of a type. Therefore, C++ supports not just object-oriented programming, but other decomposition paradigms such as modular programming.
 It is generally considered good practice to make all data private or protected, and to make public only those functions that are part of a minimal interface for users of the class. This can hide the details of data implementation, allowing the designer to later fundamentally change the implementation without changing the interface in any way.[72][73]
 Inheritance allows one data type to acquire properties of other data types. Inheritance from a base class may be declared as public, protected, or private. This access specifier determines whether unrelated and derived classes can access the inherited public and protected members of the base class. Only public inheritance corresponds to what is usually meant by "inheritance". The other two forms are much less frequently used. If the access specifier is omitted, a "class" inherits privately, while a "struct" inherits publicly. Base classes may be declared as virtual; this is called virtual inheritance. Virtual inheritance ensures that only one instance of a base class exists in the inheritance graph, avoiding some of the ambiguity problems of multiple inheritance.
 Multiple inheritance is a C++ feature allowing a class to be derived from more than one base class; this allows for more elaborate inheritance relationships. For example, a "Flying Cat" class can inherit from both "Cat" and "Flying Mammal". Some other languages, such as C# or Java, accomplish something similar (although more limited) by allowing inheritance of multiple interfaces while restricting the number of base classes to one (interfaces, unlike classes, provide only declarations of member functions, no implementation or member data). An interface as in C# and Java can be defined in C++ as a class containing only pure virtual functions, often known as an abstract base class or "ABC". The member functions of such an abstract base class are normally explicitly defined in the derived class, not inherited implicitly. C++ virtual inheritance exhibits an ambiguity resolution feature called dominance.
 C++ provides more than 35 operators, covering basic arithmetic, bit manipulation, indirection, comparisons, logical operations and others. Almost all operators can be overloaded for user-defined types, with a few notable exceptions such as member access (. and .*) as well as the conditional operator. The rich set of overloadable operators is central to making user-defined types in C++ seem like built-in types.
 Overloadable operators are also an essential part of many advanced C++ programming techniques, such as smart pointers. Overloading an operator does not change the precedence of calculations involving the operator, nor does it change the number of operands that the operator uses (any operand may however be ignored by the operator, though it will be evaluated prior to execution). Overloaded "&&" and "||" operators lose their short-circuit evaluation property.
 Polymorphism enables one common interface for many implementations, and for objects to act differently under different circumstances.
 C++ supports several kinds of static (resolved at compile-time) and dynamic (resolved at run-time) polymorphisms, supported by the language features described above. Compile-time polymorphism does not allow for certain run-time decisions, while runtime polymorphism typically incurs a performance penalty.
 Function overloading allows programs to declare multiple functions having the same name but with different arguments (i.e. ad hoc polymorphism). The functions are distinguished by the number or types of their formal parameters. Thus, the same function name can refer to different functions depending on the context in which it is used. The type returned by the function is not used to distinguish overloaded functions and differing return types would result in a compile-time error message.
 When declaring a function, a programmer can specify for one or more parameters a default value. Doing so allows the parameters with defaults to optionally be omitted when the function is called, in which case the default arguments will be used. When a function is called with fewer arguments than there are declared parameters, explicit arguments are matched to parameters in left-to-right order, with any unmatched parameters at the end of the parameter list being assigned their default arguments. In many cases, specifying default arguments in a single function declaration is preferable to providing overloaded function definitions with different numbers of parameters.
 Templates in C++ provide a sophisticated mechanism for writing generic, polymorphic code (i.e. parametric polymorphism). In particular, through the curiously recurring template pattern, it is possible to implement a form of static polymorphism that closely mimics the syntax for overriding virtual functions. Because C++ templates are type-aware and Turing-complete, they can also be used to let the compiler resolve recursive conditionals and generate substantial programs through template metaprogramming. Contrary to some opinion, template code will not generate a bulk code after compilation with the proper compiler settings.[71]
 Variable pointers and references to a base class type in C++ can also refer to objects of any derived classes of that type. This allows arrays and other kinds of containers to hold pointers to objects of differing types (references cannot be directly held in containers). This enables dynamic (run-time) polymorphism, where the referred objects can behave differently, depending on their (actual, derived) types.
 C++ also provides the dynamic_cast operator, which allows code to safely attempt conversion of an object, via a base reference/pointer, to a more derived type: downcasting. The attempt is necessary as often one does not know which derived type is referenced. (Upcasting, conversion to a more general type, can always be checked/performed at compile-time via static_cast, as ancestral classes are specified in the derived class's interface, visible to all callers.) dynamic_cast relies on run-time type information (RTTI), metadata in the program that enables differentiating types and their relationships. If a dynamic_cast to a pointer fails, the result is the nullptr constant, whereas if the destination is a reference (which cannot be null), the cast throws an exception. Objects known to be of a certain derived type can be cast to that with static_cast, bypassing RTTI and the safe runtime type-checking of dynamic_cast, so this should be used only if the programmer is very confident the cast is, and will always be, valid.
 Ordinarily, when a function in a derived class overrides a function in a base class, the function to call is determined by the type of the object. A given function is overridden when there exists no difference in the number or type of parameters between two or more definitions of that function. Hence, at compile time, it may not be possible to determine the type of the object and therefore the correct function to call, given only a base class pointer; the decision is therefore put off until runtime. This is called dynamic dispatch. Virtual member functions or methods[74] allow the most specific implementation of the function to be called, according to the actual run-time type of the object. In C++ implementations, this is commonly done using virtual function tables. If the object type is known, this may be bypassed by prepending a fully qualified class name before the function call, but in general calls to virtual functions are resolved at run time.
 In addition to standard member functions, operator overloads and destructors can be virtual. An inexact rule based on practical experience states that if any function in the class is virtual, the destructor should be as well. As the type of an object at its creation is known at compile time, constructors, and by extension copy constructors, cannot be virtual. Nonetheless, a situation may arise where a copy of an object needs to be created when a pointer to a derived object is passed as a pointer to a base object. In such a case, a common solution is to create a clone() (or similar) virtual function that creates and returns a copy of the derived class when called.
 A member function can also be made "pure virtual" by appending it with = 0 after the closing parenthesis and before the semicolon. A class containing a pure virtual function is called an abstract class. Objects cannot be created from an abstract class; they can only be derived from. Any derived class inherits the virtual function as pure and must provide a non-pure definition of it (and all other pure virtual functions) before objects of the derived class can be created. A program that attempts to create an object of a class with a pure virtual member function or inherited pure virtual member function is ill-formed.
 C++ provides support for anonymous functions, also known as lambda expressions, with the following form:[75]
 Since C++20, the keyword template is optional for template parameters of lambda expressions:
 If the lambda takes no parameters, and no return type or other specifiers are used, the () can be omitted; that is,
 The return type of a lambda expression can be automatically inferred, if possible; e.g.:
 The [capture] list supports the definition of closures. Such lambda expressions are defined in the standard as syntactic sugar for an unnamed function object.
 Exception handling is used to communicate the existence of a runtime problem or error from where it was detected to where the issue can be handled.[76] It permits this to be done in a uniform manner and separately from the main code, while detecting all errors.[77] Should an error occur, an exception is thrown (raised), which is then caught by the nearest suitable exception handler. The exception causes the current scope to be exited, and also each outer scope (propagation) until a suitable handler is found, calling in turn the destructors of any objects in these exited scopes.[78] At the same time, an exception is presented as an object carrying the data about the detected problem.[79]
 Some C++ style guides, such as Google's,[80] LLVM's,[81] and Qt's,[82] forbid the usage of exceptions.
 The exception-causing code is placed inside a try block. The exceptions are handled in separate catch blocks (the handlers); each try block can have multiple exception handlers, as it is visible in the example below.[83]
 It is also possible to raise exceptions purposefully, using the throw keyword; these exceptions are handled in the usual way. In some cases, exceptions cannot be used due to technical reasons. One such example is a critical component of an embedded system, where every operation must be guaranteed to complete within a specified amount of time. This cannot be determined with exceptions as no tools exist to determine the maximum time required for an exception to be handled.[84]
 Unlike signal handling, in which the handling function is called from the point of failure, exception handling exits the current scope before the catch block is entered, which may be located in the current function or any of the previous function calls currently on the stack.
 C++ has enumeration types that are directly inherited from C's and work mostly like these, except that an enumeration is a real type in C++, giving added compile-time checking. Also (as with structs), the C++ enum keyword is combined with a typedef, so that instead of naming the type enum name, simply name it name. This can be simulated in C using a typedef: typedef enum {Value1, Value2} name;
 C++11 also provides a second kind of enumeration, called a scoped enumeration. These are type-safe: the enumerators are not implicitly converted to an integer type. Among other things, this allows I/O streaming to be defined for the enumeration type. Another feature of scoped enumerations is that the enumerators do not leak, so usage requires prefixing with the name of the enumeration (e.g., Color::Red for the first enumerator in the example below), unless a using enum declaration (introduced in C++20) has been used to bring the enumerators into the current scope. A scoped enumeration is specified by the phrase enum class (or enum struct). For example:
 The underlying type of an enumeration is an implementation-defined integral type that is large enough to hold all enumerated values; it does not have to be the smallest possible type. The underlying type can be specified directly, which allows "forward declarations" of enumerations:
 The C++ standard consists of two parts: the core language and the standard library. C++ programmers expect the latter on every major implementation of C++; it includes aggregate types (vectors, lists, maps, sets, queues, stacks, arrays, tuples), algorithms (find, for_each, binary_search, random_shuffle, etc.), input/output facilities (iostream, for reading from and writing to the console and files), filesystem library, localisation support, smart pointers for automatic memory management, regular expression support, multi-threading library, atomics support (allowing a variable to be read or written to by at most one thread at a time without any external synchronisation), time utilities (measurement, getting current time, etc.), a system for converting error reporting that does not use C++ exceptions into C++ exceptions, a random number generator, and a slightly modified version of the C standard library (to make it comply with the C++ type system).
 A large part of the C++ library is based on the Standard Template Library (STL). Useful tools provided by the STL include containers as the collections of objects (such as vectors and lists), iterators that provide array-like access to containers, and algorithms that perform operations such as searching and sorting.
 Furthermore, (multi)maps (associative arrays) and (multi)sets are provided, all of which export compatible interfaces. Therefore, using templates it is possible to write generic algorithms that work with any container or on any sequence defined by iterators. As in C, the features of the library are accessed by using the #include directive to include a standard header. The C++ Standard Library provides 105 standard headers, of which 27 are deprecated.
 The standard incorporates the STL that was originally designed by Alexander Stepanov, who experimented with generic algorithms and containers for many years. When he started with C++, he finally found a language where it was possible to create generic algorithms (e.g., STL sort) that perform even better than, for example, the C standard library qsort, thanks to C++ features like using inlining and compile-time binding instead of function pointers. The standard does not refer to it as "STL", as it is merely a part of the standard library, but the term is still widely used to distinguish it from the rest of the standard library (input/output streams, internationalization, diagnostics, the C library subset, etc.).[85]
 Most C++ compilers, and all major ones, provide a standards-conforming implementation of the C++ standard library.
 The C++ Core Guidelines[86] are an initiative led by Bjarne Stroustrup, the inventor of C++, and Herb Sutter, the convener and chair of the C++ ISO Working Group, to help programmers write 'Modern C++' by using best practices for the language standards C++11 and newer, and to help developers of compilers and static checking tools to create rules for catching bad programming practices.
 The main aim is to efficiently and consistently write type and resource safe C++.
 The Core Guidelines were announced[87] in the opening keynote at CPPCon 2015.
 The Guidelines are accompanied by the Guideline Support Library (GSL),[88] a header only library of types and functions to implement the Core Guidelines and static checker tools for enforcing Guideline rules.[89]
 To give compiler vendors greater freedom, the C++ standards committee decided not to dictate the implementation of name mangling, exception handling, and other implementation-specific features. The downside of this decision is that object code produced by different compilers is expected to be incompatible. There are, however, attempts to standardize compilers for particular machines or operating systems. For example, the Itanium C++ ABI is processor-independent (despite its name) and is implemented by GCC and Clang.[90]
 C++ is often considered to be a superset of C but this is not strictly true.[91] Most C code can easily be made to compile correctly in C++ but there are a few differences that cause some valid C code to be invalid or behave differently in C++. For example, C allows implicit conversion from void* to other pointer types but C++ does not (for type safety reasons). Also, C++ defines many new keywords, such as new and class, which may be used as identifiers (for example, variable names) in a C program.
 Some incompatibilities have been removed by the 1999 revision of the C standard (C99), which now supports C++ features such as line comments (//) and declarations mixed with code. On the other hand, C99 introduced a number of new features that C++ did not support that were incompatible or redundant in C++, such as variable-length arrays, native complex-number types (however, the std::complex class in the C++ standard library provides similar functionality, although not code-compatible), designated initializers, compound literals, and the restrict keyword.[92] Some of the C99-introduced features were included in the subsequent version of the C++ standard, C++11 (out of those which were not redundant).[93][94][95] However, the C++11 standard introduces new incompatibilities, such as disallowing assignment of a string literal to a character pointer, which remains valid C.
 To intermix C and C++ code, any function declaration or definition that is to be called from/used both in C and C++ must be declared with C linkage by placing it within an extern "C" {/*...*/} block. Such a function may not rely on features depending on name mangling (i.e., function overloading).
 Despite its widespread adoption, some notable programmers have criticized the C++ language, including Linus Torvalds,[96] Richard Stallman,[97] Joshua Bloch, Ken Thompson,[98][99][100] and Donald Knuth.[101][102]
 
One of the most often criticised points of C++ is its perceived complexity as a language, with the criticism that a large number of non-orthogonal features in practice necessitates restricting code to a subset of C++, thus eschewing the readability benefits of common style and idioms. As expressed by Joshua Bloch:   I think C++ was pushed well beyond its complexity threshold, and yet there are a lot of people programming it. But what you do is you force people to subset it. So almost every shop that I know of that uses C++ says, "Yes, we're using C++ but we're not doing multiple-implementation inheritance and we're not using operator overloading." There are just a bunch of features that you're not going to use because the complexity of the resulting code is too high. And I don't think it's good when you have to start doing that. You lose this programmer portability where everyone can read everyone else's code, which I think is such a good thing.  Donald Knuth (1993, commenting on pre-standardized C++), who said of Edsger Dijkstra that "to think of programming in C++" "would make him physically ill":[101][102]   The problem that I have with them today is that... C++ is too complicated. At the moment, it's impossible for me to write portable code that I believe would work on lots of different systems, unless I avoid all exotic features. Whenever the C++ language designers had two competing ideas as to how they should solve some problem, they said "OK, we'll do them both". So the language is too baroque for my taste.  Ken Thompson, who was a colleague of Stroustrup at Bell Labs, gives his assessment:[99][100]   It certainly has its good points. But by and large I think it's a bad language. It does a lot of things half well and it's just a garbage heap of ideas that are mutually exclusive. Everybody I know, whether it's personal or corporate, selects a subset and these subsets are different. So it's not a good language to transport an algorithm—to say, "I wrote it; here, take it." It's way too big, way too complex. And it's obviously built by a committee.
Stroustrup campaigned for years and years and years, way beyond any sort of technical contributions he made to the language, to get it adopted and used. And he sort of ran all the standards committees with a whip and a chair. And he said "no" to no one. He put every feature in that language that ever existed. It wasn't cleanly designed—it was just the union of everything that came along. And I think it suffered drastically from that.  
However Brian Kernighan, also a colleague at Bell Labs, disputes this assessment:[103]  C++ has been enormously influential. ... Lots of people say C++ is too big and too complicated etc. etc. but in fact it is a very powerful language and pretty much everything that is in there is there for a really sound reason: it is not somebody doing random invention, it is actually people trying to solve real world problems. Now a lot of the programs that we take for granted today, that we just use, are C++ programs.  Stroustrup himself comments that C++ semantics are much cleaner than its syntax: "within C++, there is a much smaller and cleaner language struggling to get out."[104]
 Other complaints may include a lack of reflection or garbage collection, long compilation times, perceived feature creep,[105] and verbose error messages, particularly from template metaprogramming.[106]


Source: https://en.wikipedia.org/wiki/Datei:Python_3._The_standard_type_hierarchy.png
Content: Other reasons this message may be displayed:


Source: https://en.wikipedia.org/wiki/Arithmetik
Content: 
 Arithmetic is an elementary branch of mathematics that studies numerical operations like addition, subtraction, multiplication, and division. In a wider sense, it also includes exponentiation, extraction of roots, and taking logarithms. Arithmetic systems can be distinguished based on the type of number they operate on. Integer arithmetic restricts itself to calculations with positive and negative whole numbers. Rational number arithmetic involves operations on fractions that lie between integers. Real number arithmetic includes calculations with both rational and irrational numbers and covers the complete number line. Another distinction is based on the numeral system employed to perform calculations. Decimal arithmetic is the most common. It uses the basic numerals from 0 to 9 and their combinations to express numbers. Binary arithmetic, by contrast, is used by most computers and represents numbers as combinations of the basic numerals 0 and 1. Some arithmetic systems operate on mathematical objects other than numbers, such as interval arithmetic and matrix arithmetic.
 Arithmetic operations form the basis of many branches of mathematics, such as algebra, calculus, and statistics. They play a similar role in the sciences, like physics and economics. Arithmetic is present in many aspects of daily life, for example, to calculate change while shopping or to manage personal finances. It is one of the earliest forms of mathematics education that students encounter. Its cognitive and conceptual foundations are studied by psychology and philosophy.
 The practice of arithmetic is at least thousands and possibly tens of thousands of years old. Ancient civilizations like the Egyptians and the Sumerians invented numeral systems to solve practical arithmetic problems in about 3000 BCE. Starting in the 7th and 6th centuries BCE, the ancient Greeks initiated a more abstract study of numbers and introduced the method of rigorous mathematical proofs. The ancient Indians developed the concept of zero and the decimal system, which Arab mathematicians further refined and spread to the Western world during the medieval period. The first mechanical calculators were invented in the 17th century. The 18th and 19th centuries saw the development of modern number theory and the formulation of axiomatic foundations of arithmetic. In the 20th century, the emergence of electronic calculators and computers revolutionized the accuracy and speed with which arithmetic calculations could be performed.
 Arithmetic is the fundamental branch of mathematics that studies numbers and their operations. In particular, it deals with numerical calculations using the arithmetic operations of addition, subtraction, multiplication, and division.[1] In a wider sense, it also includes  exponentiation, extraction of roots, and logarithm.[2] The term "arithmetic" has its root in the Latin term "arithmetica" which derives from the Ancient Greek words ἀριθμός (arithmos), meaning "number", and ἀριθμητική τέχνη (arithmetike tekhne), meaning "the art of counting".[3]
 There are disagreements about its precise definition. According to a narrow characterization, arithmetic deals only with natural numbers.[4] However, the more common view is to include operations on integers, rational numbers, real numbers, and sometimes also complex numbers in its scope.[5] Some definitions restrict arithmetic to the field of numerical calculations.[6] When understood in a wider sense, it also includes the study of how the concept of numbers developed, the analysis of properties of and relations between numbers, and the examination of the axiomatic structure of arithmetic operations.[7]
 Arithmetic is closely related to number theory and some authors use the terms as synonyms.[8] However, in a more specific sense, number theory is restricted to the study of integers and focuses on their properties and relationships such as divisibility, factorization, and primality.[9] Traditionally, it is known as higher arithmetic.[10]
 Numbers are mathematical objects used to count quantities and measure magnitudes. They are fundamental elements in arithmetic since all arithmetic operations are performed on numbers. There are different kinds of numbers and different numeral systems to represent them.[11]
 The main kinds of numbers employed in arithmetic are natural numbers, whole numbers, integers, rational numbers, and real numbers.[12] The natural numbers are whole numbers that start from 1 and go to infinity. They exclude 0 and negative numbers. They are also known as counting numbers and can be expressed as 



{
1
,
2
,
3
,
4
,
.
.
.
}


{\displaystyle \{1,2,3,4,...\}}

. The symbol of the natural numbers is 




N



{\displaystyle \mathbb {N} }

.[a] The whole numbers are identical to the natural numbers with the only difference being that they include 0. They can be represented as 



{
0
,
1
,
2
,
3
,
4
,
.
.
.
}


{\displaystyle \{0,1,2,3,4,...\}}

 and have the symbol 





N


0




{\displaystyle \mathbb {N} _{0}}

.[14][b] Some mathematicians do not draw the distinction between the natural and the whole numbers by including 0 in the set of natural numbers.[16] The set of integers encompasses both positive and negative whole numbers. It has the symbol 




Z



{\displaystyle \mathbb {Z} }

 and can be expressed as 



{
.
.
.
,
−
2
,
−
1
,
0
,
1
,
2
,
.
.
.
}


{\displaystyle \{...,-2,-1,0,1,2,...\}}

.[17]
 Based on how natural and whole numbers are used, they can be distinguished into cardinal and ordinal numbers. Cardinal numbers, like one, two, and three, are numbers that express the quantity of objects. They answer the question "how many?". Ordinal numbers, such as first, second, and third, indicate order or placement in a series. They answer the question "what position?".[18]
 A number is rational if it can be represented as the ratio of two integers. For instance, the rational number 






1
2





{\displaystyle {\tfrac {1}{2}}}

 is formed by dividing the integer 1, called the numerator, by the integer 2, called the denominator. Other examples are 






3
4





{\displaystyle {\tfrac {3}{4}}}

 and 






281
3





{\displaystyle {\tfrac {281}{3}}}

. The set of rational numbers includes all integers, which are fractions with a denominator of 1. The symbol of the rational numbers is 




Q



{\displaystyle \mathbb {Q} }

.[19] Decimal fractions like 0.3 and 25.12 are a special type of rational numbers since their denominator is a power of 10. For instance, 0.3 is equal to 






3
10





{\displaystyle {\tfrac {3}{10}}}

, and 25.12 is equal to 






2512
100





{\displaystyle {\tfrac {2512}{100}}}

.[20] Every rational number corresponds to a finite or a repeating decimal.[21][c]
 Irrational numbers are numbers that cannot be expressed through the ratio of two integers. They are often required to describe geometric magnitudes. For example, if a right triangle has legs of the length 1 then the length of its hypotenuse is given by the irrational number 





2




{\displaystyle {\sqrt {2}}}

. π is another irrational number and describes the ratio of a circle's circumference to its diameter.[22] The decimal representation of an irrational number is infinite without repeating decimals.[23] The set of rational numbers together with the set of irrational numbers makes up the set of real numbers. The symbol of the real numbers is 




R



{\displaystyle \mathbb {R} }

.[24] Even wider classes of numbers include complex numbers and quaternions.[25]
 A numeral is a symbol to represent a number and numeral systems are representational frameworks.[26] They usually have a limited amount of basic numerals, which directly refer to certain numbers. The system governs how these basic numerals may be combined to express any number.[27] Numeral systems are either positional or non-positional. All early numeral systems were non-positional.[28] For non-positional numeral systems, the value of a digit does not depend on its position in the numeral.[29]
 The simplest non-positional system is the unary numeral system. It relies on one symbol for the number 1. All higher numbers are written by repeating this symbol. For example, the number 7 can be represented by repeating the symbol for 1 seven times. This system makes it cumbersome to write large numbers, which is why many non-positional systems include additional symbols to directly represent larger numbers.[30] Variations of the unary numeral systems are employed in tally sticks using dents and in tally marks.[31]
 Egyptian hieroglyphics had a more complex non-positional numeral system. They have additional symbols for numbers like 10, 100, 1000, and 10,000. These symbols can be combined into a sum to more conveniently express larger numbers. For instance, the numeral for 10,405 uses one time the symbol for 10,000, four times the symbol for 100, and five times the symbol for 1. A similar well-known framework is the Roman numeral system. It has the symbols I, V, X, L, C, D, M as its basic numerals to represent the numbers 1, 5, 10, 50, 100, 500, and 1000.[33]
 A numeral system is positional if the position of a basic numeral in a compound expression determines its value. Positional numeral systems have a radix that acts as a multiplicand of the different positions. For each subsequent position, the radix is raised to a higher power. In the common decimal system, also called the Hindu–Arabic numeral system, the radix is 10. This means that the first digit is multiplied by 




10

0




{\displaystyle 10^{0}}

, the next digit is multiplied by 




10

1




{\displaystyle 10^{1}}

, and so on. For example, the decimal numeral 532 stands for 



5
⋅

10

2


+
3
⋅

10

1


+
2
⋅

10

0




{\displaystyle 5\cdot 10^{2}+3\cdot 10^{1}+2\cdot 10^{0}}

. Because of the effect of the digits' positions, the numeral 532 differs from the numerals 325 and 253 even though they have the same digits.[34]
 Another positional numeral system used extensively in computer arithmetic is the binary system, which has a radix of 2. This means that the first digit is multiplied by 




2

0




{\displaystyle 2^{0}}

, the next digit by 




2

1




{\displaystyle 2^{1}}

, and so on. For example, the number 13 is written as 1101 in the binary notation, which stands for 



1
⋅

2

3


+
1
⋅

2

2


+
0
⋅

2

1


+
1
⋅

2

0




{\displaystyle 1\cdot 2^{3}+1\cdot 2^{2}+0\cdot 2^{1}+1\cdot 2^{0}}

. In computing, each digit in the binary notation corresponds to one bit.[35] The earliest positional system was developed by ancient Babylonians and had a radix of 60.[36]
 Arithmetic operations are ways of combining, transforming, or manipulating numbers. They are functions that have numbers both as input and output.[37] The most important operations in arithmetic are addition, subtraction, multiplication, and division.[38] Further operations include exponentiation, extraction of roots, and logarithm.[39] If these operations are performed on variables rather than numbers, they are sometimes referred to as algebraic operations.[40]
 Two important concepts in relation to arithmetic operations are identity elements and inverse elements. The identity element or neutral element of an operation does not cause any change if it is applied to another element. For example, the identity element of addition is 0 since any sum of a number and 0 results in the same number. The inverse element is the element that results in the identity element when combined with another element. For instance, the additive inverse of the number 6 is -6 since their sum is 0.[41]
 There are not only inverse elements but also inverse operations. In an informal sense, one operation is the inverse of another operation if it undoes the first operation. For example, subtraction is the inverse of addition since a number returns to its original value if a second number is first added and subsequently subtracted, as in 



13
+
4
−
4
=
13


{\displaystyle 13+4-4=13}

. Defined more formally, the operation "



⋆


{\displaystyle \star }

" is an inverse of the operation "



∘


{\displaystyle \circ }

" if it fulfills the following condition: 



t
⋆
s
=
r


{\displaystyle t\star s=r}

 if and only if 



r
∘
s
=
t


{\displaystyle r\circ s=t}

.[42]
 Commutativity and associativity are laws governing the order in which some arithmetic operations can be carried out. An operation is commutative if the order of the arguments can be changed without affecting the results. This is the case for addition, for instance, 



7
+
9


{\displaystyle 7+9}

 is the same as 



9
+
7


{\displaystyle 9+7}

. Associativity is a rule that affects the order in which a series of operations can be carried out. An operation is associative if, in a series of two operations, it does not matter which operation is carried out first. This is the case for multiplication, for example, since 



(
5
×
4
)
×
2


{\displaystyle (5\times 4)\times 2}

 is the same as 



5
×
(
4
×
2
)


{\displaystyle 5\times (4\times 2)}

.[43]
 Addition is an arithmetic operation in which two numbers, called the addends, are combined into a single number, called the sum. The symbol of addition is 



+


{\displaystyle +}

. Examples are 



2
+
2
=
4


{\displaystyle 2+2=4}

 and 



6.3
+
1.26
=
7.56


{\displaystyle 6.3+1.26=7.56}

.[44] The term summation is used if several additions are performed in a row.[45] Counting is a type of repeated addition in which the number 1 is continuously added.[46]
 Subtraction is the inverse of addition. In it, one number, known as the subtrahend, is taken away from another, known as the minuend. The result of this operation is called the difference. The symbol of subtraction is 



−


{\displaystyle -}

.[47] Examples are 



14
−
8
=
6


{\displaystyle 14-8=6}

 and 



45
−
1.7
=
43.3


{\displaystyle 45-1.7=43.3}

. Subtraction is often treated as a special case of addition: instead of subtracting a positive number, it is also possible to add a negative number. For instance 



14
−
8
=
14
+
(
−
8
)


{\displaystyle 14-8=14+(-8)}

. This helps to simplify mathematical computations by reducing the number of basic arithmetic operations needed to perform calculations.[48]
 The additive identity element is 0 and the additive inverse of a number is the negative of that number. For instance, 



13
+
0
=
13


{\displaystyle 13+0=13}

 and 



13
+
(
−
13
)
=
0


{\displaystyle 13+(-13)=0}

. Addition is both commutative and associative.[49]
 Multiplication  is an arithmetic operation in which two numbers, called the multiplier and the multiplicand, are combined into a single number called the product.[50][d] The symbols of multiplication are 



×


{\displaystyle \times }

, 



⋅


{\displaystyle \cdot }

, and *. Examples are 



2
×
3
=
6


{\displaystyle 2\times 3=6}

 and 



0.3
⋅
5
=
1.5


{\displaystyle 0.3\cdot 5=1.5}

. If the multiplicand is a natural number then multiplication is the same as repeated addition, as in 



2
×
3
=
2
+
2
+
2


{\displaystyle 2\times 3=2+2+2}

.[52]
 Division is the inverse of multiplication. In it, one number, known as the dividend, is split into several equal parts by another number, known as the divisor. The result of this operation is called the quotient. The symbols of division are 



÷


{\displaystyle \div }

 and 




/



{\displaystyle /}

. Examples are 



48
÷
8
=
6


{\displaystyle 48\div 8=6}

 and 



29.4

/

1.4
=
21


{\displaystyle 29.4/1.4=21}

.[53] Division is often treated as a special case of multiplication: instead of dividing by a number, it is also possible to multiply by its reciprocal. The reciprocal of a number is 1 divided by that number. For instance, 



48
÷
8
=
48
×



1
8





{\displaystyle 48\div 8=48\times {\tfrac {1}{8}}}

.[54]
 The multiplicative identity element is 1 and the multiplicative inverse of a number is the reciprocal of that number. For example, 



13
×
1
=
13


{\displaystyle 13\times 1=13}

 and 



13
×



1
13



=
1


{\displaystyle 13\times {\tfrac {1}{13}}=1}

. Multiplication is both commutative and associative.[55]
 Exponentiation is an arithmetic operation in which a number, known as the base, is raised to the power of another number, known as the exponent. The result of this operation is called the power. Exponentiation is sometimes expressed using the symbol ^ but the more common way is to write the exponent in superscript right after the base. Examples are 




2

4


=
16


{\displaystyle 2^{4}=16}

 and 



3


{\displaystyle 3}

^



3
=
27


{\displaystyle 3=27}

. If the exponent is a natural number then exponentiation is the same as repeated multiplication, as in 




2

4


=
2
×
2
×
2
×
2


{\displaystyle 2^{4}=2\times 2\times 2\times 2}

.[56][e]
 Roots are a special type of exponentiation using a fractional exponent. For example, the square root of a number is the same as raising the number to the power of 






1
2





{\displaystyle {\tfrac {1}{2}}}

 and the cube root of a number is the same as raising the number to the power of 






1
3





{\displaystyle {\tfrac {1}{3}}}

. Examples are 





4


=

4


1
2



=
2


{\displaystyle {\sqrt {4}}=4^{\frac {1}{2}}=2}

 and 





27

3



=

27


1
3



=
3


{\displaystyle {\sqrt[{3}]{27}}=27^{\frac {1}{3}}=3}

.[58]
 Logarithm is the inverse of exponentiation. The logarithm of a number 



x


{\displaystyle x}

 to the base 



b


{\displaystyle b}

 is the exponent to which 



b


{\displaystyle b}

 must be raised to produce 



x


{\displaystyle x}

. For instance, since 



1000
=

10

3




{\displaystyle 1000=10^{3}}

, the logarithm base 10 of 1000 is 3. The logarithm of 



x


{\displaystyle x}

 to base 



b


{\displaystyle b}

 is denoted as 




log

b


⁡
(
x
)


{\displaystyle \log _{b}(x)}

, or without parentheses, 




log

b


⁡
x


{\displaystyle \log _{b}x}

, or even without the explicit base, 



log
⁡
x


{\displaystyle \log x}

, when the base can be understood from context. So, the previous example can be written 




log

10


⁡
1000
=
3


{\displaystyle \log _{10}1000=3}

.[59]
 Exponentiation and logarithm do not have general identity elements and inverse elements like addition and multiplication. The neutral element of exponentiation in relation to the exponent is 1, as in 




14

1


=
14


{\displaystyle 14^{1}=14}

. However, exponentiation does not have a general identity element since 1 is not the neutral element for the base.[60] Exponentiation and logarithm are neither commutative nor associative.[61]
 Different types of arithmetic systems are discussed in the academic literature. They differ from each other based on what type of number they operate on, what numeral system they use to represent them, and whether they operate on mathematical objects other than numbers.[62]
 Integer arithmetic is the branch of arithmetic that deals with the manipulation of positive and negative whole numbers.[63] Simple one-digit operations can be performed by following or memorizing a table that presents the results of all possible combinations, like an addition table or a multiplication table. Other common methods are verbal counting and finger-counting.[64]
 For operations on numbers with more than one digit, different techniques can be employed to calculate the result by using several one-digit operations in a row. For example, in the method addition with carries, the two numbers are written one above the other. Starting from the rightmost digit, each pair of digits is added together. The rightmost digit of the sum is written below them. If the sum is a two-digit number then the leftmost digit, called the "carry", is added to the next pair of digits to the left. This process is repeated until all digits have been added.[65] Other methods used for integer additions are the number line method, the partial sum method, and the compensation method.[66] A similar technique is utilized for subtraction: it also starts with the rightmost digit and uses a "borrow" or a negative carry for the column on the left if the result of the one-digit subtraction is negative.[67]
 A basic technique of integer multiplication employs repeated addition. For example, the product of 



3
×
4


{\displaystyle 3\times 4}

 can be calculated as 



3
+
3
+
3
+
3


{\displaystyle 3+3+3+3}

.[68] A common technique for multiplication with larger numbers is called long multiplication. This method starts by writing the multiplier above the multiplicand. The calculation begins by multiplying the multiplier only with the rightmost digit of the multiplicand and writing the result below, starting in the rightmost column. The same is done for each digit of the multiplicand and the result in each case is shifted one position to the left. As a final step, all the individual products are added to arrive at the total product of the two multi-digit numbers.[69] Other techniques used for multiplication are the grid method and the lattice method.[70] Computer science is interested in multiplication algorithms with a low computational complexity to be able to efficiently multiply very large integers, such as the Karatsuba algorithm, the Schönhage-Strassen algorithm, and the Toom-Cook algorithm.[71] A common technique used for division is called long division. Other methods include short division and chunking.[72]
 Integer arithmetic is not closed under division. This means that when dividing one integer by another integer, the result is not always an integer. For instance, 7 divided by 2 is not a whole number but 3.5.[73] One way to ensure that the result is an integer is to round the result to a whole number. However, this method leads to inaccuracies as the original value is altered.[74] Another method is to perform the division only partially and retain the remainder. For example, 7 divided by 2 is 3 with a remainder of 1. These difficulties are avoided by rational number arithmetic, which allows for the exact representation of fractions.[75]
 A simple method to calculate exponentiation is by repeated multiplication. For instance, the exponentiation of 




3

4




{\displaystyle 3^{4}}

 can be calculated as 



3
×
3
×
3
×
3


{\displaystyle 3\times 3\times 3\times 3}

.[76] A more efficient technique used for large exponents is exponentiation by squaring. It breaks down the calculation into a number of squaring operations. For example, the exponentiation 




3

65




{\displaystyle 3^{65}}

 can be written as 



(
(
(
(
(

3

2



)

2



)

2



)

2



)

2



)

2


×
3


{\displaystyle (((((3^{2})^{2})^{2})^{2})^{2})^{2}\times 3}

. By taking advantage of repeated squaring operations, only 7 individual operations are needed rather than the 64 operations required for regular repeated multiplication.[77] Methods to calculate logarithms include the Taylor series and continued fractions.[78] Integer arithmetic is not closed under logarithm and under exponentiation with negative exponents, meaning that the result of these operations is not always an integer.[79]
 Number theory studies the structure and properties of integers as well as the relations and laws between them.[80] Some of the main branches of modern number theory include elementary number theory, analytic number theory, algebraic number theory, and geometric number theory.[81] Elementary number theory studies aspects of integers that can be investigated using elementary methods. Its topics include divisibility, factorization, and primality.[82] Analytic number theory, by contrast, relies on techniques from analysis and calculus. It examines problems like how prime numbers are distributed and the claim that every even number is a sum of two prime numbers.[83] Algebraic number theory employs algebraic structures to analyze the properties of and relations between numbers. Examples are the use of fields and rings, as in algebraic number fields like the ring of integers. Geometric number theory uses concepts from geometry to study numbers. For instance, it investigates how lattice points with integer coordinates behave in a plane.[84] Further branches of number theory are probabilistic number theory, which employs methods from probability theory,[85] combinatorial number theory, which relies on the field of combinatorics,[86] computational number theory, which approaches number-theoretic problems with computational methods,[87] and applied number theory, which examines the application of number theory to fields like physics, biology, and cryptography.[88]
 Influential theorems in number theory include the fundamental theorem of arithmetic, Euclid's theorem, and Fermat's last theorem.[89] According to the fundamental theorem of arithmetic, every integer greater than 1 is either a prime number or can be represented as a unique product of prime numbers. For example, the number 18 is not a prime number and can be represented as 



2
×
3
×
3


{\displaystyle 2\times 3\times 3}

, all of which are prime numbers. The number 19, by contrast, is a prime number that has no other prime factorization.[90] Euclid's theorem states that there are infinitely many prime numbers.[91] Fermat's last theorem is the statement that no positive integer values can be found for 



a


{\displaystyle a}

, 



b


{\displaystyle b}

, and 



c


{\displaystyle c}

, to solve the equation 




a

n


+

b

n


=

c

n




{\displaystyle a^{n}+b^{n}=c^{n}}

 if 



n


{\displaystyle n}

 is greater than 



2


{\displaystyle 2}

.[92]
 Rational number arithmetic is the branch of arithmetic that deals with the manipulation of numbers that can be expressed as a ratio of two integers.[93] Most arithmetic operations on rational numbers can be calculated by performing a series of integer arithmetic operations on the numerators and the denominators of the involved numbers. If two rational numbers have the same denominator then they can be added by adding their numerators and keeping the common denominator. For example, 






2
7



+



3
7



=



5
7





{\displaystyle {\tfrac {2}{7}}+{\tfrac {3}{7}}={\tfrac {5}{7}}}

. A similar procedure is used for subtraction. If the two numbers do not have the same denominator then they must be transformed to find a common denominator. This can be achieved by scaling the first number with the denominator of the second number while scaling the second number with the denominator of the first number. For instance, 






1
3



+



1
2



=




1
⋅
2


3
⋅
2




+




1
⋅
3


2
⋅
3




=



2
6



+



3
6



=



5
6





{\displaystyle {\tfrac {1}{3}}+{\tfrac {1}{2}}={\tfrac {1\cdot 2}{3\cdot 2}}+{\tfrac {1\cdot 3}{2\cdot 3}}={\tfrac {2}{6}}+{\tfrac {3}{6}}={\tfrac {5}{6}}}

.[94]
 Two rational numbers are multiplied by multiplying their numerators and their denominators respectively, as in 






2
3



⋅



2
5



=




2
⋅
2


3
⋅
5




=



4
15





{\displaystyle {\tfrac {2}{3}}\cdot {\tfrac {2}{5}}={\tfrac {2\cdot 2}{3\cdot 5}}={\tfrac {4}{15}}}

. Dividing one rational number by another can be achieved by multiplying the first number with the reciprocal of the second number. This means that the numerator and the denominator of the second number change position. For example, 






3
5



:



2
7



=



3
5



⋅



7
2



=



21
10





{\displaystyle {\tfrac {3}{5}}:{\tfrac {2}{7}}={\tfrac {3}{5}}\cdot {\tfrac {7}{2}}={\tfrac {21}{10}}}

.[95] Unlike integer arithmetic, rational number arithmetic is closed under division as long as the divisor is not 0.[96]
 Both integer arithmetic and rational number arithmetic are not closed under exponentiation and logarithm.[97] One way to calculate exponentiation with a fractional exponent is to perform two separate calculations: one exponentiation using the numerator of the exponent followed by drawing the nth root of the result based on the denominator of the exponent. For example, 




5


2
3



=



5

2



3





{\displaystyle 5^{\frac {2}{3}}={\sqrt[{3}]{5^{2}}}}

. The first operation can be completed using methods like repeated multiplication or exponentiation by squaring. One way to get an approximate result for the second operation is to employ Newton's method, which uses a series of steps to gradually refine an initial guess until it reaches the desired level of accuracy.[98] The Taylor series or the continued fraction method can be utilized to calculate logarithms.[99]
 The decimal fraction notation is a special way of representing rational numbers whose denominator is a power of 10. For instance, the rational numbers 






1
10





{\displaystyle {\tfrac {1}{10}}}

, 






371
100





{\displaystyle {\tfrac {371}{100}}}

, and 






44
10000





{\displaystyle {\tfrac {44}{10000}}}

 are written as 0.1, 3.71, and 0.0044 in the decimal fraction notation.[100] Modified versions of integer calculation methods like addition with carry and long multiplication can be applied to calculations with decimal fractions.[101] Not all rational numbers have a finite representation in the decimal notation. For example, the rational number 






1
3





{\displaystyle {\tfrac {1}{3}}}

 corresponds to 0.333... with an infinite number of 3s. The shortened notation for this type of repeating decimal is 0.3.[102] Every repeating decimal expresses a rational number.[103]
 Real number arithmetic is the branch of arithmetic that deals with the manipulation of both rational and irrational numbers. Irrational numbers are numbers that cannot be expressed through fractions or repeated decimals, like the root of 2 and π.[104] Unlike rational number arithmetic, real number arithmetic is closed under exponentiation as long as it uses a positive number as its base. The same is true for the logarithm of positive real numbers as long as the logarithm base is positive and not 1.[105]
 Irrational numbers involve an infinite non-repeating series of decimal digits. Because of this, there is often no simple and accurate way to express the results of arithmetic operations like 





2


+
π


{\displaystyle {\sqrt {2}}+\pi }

 or 



e
⋅


3




{\displaystyle e\cdot {\sqrt {3}}}

.[106] In cases where absolute precision is not required, the problem of calculating arithmetic operations on real numbers is usually addressed by truncation or rounding. For truncation, a certain number of leftmost digits are kept and remaining digits are discarded or replaced by zeros. For example, the number π has an infinite number of digits starting with 3.14159.... If this number is truncated to 4 decimal places, the result is 3.141. Rounding is a similar process in which the last preserved digit is increased by one if the next digit is 5 or greater but remains the same if the next digit is less than 5, so that the rounded number is the best approximation of a given precision for the original number. For instance, if the number π is rounded to 4 decimal places, the result is 3.142 because the following digit is a 5, so 3.142 is closer to π than 3.141.[107] These methods allow computers to efficiently perform approximate calculations on real numbers.[108]
 In science and engineering, numbers represent estimates of physical quantities derived from measurement or modeling. Unlike mathematically exact numbers such as π or 





2




{\displaystyle {\sqrt {2}}}

, scientifically relevant numerical data are inherently inexact, involving some measurement uncertainty.[109] One basic way to express the degree of certainty about each number's value and avoid false precision is to round each measurement to a certain number of digits, called significant digits, which are implied to be accurate. For example, a person's height measured with a tape measure might only be precisely known to the nearest centimeter, so should be presented as 1.62 meters rather than 1.6217 meters. If converted to imperial units, this quantity should be rounded to 64 inches or 63.8 inches rather than 63.7795 inches, to clearly convey the precision of the measurement. When a number is written using ordinary decimal notation, leading zeros are not significant, and trailing zeros of numbers not written with a decimal point are implicitly considered to be non-significant.[110] For example, the numbers 0.056 and 1200 each have only 2 significant digits, but the number 40.00 has 4 significant digits. Representing uncertainty using only significant digits is a relatively crude method, with some unintuitive subtleties; explicitly keeping track of an estimate or upper bound of the approximation error is a more sophisticated approach.[111] In the example, the person's height might be represented as 1.62 ± 0.005 meters or 63.8 ± 0.2 inches.[112]
 In performing calculations with uncertain quantities, the uncertainty should be propagated to calculated quantities. When adding or subtracting two or more quantities, add the absolute uncertainties of each summand together to obtain the absolute uncertainty of the sum. When multiplying or dividing two or more quantities, add the relative uncertainties of each factor together to obtain the relative uncertainty of the product.[113] When representing uncertainty by significant digits, uncertainty can be coarsely propagated by rounding the result of adding or subtracting two or more quantities to the leftmost last significant decimal place among the summands, and by rounding the result of multiplying or dividing two or more quantities to the least number of significant digits among the factors.[114] (See Significant figures § Arithmetic.)
 More sophisticated methods of dealing with uncertain values include interval arithmetic and affine arithmetic. Interval arithmetic describes operations on intervals. Intervals can be used to represent a range of values if one does not know the precise magnitude, for example, because of measurement errors. Interval arithmetic includes operations like addition and multiplication on intervals, as in 



[
1
,
2
]
+
[
3
,
4
]
=
[
4
,
6
]


{\displaystyle [1,2]+[3,4]=[4,6]}

 and 



[
1
,
2
]
×
[
3
,
4
]
=
[
3
,
8
]


{\displaystyle [1,2]\times [3,4]=[3,8]}

.[115] It is closely related to affine arithmetic, which aims to give more precise results by performing calculations on affine forms rather than intervals. An affine form is a number together with error terms that describe how the number may deviate from the actual magnitude.[116]
 The precision of numerical quantities can be expressed uniformly using normalized scientific notation, which is also convenient for concisely representing numbers which are much larger or smaller than 1. Using scientific notation, a number is decomposed into the product of a number between 1 and 10, called the significand, and 10 raised to some integer power, called the exponent. The significand consists of the significant digits of the number, and is written as a leading digit 1–9 followed by a decimal point and a sequence of digits 0–9. For example, the normalized scientific notation of the number 8276000 is 



8.276
×

10

6




{\displaystyle 8.276\times 10^{6}}

 with significand 8.276 and exponent 6, and the normalized scientific notation of the number 0.00735 is 



7.35
×

10

−
3




{\displaystyle 7.35\times 10^{-3}}

 with significand 7.35 and exponent −3.[117] Unlike ordinary decimal notation, where trailing zeros of large numbers are implicitly considered to be non-significant, in scientific notation every digit in the significand is considered significant, and adding trailing zeros indicates higher precision. For example, while the number 1200 implicitly has only 2 significant digits, the number 



1.20
×

10

3




{\displaystyle 1.20\times 10^{3}}

 explicitly has 3.[118]
 A common method employed by computers to approximate real number arithmetic is called floating-point arithmetic. It represents real numbers similar to the scientific notation through three numbers: a significand, a base, and an exponent.[119] The precision of the significand is limited by the number of bits allocated to represent it. If an arithmetic operation results in a number that requires more bits than are available, the computer rounds the result to the closest representable number. This leads to rounding errors.[120] A consequence of this behavior is that certain laws of arithmetic are violated by floating-point arithmetic. For example, floating-point addition is not associative since the rounding errors introduced can depend on the order of the additions. This means that the result of 



(
a
+
b
)
+
c


{\displaystyle (a+b)+c}

 is sometimes different from the result of 



a
+
(
b
+
c
)


{\displaystyle a+(b+c)}

.[121] The most common technical standard used for floating-point arithmetic is called IEEE 754. Among other things, it determines how numbers are represented, how arithmetic operations and rounding are performed, and how errors and exceptions are handled.[122] In cases where computation speed is not a limiting factor, it is possible to use arbitrary-precision arithmetic, for which the precision of calculations is only restricted by the computer's memory.[123]
 Forms of arithmetic can also be distinguished by the tools employed to perform calculations and include many approaches besides the regular use of pen and paper. Mental arithmetic relies exclusively on the mind without external tools. Instead, it utilizes visualization, memorization, and certain calculation techniques to solve arithmetic problems.[124] One such technique is the compensation method, which consists in altering the numbers to make the calculation easier and then adjusting the result afterward. For example, instead of calculating 



85
−
47


{\displaystyle 85-47}

, one calculates 



85
−
50


{\displaystyle 85-50}

 which is easier because it uses a round number. In the next step, one adds 



3


{\displaystyle 3}

 to the result to compensate for the earlier adjustment.[125] Mental arithmetic is often taught in primary education to train the numerical abilities of the students.[126]
 The human body can also be employed as an arithmetic tool. The use of hands in finger counting is often introduced to young children to teach them numbers and simple calculations. In its most basic form, the number of extended fingers corresponds to the represented quantity and arithmetic operations like addition and subtraction are performed by extending or retracting fingers. This system is limited to small numbers while more advanced systems employ different approaches to represent larger quantities as well.[127] The human voice is used as an arithmetic aid in verbal counting.[128]
 Tally marks are a simple system based on external tools other than the body. It relies on strokes drawn on a surface or notches in a wooden stick to keep track of quantities. Some forms of tally marks arrange the strokes in groups of five to make them easier to read.[129] The abacus is a more advanced tool to represent numbers and perform calculations. An abacus usually consists of a series of rods, each holding several beads. Each bead represents a quantity, which is counted if the bead is moved from one end of a rod to the other. Calculations happen by manipulating the positions of beads until the final bead pattern reveals the result.[130] Related aids include counting boards, which use tokens whose value depends on the area on the board in which they are placed,[131] and counting rods, which are arranged in horizontal and vertical patterns to represent different numbers.[132][f] Sectors and slide rules are more refined calculating instruments that rely on geometric relationships between different scales to perform both basic and advanced arithmetic operations.[134][g] Printed tables were particularly relevant as an aid to look up the results of operations like logarithm and trigonometric functions.[136]
 Mechanical calculators automate manual calculation processes. They present the user with some form of input device to enter numbers by turning dials or pressing keys. They include an internal mechanism usually consisting of gears, levers, and wheels to perform calculations and display the results.[137] For electronic calculators and computers, this procedure is further refined by replacing the mechanical components with electronic circuits like processors that combine and transform electric signals to perform calculations.[138]
 There are many other types of arithmetic. Modular arithmetic operates on a finite set of numbers. If an operation would result in a number outside this finite set then the number is adjusted back into the set, similar to how the hands of clocks start at the beginning again after having completed one cycle. The number at which this adjustment happens is called the modulus. For example, a regular clock has a modulus of 12. In the case of adding 4 to 9, this means that the result is not 13 but 1. The same principle applies also to other operations, such as subtraction, multiplication, and division.[139]
 Some forms of arithmetic deal with operations performed on mathematical objects other than numbers. Interval arithmetic describes operations on intervals.[140] Vector arithmetic and matrix arithmetic describe arithmetic operations on vectors and matrices, like vector addition and matrix multiplication.[141]
 Arithmetic systems can be classified based on the numeral system they rely on. For instance, decimal arithmetic describes arithmetic operations in the decimal system. Other examples are binary arithmetic, octal arithmetic, and hexadecimal arithmetic.[142]
 Compound unit arithmetic describes arithmetic operations performed on magnitudes with compound units. It involves additional operations to govern the transformation between single unit and compound unit quantities. For example, the operation of reduction is used to transform the compound quantity 1 h 90 min into the single unit quantity 150 min.[143]
 Non-Diophantine arithmetics are arithmetic systems that violate traditional arithmetic intuitions and include equations like 



1
+
1
=
1


{\displaystyle 1+1=1}

 and 



2
+
2
=
5


{\displaystyle 2+2=5}

.[144] They can be employed to represent some real-world situations in modern physics and everyday life. For instance, the equation 



1
+
1
=
1


{\displaystyle 1+1=1}

 can be used to describe the observation that if one raindrop is added to another raindrop then they do not remain two separate entities but become one.[145]
 Axiomatic foundations of arithmetic try to provide a small set of laws, called axioms, from which all fundamental properties of and operations on numbers can be derived. They constitute logically consistent and systematic frameworks that can be used to formulate mathematical proofs in a rigorous manner. Two well-known approaches are the Dedekind–Peano axioms and set-theoretic constructions.[146]
 The Dedekind–Peano axioms provide an axiomatization of the arithmetic of natural numbers. Their basic principles were first formulated by Richard Dedekind and later refined by Giuseppe Peano. They rely only on a small number of primitive mathematical concepts, such as 0, natural number, and successor.[h] The Peano axioms determine how these concepts are related to each other. All other arithmetic concepts can then be defined in terms of these primitive concepts.[147]
 Numbers greater than 0 are expressed by repeated application of the successor function 



s


{\displaystyle s}

. For example, 



1


{\displaystyle 1}

 is 



s
(
0
)


{\displaystyle s(0)}

 and 



3


{\displaystyle 3}

 is 



s
(
s
(
s
(
0
)
)
)


{\displaystyle s(s(s(0)))}

. Arithmetic operations can be defined as mechanisms that affect how the successor function is applied. For instance, to add 



2


{\displaystyle 2}

 to any number is the same as applying the successor function two times to this number.[150]
 Various axiomatizations of arithmetic rely on set theory. They cover natural numbers but can also be extended to integers, rational numbers, and real numbers. Each natural number is represented by a unique set. 0 is usually defined as the empty set 



∅


{\displaystyle \varnothing }

. Each subsequent number can be defined as the union of the previous number with the set containing the previous number. For example, 



1
=
0
∪
{
0
}
=
{
0
}


{\displaystyle 1=0\cup \{0\}=\{0\}}

, 



2
=
1
∪
{
1
}
=
{
0
,
1
}


{\displaystyle 2=1\cup \{1\}=\{0,1\}}

, and 



3
=
2
∪
{
2
}
=
{
0
,
1
,
2
}


{\displaystyle 3=2\cup \{2\}=\{0,1,2\}}

.[151] Integers can be defined as ordered pairs of natural numbers where the second number is subtracted from the first one. For instance, the pair (9, 0) represents the number 9 while the pair (0, 9) represents the number -9.[152] Rational numbers are defined as pairs of integers where the first number represents the numerator and the second number represents the denominator. For example, the pair (3, 7) represents the rational number 






3
7





{\displaystyle {\tfrac {3}{7}}}

.[153] One way to construct the real numbers relies on the concept of Dedekind cuts. According to this approach, each real number is represented by a partition of all rational numbers into two sets, one for all numbers below the represented real number and the other for the rest.[154] Arithmetic operations are defined as functions that perform various set-theoretic transformations on the sets representing the input numbers to arrive at the set representing the result.[155]
 The earliest forms of arithmetic are sometimes traced back to counting and tally marks used to keep track of quantities. Some historians suggest that the Lebombo bone (dated about 43,000 years ago) and the Ishango bone (dated about 22,000 to 30,000 years ago) are the oldest arithmetic artifacts but this interpretation is disputed.[156] However, a basic sense of numbers may predate these findings and might even have existed before the development of language.[157]
 It was not until the emergence of ancient civilizations that a more complex and structured approach to arithmetic began to evolve, starting around 3000 BCE. This became necessary because of the increased need to keep track of stored items, manage land ownership, and arrange exchanges.[158] All the major ancient civilizations developed non-positional numeral systems to facilitate the representation of numbers. They also had symbols for operations like addition and subtraction and were aware of fractions. Examples are Egyptian hieroglyphics as well as the numeral systems invented in Sumeria, China, and India.[159] The first positional numeral system was developed by the Babylonians starting around 1800 BCE. This was a significant improvement over earlier numeral systems since it made the representation of large numbers and calculations on them more efficient.[160] Abacuses have been utilized as hand-operated calculating tools since ancient times as efficient means for performing complex calculations.[161]
 Early civilizations primarily used numbers for concrete practical purposes, like commercial activities and tax records, but lacked an abstract concept of number itself.[162] This changed with the ancient Greek mathematicians, who began to explore the abstract nature of numbers rather than studying how they are applied to specific problems.[163] Another novel feature was their use of proofs to establish mathematical truths and validate theories.[164] A further contribution was their distinction of various classes of numbers, such as even numbers, odd numbers, and prime numbers.[165] This included the discovery that numbers for certain geometrical lengths are irrational and therefore cannot be expressed as a fraction.[166] The works of Thales of Miletus and Pythagoras in the 7th and 6th centuries BCE are often regarded as the inception of Greek mathematics.[167] Diophantus was an influential figure in Greek arithmetic in the 3rd century BCE because of his numerous contributions to number theory and his exploration of the application of arithmetic operations to algebraic equations.[168]
 The ancient Indians were the first to develop the concept of zero as a number to be used in calculations. The exact rules of its operation were written down by Brahmagupta in around 628 CE.[169] The concept of zero or none existed long before, but it was not considered an object of arithmetic operations.[170] Brahmagupta further provided a detailed discussion of calculations with negative numbers and their application to problems like credit and debt.[171] The concept of negative numbers itself is significantly older and was first explored in Chinese mathematics in the first millennium BCE.[172]
 Indian mathematicians also developed the positional decimal system used today, in particular the concept of a zero digit instead of empty or missing positions.[173] For example, a detailed treatment of its operations was provided by Aryabhata around the turn of the 6th century CE.[174] The Indian decimal system was further refined and expanded to non-integers during the Islamic Golden Age by Arab mathematicians such as Al-Khwarizmi. His work was influential in introducing the decimal numeral system to the Western world, which at that time relied on the Roman numeral system.[175] There, it was popularized by mathematicians like Leonardo Fibonacci, who lived in the 12th and 13th centuries and also developed the Fibonacci sequence.[176] During the Middle Ages and Renaissance, many popular textbooks were published to cover the practical calculations for commerce. The use of abacuses also became widespread in this period.[177] In the 16th century, the mathematician Gerolamo Cardano conceived the concept of complex numbers as a way to solve cubic equations.[178]
 The first mechanical calculators were developed in the 17th century and greatly facilitated complex mathematical calculations, such as Blaise Pascal's calculator and Gottfried Wilhelm Leibniz's stepped reckoner.[180] The 17th century also saw the discovery of the logarithm by John Napier.[181]
 In the 18th and 19th centuries, mathematicians such as Leonhard Euler and Carl Friedrich Gauss laid the foundations of modern number theory.[182] Another development in this period concerned work on the formalization and foundations of arithmetic, such as Georg Cantor's set theory and the Dedekind–Peano axioms used as an axiomatization of natural-number arithmetic.[183] Computers and electronic calculators were first developed in the 20th century. Their widespread use revolutionized both the accuracy and speed with which even complex arithmetic computations can be calculated.[184]
 Arithmetic education forms part of primary education. It is one of the first forms of mathematics education that children encounter. Elementary arithmetic aims to give students a basic sense of numbers and to familiarize them with fundamental numerical operations like addition, subtraction, multiplication, and division.[185] It is usually introduced in relation to concrete scenarios, like counting beads, dividing the class into groups of children of the same size, and calculating change when buying items. Common tools in early arithmetic education are number lines, addition and multiplication tables, counting blocks, and abacuses.[186]
 Later stages focus on a more abstract understanding and introduce the students to different types of numbers, such as negative numbers, fractions, real numbers, and complex numbers. They further cover more advanced numerical operations, like exponentiation, extraction of roots, and logarithm.[187] They also show how arithmetic operations are employed in other branches of mathematics, such as their application to describe geometrical shapes and the use of variables in algebra. Another aspect is to teach the students the use of algorithms and calculators to solve complex arithmetic problems.[188]
 The psychology of arithmetic is interested in how humans and animals learn about numbers, represent them, and use them for calculations. It examines how mathematical problems are understood and solved and how arithmetic abilities are related to perception, memory, judgment, and decision making.[189] For example, it investigates how collections of concrete items are first encountered in perception and subsequently associated with numbers.[190] A further field of inquiry concerns the relation between numerical calculations and the use of language to form representations.[191] Psychology also explores the biological origin of arithmetic as an inborn ability. This concerns pre-verbal and pre-symbolic cognitive processes implementing arithmetic-like operations required to successfully represent the world and perform tasks like spatial navigation.[192]
 One of the concepts studied by psychology is numeracy, which is the capability to comprehend numerical concepts, apply them to concrete situations, and reason with them. It includes a fundamental number sense as well as being able to estimate and compare quantities. It further encompasses the abilities to symbolically represent numbers in numbering systems, interpret numerical data, and evaluate arithmetic calculations.[193] Numeracy is a key skill in many academic fields. A lack of numeracy can inhibit academic success and lead to bad economic decisions in everyday life, for example, by misunderstanding mortgage plans and insurance policies.[194]
 The philosophy of arithmetic studies the fundamental concepts and principles underlying numbers and arithmetic operations. It explores the nature and ontological status of numbers, the relation of arithmetic to language and logic, and how it is possible to acquire arithmetic knowledge.[195]
 According to Platonism, numbers have mind-independent existence: they exist as abstract objects outside spacetime and without causal powers.[196][j] This view is rejected by intuitionists, who claim that mathematical objects are mental constructions.[198] Further theories are logicism, which holds that mathematical truths are reducible to logical truths,[199] and formalism, which states that mathematical principles are rules of how symbols are manipulated without claiming that they correspond to entities outside the rule-governed activity.[200]
 The traditionally dominant view in the epistemology of arithmetic is that arithmetic truths are knowable a priori. This means that they can be known by thinking alone without the need to rely on sensory experience.[201] Some proponents of this view state that arithmetic knowledge is innate while others claim that there is some form of rational intuition through which mathematical truths can be apprehended.[202] A more recent alternative view was suggested by naturalist philosophers like Willard Van Orman Quine, who argue that mathematical principles are high-level generalizations that are ultimately grounded in the sensory world as described by the empirical sciences.[203]
 Arithmetic is relevant to many fields. In daily life, it is required to calculate change when shopping, manage personal finances, and adjust a cooking recipe for a different number of servings. Businesses use arithmetic to calculate profits and losses and analyze market trends. In the field of engineering, it is used to measure quantities, calculate loads and forces, and design structures.[204] Cryptography relies on arithmetic operations to protect sensitive information by encrypting data and messages.[205]
 Arithmetic is intimately connected to many branches of mathematics that depend on numerical operations. Algebra relies on arithmetic principles to solve equations using variables. These principles also play a key role in calculus in its attempt to determine rates of change and areas under curves. Geometry uses arithmetic operations to measure the properties of shapes while statistics utilizes them to analyze numerical data.[206] Due to the relevance of arithmetic operations throughout mathematics, the influence of arithmetic extends to most sciences such as physics, computer science, and economics. These operations are used in calculations, problem-solving, data analysis, and algorithms, making them integral to scientific research, technological development, and economic modeling.[207]


Source: https://en.wikipedia.org/wiki/Komplexe_Zahl
Content: Other reasons this message may be displayed:


Source: https://en.wikipedia.org/wiki/Zeichenkette
Content: Other reasons this message may be displayed:


Source: https://en.wikipedia.org/wiki/Java_(Programmiersprache)
Content: Other reasons this message may be displayed:


Source: https://en.wikipedia.org/wiki/Smalltalk_(Programmiersprache)
Content: Other reasons this message may be displayed:


Source: https://en.wikipedia.org/wiki/Lisp
Content: A lisp is a  speech impairment in which a person misarticulates sibilants ([s], [z], [ts], [dz], [ʃ], [ʒ], [t͡ʃ], [d͡ʒ]).[1] These misarticulations often result in unclear speech.
 Successful treatments have shown that causes are functional rather than physical: that is, most lisps are caused by errors in tongue placement or density of the tongue within the mouth rather than caused by any injury or congenital or acquired deformity to the mouth. The most frequently discussed of these problems is tongue thrust in which the tongue protrudes beyond the front teeth.[3] This protrusion affects speech as well as swallowing and can lead to lisping. Ankyloglossia or tongue tie can also be responsible for lisps in children — however, it is unclear whether these deficiencies are caused by the tongue tie itself or the muscle weakness following the correction of the tongue tie.[4] Overbites and underbites may also contribute to non lingual lisping. Temporary lisps can be caused by dental work, dental appliances such as dentures or retainers or by swollen or bruised tongues.
 Lisps caused by tongue tie can be treated by a dentist or otolaryngologist (ENT) with a lingual frenectomy, or laser incision, which takes less than 10 to 15 minutes to complete.[5][6][7]
 With an interdental lisp, the therapist teaches the student how to keep the tongue behind the two front incisors.[8]
 One popular method of correcting articulation or lisp disorders is to isolate sounds and work on correcting the sound in isolation. The basic sound, or phoneme, is selected as a target for treatment. Typically the position of the sound within a word is considered and targeted. The sound appears in the beginning of the word, middle, or end of the word (initial, medial, or final).
 Take for example, correction of an "S" sound (lisp). Most likely, a speech language pathologist (SLP) would employ exercises to work on "Sssssss."[clarify] Starting practice words would most likely consist of "S-initial" words such as "say, sun, soap, sip, sick, said, sail." According to this protocol, the SLP slowly increases the complexity of tasks (context of pronunciations) as the production of the sound improves. Examples of increased complexity could include saying words in phrases and sentences, saying longer multi syllabic words, or increasing the tempo of pronunciation.
 Using this method, the SLP achieves success with their student by targeting a sound in a phonetically consistent manner. Phonetic consistency means that a target sound is isolated at the smallest possible level (phoneme, phone, or allophone) and that the context of production must be consistent. Consistency is critical, because factors such as the position within the word, grouping with other sounds (vowels or consonants), and the complexity all may affect production.
 Another popular method for treating a lisp is using specially designed devices that go in the mouth to provide a tactile cue of exactly where the tongue should be positioned when saying the "S" sound. This tactile feedback has been shown to correct lisp errors twice as fast as traditional therapy. 
 Using either or both methods, the repetition of consistent contexts allows the student to align all the necessary processes required to properly produce language; language skills (ability to formulate correct sounds in the brain: What sounds do I need to make?), motor planning (voicing and jaw and tongue movements: How do I produce the sound?), and auditory processing (receptive feedback: Was the sound produced correctly? Do I need to correct?).
 A student with an articulation or lisp disorder has a deficiency in one or more of these areas. To correct the deficiency, adjustments have to be made in one or more of these processes. The process to correct it is more often than not, trial and error. With so many factors, however, isolating the variables (the sound) is imperative to getting to the result faster. 
 A phonetically consistent treatment strategy means practicing the same thing over and over. What is practiced is consistent and does not change. The words might change, but the phoneme and its positioning is the same (say, sip, sill, soap, ...). Thus, successful correction of the disorder is found in manipulating or changing the other factors involved with speech production (tongue positioning, cerebral processing, etc.). Once a successful result (speech) is achieved, then consistent practice becomes essential to reinforcing correct productions.
 When the difficult sound is mastered, the student will then learn to say the sound in syllables, then words, then phrases and then sentences. When a student can speak a whole sentence without lisping, attention is then focused on making correct sounds throughout natural conversation. Towards the end of the course of therapy, the student will be taught how to monitor his or her own speech, and how to correct as necessary. Speech therapy can sometimes fix the problem, but in some cases speech therapy fails to work.


Source: https://en.wikipedia.org/wiki/Perl_(Programmiersprache)
Content: Other reasons this message may be displayed:


Source: https://en.wikipedia.org/wiki/Objective_CAML
Content: 
 OCaml (/oʊˈkæməl/ oh-KAM-əl, formerly Objective Caml) is a general-purpose, high-level, multi-paradigm programming language which extends the Caml dialect of ML with object-oriented features. OCaml was created in 1996 by Xavier Leroy, Jérôme Vouillon, Damien Doligez, Didier Rémy, Ascánder Suárez, and others.
 The OCaml toolchain includes an interactive top-level interpreter, a bytecode compiler, an optimizing native code compiler, a reversible debugger, and a package manager (OPAM).  OCaml was initially developed in the context of automated theorem proving, and is used in static analysis and formal methods software. Beyond these areas, it has found use in systems programming, web development, and specific financial utilities, among other application domains.
 The acronym CAML originally stood for Categorical Abstract Machine Language, but OCaml omits this abstract machine.[5] OCaml is a free and open-source software project managed and principally maintained by the French Institute for Research in Computer Science and Automation (Inria). In the early 2000s, elements from OCaml were adopted by many languages, notably F# and Scala.
 ML-derived languages are best known for their static type systems and type-inferring compilers. OCaml unifies functional, imperative, and object-oriented programming under an ML-like type system. Thus, programmers need not be highly familiar with the pure functional language paradigm to use OCaml.
 By requiring the programmer to work within the constraints of its static type system, OCaml eliminates many of the type-related runtime problems associated with dynamically typed languages. Also, OCaml's type-inferring compiler greatly reduces the need for the manual type annotations that are required in most statically typed languages. For example, the data types of variables and the signatures of functions usually need not be declared explicitly, as they do in languages like Java and C#, because they can be inferred from the operators and other functions that are applied to the variables and other values in the code. Effective use of OCaml's type system can require some sophistication on the part of a programmer, but this discipline is rewarded with reliable, high-performance software. 
 OCaml is perhaps most distinguished from other languages with origins in academia by its emphasis on performance. Its static type system prevents runtime type mismatches and thus obviates runtime type and safety checks that burden the performance of dynamically typed languages, while still guaranteeing runtime safety, except when array bounds checking is turned off or when some type-unsafe features like serialization are used. These are rare enough that avoiding them is quite possible in practice.
 Aside from type-checking overhead, functional programming languages are, in general, challenging to compile to efficient machine language code, due to issues such as the funarg problem. Along with standard loop, register, and instruction optimizations, OCaml's optimizing compiler employs static program analysis methods to optimize value boxing and closure allocation, helping to maximize the performance of the resulting code even if it makes extensive use of functional programming constructs.
 Xavier Leroy has stated that "OCaml delivers at least 50% of the performance of a decent C compiler",[6] although a direct comparison is impossible. Some functions in the OCaml standard library are implemented with faster algorithms than equivalent functions in the standard libraries of other languages. For example, the implementation of set union in the OCaml standard library in theory is asymptotically faster than the equivalent function in the standard libraries of imperative languages (e.g., C++, Java) because the OCaml implementation can exploit the immutability of sets to reuse parts of input sets in the output (see persistent data structure).
 Between the 1970s and 1980s, Robin Milner, a British computer scientist and Turing Award winner, worked at the University of Edinburgh's Laboratory for Foundations of Computer Science.[7][8] Milner and others were working on theorem provers, which were historically developed in languages such as Lisp. Milner repeatedly ran into the issue that the theorem provers would attempt to claim a proof was valid by putting non-proofs together.[8] As a result, he went on to develop the meta language for his Logic for Computable Functions, a language that would only allow the writer to construct valid proofs with its polymorphic type system.[9] ML was turned into a compiler in order to simplify using LCF on different machines, and, by the 1980s, was turned into a complete system of its own.[9] ML would eventually serve as a basis for the creation of OCaml.
 In the early 1980s, there were some developments that prompted INRIA's Formel team to become interested in the ML language. Luca Cardelli, a research professor at University of Oxford, used his Functional Abstract Machine to develop a faster implementation of ML, and Robin Milner proposed a new definition of ML in order to avoid divergence between various implementations. Simultaneously, Pierre-Louis Curien, a senior researcher at Paris Diderot University, developed a calculus of categorical combinators and linked it to lambda calculus, which led to the definition of the Categorical Abstract Machine (CAM). Guy Cousineau, a researcher at Paris Diderot University, recognized that this could be applied as a compilation technique for ML.[10]
 Caml was initially designed and developed by INRIA's Formel team headed by Gérard Huet. The first implementation of Caml was created in 1987 and was further developed until 1992. Though it was spearheaded by Ascánder Suárez, Pierre Weis and Michel Mauny carried on with development after he left in 1988.[10]
 Guy Cousineau is quoted recalling that his experience with programming language implementation was initially very limited, and that there were multiple inadequacies for which he is responsible. Despite this, he believes that "Ascander, Pierre and Michel did quite a nice piece of work.”[10]
 Between 1990 and 1991, Xavier Leroy designed a new implementation of Caml based on a bytecode interpreter written in C. In addition to this, Damien Doligez wrote a memory management system, also known as a sequential garbage collector, for this implementation.[9] This new implementation, known as Caml Light, replaced the old Caml implementation and ran on small desktop machines.[10] In the following years, libraries such as Michel Mauny's syntax manipulation tools appeared and helped promote the use of Caml in educational and research teams.[9]
 In 1995, Xavier Leroy released Caml Special Light, which was an improved version of Caml.[10] An optimizing native-code compiler was added to the bytecode compiler, which greatly increased performance to comparable levels with mainstream languages such as C++.[9][10] Additionally, Leroy designed a high-level module system inspired by the module system of Standard ML which provided powerful facilities for abstraction and parameterization and made larger-scale programs easier to construct.[9]
 Didier Rémy and Jérôme Vouillon designed an expressive type system for objects and classes, which was integrated within Caml Special Light. This led to the emergence of the Objective Caml language, first released in 1996 and subsequently renamed to OCaml in 2011. This object system notably supported many prevalent object-oriented idioms in a statically type-safe way, while those same idioms caused unsoundness or required runtime checks in languages such as C++ or Java. In 2000, Jacques Garrigue extended Objective Caml with multiple new features such as polymorphic methods, variants, and labeled and optional arguments.[9][10]
 Language improvements have been incrementally added for the last two decades in order to support the growing commercial and academic codebases in OCaml.[9] The OCaml 4.0 release in 2012 added Generalized Algebraic Data Types (GADTs) and first-class modules in order to increase the flexibility of the language.[9] The OCaml 5.0.0 release in 2022[11] is a complete rewrite of the language runtime, removing the global GC lock and adding effect handlers via delimited continuations. These changes enable support for shared-memory parallelism and color-blind concurrency respectively.
 OCaml's development continued within the Cristal team at INRIA until 2005, when it was succeeded by the Gallium team.[12] Subsequently, Gallium was succeeded by the Cambium team in 2019.[13][14] As of 2023, there are 23 core developers of the compiler distribution from a variety of organizations[15] and 41 developers for the broader OCaml tooling and packaging ecosystem.[16]
 OCaml features a static type system, type inference, parametric polymorphism, tail recursion, pattern matching, first class lexical closures, functors (parametric modules), exception handling, effect handling, and incremental generational automatic garbage collection.
 OCaml is notable for extending ML-style type inference to an object system in a general-purpose language. This permits structural subtyping, where object types are compatible if their method signatures are compatible, regardless of their declared inheritance (an unusual feature in statically typed languages).
 A foreign function interface for linking to C primitives is provided, including language support for efficient numerical arrays in formats compatible with both C and Fortran. OCaml also supports creating libraries of OCaml functions that can be linked to a main program in C, so that an OCaml library can be distributed to C programmers who have no knowledge or installation of OCaml.
 The OCaml distribution contains:
 The native code compiler is available for many platforms, including Unix, Microsoft Windows, and Apple macOS. Portability is achieved through native code generation support for major architectures: 
 The bytecode compiler supports operation on any 32- or 64-bit architecture when native code generation is not available, requiring only a C compiler.
 OCaml bytecode and native code programs can be written in a multithreaded style, with preemptive context switching. OCaml threads in the same domain[18] execute by time sharing only. However, an OCaml program can contain several domains. There are several libraries for distributed computing such as Functory Archived 20 January 2022 at the Wayback Machine and ocamlnet/Plasma.
 Since 2011, many new tools and libraries have been contributed to the OCaml development environment:[19]
 Snippets of OCaml code are most easily studied by entering them into the top-level REPL. This is an interactive OCaml session that prints the inferred types of resulting or defined expressions.[20] The OCaml top-level is started by simply executing the OCaml program:
 Code can then be entered at the "#" prompt. For example, to calculate 1+2*3:
 OCaml infers the type of the expression to be "int" (a machine-precision integer) and gives the result "7".
 The following program "hello.ml":
 can be compiled into a bytecode executable:
 or compiled into an optimized native-code executable:
 and executed:
 The first argument to ocamlc, "hello.ml", specifies the source file to compile and the "-o hello" flag specifies the output file.[21]
 
The option type constructor in OCaml, similar to the Maybe type in Haskell, augments a given data type to either return Some value of the given data type, or to return None.[22] This is used to express that a value might or might not be present. This is an example of a function that either extracts an int from an option, if there is one inside, and converts it into a string, or if not, returns an empty string: Lists are one of the fundamental datatypes in OCaml. The following code example defines a recursive function sum that accepts one argument, integers, which is supposed to be a list of integers. Note the keyword rec which denotes that the function is recursive. The function recursively iterates over the given list of integers and provides a sum of the elements. The match statement has similarities to C's switch element, though it is far more general.
 Another way is to use standard fold function that works with lists.
 Since the anonymous function is simply the application of the + operator, this can be shortened to:
 Furthermore, one can omit the list argument by making use of a partial application:
 OCaml lends itself to concisely expressing recursive algorithms. The following code example implements an algorithm similar to quicksort that sorts a list in increasing order.
 Or using partial application of the >= operator.
 The following program calculates the smallest number of people in a room for whom the probability of completely unique birthdays is less than 50% (the birthday problem, where for 1 person the probability is 365/365 (or 100%), for 2 it is 364/365, for 3 it is 364/365 × 363/365, etc.) (answer = 23).
 The following code defines a Church encoding of natural numbers, with successor (succ) and addition (add). A Church numeral n is a higher-order function that accepts a function f and a value x and applies f to x exactly n times. To convert a Church numeral from a functional value to a string, we pass it a function that prepends the string "S" to its input and the constant string "0".
 A variety of libraries are directly accessible from OCaml. For example, OCaml has a built-in library for arbitrary-precision arithmetic. As the factorial function grows very rapidly, it quickly overflows machine-precision numbers (typically 32- or 64-bits). Thus, factorial is a suitable candidate for arbitrary-precision arithmetic.
 In OCaml, the Num module (now superseded by the ZArith module) provides arbitrary-precision arithmetic and can be loaded into a running top-level using:
 The factorial function may then be written using the arbitrary-precision numeric operators =/, */ and -/ :
 This function can compute much larger factorials, such as 120!:
 The following program renders a rotating triangle in 2D using OpenGL:
 The LablGL bindings to OpenGL are required. The program may then be compiled to bytecode with:
 or to nativecode with:
 or, more simply, using the ocamlfind build command
 and run:
 Far more sophisticated, high-performance 2D and 3D graphical programs can be developed in OCaml. Thanks to the use of OpenGL and OCaml, the resulting programs can be cross-platform, compiling without any changes on many major platforms.
 The following code calculates the Fibonacci sequence of a number n inputted. It uses tail recursion and pattern matching.
 Functions may take functions as input and return functions as result. For example, applying twice to a function f yields a function that applies f two times to its argument.
 The function twice uses a type variable 'a to indicate that it can be applied to any function f mapping from a type 'a to itself, rather than only to int->int functions. In particular, twice can even be applied to itself.
 MetaOCaml[23] is a multi-stage programming extension of OCaml enabling incremental compiling of new machine code during runtime. Under some circumstances, significant speedups are possible using multistage programming, because more detailed information about the data to process is available at runtime than at the regular compile time, so the incremental compiler can optimize away many cases of condition checking, etc.
 As an example: if at compile time it is known that some power function x -> x^n is needed often, but the value of n is known only at runtime, a two-stage power function can be used in MetaOCaml:
 As soon as n is known at runtime, a specialized and very fast power function can be created:
 The result is:
 The new function is automatically compiled.
 At least several dozen companies use OCaml to some degree.[30] Notable examples include:


Source: https://en.wikipedia.org/wiki/Duck-Typing
Content: Other reasons this message may be displayed:


Source: https://en.wikipedia.org/wiki/Identit%C3%A4t_(Logik)
Content: Other reasons this message may be displayed:


Source: https://en.wikipedia.org/wiki/Tupel_(Informatik)
Content: Other reasons this message may be displayed:


Source: https://en.wikipedia.org/wiki/Menge_(Datenstruktur)
Content: Other reasons this message may be displayed:


Source: https://en.wikipedia.org/wiki/Assoziatives_Datenfeld
Content: Other reasons this message may be displayed:


Source: https://en.wikipedia.org/wiki/Feld_(Datentyp)
Content: Other reasons this message may be displayed:


Source: https://en.wikipedia.org/wiki/Iteration
Content: 
 Iteration is the repetition of a process in order to generate a (possibly unbounded) sequence of outcomes. Each repetition of the process is a single iteration, and the outcome of each iteration is then the starting point of the next iteration. 
 In mathematics and computer science, iteration (along with the related technique of recursion) is a standard element of algorithms.
 In mathematics, iteration may refer to the process of iterating a function, i.e. applying a function repeatedly, using the output from one iteration as the input to the next. Iteration of apparently simple functions can produce complex behaviors and difficult problems – for examples, see the Collatz conjecture and juggler sequences.
 Another use of iteration in mathematics is in iterative methods which are used to produce approximate numerical solutions to certain mathematical problems. Newton's method is an example of an iterative method. Manual calculation of a number's square root is a common use and a well-known example.
 In computing, iteration is the technique marking out of a block of statements within a computer program for a defined number of repetitions.  That block of statements is said to be iterated; a computer scientist might also refer to that block of statements as an "iteration".
 Loops constitute the most common language constructs for performing iterations. The following pseudocode "iterates" three times the line of code between begin & end through a for loop, and uses the values of i as increments.
 It is permissible, and often necessary, to use values from other parts of the program outside the bracketed block of statements, to perform the desired function.
 Iterators constitute alternative language constructs to loops, which ensure consistent iterations over specific data structures. They can eventually save time and effort in later coding attempts. In particular, an iterator allows one to repeat the same kind of operation at each node of such a data structure, often in some pre-defined order.
 Iteratees are purely functional language constructs, which accept or reject data during the iterations.
 Recursions and iterations have different algorithmic definitions, even though they can generate identical effects/results.  The primary difference is that recursion can be employed as a solution without prior knowledge as to how many times the action will have to repeat, while a successful iteration requires that foreknowledge. 
 Some types of programming languages, known as functional programming languages, are designed such that they do not set up a block of statements for explicit repetition, as with the for loop.  Instead, those programming languages exclusively use recursion.  Rather than call out a block of code to be repeated a pre-defined number of times, the executing code block instead "divides" the work to be done into a number of separate pieces, after which the code block executes itself on each individual piece.  Each piece of work will be divided repeatedly until the "amount" of work is as small as it can possibly be, at which point the algorithm will do that work very quickly.  The algorithm then "reverses" and reassembles the pieces into a complete whole.
 The classic example of recursion is in list-sorting algorithms, such as merge sort.  The merge sort recursive algorithm will first repeatedly divide the list into consecutive pairs; each pair is then ordered, then each consecutive pair of pairs, and so forth until the elements of the list are in the desired order.
 The code below is an example of a recursive algorithm in the Scheme programming language that will output the same result as the pseudocode under the previous heading.  
 In some schools of pedagogy, iterations are used to describe the process of teaching or guiding students to repeat experiments, assessments, or projects, until more accurate results are found, or the student has mastered the technical skill.  This idea is found in the old adage, "Practice makes perfect." In particular, "iterative" is defined as the "process of learning and development that involves cyclical inquiry, enabling multiple opportunities for people to revisit ideas and critically reflect on their implication."[1]
 Unlike computing and math, educational iterations are not predetermined; instead, the task is repeated until success according to some external criteria (often a test) is achieved.


Source: https://en.wikipedia.org/wiki/Menge_(Mathematik)#Durchschnitt_(Schnittmenge,_Schnitt)
Content: Other reasons this message may be displayed:


Source: https://en.wikipedia.org/wiki/Mengenlehre#Differenz_und_Komplement
Content: Grundzüge der Mengenlehre (German for "Basics of Set Theory") is a book on set theory written by Felix Hausdorff.
 First published in April 1914, Grundzüge der Mengenlehre was the first comprehensive introduction to set theory. Besides the systematic treatment of known results in set theory, the book also contains chapters on measure theory and topology, which were then still considered parts of set theory. Hausdorff presented and developed original material which was later to become the basis for those areas. In 1927 Hausdorff published an extensively revised second edition under the title Mengenlehre (German for "Set Theory"), with many of the topics of the first edition omitted. In 1935 there was a third German edition, which in 1957 was translated by John R. Aumann et al. into English under the title Set Theory.
 Chelsea Publishing Company reprinted the German 1914 edition in New York City in German in 1944, 1949, 1965, 1978 and 1991 but never issued an English translation of this first edition (or the 1927 second edition) to date. When the American Mathematical Society took over and set up AMS Chelsea Publishing it published editions in 2005 and 2021. 
 
 This article about a mathematical publication is a stub. You can help Wikipedia by expanding it.

Source: https://en.wikipedia.org/wiki/Menge_(Mathematik)#Vereinigung_(Vereinigungsmenge)
Content: Other reasons this message may be displayed:


Source: https://en.wikipedia.org/wiki/Typsystem
Content: Other reasons this message may be displayed:


Source: https://en.wikipedia.org/wiki/Klasse_(Objektorientierung)
Content: Other reasons this message may be displayed:


Source: https://en.wikipedia.org/wiki/Vererbung_(Programmierung)
Content: Other reasons this message may be displayed:


Source: https://en.wikipedia.org/wiki/Mehrfachvererbung
Content: Other reasons this message may be displayed:


Source: https://en.wikipedia.org/wiki/Strukturierte_Programmierung
Content: Other reasons this message may be displayed:


Source: https://en.wikipedia.org/wiki/Pattern_Matching#Programmierung
Content: In computer science, pattern matching is the act of checking a given sequence of tokens for the presence of the constituents of some pattern. In contrast to pattern recognition, the match usually has to be exact: "either it will or will not be a  match."  The patterns generally have the form of either sequences or tree structures. Uses of pattern matching include outputting the locations (if any) of a pattern within a token sequence, to output some component of the matched pattern, and to substitute the matching pattern with some other token sequence (i.e., search and replace).
 Sequence patterns (e.g., a text string) are often described using regular expressions and matched using techniques such as backtracking.
 Tree patterns are used in some programming languages as a general tool to process data based on its structure, e.g. C#,[1] F#,[2] Haskell,[3] ML, Python,[4] Ruby,[5] Rust,[6] Scala,[7] Swift[8] and the symbolic mathematics language Mathematica have special syntax for expressing tree patterns and a language construct for conditional execution and value retrieval based on it.
 Often it is possible to give alternative patterns that are tried one by one, which yields a powerful conditional programming construct. Pattern matching sometimes includes support for guards.[citation needed]
 Early programming languages with pattern matching constructs include COMIT (1957), SNOBOL (1962), Refal (1968) with tree-based pattern matching, Prolog (1972), St Andrews Static Language (SASL) (1976), NPL (1977), and Kent Recursive Calculator (KRC) (1981).
 Many text editors support pattern matching of various kinds: the QED editor supports regular expression search, and some versions of TECO support the OR operator in searches.
 Computer algebra systems generally support pattern matching on algebraic expressions.[9]
 The simplest pattern in pattern matching is an explicit value or a variable. For an example, consider a simple function definition in Haskell syntax (function parameters are not in parentheses but are separated by spaces, = is not assignment but definition):
 Here, 0 is a single value pattern. Now, whenever f is given 0 as argument the pattern matches and the function returns 1. With any other argument, the matching and thus the function fail. As the syntax supports alternative patterns in function definitions, we can continue the definition extending it to take more generic arguments:
 Here, the first n is a single variable pattern, which will match absolutely any argument and bind it to name n to be used in the rest of the definition. In Haskell (unlike at least Hope), patterns are tried in order so the first definition still applies in the very specific case of the input being 0, while for any other argument the function returns n * f (n-1) with n being the argument.
 The wildcard pattern (often written as _) is also simple: like a variable name, it matches any value, but does not bind the value to any name. Algorithms for matching wildcards in simple string-matching situations have been developed in a number of recursive and non-recursive varieties.[10]
 More complex patterns can be built from the primitive ones of the previous section, usually in the same way as values are built by combining other values. The difference then is that with variable and wildcard parts, a pattern doesn't build into a single value, but matches a group of values that are the combination of the concrete elements and the elements that are allowed to vary within the structure of the pattern.
 A tree pattern describes a part of a tree by starting with a node and specifying some branches and nodes and leaving some unspecified with a variable or wildcard pattern. It may help to think of the abstract syntax tree of a programming language and algebraic data types.
 In Haskell, the following line defines an algebraic data type Color that has a single data constructor ColorConstructor that wraps an integer and a string.
 The constructor is a node in a tree and the integer and string are leaves in branches.
 When we want to write functions to make Color an abstract data type, we wish to write functions to interface with the data type, and thus we want to extract some data from the data type, for example, just the string or just the integer part of Color.
 If we pass a variable that is of type Color, how can we get the data out of this variable? For example, for a function to get the integer part of Color, we can use a simple tree pattern and write:
 As well:
 The creations of these functions can be automated by Haskell's data record syntax.
 This OCaml example which defines a red–black tree and a function to re-balance it after element insertion shows how to match on a more complex structure generated by a recursive data type. The compiler verifies at compile-time that the list of cases is exhaustive and none are redundant.
 Pattern matching can be used to filter data of a certain structure. For instance, in Haskell a list comprehension could be used for this kind of filtering:
 evaluates to
 In Mathematica, the only structure that exists is the tree, which is populated by symbols. In the Haskell syntax used thus far, this could be defined as
 An example tree could then look like
 In the traditional, more suitable syntax, the symbols are written as they are and the levels of the tree are represented using [], so that for instance a[b,c] is a tree with a as the parent, and b and c as the children.
 A pattern in Mathematica involves putting "_" at positions in that tree. For instance, the pattern
 will match elements such as A[1], A[2], or more generally A[x] where x is any entity. In this case, A is the concrete element, while _ denotes the piece of tree that can be varied. A symbol prepended to _ binds the match to that variable name while a symbol appended to _ restricts the matches to nodes of that symbol. Note that even blanks themselves are internally represented as Blank[] for _ and Blank[x] for _x.
 The Mathematica function Cases filters elements of the first argument that match the pattern in the second argument:[11]
 evaluates to
 Pattern matching applies to the structure of expressions. In the example below,
 returns
 because only these elements will match the pattern a[b[_],_] above.
 In Mathematica, it is also possible to extract structures as they are created in the course of computation, regardless of how or where they appear. The function Trace can be used to monitor a computation, and return the elements that arise which match a pattern. For example, we can define the Fibonacci sequence as
 Then, we can ask the question: Given fib[3], what is the sequence of recursive Fibonacci calls?
 returns a structure that represents the occurrences of the pattern fib[_] in the computational structure:
 In symbolic programming languages, it is easy to have patterns as arguments to functions or as elements of data structures. A consequence of this is the ability to use patterns to declaratively make statements about pieces of data and to flexibly instruct functions how to operate.
 For instance, the Mathematica function Compile can be used to make more efficient versions of the code. In the following example the details do not particularly matter; what matters is that the subexpression {{com[_],  Integer}} instructs Compile that expressions of the form com[_] can be assumed to be integers for the purposes of compilation:
 Mailboxes in Erlang also work this way.
 The Curry–Howard correspondence between proofs and programs relates ML-style pattern matching to case analysis and proof by exhaustion.
 By far the most common form of pattern matching involves strings of characters. In many programming languages, a particular syntax of strings is used to represent regular expressions, which are patterns describing string characters.
 However, it is possible to perform some string pattern matching within the same framework that has been discussed throughout this article.
 In Mathematica, strings are represented as trees of root StringExpression and all the characters in order as children of the root. Thus, to match "any amount of trailing characters", a new wildcard ___ is needed in contrast to _ that would match only a single character.
 In Haskell and functional programming languages in general, strings are represented as functional lists of characters. A functional list is defined as an empty list, or an element constructed on an existing list. In Haskell syntax:
 The structure for a list with some elements is thus element:list. When pattern matching, we assert that a certain piece of data is equal to a certain pattern. For example, in the function:
 We assert that the first element of head's argument is called element, and the function returns this. We know that this is the first element because of the way lists are defined, a single element constructed onto a list. This single element must be the first. The empty list would not match the pattern at all, as an empty list does not have a head (the first element that is constructed).
 In the example, we have no use for list, so we can disregard it, and thus write the function:
 The equivalent Mathematica transformation is expressed as
 In Mathematica, for instance,
 will match a string that has two characters and begins with "a".
 The same pattern in Haskell:
 Symbolic entities can be introduced to represent many different classes of relevant features of a string. For instance,
 will match a string that consists of a letter first, and then a number.
 In Haskell, guards could be used to achieve the same matches:
 The main advantage of symbolic string manipulation is that it can be completely integrated with the rest of the programming language, rather than being a separate, special purpose subunit. The entire power of the language can be leveraged to build up the patterns themselves or analyze and transform the programs that contain them.
 SNOBOL (StriNg Oriented and symBOlic Language) is a computer programming language developed between 1962 and 1967 at AT&T Bell Laboratories by David J. Farber, Ralph E. Griswold and Ivan P. Polonsky.
 SNOBOL4 stands apart from most programming languages by having patterns as a first-class data type (i.e. a data type whose values can be manipulated in all ways permitted to any other data type in the programming language) and by providing operators for pattern concatenation and alternation. Strings generated during execution can be treated as programs and executed.
 SNOBOL was quite widely taught in larger US universities in the late 1960s and early 1970s and was widely used in the 1970s and 1980s as a text manipulation language in the humanities.
 Since SNOBOL's creation, newer languages such as Awk and Perl have made string manipulation by means of regular expressions fashionable. SNOBOL4 patterns, however, subsume BNF grammars, which are equivalent to context-free grammars and more powerful than regular expressions.[12]


Source: https://en.wikipedia.org/wiki/Miranda_(Programmiersprache)
Content: Other reasons this message may be displayed:


Source: https://en.wikipedia.org/wiki/Haskell_(Programmiersprache)
Content: Other reasons this message may be displayed:


Source: https://en.wikipedia.org/wiki/Einr%C3%BCckungsstil
Content: Other reasons this message may be displayed:


Source: https://en.wikipedia.org/wiki/Peter_J._Landin
Content: 
 Peter John Landin (5 June 1930 – 3 June 2009[1][2]) was a British computer scientist. He was one of the first to realise that the lambda calculus could be used to model a programming language, an insight that is essential to the development of both functional programming and denotational semantics.
 Landin was born in Sheffield, where he attended King Edward VII School; he graduated from Clare College, Cambridge.[2] From 1960 to 1964, he was the assistant to Christopher Strachey when the latter was an independent computer consultant in London.[3] Most of his work was published during this period and the brief time he worked for Univac and at the Massachusetts Institute of Technology in the United States, before taking a position at Queen Mary University of London. During the 1970s and 1980s, his efforts went into building the computer science department in Queen Mary College, developing courses, and teaching students, as set forth in the foreword to the textbook Programming from First Principles.[4] On his retirement, he was appointed Emeritus Professor of Theoretical Computation at Queen Mary University of London, where in 2012, the computer science building was renamed the Peter Landin Building in his honour.
 At a workshop at the Science Museum, London, in 2001, on the history of programming semantics he spoke of how his scholarly career in computer science began in the late 1950s and of how he was much influenced by a study of John McCarthy's Lisp language when the most commonly used language was Fortran.[5]
 He was active in the definition of the ALGOL programming language. He is listed among those who attended the November 1959 conference in Paris,[6] and the 1962 conference,[7][8] and cited by Tony Hoare as one of the people who taught him ALGOL 60 and hence facilitated his expression of powerful recursive algorithms:
 "Around Easter 1961, a course on ALGOL 60 was offered in Brighton, England, with Peter Naur, Edsger W. Dijkstra, and Peter Landin as tutors. ... It was there that I first learned about recursive procedures and saw how to program the sorting method which I had earlier found such difficulty in explaining. It was there that I wrote the procedure, immodestly named QUICKSORT, on which my career as a computer scientist is founded. Due credit must be paid to the genius of the designers of ALGOL 60 who included recursion in their language and enabled me to describe my invention so elegantly to the world. I have regarded it as the highest goal of programming language design to enable good ideas to be elegantly expressed."[9] Landin was involved with international standards in programming and informatics, as a member of the International Federation for Information Processing (IFIP) IFIP Working Group 2.1 on Algorithmic Languages and Calculi,[10] which specified, maintains, and supports the programming languages ALGOL 60 and ALGOL 68.[11]
 Landin is responsible for inventing the stack, environment, control, dump SECD machine, the first abstract machine for a functional programming language,[12] and the ISWIM programming language, defining the Landin off-side rule and for coining the term syntactic sugar. The off-side rule allows bounding scope declaration by use of white spaces as seen in languages such as Miranda, Haskell, Python, and F# (using the light syntax).
 Another phrase originating with Landin is "The next 700 ..." after his influential paper The next 700 programming languages.[13] "700" was chosen because Landin had read in the Journal of the ACM that there were already 700 programming languages in existence.[14] The paper opens with the quotation "... today ... 1,700 special programming languages used to 'communicate' in over 700 application areas."[15] It also includes the joke that
 A possible first step in the research program is 1700 doctoral theses called "A Correspondence between x and Church's λ-notation." a reference to his earlier paper.[16] This dry sense of humour is expressed in many of his papers.
 Landin, who was bisexual,[2] became involved with the Gay Liberation Front (GLF) during the early 1970s. He was once arrested as part of an anti-nuclear demonstration.[17]
He was a dedicated cyclist and moved around London on his bike until it became physically impossible for him to do so.[citation needed]
 The Bodleian Library in Oxford holds an archive of material relating to Peter Landin.[18] Since 2010, there has been an Annual Peter Landin Semantics Seminar held annually each December in memory of Peter Landin and organized by the BCS-FACS Specialist Group on Formal Aspects of Computing Science.[19] The first seminar was delivered by the American computer scientist John C. Reynolds (1935–2013).[20] There is a Peter Landin Building at Queen Mary University of London housing teaching and research facilities for computer science.[21]


Source: https://en.wikipedia.org/wiki/Leerraum
Content: Other reasons this message may be displayed:


Source: https://en.wikipedia.org/wiki/Fakult%C3%A4t_(Mathematik)
Content: Other reasons this message may be displayed:


Source: https://en.wikipedia.org/wiki/C_(Programmiersprache)
Content: Other reasons this message may be displayed:


Source: https://en.wikipedia.org/wiki/Tabulatorzeichen
Content: Other reasons this message may be displayed:


Source: https://en.wikipedia.org/wiki/Tern%C3%A4rer_Operator
Content: Other reasons this message may be displayed:


Source: https://en.wikipedia.org/wiki/Continuation-Passing_Style
Content: Other reasons this message may be displayed:


Source: https://en.wikipedia.org/wiki/Haskell_Brooks_Curry
Content: Haskell Brooks Curry (/ˈhæskəl/; September 12, 1900 – September 1, 1982) was an American mathematician and logician. Curry is best known for his work in combinatory logic, whose initial concept is based on a paper by Moses Schönfinkel,[1] for which Curry did much of the development. Curry is also known for Curry's paradox and the Curry–Howard correspondence. Named for him are three programming languages: Haskell, Brook, and Curry, and the concept of currying, a method to transform functions, used in mathematics and computer science.
 Curry was born on (1900-09-12)12 September 1900 in Millis, Massachusetts, to Samuel Silas Curry and Anna Baright Curry, who ran a school for elocution. He entered Harvard University in 1916 to study medicine but switched to mathematics before graduating in 1920. After two years of graduate work in electrical engineering at Massachusetts Institute of Technology (MIT), he returned to Harvard to study physics, earning a Master of Arts (M.A.) in 1924. Curry's interest in mathematical logic began during this period when he was introduced to the Principia Mathematica, the attempt by Alfred North Whitehead and Bertrand Russell to ground mathematics in symbolic logic. Remaining at Harvard, Curry pursued a Doctor of Philosophy (Ph.D.) in mathematics. While he was directed by George David Birkhoff to work on differential equations, his interests continued to shift to logic. In 1927, while an instructor at Princeton University, he discovered the work of Moses Schönfinkel in combinatory logic. Schönfinkel's work had anticipated much of Curry's own research, and as a consequence, he moved to University of Göttingen where he could work with Heinrich Behmann and Paul Bernays, who were familiar with Schönfinkel's work. Curry was supervised by David Hilbert and worked closely with Bernays, receiving a Ph.D. in 1930 with a dissertation on combinatory logic.[2]
 In 1928, before leaving for Göttingen, Curry married Mary Virginia Wheatley. The couple lived in Germany while Curry completed his dissertation, then, in 1929, moved to State College, Pennsylvania where Curry accepted a position at Pennsylvania State College. They had two children, Anne Wright Curry (July 27, 1930) and Robert Wheatley Curry (July 6, 1934). Curry remained at Penn State for the next 37 years. He spent one year at University of Chicago in 1931–1932 under a National Research Fellowship and one year in 1938–1939 at the Institute for Advanced Study in Princeton. In 1942 he took a leave of absence to do applied mathematics for the United States government during World War II, notably at the Frankford Arsenal. Immediately after the war he worked on the ENIAC project, in 1945 and 1946. Under a Fulbright fellowship, he collaborated with Robert Feys in Louvain, Belgium. After retiring from Penn State in 1966, Curry accepted a position at the University of Amsterdam. In 1970, after finishing the second volume of his treatise on the combinatory logic, Curry retired from the University of Amsterdam and returned to State College, Pennsylvania.
 Haskell Curry died on 1 September 1982(1982-09-01) (aged 81), in State College, Pennsylvania.
 The focus of Curry's work were attempts to show that combinatory logic could provide a foundation for mathematics. Towards the end of 1933, he learned of the Kleene–Rosser paradox from correspondence with John Rosser. The paradox, developed by Rosser and Stephen Kleene, had proved the inconsistency of a number of related formal systems, including one proposed by Alonzo Church (a system which had the lambda calculus as a consistent subsystem) and Curry's own system.[2] However, unlike Church, Kleene, and Rosser, Curry did not give up on the foundational approach, saying that he did not want to "run away from paradoxes."[3]
 By working in the area of Combinatory Logic for his entire career, Curry essentially became the founder and biggest name in the field. Combinatory logic is the foundation for one style of functional programming language. The power and scope of combinatory logic are quite similar to that of the lambda calculus of Church, and the latter formalism has tended to predominate in recent decades.
 In 1947 Curry also described one of the first high-level programming languages and provided the first description of a procedure to convert a general arithmetic expression into a code for one-address computer.[4]
 He taught at Harvard, Princeton, and from 1929 to 1966, at the Pennsylvania State University. In 1942, he published Curry's paradox. In 1966 he became professor of logic and its history and philosophy of exact sciences at the University of Amsterdam, the successor of Evert Willem Beth.[5]
 Curry also wrote and taught mathematical logic more generally; his teaching in this area culminated in his 1963 Foundations of Mathematical Logic. His preferred philosophy of mathematics was formalism (cf. his 1951 book), following his mentor Hilbert, but his writings betray substantial philosophical curiosity and a very open mind about intuitionistic logic.


Source: https://en.wikipedia.org/wiki/Currying
Content: In mathematics and computer science, currying is the technique of translating a function that takes multiple arguments into a sequence of families of functions, each taking a single argument.
 In the prototypical example, one begins with a function 



f
:
(
X
×
Y
)
→
Z


{\displaystyle f:(X\times Y)\to Z}

 that takes two arguments, one from 



X


{\displaystyle X}

 and one from 



Y
,


{\displaystyle Y,}

 and produces objects in 



Z
.


{\displaystyle Z.}

 The curried form of this function treats the first argument as a parameter, so as to create a family of functions 




f

x


:
Y
→
Z
.


{\displaystyle f_{x}:Y\to Z.}

 The family is arranged so that for each object 



x


{\displaystyle x}

 in 



X
,


{\displaystyle X,}

 there is exactly one function 




f

x


.


{\displaystyle f_{x}.}


 In this example, 





curry




{\displaystyle {\mbox{curry}}}

 itself becomes a function, that takes 



f


{\displaystyle f}

 as an argument, and returns a function that maps each 



x


{\displaystyle x}

 to 




f

x


.


{\displaystyle f_{x}.}

 The proper notation for expressing this is verbose. The function 



f


{\displaystyle f}

 belongs to the set of functions 



(
X
×
Y
)
→
Z
.


{\displaystyle (X\times Y)\to Z.}

  Meanwhile, 




f

x




{\displaystyle f_{x}}

 belongs to the set of functions 



Y
→
Z
.


{\displaystyle Y\to Z.}

 Thus, something that maps 



x


{\displaystyle x}

 to 




f

x




{\displaystyle f_{x}}

 will be of the type 



X
→
[
Y
→
Z
]
.


{\displaystyle X\to [Y\to Z].}

 With this notation, 





curry




{\displaystyle {\mbox{curry}}}

 is a function that takes objects from the first set, and returns objects in the second set, and so one writes 





curry


:
[
(
X
×
Y
)
→
Z
]
→
(
X
→
[
Y
→
Z
]
)
.


{\displaystyle {\mbox{curry}}:[(X\times Y)\to Z]\to (X\to [Y\to Z]).}

 This is a somewhat informal example; more precise definitions of what is meant by "object" and "function" are given below. These definitions vary from context to context, and take different forms, depending on the theory that one is working in.
 Currying is related to, but not the same as, partial application.[1][2] The example above can be used to illustrate partial application; it is quite similar.  Partial application is the function 





apply




{\displaystyle {\mbox{apply}}}

 that takes the pair 



f


{\displaystyle f}

 and 



x


{\displaystyle x}

 together as arguments, and returns 




f

x


.


{\displaystyle f_{x}.}

 Using the same notation as above, partial application has the signature 





apply


:
(
[
(
X
×
Y
)
→
Z
]
×
X
)
→
[
Y
→
Z
]
.


{\displaystyle {\mbox{apply}}:([(X\times Y)\to Z]\times X)\to [Y\to Z].}

 Written this way, application can be seen to be adjoint to currying.
 The currying of a function with more than two arguments can be defined by induction. 
 Currying is useful in both practical and theoretical settings. In functional programming languages, and many others, it provides a way of automatically managing how arguments are passed to functions and exceptions. In theoretical computer science, it provides a way to study functions with multiple arguments in simpler theoretical models which provide only one argument. The most general setting for the strict notion of currying and uncurrying is in the closed monoidal categories, which underpins a vast generalization of the Curry–Howard correspondence of proofs and programs to a correspondence with many other structures, including quantum mechanics, cobordisms and string theory.[3]
 The concept of currying was introduced by Gottlob Frege,[4][5] developed by Moses Schönfinkel,[6][5][7][8][9][10][11]
and further developed by Haskell Curry.[8][10][12][13]
 Uncurrying is the dual transformation to currying, and can be seen as a form of defunctionalization. It takes a function 



f


{\displaystyle f}

 whose return value is another function 



g


{\displaystyle g}

, and yields a new function 




f
′



{\displaystyle f'}

 that takes as parameters the arguments for both 



f


{\displaystyle f}

 and 



g


{\displaystyle g}

, and returns, as a result, the application of 



f


{\displaystyle f}

 and subsequently, 



g


{\displaystyle g}

, to those arguments. The process can be iterated.
 Currying provides a way for working with functions that take multiple arguments, and using them in frameworks where functions might take only one argument. For example, some analytical techniques can only be applied to functions with a single argument. Practical functions frequently take more arguments than this. Frege showed that it was sufficient to provide solutions for the single argument case, as it was possible to transform a function with multiple arguments into a chain of single-argument functions instead. This transformation is the process now known as currying.[14] All "ordinary" functions that might typically be encountered in mathematical analysis or in computer programming can be curried.  However, there are categories in which currying is not possible; the most general categories which allow currying are the closed monoidal categories.
 Some programming languages almost always use curried functions to achieve multiple arguments; notable examples are ML and Haskell, where in both cases all functions have exactly one argument. This property is inherited from lambda calculus, where multi-argument functions are usually represented in curried form.
 Currying is related to, but not the same as partial application.[1][2] In practice, the programming technique of closures can be used to perform partial application and a kind of currying, by hiding arguments in an environment that travels with the curried function.
 The "Curry" in "Currying" is a reference to logician Haskell Curry, who used the concept extensively, but Moses Schönfinkel had the idea six years before Curry.[10] The alternative name "Schönfinkelisation" has been proposed.[15] In the mathematical context, the principle can be traced back to work in 1893 by Frege.[4][5]
 The originator of the word "currying" is not clear. David Turner says the word was coined by Christopher Strachey in his 1967 lecture notes Fundamental Concepts in Programming Languages,[16] but although the concept is mentioned and Curry is mentioned in the context of higher-order functions, the word "currying" does not appear in the notes and Curry is not associated with the concept.[7] John C. Reynolds defined "currying" in a 1972 paper, but did not claim to have coined the term.[8]
 Currying is most easily understood by starting with an informal definition, which can then be molded to fit many different domains. First, there is some notation to be established. The notation 



X
→
Y


{\displaystyle X\to Y}

 denotes all functions from 



X


{\displaystyle X}

 to 



Y


{\displaystyle Y}

. If 



f


{\displaystyle f}

 is such a function, we write 



f
:
X
→
Y


{\displaystyle f\colon X\to Y}

. Let 



X
×
Y


{\displaystyle X\times Y}

 denote the ordered pairs of the elements of 



X


{\displaystyle X}

 and 



Y


{\displaystyle Y}

 respectively, that is, the Cartesian product of 



X


{\displaystyle X}

 and 



Y


{\displaystyle Y}

. Here, 



X


{\displaystyle X}

 and 



Y


{\displaystyle Y}

 may be sets, or they may be types, or they may be other kinds of objects, as explored below.
 Given a function
 currying constructs a new function
 That is, 



g


{\displaystyle g}

 takes an argument of type 



X


{\displaystyle X}

 and returns a function of type 



Y
→
Z


{\displaystyle Y\to Z}

. It is defined by
 for 



x


{\displaystyle x}

 of type 



X


{\displaystyle X}

 and 



y


{\displaystyle y}

 of type 



Y


{\displaystyle Y}

. We then also write
 Uncurrying is the reverse transformation, and is most easily understood in terms of its right adjoint, the function 



apply
.


{\displaystyle \operatorname {apply} .}


 In set theory, the notation 




Y

X




{\displaystyle Y^{X}}

 is used to denote the set of functions from the set 



X


{\displaystyle X}

 to the set 



Y


{\displaystyle Y}

. Currying is the natural bijection between the set 




A

B
×
C




{\displaystyle A^{B\times C}}

 of functions from 



B
×
C


{\displaystyle B\times C}

 to 



A


{\displaystyle A}

, and the set 



(

A

C



)

B




{\displaystyle (A^{C})^{B}}

 of functions from 



B


{\displaystyle B}

 to the set of functions from 



C


{\displaystyle C}

 to 



A


{\displaystyle A}

. In symbols:
 Indeed, it is this natural bijection that justifies the exponential notation for the set of functions. As is the case in all instances of currying, the formula above describes an adjoint pair of functors: for every fixed set 



C


{\displaystyle C}

, the functor 



B
↦
B
×
C


{\displaystyle B\mapsto B\times C}

 is left adjoint to the functor 



A
↦

A

C




{\displaystyle A\mapsto A^{C}}

.
 In the category of sets, the object 




Y

X




{\displaystyle Y^{X}}

 is called the exponential object.
 In the theory of function spaces, such as in functional analysis or homotopy theory, one is commonly interested in continuous functions between topological spaces. One writes 




Hom

(
X
,
Y
)


{\displaystyle {\text{Hom}}(X,Y)}

 (the Hom functor) for the set of all functions from 



X


{\displaystyle X}

 to 



Y


{\displaystyle Y}

, and uses the notation 




Y

X




{\displaystyle Y^{X}}

 to denote the subset of continuous functions. Here, 




curry



{\displaystyle {\text{curry}}}

 is the bijection
 while uncurrying is the inverse map. If the set 




Y

X




{\displaystyle Y^{X}}

 of continuous functions from 



X


{\displaystyle X}

 to 



Y


{\displaystyle Y}

 is given the compact-open topology, and if the space 



Y


{\displaystyle Y}

 is locally compact Hausdorff, then
 is a homeomorphism. This is also the case when 



X


{\displaystyle X}

, 



Y


{\displaystyle Y}

 and 




Y

X




{\displaystyle Y^{X}}

 are compactly generated,[17]: chapter 5 [18] although there are more cases.[19][20]
 One useful corollary is that a function is continuous if and only if its curried form is continuous. Another important result is that the application map, usually called "evaluation" in this context, is continuous (note that eval is a strictly different concept in computer science.) That is,
 










eval

:

Y

X


×
X
→
Y






(
f
,
x
)
↦
f
(
x
)






{\displaystyle {\begin{aligned}&&{\text{eval}}:Y^{X}\times X\to Y\\&&(f,x)\mapsto f(x)\end{aligned}}}


 is continuous when 




Y

X




{\displaystyle Y^{X}}

 is compact-open and 



Y


{\displaystyle Y}

 locally compact Hausdorff.[21] These two results are central for establishing the continuity of homotopy, i.e. when 



X


{\displaystyle X}

 is the unit interval 



I


{\displaystyle I}

, so that 




Z

I
×
Y


≅
(

Z

Y



)

I




{\displaystyle Z^{I\times Y}\cong (Z^{Y})^{I}}

 can be thought of as either a homotopy of two functions from 



Y


{\displaystyle Y}

 to 



Z


{\displaystyle Z}

, or, equivalently, a single (continuous) path in 




Z

Y




{\displaystyle Z^{Y}}

.
 In algebraic topology, currying serves as an example of Eckmann–Hilton duality, and, as such, plays an important role in a variety of different settings. For example, loop space is adjoint to reduced suspensions; this is commonly written as
 where 



[
A
,
B
]


{\displaystyle [A,B]}

 is the set of homotopy classes of maps 



A
→
B


{\displaystyle A\rightarrow B}

, and 



Σ
A


{\displaystyle \Sigma A}

 is the suspension of A, and 



Ω
A


{\displaystyle \Omega A}

 is the loop space of A. In essence, the suspension 



Σ
X


{\displaystyle \Sigma X}

 can be seen as the cartesian product of 



X


{\displaystyle X}

 with the unit interval, modulo an equivalence relation to turn the interval into a loop. The curried form then maps the space 



X


{\displaystyle X}

 to the space of functions from loops into 



Z


{\displaystyle Z}

, that is, from 



X


{\displaystyle X}

 into 



Ω
Z


{\displaystyle \Omega Z}

.[21] Then 




curry



{\displaystyle {\text{curry}}}

 is the adjoint functor that maps suspensions to loop spaces, and uncurrying is the dual.[21]
 The duality between the mapping cone and the mapping fiber (cofibration and fibration)[17]: chapters 6,7  can be understood as a form of currying, which in turn leads to the duality of the long exact and coexact Puppe sequences.
 In homological algebra, the relationship between currying and uncurrying is known as tensor-hom adjunction. Here, an interesting twist arises: the Hom functor and the tensor product functor might not lift to an exact sequence; this leads to the definition of the Ext functor and the Tor functor.
 In order theory, that is, the theory of lattices of partially ordered sets, 




curry



{\displaystyle {\text{curry}}}

 is a continuous function when the lattice is given the Scott topology.[22] Scott-continuous functions were first investigated in the attempt to provide a semantics for lambda calculus (as ordinary set theory is inadequate to do this). More generally, Scott-continuous functions are now studied in domain theory, which encompasses the study of denotational semantics of computer algorithms. Note that the Scott topology is quite different than many common topologies one might encounter in the category of topological spaces; the Scott topology is typically finer, and is not sober.
 The notion of continuity makes its appearance in homotopy type theory, where, roughly speaking, two computer programs can be considered to be homotopic, i.e. compute the same results, if they can be "continuously" refactored from one to the other.
 In theoretical computer science, currying provides a way to study functions with multiple arguments in very simple theoretical models, such as the lambda calculus, in which functions only take a single argument. Consider a function 



f
(
x
,
y
)


{\displaystyle f(x,y)}

 taking two arguments, and having the type 



(
X
×
Y
)
→
Z


{\displaystyle (X\times Y)\to Z}

, which should be understood to mean that x must have the type 



X


{\displaystyle X}

, y must have the type 



Y


{\displaystyle Y}

, and the function itself returns the type 



Z


{\displaystyle Z}

. The curried form of f is defined as
 where 



λ


{\displaystyle \lambda }

 is the abstractor of lambda calculus. Since curry takes, as input, functions with the type 



(
X
×
Y
)
→
Z


{\displaystyle (X\times Y)\to Z}

, one concludes that the type of curry itself is
 The → operator is often considered right-associative, so the curried function type 



X
→
(
Y
→
Z
)


{\displaystyle X\to (Y\to Z)}

 is often written as 



X
→
Y
→
Z


{\displaystyle X\to Y\to Z}

. Conversely, function application is considered to be left-associative, so that 



f
(
x
,
y
)


{\displaystyle f(x,y)}

 is equivalent to
 That is, the parenthesis are not required to disambiguate the order of the application.
 Curried functions may be used in any programming language that supports closures; however, uncurried functions are generally preferred for efficiency reasons, since the overhead of partial application and closure creation can then be avoided for most function calls.
 In type theory, the general idea of a type system in computer science is formalized into a specific algebra of types. For example, when writing 



f
:
X
→
Y


{\displaystyle f\colon X\to Y}

, the intent is that 



X


{\displaystyle X}

 and 



Y


{\displaystyle Y}

 are types, while the arrow 



→


{\displaystyle \to }

 is a type constructor, specifically, the function type or arrow type. Similarly, the Cartesian product 



X
×
Y


{\displaystyle X\times Y}

 of types is constructed by the product type constructor 



×


{\displaystyle \times }

.
 The type-theoretical approach is expressed in programming languages such as ML and the languages derived from and inspired by it: CaML, Haskell and F#.
 The type-theoretical approach provides a natural complement to the language of category theory, as discussed below. This is because categories, and specifically, monoidal categories, have an internal language, with simply-typed lambda calculus being the most prominent example of such a language. It is important in this context, because it can be built from a single type constructor, the arrow type. Currying then endows the language with a natural product type. The correspondence between objects in categories and types then allows programming languages to be re-interpreted as logics (via Curry–Howard correspondence), and as other types of mathematical systems, as explored further, below.
 Under the Curry–Howard correspondence, the existence of currying and uncurrying is equivalent to the logical theorem 



(
(
A
∧
B
)
→
C
)
⇔
(
A
→
(
B
→
C
)
)


{\displaystyle ((A\land B)\to C)\Leftrightarrow (A\to (B\to C))}

, as tuples (product type) corresponds to conjunction in logic, and function type corresponds to implication.
 The exponential object 




Q

P




{\displaystyle Q^{P}}

 in the category of Heyting algebras is normally written as material implication 



P
→
Q


{\displaystyle P\to Q}

. Distributive Heyting algebras are Boolean algebras, and the exponential object has the explicit form 



¬
P
∨
Q


{\displaystyle \neg P\lor Q}

, thus making it clear that the exponential object really is material implication.[23]
 The above notions of currying and uncurrying find their most general, abstract statement in category theory. Currying is a universal property of an exponential object, and gives rise to an adjunction in cartesian closed categories. That is, there is a natural isomorphism between the morphisms from a binary product 



f
:
(
X
×
Y
)
→
Z


{\displaystyle f\colon (X\times Y)\to Z}

 and the morphisms to an exponential object 



g
:
X
→

Z

Y




{\displaystyle g\colon X\to Z^{Y}}

.
 This generalizes to a broader result in closed monoidal categories: Currying is the statement that the tensor product and the internal Hom are adjoint functors; that is, for every object 



B


{\displaystyle B}

 there is a natural isomorphism:
 Here, Hom denotes the (external) Hom-functor of all morphisms in the category, while 



B
⇒
C


{\displaystyle B\Rightarrow C}

 denotes the internal hom functor in the closed monoidal category. For the category of sets, the two are the same. When the product is the cartesian product, then the internal hom 



B
⇒
C


{\displaystyle B\Rightarrow C}

 becomes the exponential object 




C

B




{\displaystyle C^{B}}

.
 Currying can break down in one of two ways. One is if a category is not closed, and thus lacks an internal hom functor (possibly because there is more than one choice for such a functor). Another way is if it is not monoidal, and thus lacks a product (that is, lacks a way of writing down pairs of objects). Categories that do have both products and internal homs are exactly the closed monoidal categories.
 The setting of cartesian closed categories is sufficient for the discussion of classical logic; the more general setting of closed monoidal categories is suitable for quantum computation.[24]
 The difference between these two is that the product for cartesian categories (such as the category of sets, complete partial orders or Heyting algebras) is just the Cartesian product; it is interpreted as an ordered pair of items (or a list). Simply typed lambda calculus is the internal language of cartesian closed categories; and it is for this reason that pairs and lists are the primary types in the type theory of LISP, Scheme and many functional programming languages.
 By contrast, the product for monoidal categories (such as Hilbert space and the vector spaces of functional analysis) is the tensor product. The internal language of such categories is linear logic, a form of quantum logic; the corresponding type system is the linear type system. Such categories are suitable for describing entangled quantum states, and, more generally, allow a vast generalization of the Curry–Howard correspondence to quantum mechanics, to cobordisms in algebraic topology, and to string theory.[3] The linear type system, and linear logic are useful for describing synchronization primitives, such as mutual exclusion locks, and the operation of vending machines.
 Currying and partial function application are often conflated.[1][2] One of the significant differences between the two is that a call to a partially applied function returns the result right away, not another function down the currying chain; this distinction can be illustrated clearly for functions whose arity is greater than two.[25]
 Given a function of type 



f
:
(
X
×
Y
×
Z
)
→
N


{\displaystyle f\colon (X\times Y\times Z)\to N}

, currying produces 




curry

(
f
)
:
X
→
(
Y
→
(
Z
→
N
)
)


{\displaystyle {\text{curry}}(f)\colon X\to (Y\to (Z\to N))}

. That is, while an evaluation of the first function might be represented as 



f
(
1
,
2
,
3
)


{\displaystyle f(1,2,3)}

, evaluation of the curried function would be represented as 




f

curried


(
1
)
(
2
)
(
3
)


{\displaystyle f_{\text{curried}}(1)(2)(3)}

, applying each argument in turn to a single-argument function returned by the previous invocation. Note that after calling 




f

curried


(
1
)


{\displaystyle f_{\text{curried}}(1)}

, we are left with a function that takes a single argument and returns another function, not a function that takes two arguments.
 In contrast, partial function application refers to the process of fixing a number of arguments to a function, producing another function of smaller arity. Given the definition of 



f


{\displaystyle f}

 above, we might fix (or 'bind') the first argument, producing a function of type 




partial

(
f
)
:
(
Y
×
Z
)
→
N


{\displaystyle {\text{partial}}(f)\colon (Y\times Z)\to N}

. Evaluation of this function might be represented as 




f

partial


(
2
,
3
)


{\displaystyle f_{\text{partial}}(2,3)}

. Note that the result of partial function application in this case is a function that takes two arguments.
 Intuitively, partial function application says "if you fix the first argument of the function, you get a function of the remaining arguments". For example, if function div stands for the division operation x/y, then div with the parameter x fixed at 1 (i.e., div 1) is another function: the same as the function inv that returns the multiplicative inverse of its argument, defined by inv(y) = 1/y.
 The practical motivation for partial application is that very often the functions obtained by supplying some but not all of the arguments to a function are useful; for example, many languages have a function or operator similar to plus_one. Partial application makes it easy to define these functions, for example by creating a function that represents the addition operator with 1 bound as its first argument.
 Partial application can be seen as evaluating a curried function at a fixed point, e.g. given 



f
:
(
X
×
Y
×
Z
)
→
N


{\displaystyle f\colon (X\times Y\times Z)\to N}

 and 



a
∈
X


{\displaystyle a\in X}

 then 




curry

(

partial

(
f

)

a


)
(
y
)
(
z
)
=

curry

(
f
)
(
a
)
(
y
)
(
z
)


{\displaystyle {\text{curry}}({\text{partial}}(f)_{a})(y)(z)={\text{curry}}(f)(a)(y)(z)}

 or simply 




partial

(
f

)

a


=


curry


1


(
f
)
(
a
)


{\displaystyle {\text{partial}}(f)_{a}={\text{curry}}_{1}(f)(a)}

 where 





curry


1




{\displaystyle {\text{curry}}_{1}}

 curries f's first parameter.
 Thus, partial application is reduced to a curried function at a fixed point. Further, a curried function at a fixed point is (trivially), a partial application. For further evidence, note that, given any function 



f
(
x
,
y
)


{\displaystyle f(x,y)}

, a function 



g
(
y
,
x
)


{\displaystyle g(y,x)}

 may be defined such that 



g
(
y
,
x
)
=
f
(
x
,
y
)


{\displaystyle g(y,x)=f(x,y)}

. Thus, any partial application may be reduced to a single curry operation. As such, curry is more suitably defined as an operation which, in many theoretical cases, is often applied recursively, but which is theoretically indistinguishable (when considered as an operation) from a partial application.
 So, a partial application can be defined as the objective result of a single application of the curry operator on some ordering of the inputs of some function.


Source: https://en.wikipedia.org/wiki/Closure_(Funktion)
Content: Other reasons this message may be displayed:


Source: https://en.wikipedia.org/wiki/Stapelspeicher
Content: Other reasons this message may be displayed:


Source: https://en.wikipedia.org/wiki/Ausnahmebehandlung
Content: Other reasons this message may be displayed:


Source: https://en.wikipedia.org/wiki/Syntaxfehler
Content: Other reasons this message may be displayed:


Source: https://en.wikipedia.org/wiki/Thread_(Informatik)
Content: Other reasons this message may be displayed:


Source: https://en.wikipedia.org/wiki/Race_Condition
Content: A race condition or race hazard is the condition of an electronics, software, or other system where the system's substantive behavior is dependent on the sequence or timing of other uncontrollable events, leading to unexpected or inconsistent results. It becomes a bug when one or more of the possible behaviors is undesirable.
 The term race condition was already in use by 1954, for example in David A. Huffman's doctoral thesis "The synthesis of sequential switching circuits".[1]
 Race conditions can occur especially in logic circuits, multithreaded, or distributed software programs.
 A typical example of a race condition may occur when a logic gate combines signals that have traveled along different paths from the same source. The inputs to the gate can change at slightly different times in response to a change in the source signal. The output may, for a brief period, change to an unwanted state before settling back to the designed state. Certain systems can tolerate such glitches but if this output functions as a clock signal for further systems that contain memory, for example, the system can rapidly depart from its designed behaviour (in effect, the temporary glitch becomes a permanent glitch).
 Consider, for example, a two-input AND gate fed with the following logic: 




output

=
A
∧


A
¯




{\displaystyle {\text{output}}=A\wedge {\overline {A}}}

A logic signal 



A


{\displaystyle A}

 on one input and its negation, 



¬
A


{\displaystyle \neg A}

 (the ¬ is a Boolean negation), on another input in theory never output a true value: 



A
∧


A
¯


≠
1


{\displaystyle A\wedge {\overline {A}}\neq 1}

. If, however, changes in the value of 



A


{\displaystyle A}

 take longer to propagate to the second input than the first when 



A


{\displaystyle A}

 changes from false to true then a brief period will ensue during which both inputs are true, and so the gate's output will also be true.[2]
 A practical example of a race condition can occur when logic circuitry is used to detect certain outputs of a counter.  If all the bits of the counter do not change exactly simultaneously, there will be intermediate patterns that can trigger false matches.
 A critical race condition occurs when the order in which internal variables are changed determines the eventual state that the state machine will end up in.
 A non-critical race condition occurs when the order in which internal variables are changed does not determine the eventual state that the state machine will end up in.
 A static race condition occurs when a signal and its complement are combined.
 A dynamic race condition occurs when it results in multiple transitions when only one is intended. They are due to interaction between gates. It can be eliminated by using no more than two levels of gating.
 An essential race condition occurs when an input has two transitions in less than the total feedback propagation time. Sometimes they are cured using inductive delay line elements to effectively increase the time duration of an input signal.
 Design techniques such as Karnaugh maps encourage designers to recognize and eliminate race conditions before they cause problems. Often logic redundancy can be added to eliminate some kinds of races.
 As well as these problems, some logic elements can enter metastable states, which create further problems for circuit designers.
 A race condition can arise in software when a computer program has multiple code paths that are executing at the same time. If the multiple code paths take a different amount of time than expected, they can finish in a different order than expected, which can cause software bugs due to unanticipated behavior. A race can also occur between two programs, resulting in security issues (see below.)
 Critical race conditions cause invalid execution and software bugs. Critical race conditions often happen when the processes or threads depend on some shared state. Operations upon shared states are done in critical sections that must be mutually exclusive. Failure to obey this rule can corrupt the shared state.
 A data race is a type of race condition. Data races are important parts of various formal memory models. The memory model defined in the C11 and C++11 standards specify that a C or C++ program containing a data race has undefined behavior.[3][4]
 A race condition can be difficult to reproduce and debug because the end result is nondeterministic and depends on the relative timing between interfering threads. Problems of this nature can therefore disappear when running in debug mode, adding extra logging, or attaching a debugger. A bug that disappears like this during debugging attempts is often referred to as a "Heisenbug". It is therefore better to avoid race conditions by careful software design.
 Assume that two threads each increment the value of a global integer variable by 1. Ideally, the following sequence of operations would take place:
 In the case shown above, the final value is 2, as expected. However, if the two threads run simultaneously without locking or synchronization (via semaphores), the outcome of the operation could be wrong. The alternative sequence of operations below demonstrates this scenario:
 In this case, the final value is 1 instead of the expected result of 2. This occurs because here the increment operations are not mutually exclusive. Mutually exclusive operations are those that cannot be interrupted while accessing some resource such as a memory location.
 Not all regard data races as a subset of race conditions.[5] The precise definition of data race is specific to the formal concurrency model being used, but typically it refers to a situation where a memory operation in one thread could potentially attempt to access a memory location at the same time that a memory operation in another thread is writing to that memory location, in a context where this is dangerous. This implies that a data race is different from a race condition as it is possible to have nondeterminism due to timing even in a program without data races, for example, in a program in which all memory accesses use only atomic operations.
 This can be dangerous because on many platforms, if two threads write to a memory location at the same time, it may be possible for the memory location to end up holding a value that is some arbitrary and meaningless combination of the bits representing the values that each thread was attempting to write; this could result in memory corruption if the resulting value is one that neither thread attempted to write (sometimes this is called a 'torn write'). Similarly, if one thread reads from a location while another thread is writing to it, it may be possible for the read to return a value that is some arbitrary and meaningless combination of the bits representing the value that the memory location held before the write, and of the bits representing the value being written.
 On many platforms, special memory operations are provided for simultaneous access; in such cases, typically simultaneous access using these special operations is safe, but simultaneous access using other memory operations is dangerous. Sometimes such special operations (which are safe for simultaneous access) are called atomic or synchronization operations, whereas the ordinary operations (which are unsafe for simultaneous access) are called data operations. This is probably why the term is data race; on many platforms, where there is a race condition involving only synchronization operations, such a race may be nondeterministic but otherwise safe; but a data race could lead to memory corruption or undefined behavior.
 The precise definition of data race differs across formal concurrency models. This matters because concurrent behavior is often non-intuitive and so formal reasoning is sometimes applied.
 The C++ standard, in draft N4296 (2014-11-19), defines data race as follows in section 1.10.23 (page 14)[6]
 Two actions are potentially concurrent if
 The execution of a program contains a data race if it contains two potentially concurrent conflicting actions, at least one of which is not atomic, and neither happens before the other, except for the special case for signal handlers described below [omitted]. Any such data race results in undefined behavior.
 The parts of this definition relating to signal handlers are idiosyncratic to C++ and are not typical of definitions of data race.
 The paper Detecting Data Races on Weak Memory Systems[7] provides a different definition:
 "two memory operations conflict if they access the same location and at least one of them is a write operation...
"Two memory operations, x and y, in a sequentially consistent execution form a race 〈x,y〉, iff x and y conflict, and they are not ordered by the hb1 relation of the execution. The race 〈x,y〉, is a data race iff at least one of x or y is a data operation. Here we have two memory operations accessing the same location, one of which is a write.
 The hb1 relation is defined elsewhere in the paper, and is an example of a typical "happens-before" relation; intuitively, if we can prove that we are in a situation where one memory operation X is guaranteed to be executed to completion before another memory operation Y begins, then we say that "X happens-before Y". If neither "X happens-before Y" nor "Y happens-before X", then we say that X and Y are "not ordered by the hb1 relation". So, the clause "...and they are not ordered by the hb1 relation of the execution" can be intuitively translated as "...and X and Y are potentially concurrent".
 The paper considers dangerous only those situations in which at least one of the memory operations is a "data operation"; in other parts of this paper, the paper also defines a class of "synchronization operations" which are safe for potentially simultaneous use, in contrast to "data operations".
 The Java Language Specification[8] provides a different definition:
 Two accesses to (reads of or writes to) the same variable are said to be conflicting if at least one of the accesses is a write...When a program contains two conflicting accesses (§17.4.1) that are not ordered by a happens-before relationship, it is said to contain a data race...a data race cannot cause incorrect behavior such as returning the wrong length for an array. A critical difference between the C++ approach and the Java approach is that in C++, a data race is undefined behavior, whereas in Java, a data race merely affects "inter-thread actions".[8] This means that in C++, an attempt to execute a program containing a data race could (while still adhering to the spec) crash or could exhibit insecure or bizarre behavior, whereas in Java, an attempt to execute a program containing a data race may produce undesired concurrency behavior but is otherwise (assuming that the implementation adheres to the spec) safe.
 An important facet of data races is that in some contexts, a program that is free of data races is guaranteed to execute in a sequentially consistent manner, greatly easing reasoning about the concurrent behavior of the program. Formal memory models that provide such a guarantee are said to exhibit an "SC for DRF" (Sequential Consistency for Data Race Freedom) property. This approach has been said to have achieved recent consensus (presumably compared to approaches which guarantee sequential consistency in all cases, or approaches which do not guarantee it at all).[9]
 For example, in Java, this guarantee is directly specified:[8]
 A program is correctly synchronized if and only if all sequentially consistent executions are free of data races.
 If a program is correctly synchronized, then all executions of the program will appear to be sequentially consistent (§17.4.3).
 This is an extremely strong guarantee for programmers. Programmers do not need to reason about reorderings to determine that their code contains data races. Therefore they do not need to reason about reorderings when determining whether their code is correctly synchronized. Once the determination that the code is correctly synchronized is made, the programmer does not need to worry that reorderings will affect his or her code.
 A program must be correctly synchronized to avoid the kinds of counterintuitive behaviors that can be observed when code is reordered. The use of correct synchronization does not ensure that the overall behavior of a program is correct. However, its use does allow a programmer to reason about the possible behaviors of a program in a simple way; the behavior of a correctly synchronized program is much less dependent on possible reorderings. Without correct synchronization, very strange, confusing and counterintuitive behaviors are possible.
 By contrast, a draft C++ specification does not directly require an SC for DRF property, but merely observes that there exists a theorem providing it:
 [Note:It can be shown that programs that correctly use mutexes and memory_order_seq_cst operations to prevent all data races and use no other synchronization operations behave as if the operations executed by their constituent threads were simply interleaved, with each value computation of an object being taken from the last side effect on that object in that interleaving. This is normally referred to as “sequential consistency”. However, this applies only to data-race-free programs, and data-race-free programs cannot observe most program transformations that do not change single-threaded program semantics. In fact, most single-threaded program transformations continue to be allowed, since any program that behaves differently as a result must perform an undefined operation.— end note
 Note that the C++ draft specification admits the possibility of programs that are valid but use synchronization operations with a memory_order other than memory_order_seq_cst, in which case the result may be a program which is correct but for which no guarantee of sequentially consistency is provided. In other words, in C++, some correct programs are not sequentially consistent. This approach is thought to give C++ programmers the freedom to choose faster program execution at the cost of giving up ease of reasoning about their program.[9]
 There are various theorems, often provided in the form of memory models, that provide SC for DRF guarantees given various contexts. The premises of these theorems typically place constraints upon both the memory model (and therefore upon the implementation), and also upon the programmer; that is to say, typically it is the case that there are programs which do not meet the premises of the theorem and which could not be guaranteed to execute in a sequentially consistent manner.
 The DRF1 memory model[10] provides SC for DRF and allows the optimizations of the WO (weak ordering), RCsc (Release Consistency with sequentially consistent special operations), VAX memory model, and data-race-free-0 memory models. The PLpc memory model[11] provides SC for DRF and allows the optimizations of the TSO (Total Store Order), PSO, PC (Processor Consistency), and RCpc (Release Consistency with processor consistency special operations) models. DRFrlx[12] provides a sketch of an SC for DRF theorem in the presence of relaxed atomics.
 Many software race conditions have associated computer security implications. A race condition allows an attacker with access to a shared resource to cause other actors that utilize that resource to malfunction, resulting in effects including denial of service[13] and privilege escalation.[14][15]
 A specific kind of race condition involves checking for a predicate (e.g. for authentication), then acting on the predicate, while the state can change between the time of check and the time of use. When this kind of bug exists in security-sensitive code, a security vulnerability called a time-of-check-to-time-of-use (TOCTTOU) bug is created.
 Race conditions are also intentionally used to create hardware random number generators and physically unclonable functions.[16][citation needed] PUFs can be created by designing circuit topologies with identical paths to a node and relying on manufacturing variations to randomly determine which paths will complete first. By measuring each manufactured circuit's specific set of race condition outcomes, a profile can be collected for each circuit and kept secret in order to later verify a circuit's identity.
 Two or more programs may collide in their attempts to modify or access a file system, which can result in data corruption or privilege escalation.[14] File locking provides a commonly used solution. A more cumbersome remedy involves organizing the system in such a way that one unique process (running a daemon or the like) has exclusive access to the file, and all other processes that need to access the data in that file do so only via interprocess communication with that one process. This requires synchronization at the process level.
 A different form of race condition exists in file systems where unrelated programs may affect each other by suddenly using up available resources such as disk space, memory space, or processor cycles. Software not carefully designed to anticipate and handle this race situation may then become unpredictable. Such a risk may be overlooked for a long time in a system that seems very reliable. But eventually enough data may accumulate or enough other software may be added to critically destabilize many parts of a system. An example of this occurred with the near loss of the Mars Rover "Spirit" not long after landing. A solution is for software to request and reserve all the resources it will need before beginning a task; if this request fails then the task is postponed, avoiding the many points where failure could have occurred. Alternatively, each of those points can be equipped with error handling, or the success of the entire task can be verified afterwards, before continuing. A more common approach is to simply verify that enough system resources are available before starting a task; however, this may not be adequate because in complex systems the actions of other running programs can be unpredictable.
 In networking, consider a distributed chat network like IRC, where a user who starts a channel automatically acquires channel-operator privileges. If two users on different servers, on different ends of the same network, try to start the same-named channel at the same time, each user's respective server will grant channel-operator privileges to each user, since neither server will yet have received the other server's signal that it has allocated that channel. (This problem has been largely solved by various IRC server implementations.)
 In this case of a race condition, the concept of the "shared resource" covers the state of the network (what channels exist, as well as what users started them and therefore have what privileges), which each server can freely change as long as it signals the other servers on the network about the changes so that they can update their conception of the state of the network. However, the latency across the network makes possible the kind of race condition described. In this case, heading off race conditions by imposing a form of control over access to the shared resource—say, appointing one server to control who holds what privileges—would mean turning the distributed network into a centralized one (at least for that one part of the network operation).
 Race conditions can also exist when a computer program is written with non-blocking sockets, in which case the performance of the program can be dependent on the speed of the network link.
 Software flaws in life-critical systems can be disastrous. Race conditions were among the flaws in the Therac-25 radiation therapy machine, which led to the death of at least three patients and injuries to several more.[17]
 Another example is the energy management system provided by GE Energy and used by Ohio-based FirstEnergy Corp (among other power facilities). A race condition existed in the alarm subsystem; when three sagging power lines were tripped simultaneously, the condition prevented alerts from being raised to the monitoring technicians, delaying their awareness of the problem. This software flaw eventually led to the North American Blackout of 2003.[18] GE Energy later developed a software patch to correct the previously undiscovered error.
 Many software tools exist to help detect race conditions in software. They can be largely categorized into two groups: static analysis tools and dynamic analysis tools.
 Thread Safety Analysis is a static analysis tool for annotation-based intra-procedural static analysis, originally implemented as a branch of gcc, and now reimplemented in Clang, supporting PThreads.[19][non-primary source needed]
 Dynamic analysis tools include: 
 There are several benchmarks designed to evaluate the effectiveness of data race detection tools
 Neuroscience is demonstrating that race conditions can occur in mammal brains as well.[24][25]
 In UK railway signalling, a race condition would arise in the carrying out of Rule 55. According to this rule, if a train was stopped on a running line by a signal, the locomotive fireman would walk to the signal box in order to remind the signalman that the train was present. In at least one case, at Winwick in 1934, an accident occurred because the signalman accepted another train before the fireman arrived. Modern signalling practice removes the race condition by making it possible for the driver to instantaneously contact the signal box by radio.


Source: https://en.wikipedia.org/wiki/Laufzeitfehler
Content: Other reasons this message may be displayed:


Source: https://en.wikipedia.org/wiki/Interrupt
Content: In digital computers, an interrupt (sometimes referred to as a trap)[1] is a request for the processor to interrupt currently executing code (when permitted), so that the event can be processed in a timely manner. If the request is accepted, the processor will suspend its current activities, save its state, and execute a function called an interrupt handler (or an interrupt service routine, ISR) to deal with the event. This interruption is often temporary, allowing the software to resume[a] normal activities after the interrupt handler finishes, although the interrupt could instead indicate a fatal error.[2]
 Interrupts are commonly used by hardware devices to indicate electronic or physical state changes that require time-sensitive attention. Interrupts are also commonly used to implement computer multitasking and system calls, especially in real-time computing. Systems that use interrupts in these ways are said to be interrupt-driven.[3]
 Hardware interrupts were introduced as an optimization, eliminating unproductive waiting time in polling loops, waiting for external events. The first system to use this approach was the DYSEAC, completed in 1954, although earlier systems provided error trap functions.[4]
 The UNIVAC 1103A computer is generally credited with the earliest use of interrupts in 1953.[5][6] Earlier, on the UNIVAC I (1951) "Arithmetic overflow either triggered the execution of a two-instruction fix-up routine at address 0, or, at the programmer's option, caused the computer to stop." The IBM 650 (1954) incorporated the first occurrence of interrupt masking. The National Bureau of Standards DYSEAC (1954) was the first to use interrupts for I/O. The IBM 704 was the first to use interrupts for debugging, with a "transfer trap", which could invoke a special routine when a branch instruction was encountered. The MIT Lincoln Laboratory TX-2 system (1957) was the first to provide multiple levels of priority interrupts.[6]
 Interrupt signals may be issued in response to hardware or software events. These are classified as hardware interrupts or software interrupts, respectively. For any particular processor, the number of interrupt types is limited by the architecture.
 A hardware interrupt is a condition related to the state of the hardware that may be signaled by an external hardware device, e.g., an interrupt request (IRQ) line on a PC, or detected by devices embedded in processor logic (e.g., the CPU timer in IBM System/370), to communicate that the device needs attention from the operating system (OS)[7] or, if there is no OS, from the bare metal program running on the CPU. Such external devices may be part of the computer (e.g., disk controller) or they may be external peripherals. For example, pressing a keyboard key or moving a mouse plugged into a PS/2 port triggers hardware interrupts that cause the processor to read the keystroke or mouse position.
 Hardware interrupts can arrive asynchronously with respect to the processor clock, and at any time during instruction execution. Consequently, all incoming hardware interrupt signals are conditioned by synchronizing them to the processor clock, and acted upon only at instruction execution boundaries.
 In many systems, each device is associated with a particular IRQ signal. This makes it possible to quickly determine which hardware device is requesting service, and to expedite servicing of that device.
 On some older systems, such as the 1964 CDC 3600,[8] all interrupts went to the same location, and the OS used a specialized instruction to determine the highest-priority outstanding unmasked interrupt. On contemporary systems, there is generally a distinct interrupt routine for each type of interrupt (or for each interrupt source), often implemented as one or more interrupt vector tables.
 To mask an interrupt is to disable it, so it is deferred[b] or ignored[c] by the processor, while to unmask an interrupt is to enable it.[9]
 Processors typically have an internal interrupt mask register,[d] which allows selective enabling[2] (and disabling) of hardware interrupts. Each interrupt signal is associated with a bit in the mask register. On some systems, the interrupt is enabled when the bit is set, and disabled when the bit is clear. On others, the reverse is true, and a set bit disables the interrupt. When the interrupt is disabled, the associated interrupt signal may be ignored by the processor, or it may remain pending. Signals which are affected by the mask are called maskable interrupts.
 Some interrupt signals are not affected by the interrupt mask and therefore cannot be disabled; these are called non-maskable interrupts (NMIs). These indicate high-priority events which cannot be ignored under any circumstances, such as the timeout signal from a watchdog timer. With regard to SPARC, the Non-Maskable Interrupt (NMI), despite having the highest priority among interrupts, can be prevented from occurring through the use of an interrupt mask.[10]
 One failure mode is when the hardware does not generate the expected interrupt for a change in state, causing the operating system to wait indefinitely. Depending on the details, the failure might affect only a single process or might have global impact. Some operating systems have code specifically to deal with this.
 As an example, IBM Operating System/360 (OS/360) relies on a not-ready to ready device-end interrupt when a tape has been mounted on a tape drive, and will not read the tape label until that interrupt occurs or is simulated. IBM added code in OS/360 so that the VARY ONLINE command will simulate a device end interrupt on the target device.
 A spurious interrupt is a hardware interrupt for which no source can be found.  The term "phantom interrupt" or "ghost interrupt" may also be used to describe this phenomenon.  Spurious interrupts tend to be a problem with a wired-OR interrupt circuit attached to a level-sensitive processor input.  Such interrupts may be difficult to identify when a system misbehaves.
 In a wired-OR circuit, parasitic capacitance charging/discharging through the interrupt line's bias resistor will cause a small delay before the processor recognizes that the interrupt source has been cleared.  If the interrupting device is cleared too late in the interrupt service routine (ISR), there will not be enough time for the interrupt circuit to return to the quiescent state before the current instance of the ISR terminates.  The result is the processor will think another interrupt is pending, since the voltage at its interrupt request input will be not high or low enough to establish an unambiguous internal logic 1 or logic 0.  The apparent interrupt will have no identifiable source, hence the "spurious" moniker.
 A spurious interrupt may also be the result of electrical anomalies due to faulty circuit design, high noise levels, crosstalk, timing issues, or more rarely, device errata.[11]
 A spurious interrupt may result in system deadlock or other undefined operation if the ISR does not account for the possibility of such an interrupt occurring.  As spurious interrupts are mostly a problem with wired-OR interrupt circuits, good programming practice in such systems is for the ISR to check all interrupt sources for activity and take no action (other than possibly logging the event) if none of the sources is interrupting. They may even lead to crashing of the computer in adverse scenarios.
 A software interrupt is requested by the processor itself upon executing particular instructions or when certain conditions are met. Every software interrupt signal is associated with a particular interrupt handler.
 A software interrupt may be intentionally caused by executing a special instruction which, by design, invokes an interrupt when executed.[e] Such instructions function similarly to subroutine calls and are used for a variety of purposes, such as requesting operating system services and interacting with device drivers (e.g., to read or write storage media). Software interrupts may also be triggered by program execution errors or by the virtual memory system.
 Typically, the operating system kernel will catch and handle such interrupts. Some interrupts are handled transparently to the program - for example, the normal resolution of a page fault is to make the required page accessible in physical memory. But in other cases such as a segmentation fault the operating system executes a process callback. On Unix-like operating systems this involves sending a signal such as SIGSEGV, SIGBUS, SIGILL or SIGFPE, which may either call a signal handler or execute a default action (terminating the program). On Windows the callback is made using Structured Exception Handling with an exception code such as STATUS_ACCESS_VIOLATION or STATUS_INTEGER_DIVIDE_BY_ZERO.[12]
 In a kernel process, it is often the case that some types of software interrupts are not supposed to happen. If they occur nonetheless, an operating system crash may result.
 The terms interrupt, trap, exception, fault, and abort are used to distinguish types of interrupts, although "there is no clear consensus as to the exact meaning of these terms".[13] The term trap may refer to any interrupt, to any software interrupt, to any synchronous software interrupt, or only to interrupts caused by instructions with trap in their names. In some usages, the term trap refers specifically to a breakpoint intended to initiate a context switch to a monitor program or debugger.[1] It may also refer to a synchronous interrupt caused by an exceptional condition (e.g., division by zero, invalid memory access, illegal opcode),[13] although the term exception is more common for this.
 x86 divides interrupts into (hardware) interrupts and software exceptions, and identifies three types of exceptions: faults, traps, and aborts.[14][15] (Hardware) interrupts are interrupts triggered asynchronously by an I/O device, and allow the program to be restarted with no loss of continuity.[14] A fault is restartable as well but is tied to the synchronous execution of an instruction - the return address points to the faulting instruction. A trap is similar to a fault except that the return address points to the instruction to be executed after the trapping instruction;[16] one prominent use is to implement system calls.[15] An abort is used for severe errors, such as hardware errors and illegal values in system tables, and often[f] does not allow a restart of the program.[16]
 Arm uses the term exception to refer to all types of interrupts,[17] and divides exceptions into (hardware) interrupts, aborts, reset, and exception-generating instructions. Aborts correspond to x86 exceptions and may be prefetch aborts (failed instruction fetches) or data aborts (failed data accesses), and may be synchronous or asynchronous. Asynchronous aborts may be precise or imprecise. MMU aborts (page faults) are synchronous.[18]
 RISC-V uses interrupt as the overall term as well as for the external subset; internal interrupts are called exceptions.
 Each interrupt signal input is designed to be triggered by either a logic signal level or a particular signal edge (level transition). Level-sensitive inputs continuously request processor service so long as a particular (high or low) logic level is applied to the input. Edge-sensitive inputs react to signal edges: a particular (rising or falling) edge will cause a service request to be latched; the processor resets the latch when the interrupt handler executes.
 A level-triggered interrupt is requested by holding the interrupt signal at its particular (high or low) active logic level. A device invokes a level-triggered interrupt by driving the signal to and holding it at the active level. It negates the signal when the processor commands it to do so, typically after the device has been serviced.
 The processor samples the interrupt input signal during each instruction cycle. The processor will recognize the interrupt request if the signal is asserted when sampling occurs.
 Level-triggered inputs allow multiple devices to share a common interrupt signal via wired-OR connections. The processor polls to determine which devices are requesting service. After servicing a device, the processor may again poll and, if necessary, service other devices before exiting the ISR.
 An edge-triggered interrupt is an interrupt signaled by a level transition on the interrupt line, either a falling edge (high to low) or a rising edge (low to high). A device wishing to signal an interrupt drives a pulse onto the line and then releases the line to its inactive state. If the pulse is too short to be detected by polled I/O then special hardware may be required to detect it. The important part of edge triggering is that the signal must transition to trigger the interrupt; for example, if the signal was high-low-low, there would only be one falling edge interrupt triggered, and the continued low level would not trigger a further interrupt. The signal must return to the high level and fall again in order to trigger a further interrupt. This contrasts with a level trigger where the low level would continue to create interrupts (if they are enabled) until the signal returns to its high level.
 Computers with edge-triggered interrupts may include an interrupt register that retains the status of pending interrupts. Systems with interrupt registers generally have interrupt mask registers as well.
 The processor samples the interrupt trigger signals or interrupt register during each instruction cycle, and will process the highest priority enabled interrupt found.
Regardless of the triggering method, the processor will begin interrupt processing at the next instruction boundary following a detected trigger, thus ensuring:
 There are several different architectures for handling interrupts. In some, there is a single interrupt handler[19] that must scan for the highest priority enabled interrupt. In others, there are separate interrupt handlers for separate interrupt types,[20] separate I/O channels or devices, or both.[21][22]  Several interrupt causes may have the same interrupt type and thus the same interrupt handler, requiring the interrupt handler to determine the cause.[20]
 Interrupts may be fully handled in hardware by the CPU, or may be handled by both the CPU and another component such as a programmable interrupt controller or a southbridge.
 If an additional component is used, that component would be connected between the interrupting device and the processor's interrupt pin to multiplex several sources of interrupt onto the one or two CPU lines typically available. If implemented as part of the memory controller, interrupts are mapped into the system's memory address space.[citation needed]
 In systems on a chip (SoC) implementations, interrupts come from different blocks of the chip and are usually aggregated in an interrupt controller attached to one or several processors (in a multi-core system).[23]
 Multiple devices may share an edge-triggered interrupt line if they are designed to.  The interrupt line must have a pull-down or pull-up resistor so that when not actively driven it settles to its inactive state, which is the default state of it. Devices signal an interrupt by briefly driving the line to its non-default state, and let the line float (do not actively drive it) when not signaling an interrupt. This type of connection is also referred to as open collector. The line then carries all the pulses generated by all the devices. (This is analogous to the pull cord on some buses and trolleys that any passenger can pull to signal the driver that they are requesting a stop.) However, interrupt pulses from different devices may merge if they occur close in time. To avoid losing interrupts the CPU must trigger on the trailing edge of the pulse (e.g. the rising edge if the line is pulled up and driven low).  After detecting an interrupt the CPU must check all the devices for service requirements.
 Edge-triggered interrupts do not suffer the problems that level-triggered interrupts have with sharing.  Service of a low-priority device can be postponed arbitrarily, while interrupts from high-priority devices continue to be received and get serviced. If there is a device that the CPU does not know how to service, which may raise spurious interrupts, it will not interfere with interrupt signaling of other devices. However, it is easy for an edge-triggered interrupt to be missed - for example, when interrupts are masked for a period - and unless there is some type of hardware latch that records the event it is impossible to recover. This problem caused many "lockups" in early computer hardware because the processor did not know it was expected to do something. More modern hardware often has one or more interrupt status registers that latch interrupts requests; well-written edge-driven interrupt handling code can check these registers to ensure no events are missed.
 The elderly Industry Standard Architecture (ISA) bus uses edge-triggered interrupts, without mandating that devices be able to share IRQ lines, but all mainstream ISA motherboards include pull-up resistors on their IRQ lines, so well-behaved ISA devices sharing IRQ lines should just work fine. The parallel port also uses edge-triggered interrupts. Many older devices assume that they have exclusive use of IRQ lines, making it electrically unsafe to share them.
 There are 3 ways multiple devices "sharing the same line" can be raised.  First is by exclusive conduction (switching) or exclusive connection (to pins).  Next is by bus (all connected to the same line listening): cards on a bus must know when they are to talk and not talk (i.e., the ISA bus).  Talking can be triggered in two ways: by accumulation latch or by logic gates.  Logic gates expect a continual data flow that is monitored for key signals.  Accumulators only trigger when the remote side excites the gate beyond a threshold, thus no negotiated speed is required.  Each has its speed versus distance advantages.  A trigger, generally, is the method in which excitation is detected: rising edge, falling edge, threshold (oscilloscope can trigger a wide variety of shapes and conditions).
 Triggering for software interrupts must be built into the software (both in OS and app).  A 'C' app has a trigger table (a table of functions) in its header, which both the app and OS know of and use appropriately that is not related to hardware.  However do not confuse this with hardware interrupts which signal the CPU (the CPU enacts software from a table of functions, similarly to software interrupts).
 Multiple devices sharing an interrupt line (of any triggering style) all act as spurious interrupt sources with respect to each other.  With many devices on one line, the workload in servicing interrupts grows in proportion to the square of the number of devices.  It is therefore preferred to spread devices evenly across the available interrupt lines.  Shortage of interrupt lines is a problem in older system designs where the interrupt lines are distinct physical conductors.  Message-signaled interrupts, where the interrupt line is virtual, are favored in new system architectures (such as PCI Express) and relieve this problem to a considerable extent.
 Some devices with a poorly designed programming interface provide no way to determine whether they have requested service. They may lock up or otherwise misbehave if serviced when they do not want it.  Such devices cannot tolerate spurious interrupts, and so also cannot tolerate sharing an interrupt line. ISA cards, due to often cheap design and construction, are notorious for this problem.  Such devices are becoming much rarer, as hardware logic becomes cheaper and new system architectures mandate shareable interrupts.
 Some systems use a hybrid of level-triggered and edge-triggered signaling. The hardware not only looks for an edge, but it also verifies that the interrupt signal stays active for a certain period of time.
 A common use of a hybrid interrupt is for the NMI (non-maskable interrupt) input. Because NMIs generally signal major – or even catastrophic – system events, a good implementation of this signal tries to ensure that the interrupt is valid by verifying that it remains active for a period of time. This 2-step approach helps to eliminate false interrupts from affecting the system.
 A message-signaled interrupt does not use a physical interrupt line.  Instead, a device signals its request for service by sending a short message over some communications medium, typically a computer bus. The message might be of a type reserved for interrupts, or it might be of some pre-existing type such as a memory write.
 Message-signalled interrupts behave very much like edge-triggered interrupts, in that the interrupt is a momentary signal rather than a continuous condition.  Interrupt-handling software treats the two in much the same manner. Typically, multiple pending message-signaled interrupts with the same message (the same virtual interrupt line) are allowed to merge, just as closely spaced edge-triggered interrupts can merge.
 Message-signalled interrupt vectors can be shared, to the extent that the underlying communication medium can be shared. No additional effort is required.
 Because the identity of the interrupt is indicated by a pattern of data bits, not requiring a separate physical conductor, many more distinct interrupts can be efficiently handled.  This reduces the need for sharing. Interrupt messages can also be passed over a serial bus, not requiring any additional lines.
 PCI Express, a serial computer bus, uses message-signaled interrupts exclusively.
 In a push button analogy applied to computer systems, the term doorbell or doorbell interrupt is often used to describe a mechanism whereby a software system can signal or notify a computer hardware device that there is some work to be done.  Typically, the software system will place data in some well-known and mutually agreed upon memory locations, and "ring the doorbell" by writing to a different memory location.  This different memory location is often called the doorbell region, and there may even be multiple doorbells serving different purposes in this region. It is this act of writing to the doorbell region of memory that "rings the bell" and notifies the hardware device that the data are ready and waiting.  The hardware device would now know that the data are valid and can be acted upon.  It would typically write the data to a  hard disk drive, or send them over a network, or encrypt them, etc.
 The term doorbell interrupt is usually a misnomer.  It is similar to an interrupt, because it causes some work to be done by the device; however, the doorbell region is sometimes implemented as a polled region, sometimes the doorbell region writes through to physical device registers, and sometimes the doorbell region is hardwired directly to physical device registers. When either writing through or directly to physical device registers, this may cause a real interrupt to occur at the device's central processor unit (CPU), if it has one.
 Doorbell interrupts can be compared to Message Signaled Interrupts, as they have some similarities.
 In multiprocessor systems, a processor may send an interrupt request to another processor via inter-processor interrupts[h] (IPI).
 Interrupts provide low overhead and good latency at low load, but degrade significantly at high interrupt rate unless care is taken to prevent several pathologies. The phenomenon where the overall system performance is severely hindered by excessive amounts of processing time spent handling interrupts is called an interrupt storm.
 There are various forms of livelocks, when the system spends all of its time processing interrupts to the exclusion of other required tasks.
Under extreme conditions, a large number of interrupts (like very high network traffic) may completely stall the system. To avoid such problems, an operating system must schedule network interrupt handling as carefully as it schedules process execution.[24]
 With multi-core processors, additional performance improvements in interrupt handling can be achieved through receive-side scaling (RSS) when multiqueue NICs are used.  Such NICs provide multiple receive queues associated to separate interrupts; by routing each of those interrupts to different cores, processing of the interrupt requests triggered by the network traffic received by a single NIC can be distributed among multiple cores.  Distribution of the interrupts among cores can be performed automatically by the operating system, or the routing of interrupts (usually referred to as IRQ affinity) can be manually configured.[25][26]
 A purely software-based implementation of the receiving traffic distribution, known as receive packet steering (RPS), distributes received traffic among cores later in the data path, as part of the interrupt handler functionality.  Advantages of RPS over RSS include no requirements for specific hardware, more advanced traffic distribution filters, and reduced rate of interrupts produced by a NIC.  As a downside, RPS increases the rate of inter-processor interrupts (IPIs).  Receive flow steering (RFS) takes the software-based approach further by accounting for application locality; further performance improvements are achieved by processing interrupt requests by the same cores on which particular network packets will be consumed by the targeted application.[25][27][28]
 Interrupts are commonly used to service hardware timers, transfer data to and from storage (e.g., disk I/O) and communication interfaces (e.g., UART, Ethernet), handle keyboard and mouse events, and to respond to any other time-sensitive events as required by the application system. Non-maskable interrupts are typically used to respond to high-priority requests such as watchdog timer timeouts, power-down signals and traps.
 Hardware timers are often used to generate periodic interrupts. In some applications, such interrupts are counted by the interrupt handler to keep track of absolute or elapsed time, or used by the OS task scheduler to manage execution of running processes, or both. Periodic interrupts are also commonly used to invoke sampling from input devices such as analog-to-digital converters, incremental encoder interfaces, and GPIO inputs, and to program output devices such as digital-to-analog converters, motor controllers, and GPIO outputs.
 A disk interrupt signals the completion of a data transfer from or to the disk peripheral; this may cause a process to run which is waiting to read or write. A power-off interrupt predicts imminent loss of power, allowing the computer to perform an orderly shut-down while there still remains enough power to do so. Keyboard interrupts typically cause keystrokes to be buffered so as to implement typeahead.
 Interrupts are sometimes used to emulate instructions which are unimplemented on some computers in a product family.[29] For example floating point instructions may be implemented in hardware on some systems and emulated on lower-cost systems. In the latter case, execution of an unimplemented floating point instruction will cause an "illegal instruction" exception interrupt. The interrupt handler will implement the floating point function in software and then return to the interrupted program as if the hardware-implemented instruction had been executed.[30] This provides application software portability across the entire line.
 Interrupts are similar to signals, the difference being that signals are used for inter-process communication (IPC), mediated by the kernel (possibly via system calls) and handled by processes, while interrupts are mediated by the processor and handled by the kernel. The kernel may pass an interrupt as a signal to the process that caused it (typical examples are SIGSEGV, SIGBUS, SIGILL and SIGFPE).


Source: https://en.wikipedia.org/wiki/Standardbibliothek
Content: Other reasons this message may be displayed:


Source: https://en.wikipedia.org/wiki/Unix
Content: 
 Unix (/ˈjuːnɪks/, YOO-niks; trademarked as UNIX) is a family of multitasking, multi-user computer operating systems that derive from the original AT&T Unix, whose development started in 1969[1] at the Bell Labs research center by Ken Thompson, Dennis Ritchie, and others.[4]
 Initially intended for use inside the Bell System, AT&T licensed Unix to outside parties in the late 1970s, leading to a variety of both academic and commercial Unix variants from vendors including University of California, Berkeley (BSD), Microsoft (Xenix), Sun Microsystems (SunOS/Solaris), HP/HPE (HP-UX), and IBM (AIX). In the early 1990s, AT&T sold its rights in Unix to Novell, which then sold the UNIX trademark to The Open Group, an industry consortium founded in 1996. The Open Group allows the use of the mark for certified operating systems that comply with the Single UNIX Specification (SUS).
 Early versions of Unix ran on PDP-11 computers.
 Unix systems are characterized by a modular design that is sometimes called the "Unix philosophy". According to this philosophy, the operating system should provide a set of simple tools, each of which performs a limited, well-defined function.[5] A unified and inode-based filesystem and an inter-process communication mechanism known as "pipes" serve as the main means of communication,[4] and a shell scripting and command language (the Unix shell) is used to combine the tools to perform complex workflows.
 Unix distinguishes itself from its predecessors as the first portable operating system: almost the entire operating system is written in the C programming language, which allows Unix to operate on numerous platforms.[6]
 Unix was originally meant to be a convenient platform for programmers developing software to be run on it and on other systems, rather than for non-programmers.[7][8][9] The system grew larger as the operating system started spreading in academic circles, and as users added their own tools to the system and shared them with colleagues.[10]
 At first, Unix was not designed to be portable[6] or for multi-tasking.[11] Later, Unix gradually gained portability, multi-tasking and multi-user capabilities in a time-sharing configuration. Unix systems are characterized by various concepts: the use of plain text for storing data; a hierarchical file system; treating devices and certain types of inter-process communication (IPC) as files; and the use of a large number of software tools, small programs that can be strung together through a command-line interpreter using pipes, as opposed to using a single monolithic program that includes all of the same functionality. These concepts are collectively known as the "Unix philosophy". Brian Kernighan and Rob Pike summarize this in The Unix Programming Environment as "the idea that the power of a system comes more from the relationships among programs than from the programs themselves".[12]
 By the early 1980s, users began seeing Unix as a potential universal operating system, suitable for computers of all sizes.[13][14] The Unix environment and the client–server program model were essential elements in the development of the Internet and the reshaping of computing as centered in networks rather than in individual computers.
 Both Unix and the C programming language were developed by AT&T and distributed to government and academic institutions, which led to both being ported to a wider variety of machine families than any other operating system.
 The Unix operating system consists of many libraries and utilities along with the master control program, the kernel. The kernel provides services to start and stop programs, handles the file system and other common "low-level" tasks that most programs share, and schedules access to avoid conflicts when programs try to access the same resource or device simultaneously. To mediate such access, the kernel has special rights, reflected in the distinction of kernel space from user space, the latter being a lower priority realm where most application programs operate.
 The origins of Unix date back to the mid-1960s when the Massachusetts Institute of Technology, Bell Labs, and General Electric were developing Multics, a time-sharing operating system for the GE 645 mainframe computer.[15]
Multics featured several innovations, but also presented severe problems. Frustrated by the size and complexity of Multics, but not by its goals, individual researchers at Bell Labs started withdrawing from the project. The last to leave were Ken Thompson, Dennis Ritchie, Douglas McIlroy, and Joe Ossanna,[11] who decided to reimplement their experiences in a new project of smaller scale. This new operating system was initially without organizational backing, and also without a name.
 The new operating system was a single-tasking system.[11] In 1970, the group coined the name Unics for Uniplexed Information and Computing Service as a pun on Multics, which stood for Multiplexed Information and Computer Services. Brian Kernighan takes credit for the idea, but adds that "no one can remember" the origin of the final spelling Unix.[16] Dennis Ritchie,[11] Doug McIlroy,[1] and Peter G. Neumann[17] also credit Kernighan.
 The operating system was originally written in assembly language, but in 1973, Version 4 Unix was rewritten in C.[11] Version 4 Unix, however, still had much PDP-11 specific code, and was not suitable for porting. The first port to another platform was a port of Version 6, made four years later (1977) at the University of Wollongong for the Interdata 7/32,[18] followed by a Bell Labs port of Version 7 to the Interdata 8/32 during 1977 and 1978.[19]
 Bell Labs produced several versions of Unix that are collectively referred to as Research Unix. In 1975, the first source license for UNIX was sold to Donald B. Gillies at the University of Illinois Urbana–Champaign Department of Computer Science (UIUC).[20]
 During the late 1970s and early 1980s, the influence of Unix in academic circles led to large-scale adoption of Unix (BSD and System V) by commercial startups, which in turn led to Unix fragmenting into multiple, similar — but often slightly and mutually incompatible — systems including DYNIX, HP-UX, SunOS/Solaris, AIX, and Xenix. In the late 1980s, AT&T Unix System Laboratories and Sun Microsystems developed System V Release 4 (SVR4), which was subsequently adopted by many commercial Unix vendors.
 In the 1990s, Unix and Unix-like systems grew in popularity and became the operating system of choice for over 90% of the world's top 500 fastest supercomputers,[21] as BSD and Linux distributions were developed through collaboration by a worldwide network of programmers. In 2000, Apple released Darwin, also a Unix system, which became the core of the Mac OS X operating system, later renamed macOS.[22]
 Unix-like operating systems are widely used in modern servers, workstations, and mobile devices.[23]
 In the late 1980s, an open operating system standardization effort now known as POSIX provided a common baseline for all operating systems; IEEE based POSIX around the common structure of the major competing variants of the Unix system, publishing the first POSIX standard in 1988. In the early 1990s, a separate but very similar effort was started by an industry consortium, the Common Open Software Environment (COSE) initiative, which eventually became the Single UNIX Specification (SUS) administered by The Open Group. Starting in 1998, the Open Group and IEEE started the Austin Group, to provide a common definition of POSIX and the Single UNIX Specification, which, by 2008, had become the Open Group Base Specification.
 In 1999, in an effort towards compatibility, several Unix system vendors agreed on SVR4's Executable and Linkable Format (ELF) as the standard for binary and object code files. The common format allows substantial binary compatibility among different Unix systems operating on the same CPU architecture.
 The Filesystem Hierarchy Standard was created to provide a reference directory layout for Unix-like operating systems; it has mainly been used in Linux.
 The Unix system is composed of several components that were originally packaged together. By including the development environment, libraries, documents and the portable, modifiable source code for all of these components, in addition to the kernel of an operating system, Unix was a self-contained software system. This was one of the key reasons it emerged as an important teaching and learning tool and has had a broad influence.[according to whom?]
 The inclusion of these components did not make the system large –  the original V7 UNIX distribution, consisting of copies of all of the compiled binaries plus all of the source code and documentation occupied less than 10 MB and arrived on a single nine-track magnetic tape, earning its reputation as a portable system.[24] The printed documentation, typeset from the online sources, was contained in two volumes.
 The names and filesystem locations of the Unix components have changed substantially across the history of the system. Nonetheless, the V7 implementation is considered by many[who?] to have the canonical early structure:
 The Unix system had a significant impact on other operating systems. It achieved its reputation by its interactivity, by providing the software at a nominal fee for educational use, by running on inexpensive hardware, and by being easy to adapt and move to different machines. Unix was originally written in assembly language, but was soon rewritten in C, a high-level programming language.[26] Although this followed the lead of CTSS, Multics and Burroughs MCP, it was Unix that popularized the idea.
 Unix had a drastically simplified file model compared to many contemporary operating systems: treating all kinds of files as simple byte arrays. The file system hierarchy contained machine services and devices (such as printers, terminals, or disk drives), providing a uniform interface, but at the expense of occasionally requiring additional mechanisms such as ioctl and mode flags to access features of the hardware that did not fit the simple "stream of bytes" model. The Plan 9 operating system pushed this model even further and eliminated the need for additional mechanisms.
 Unix also popularized the hierarchical file system with arbitrarily nested subdirectories, originally introduced by Multics. Other common operating systems of the era had ways to divide a storage device into multiple directories or sections, but they had a fixed number of levels, often only one level. Several major proprietary operating systems eventually added recursive subdirectory capabilities also patterned after Multics. DEC's RSX-11M's "group, user" hierarchy evolved into OpenVMS directories, CP/M's volumes evolved into MS-DOS 2.0+ subdirectories, and HP's MPE group.account hierarchy and IBM's SSP and OS/400 library systems were folded into broader POSIX file systems.
 Making the command interpreter an ordinary user-level program, with additional commands provided as separate programs, was another Multics innovation popularized by Unix. The Unix shell used the same language for interactive commands as for scripting (shell scripts – there was no separate job control language like IBM's JCL). Since the shell and OS commands were "just another program", the user could choose (or even write) their own shell. New commands could be added without changing the shell itself. Unix's innovative command-line syntax for creating modular chains of producer-consumer processes (pipelines) made a powerful programming paradigm (coroutines) widely available. Many later command-line interpreters have been inspired by the Unix shell.
 A fundamental simplifying assumption of Unix was its focus on newline-delimited text for nearly all file formats. There were no "binary" editors in the original version of Unix – the entire system was configured using textual shell command scripts. The common denominator in the I/O system was the byte – unlike "record-based" file systems. The focus on text for representing nearly everything made Unix pipes especially useful and encouraged the development of simple, general tools that could easily be combined to perform more complicated ad hoc tasks. The focus on text and bytes made the system far more scalable and portable than other systems. Over time, text-based applications have also proven popular in application areas, such as printing languages (PostScript, ODF), and at the application layer of the Internet protocols, e.g., FTP, SMTP, HTTP, SOAP, and SIP.
 Unix popularized a syntax for regular expressions that found widespread use. The Unix programming interface became the basis for a widely implemented operating system interface standard (POSIX, see above). The C programming language soon spread beyond Unix, and is now ubiquitous in systems and applications programming.
 Early Unix developers were important in bringing the concepts of modularity and reusability into software engineering practice, spawning a "software tools" movement. Over time, the leading developers of Unix (and programs that ran on it) established a set of cultural norms for developing software, norms which became as important and influential as the technology of Unix itself; this has been termed the Unix philosophy.
 The TCP/IP networking protocols were quickly implemented on the Unix versions widely used on relatively inexpensive computers, which contributed to the Internet explosion of worldwide, real-time connectivity and formed the basis for implementations on many other platforms.
 The Unix policy of extensive on-line documentation and (for many years) ready access to all system source code raised programmer expectations, and contributed to the launch of the free software movement in 1983.
 In 1983, Richard Stallman announced the GNU (short for "GNU's Not Unix") project, an ambitious effort to create a free software Unix-like system—"free" in the sense that everyone who received a copy would be free to use, study, modify, and redistribute it. The GNU project's own kernel development project, GNU Hurd, had not yet produced a working kernel, but in 1991 Linus Torvalds released the Linux kernel as free software under the GNU General Public License. In addition to their use in the GNU operating system, many GNU packages – such as the GNU Compiler Collection (and the rest of the GNU toolchain), the GNU C library and the GNU Core Utilities – have gone on to play central roles in other free Unix systems as well.
 Linux distributions, consisting of the Linux kernel and large collections of compatible software have become popular both with individual users and in business. Popular distributions include Red Hat Enterprise Linux, Fedora, SUSE Linux Enterprise, openSUSE, Debian, Ubuntu, Linux Mint, Slackware Linux, Arch Linux and Gentoo.[27]
 A free derivative of BSD Unix, 386BSD, was released in 1992 and led to the NetBSD and FreeBSD projects. With the 1994 settlement of a lawsuit brought against the University of California and Berkeley Software Design Inc. (USL v. BSDi) by Unix System Laboratories, it was clarified that Berkeley had the right to distribute BSD Unix for free if it so desired. Since then, BSD Unix has been developed in several different product branches, including OpenBSD and DragonFly BSD.
 Linux and BSD Unix are increasingly[when?] filling the market needs traditionally served by proprietary Unix operating systems, as well as expanding into new markets such as the consumer desktop and mobile and embedded devices. Because of the modular design of the Unix model, sharing components is relatively common: most or all Unix and Unix-like systems include at least some BSD code, while some include GNU utilities in their distributions.
 In a 1999 interview, Dennis Ritchie voiced his opinion that Linux and BSD Unix operating systems are a continuation of the basis of the Unix design and are derivatives of Unix:[28]
 I think the Linux phenomenon is quite delightful, because it draws so strongly on the basis that Unix provided. Linux seems to be among the healthiest of the direct Unix derivatives, though there are also the various BSD systems as well as the more official offerings from the workstation and mainframe manufacturers. In the same interview, he states that he views both Unix and Linux as "the continuation of ideas that were started by Ken and me and many others, many years ago".[28]
 OpenSolaris was the free software counterpart to Solaris developed by Sun Microsystems, which included a CDDL-licensed kernel and a primarily GNU userland. However, Oracle discontinued the project upon their acquisition of Sun, which prompted a group of former Sun employees and members of the OpenSolaris community to fork OpenSolaris into the illumos kernel. As of 2014, illumos remains the only active, open-source System V derivative.
 In May 1975, RFC 681 described the development of Network Unix by the Center for Advanced Computation at the University of Illinois Urbana-Champaign.[29] The Unix system was said to "present several interesting capabilities as an ARPANET mini-host". At the time, Unix required a license from Bell Telephone Laboratories that cost US$20,000 for non-university institutions, while universities could obtain a license for a nominal fee of $150. It was noted that Bell was "open to suggestions" for an ARPANET-wide license.
 The RFC specifically mentions that Unix "offers powerful local processing facilities in terms of user programs, several compilers, an editor based on QED, a versatile document preparation system, and an efficient file system featuring sophisticated access control, mountable and de-mountable volumes, and a unified treatment of peripherals as special files." The latter permitted the Network Control Program (NCP) to be integrated within the Unix file system, treating network connections as special files that could be accessed through standard Unix I/O calls, which included the added benefit of closing all connections on program exit, should the user neglect to do so. In order "to minimize the amount of code added to the basic Unix kernel", much of the NCP code ran in a swappable user process, running only when needed.[29]
 In October 1993, Novell, the company that owned the rights to the Unix System V source at the time, transferred the trademarks of Unix to the X/Open Company (now The Open Group),[30] and in 1995 sold the related business operations to Santa Cruz Operation (SCO).[31][32] Whether Novell also sold the copyrights to the actual software was the subject of a federal lawsuit in 2006, SCO v. Novell, which Novell won. The case was appealed, but on August 30, 2011, the United States Court of Appeals for the Tenth Circuit affirmed the trial decisions, closing the case.[33] Unix vendor SCO Group Inc. accused Novell of slander of title.
 The present owner of the trademark UNIX is The Open Group, an industry standards consortium. Only systems fully compliant with and certified to the Single UNIX Specification qualify as "UNIX" (others are called "Unix-like").
 By decree of The Open Group, the term "UNIX" refers more to a class of operating systems than to a specific implementation of an operating system; those operating systems which meet The Open Group's Single UNIX Specification should be able to bear the UNIX 98 or UNIX 03 trademarks today, after the operating system's vendor pays a substantial certification fee and annual trademark royalties to The Open Group.[34] Systems that have been licensed to use the UNIX trademark include AIX,[35] EulerOS,[36] HP-UX,[37] Inspur K-UX,[38] IRIX,[39] macOS,[40] Solaris,[41] Tru64 UNIX (formerly "Digital UNIX", or OSF/1),[42] and z/OS.[43] Notably, EulerOS and Inspur K-UX are Linux distributions certified as UNIX 03 compliant.[44][45]
 Sometimes a representation like Un*x, *NIX, or *N?X is used to indicate all operating systems similar to Unix. This comes from the use of the asterisk (*) and the question mark characters as wildcard indicators in many utilities. This notation is also used to describe other Unix-like systems that have not met the requirements for UNIX branding from the Open Group.
 The Open Group requests that UNIX always be used as an adjective followed by a generic term such as system to help avoid the creation of a genericized trademark.
 Unix was the original formatting,[disputed  – discuss] but the usage of UNIX remains widespread because it was once typeset in small caps (Unix). According to Dennis Ritchie, when presenting the original Unix paper to the third Operating Systems Symposium of the American Association for Computing Machinery (ACM), "we had a new typesetter and troff had just been invented and we were intoxicated by being able to produce small caps".[46] Many of the operating system's predecessors and contemporaries used all-uppercase lettering, so many people wrote the name in upper case due to force of habit. It is not an acronym.[47]
 Trademark names can be registered by different entities in different countries and trademark laws in some countries allow the same trademark name to be controlled by two different entities if each entity uses the trademark in easily distinguishable categories. The result is that Unix has been used as a brand name for various products including bookshelves, ink pens, bottled glue, diapers, hair driers and food containers.[48]
 Several plural forms of Unix are used casually to refer to multiple brands of Unix and Unix-like systems. Most common is the conventional Unixes, but Unices, treating Unix as a Latin noun of the third declension, is also popular. The pseudo-Anglo-Saxon plural form Unixen is not common, although occasionally seen. Sun Microsystems, developer of the Solaris variant, has asserted that the term Unix is itself plural, referencing its many implementations.[49]


Source: https://en.wikipedia.org/wiki/Microsoft_Windows
Content: 
 23H2 (10.0.22631.3371) (March 21, 2024; 0 days ago (2024-03-21)[2][3]) [±]
 23H2 (10.0.22635.3350) (March 13, 2024; 8 days ago (2024-03-13)[4]) [±]
 24H2 (10.0.26085.1) (March 20, 2024; 1 day ago (2024-03-20)[5]) [±]
 Microsoft Windows is a product line of proprietary graphical operating systems developed and marketed by Microsoft. It is grouped into families and sub-families that cater to particular sectors of the computing industry -- Windows (unqualified) for a consumer or corporate workstation, Windows Server for a server and Windows IoT for an embedded system. Defunct families include Windows 9x, Windows Mobile, Windows Phone, and Windows Embedded Compact.
 The first version of Windows was released on November 20, 1985, as a graphical operating system shell for MS-DOS in response to the growing interest in graphical user interfaces (GUIs).[11]
 Windows is the most popular desktop operating system in the world, with a 70% market share as of March 2023[update], according to StatCounter.[12] However, Windows is not the most used operating system when including both mobile and desktop OSes, due to Android's massive growth.[13]
 As of 2024[update], the most recent version of Windows is Windows 11 for consumer PCs and tablets, Windows 11 Enterprise for corporations, and Windows Server 2022 for servers. Still supported are some editions of Windows 10, Windows Server 2016 and later (and exceptionally with paid support down to Windows Server 2012 and Windows Embedded POSReady 7).
 Microsoft registered trademarks for various products (families) of the Windows product line -- most of which target a specific sector of the computing industry. 
 As of 2024,[update] the only active top-level family is Windows NT. The first version, Windows NT 3.1, was intended for server computing and corporate workstations. It grew into a product line of its own and now consists of four sub-families that tend to be released almost simultaneously and share the same kernel.
 These top-level Windows families are no longer actively developed:
 The term Windows collectively describes any or all of several generations of Microsoft operating system products. These products are generally categorized as follows:
 The history of Windows dates back to 1981 when Microsoft started work on a program called "Interface Manager". It was announced in November 1983 (after the Apple Lisa, but before the Macintosh) under the name "Windows", but Windows 1.0 was not released until November 1985.[16] Windows 1.0 was to compete with Apple's operating system, but achieved little popularity. Windows 1.0 is not a complete operating system; rather, it extends MS-DOS. The shell of Windows 1.0 is a program known as the MS-DOS Executive. Components included Calculator, Calendar, Cardfile, Clipboard Viewer, Clock, Control Panel, Notepad, Paint, Reversi, Terminal and Write. Windows 1.0 does not allow overlapping windows. Instead, all windows are tiled. Only modal dialog boxes may appear over other windows. Microsoft sold as included Windows Development libraries with the C development environment, which included numerous windows samples.[17]
 Windows 2.0 was released in December 1987, and was more popular than its predecessor. It features several improvements to the user interface and memory management.[18] Windows 2.03 changed the OS from tiled windows to overlapping windows. The result of this change led to Apple Computer filing a suit against Microsoft alleging infringement on Apple's copyrights (eventually settled in court in Microsoft's favor in 1993).[19][20] Windows 2.0 also introduced more sophisticated keyboard shortcuts and could make use of expanded memory.
 Windows 2.1 was released in two different versions: Windows/286 and Windows/386. Windows/386 uses the virtual 8086 mode of the Intel 80386 to multitask several DOS programs and the paged memory model to emulate expanded memory using available extended memory. Windows/286, in spite of its name, runs on both Intel 8086 and Intel 80286 processors. It runs in real mode but can make use of the high memory area.[21]
 In addition to full Windows packages, there were runtime-only versions that shipped with early Windows software from third parties and made it possible to run their Windows software on MS-DOS and without the full Windows feature set.
 The early versions of Windows are often thought of as graphical shells, mostly because they ran on top of MS-DOS and used it for file system services.[22] However, even the earliest Windows versions already assumed many typical operating system functions; notably, having their own executable file format and providing their own device drivers (timer, graphics, printer, mouse, keyboard and sound). Unlike MS-DOS, Windows allowed users to execute multiple graphical applications at the same time, through cooperative multitasking. Windows implemented an elaborate, segment-based, software virtual memory scheme, which allows it to run applications larger than available memory: code segments and resources are swapped in and thrown away when memory became scarce; data segments moved in memory when a given application had relinquished processor control.
 Windows 3.0, released in 1990, improved the design, mostly because of virtual memory and loadable virtual device drivers (VxDs) that allow Windows to share arbitrary devices between multi-tasked DOS applications.[citation needed] Windows 3.0 applications can run in protected mode, which gives them access to several megabytes of memory without the obligation to participate in the software virtual memory scheme. They run inside the same address space, where the segmented memory provides a degree of protection. Windows 3.0 also featured improvements to the user interface. Microsoft rewrote critical operations from C into assembly. Windows 3.0 was the first version of Windows to achieve broad commercial success, selling 2 million copies in the first six months.[23][24]
 Windows 3.1, made generally available on March 1, 1992, featured a facelift. In August 1993, Windows for Workgroups, a special version with integrated peer-to-peer networking features and a version number of 3.11, was released. It was sold along with Windows 3.1. Support for Windows 3.1 ended on December 31, 2001.[25]
 Windows 3.2, released in 1994, is an updated version of the Chinese version of Windows 3.1.[26] The update was limited to this language version, as it fixed only issues related to the complex writing system of the Chinese language.[27] Windows 3.2 was generally sold by computer manufacturers with a ten-disk version of MS-DOS that also had Simplified Chinese characters in basic output and some translated utilities.
 The next major consumer-oriented release of Windows, Windows 95, was released on August 24, 1995. While still remaining MS-DOS-based, Windows 95 introduced support for native 32-bit applications, plug and play hardware, preemptive multitasking, long file names of up to 255 characters, and provided increased stability over its predecessors. Windows 95 also introduced a redesigned, object oriented user interface, replacing the previous Program Manager with the Start menu, taskbar, and Windows Explorer shell. Windows 95 was a major commercial success for Microsoft; Ina Fried of CNET remarked that "by the time Windows 95 was finally ushered off the market in 2001, it had become a fixture on computer desktops around the world."[28] Microsoft published four OEM Service Releases (OSR) of Windows 95, each of which was roughly equivalent to a service pack. The first OSR of Windows 95 was also the first version of Windows to be bundled with Microsoft's web browser, Internet Explorer.[29] Mainstream support for Windows 95 ended on December 31, 2000, and extended support for Windows 95 ended on December 31, 2001.[30]
 Windows 95 was followed up with the release of Windows 98 on June 25, 1998, which introduced the Windows Driver Model, support for USB composite devices, support for ACPI, hibernation, and support for multi-monitor configurations. Windows 98 also included integration with Internet Explorer 4 through Active Desktop and other aspects of the Windows Desktop Update (a series of enhancements to the Explorer shell which was also made available for Windows 95). In May 1999, Microsoft released Windows 98 Second Edition, an updated version of Windows 98. Windows 98 SE added Internet Explorer 5.0 and Windows Media Player 6.2 amongst other upgrades. Mainstream support for Windows 98 ended on June 30, 2002, and extended support for Windows 98 ended on July 11, 2006.[31]
 On September 14, 2000, Microsoft released Windows Me (Millennium Edition), the last DOS-based version of Windows. Windows Me incorporated visual interface enhancements from its Windows NT-based counterpart Windows 2000, had faster boot times than previous versions (which however, required the removal of the ability to access a real mode DOS environment, removing compatibility with some older programs),[32] expanded multimedia functionality (including Windows Media Player 7, Windows Movie Maker, and the Windows Image Acquisition framework for retrieving images from scanners and digital cameras), additional system utilities such as System File Protection and System Restore, and updated home networking tools.[33] However, Windows Me was faced with criticism for its speed and instability, along with hardware compatibility issues and its removal of real mode DOS support. PC World considered Windows Me to be one of the worst operating systems Microsoft had ever released, and the fourth worst tech product of all time.[15]
 In November 1988, a new development team within Microsoft (which included former Digital Equipment Corporation developers Dave Cutler and Mark Lucovsky) began work on a revamped version of IBM and Microsoft's OS/2 operating system known as "NT OS/2". NT OS/2 was intended to be a secure, multi-user operating system with POSIX compatibility and a modular, portable kernel with preemptive multitasking and support for multiple processor architectures. However, following the successful release of Windows 3.0, the NT development team decided to rework the project to use an extended 32-bit port of the Windows API known as Win32 instead of those of OS/2. Win32 maintained a similar structure to the Windows APIs (allowing existing Windows applications to easily be ported to the platform), but also supported the capabilities of the existing NT kernel. Following its approval by Microsoft's staff, development continued on what was now Windows NT, the first 32-bit version of Windows. However, IBM objected to the changes, and ultimately continued OS/2 development on its own.[34][35]
 Windows NT was the first Windows operating system based on a hybrid kernel. The hybrid kernel was designed as a modified microkernel, influenced by the Mach microkernel developed by Richard Rashid at Carnegie Mellon University, but without meeting all of the criteria of a pure microkernel.
 The first release of the resulting operating system, Windows NT 3.1 (named to associate it with Windows 3.1) was released in July 1993, with versions for desktop workstations and servers. Windows NT 3.5 was released in September 1994, focusing on performance improvements and support for Novell's NetWare, and was followed up by Windows NT 3.51 in May 1995, which included additional improvements and support for the PowerPC architecture. Windows NT 4.0 was released in June 1996, introducing the redesigned interface of Windows 95 to the NT series. On February 17, 2000, Microsoft released Windows 2000, a successor to NT 4.0. The Windows NT name was dropped at this point in order to put a greater focus on the Windows brand.[35]
 The next major version of Windows NT, Windows XP, was released to manufacturing (RTM) on August 24, 2001, and to the general public on October 25, 2001. The introduction of Windows XP aimed to unify the consumer-oriented Windows 9x series with the architecture introduced by Windows NT, a change which Microsoft promised would provide better performance over its DOS-based predecessors. Windows XP would also introduce a redesigned user interface (including an updated Start menu and a "task-oriented" Windows Explorer), streamlined multimedia and networking features, Internet Explorer 6, integration with Microsoft's .NET Passport services, a "compatibility mode" to help provide backwards compatibility with software designed for previous versions of Windows, and Remote Assistance functionality.[36][37]
 At retail, Windows XP was marketed in two main editions: the "Home" edition was targeted towards consumers, while the "Professional" edition was targeted towards business environments and power users, and included additional security and networking features. Home and Professional were later accompanied by the "Media Center" edition (designed for home theater PCs, with an emphasis on support for DVD playback, TV tuner cards, DVR functionality, and remote controls), and the "Tablet PC" edition (designed for mobile devices meeting its specifications for a tablet computer, with support for stylus pen input and additional pen-enabled applications).[38][39][40] Mainstream support for Windows XP ended on April 14, 2009. Extended support ended on April 8, 2014.[41]
 After Windows 2000, Microsoft also changed its release schedules for server operating systems; the server counterpart of Windows XP, Windows Server 2003, was released in April 2003.[35] It was followed in December 2005, by Windows Server 2003 R2.
 After a lengthy development process, Windows Vista was released on November 30, 2006, for volume licensing and January 30, 2007, for consumers. It contained a number of new features, from a redesigned shell and user interface to significant technical changes, with a particular focus on security features. It was available in a number of different editions, and has been subject to some criticism, such as drop of performance, longer boot time, criticism of new UAC, and stricter license agreement. Vista's server counterpart, Windows Server 2008 was released in early 2008.
 On July 22, 2009, Windows 7 and Windows Server 2008 R2 were released to manufacturing (RTM) and released to the public three months later on October 22, 2009. Unlike its predecessor, Windows Vista, which introduced a large number of new features, Windows 7 was intended to be a more focused, incremental upgrade to the Windows line, with the goal of being compatible with applications and hardware with which Windows Vista was already compatible.[42] Windows 7 has multi-touch support, a redesigned Windows shell with an updated taskbar with revealable jump lists that contain shortcuts to files frequently used with specific applications and shortcuts to tasks within the application,[43] a home networking system called HomeGroup,[44] and performance improvements.
 Windows 8, the successor to Windows 7, was released generally on October 26, 2012. A number of significant changes were made on Windows 8, including the introduction of a user interface based around Microsoft's Metro design language with optimizations for touch-based devices such as tablets and all-in-one PCs. These changes include the Start screen, which uses large tiles that are more convenient for touch interactions and allow for the display of continually updated information, and a new class of apps which are designed primarily for use on touch-based devices. The new Windows version required a minimum resolution of 1024×768 pixels,[45] effectively making it unfit for netbooks with 800×600-pixel screens.
 Other changes include increased integration with cloud services and other online platforms (such as social networks and Microsoft's own OneDrive (formerly SkyDrive) and Xbox Live services), the Windows Store service for software distribution, and a new variant known as Windows RT for use on devices that utilize the ARM architecture, and a new keyboard shortcut for screenshots.[46][47][48][49][50][51][52] An update to Windows 8, called Windows 8.1,[53] was released on October 17, 2013, and includes features such as new live tile sizes, deeper OneDrive integration, and many other revisions. Windows 8 and Windows 8.1 have been subject to some criticism, such as the removal of the Start menu.
 On September 30, 2014, Microsoft announced Windows 10 as the successor to Windows 8.1. It was released on July 29, 2015, and addresses shortcomings in the user interface first introduced with Windows 8. Changes on PC include the return of the Start Menu, a virtual desktop system, and the ability to run Windows Store apps within windows on the desktop rather than in full-screen mode. Windows 10 is said to be available to update from qualified Windows 7 with SP1, Windows 8.1 and Windows Phone 8.1 devices from the Get Windows 10 Application (for Windows 7, Windows 8.1) or Windows Update (Windows 7).[54]
 In February 2017, Microsoft announced the migration of its Windows source code repository from Perforce to Git. This migration involved 3.5 million separate files in a 300-gigabyte repository.[55] By May 2017, 90 percent of its engineering team was using Git, in about 8500 commits and 1760 Windows builds per day.[55]
 In June 2021, shortly before Microsoft's announcement of Windows 11, Microsoft updated their lifecycle policy pages for Windows 10, revealing that support for their last release of Windows 10 will end on October 14, 2025.[56][57] On April 27, 2023, Microsoft announced that version 22H2 would be the last of Windows 10.[58][59]
 On June 24, 2021, Windows 11 was announced as the successor to Windows 10 during a livestream. The new operating system was designed to be more user-friendly and understandable. It was released on October 5, 2021.[60][61] As of May 2022,[update] Windows 11 is a free upgrade to Windows 10 users who meet the system requirements.[62]
 In July 2021, Microsoft announced it will start selling subscriptions to virtualized Windows desktops as part of a new Windows 365 service in the following month. The new service will allow for cross-platform usage, aiming to make the operating system available for both Apple and Android users. It is a separate service and offers several variations including Windows 365 Frontline, Windows 365 Boot, and the Windows 365 app.[63] The subscription service will be accessible through any operating system with a web browser. The new service is an attempt at capitalizing on the growing trend, fostered during the COVID-19 pandemic, for businesses to adopt a hybrid remote work environment, in which "employees split their time between the office and home". As the service will be accessible through web browsers, Microsoft will be able to bypass the need to publish the service through Google Play or the Apple App Store.[64][65][66][67][68]
 Microsoft announced Windows 365 availability to business and enterprise customers on August 2, 2021.[69]
 Multilingual support has been built into Windows since Windows 3.0. The language for both the keyboard and the interface can be changed through the Region and Language Control Panel. Components for all supported input languages, such as Input Method Editors, are automatically installed during Windows installation (in Windows XP and earlier, files for East Asian languages, such as Chinese, and files for right-to-left scripts, such as Arabic, may need to be installed separately, also from the said Control Panel). Third-party IMEs may also be installed if a user feels that the provided one is insufficient for their needs. Since Windows 2000, English editions of Windows NT have East Asian IMEs (such as Microsoft Pinyin IME and Microsoft Japanese IME) bundled, but files for East Asian languages may be manually installed on Control Panel.
 Interface languages for the operating system are free for download, but some languages are limited to certain editions of Windows. Language Interface Packs (LIPs) are redistributable and may be downloaded from Microsoft's Download Center and installed for any edition of Windows (XP or later) – they translate most, but not all, of the Windows interface, and require a certain base language (the language which Windows originally shipped with). This is used for most languages in emerging markets. Full Language Packs, which translate the complete operating system, are only available for specific editions of Windows (Ultimate and Enterprise editions of Windows Vista and 7, and all editions of Windows 8, 8.1 and RT except Single Language). They do not require a specific base language and are commonly used for more popular languages such as French or Chinese. These languages cannot be downloaded through the Download Center, but are available as optional updates through the Windows Update service (except Windows 8).
 The interface language of installed applications is not affected by changes in the Windows interface language. The availability of languages depends on the application developers themselves.
 Windows 8 and Windows Server 2012 introduce a new Language Control Panel where both the interface and input languages can be simultaneously changed, and language packs, regardless of type, can be downloaded from a central location. The PC Settings app in Windows 8.1 and Windows Server 2012 R2 also includes a counterpart settings page for this. Changing the interface language also changes the language of preinstalled Windows Store apps (such as Mail, Maps and News) and certain other Microsoft-developed apps (such as Remote Desktop). The above limitations for language packs are however still in effect, except that full language packs can be installed for any edition except Single Language, which caters to emerging markets.
 Windows NT included support for several platforms before the x86-based personal computer became dominant in the professional world. Windows NT 4.0 and its predecessors supported PowerPC, DEC Alpha and MIPS R4000 (although some of the platforms implement 64-bit computing, the OS treated them as 32-bit). Windows 2000 dropped support for all platforms, except the third generation x86 (known as IA-32) or newer in 32-bit mode. The client line of the Windows NT family still ran on IA-32 up to Windows 10, with Windows 11 dropping support;[62] the Windows Server line ceased supporting this platform with the release of Windows Server 2008 R2.
 With the introduction of the Intel Itanium architecture (IA-64), Microsoft released new versions of Windows to support it. Itanium versions of Windows XP and Windows Server 2003 were released at the same time as their mainstream x86 counterparts. Windows XP 64-Bit Edition (Version 2003), released in 2003, is the last Windows client operating system to support Itanium. Windows Server line continues to support this platform until Windows Server 2012; Windows Server 2008 R2 is the last Windows operating system to support Itanium architecture.
 On April 25, 2005, Microsoft released Windows XP Professional x64 Edition and Windows Server 2003 x64 editions to support x86-64 (or simply x64), the 64-bit version of x86 architecture. Windows Vista was the first client version of Windows NT to be released simultaneously in IA-32 and x64 editions. As of 2024, x64 is still supported.
 An edition of Windows 8 known as Windows RT was specifically created for computers with ARM architecture, and while ARM is still used for Windows smartphones with Windows 10, tablets with Windows RT will not be updated. Starting from Windows 10 Fall Creators Update (version 1709) and later includes support for ARM-based PCs.[70]
 Windows CE (officially known as Windows Embedded Compact), is an edition of Windows that runs on minimalistic computers, like satellite navigation systems and some mobile phones. Windows Embedded Compact is based on its own dedicated kernel, dubbed Windows CE kernel. Microsoft licenses Windows CE to OEMs and device makers. The OEMs and device makers can modify and create their own user interfaces and experiences, while Windows CE provides the technical foundation to do so.
 Windows CE was used in the Dreamcast along with Sega's own proprietary OS for the console. Windows CE was the core from which Windows Mobile was derived. Its successor, Windows Phone 7, was based on components from both Windows CE 6.0 R3 and Windows CE 7.0. Windows Phone 8 however, is based on the same NT-kernel as Windows 8.
 Windows Embedded Compact is not to be confused with Windows XP Embedded or Windows NT 4.0 Embedded, modular editions of Windows based on Windows NT kernel.
 Xbox OS is an unofficial name given to the version of Windows that runs on Xbox consoles.[71] From Xbox One onwards it is an implementation with an emphasis on virtualization (using Hyper-V) as it is three operating systems running at once, consisting of the core operating system, a second implemented for games and a more Windows-like environment for applications.[72]
Microsoft updates Xbox One's OS every month, and these updates can be downloaded from the Xbox Live service to the Xbox and subsequently installed, or by using offline recovery images downloaded via a PC.[73] It was originally based on NT 6.2 (Windows 8) kernel, and the latest version runs on an NT 10.0 base. This system is sometimes referred to as "Windows 10 on Xbox One".[74][75]
Xbox One and Xbox Series operating systems also allow limited (due to licensing restrictions and testing resources) backward compatibility with previous generation hardware,[76] and the Xbox 360's system is backwards compatible with the original Xbox.[77]
 Up to and including every version before Windows 2000, Microsoft used an in-house version control system named Source Library Manager (SLM). Shortly after Windows 2000 was released, Microsoft switched to a fork of Perforce named Source Depot.[78] This system was used up until 2017 once the system could not keep up with the size of Windows.[citation needed] Microsoft had begun to integrate Git into Team Foundation Server in 2013,[79] but Windows (and Office) continued to rely on Source Depot.[80] The Windows code was divided among 65 different repositories with a kind of virtualization layer to produce unified view of all of the code.[citation needed]
 In 2017 Microsoft announced that it would start using Git, an open source version control system created by Linus Torvalds and in May 2017 they reported that the migration into a new Git repository was complete.[81][82][55]
 Because of its large, decades-long history, however, the Windows codebase is not especially well suited to the decentralized nature of Linux development that Git was originally created to manage.[citation needed] Each Git repository contains a complete history of all the files,[83] which proved unworkable for Windows developers because cloning the whole repository takes several hours.[citation needed] Microsoft has been working on a new project called the Virtual File System for Git (VFSForGit) to address these challenges.[82]
 In 2021 the VFS for Git has been superseded by Scalar.[84]
 Version market share
As a percentage of desktop and laptop systems using Microsoft Windows,[88] according to StatCounter data as of February 2024.[89]
 Use of Windows 10 has exceeded Windows 7 globally since early 2018.[90]
 For desktop and laptop computers, according to Net Applications and StatCounter (which track the use of operating systems in devices that are active on the Web), Windows was the most used operating-system family in August 2021, with around 91% usage share according to Net Applications[91] and around 76% usage share according to StatCounter.[92]
 Including personal computers of all kinds (e.g., desktops, laptops, mobile devices, and game consoles), Windows OSes accounted for 32.67% of usage share in August 2021, compared to Android (highest, at 46.03%), iOS's 13.76%, iPadOS's 2.81%, and macOS's 2.51%, according to Net Applications[93] and 30.73% of usage share in August 2021, compared to Android (highest, at 42.56%), iOS/iPadOS's 16.53%, and macOS's 6.51%, according to StatCounter.[94]
 Those statistics do not include servers (including so-called cloud computing, where Microsoft is known not to be a leader, with Linux used more than Windows) as Net Applications and StatCounter use web browsing as a proxy for all use.
 Early versions of Windows were designed at a time where malware and networking were less common, and had few built-in security features; they did not provide access privileges to allow a user to prevent other users from accessing their files, and they did not provide memory protection to prevent one process from reading or writing another process's address space or to prevent a process from code or data used by privileged-mode code.
 While the Windows 9x series offered the option of having profiles for multiple users with separate profiles and home folders, it had no concept of access privileges, allowing any user to edit others' files. In addition, while it ran separate 32-bit applications in separate address spaces, protecting an application's code and data from being read or written by another application, it did not protect the first megabyte of memory from userland applications for compatibility reasons. This area of memory contains code critical to the functioning of the operating system, and by writing into this area of memory an application can crash or freeze the operating system. This was a source of instability as faulty applications could accidentally write into this region, potentially corrupting important operating system memory, which usually resulted in some form of system error and halt.[95]
 Windows NT was far more secure, implementing access privileges and full memory protection, and, while 32-bit programs meeting the DoD's C2 security rating,[96] yet these advantages were nullified[improper synthesis?] by the fact that, prior to Windows Vista, the default user account created during the setup process was an administrator account; the user, and any program the user launched, had full access to the machine. Though Windows XP did offer an option of turning administrator accounts into limited accounts, the majority of home users did not do so, partially due to the number of programs which required administrator rights to function properly. As a result, most home users still ran as administrator all the time. These architectural flaws, combined with Windows's very high popularity, made Windows a frequent target of computer worm and virus writers.[97][98][citation needed]
 Furthermore, although Windows NT and its successors are designed for security (including on a network) and multi-user PCs, they were not initially designed with Internet security in mind as much, since, when it was first developed in the early 1990s, Internet use was less prevalent.[99]
 In a 2002 strategy memo entitled "Trustworthy computing" sent to every Microsoft employee, Bill Gates declared that security should become Microsoft's highest priority.[100][101]
 Windows Vista introduced a privilege elevation system called User Account Control.[102] When logging in as a standard user, a logon session is created and a token containing only the most basic privileges is assigned. In this way, the new logon session is incapable of making changes that would affect the entire system. When logging in as a user in the Administrators group, two separate tokens are assigned. The first token contains all privileges typically awarded to an administrator, and the second is a restricted token similar to what a standard user would receive. User applications, including the Windows shell, are then started with the restricted token, resulting in a reduced privilege environment even under an Administrator account. When an application requests higher privileges or "Run as administrator" is clicked, UAC will prompt for confirmation and, if consent is given (including administrator credentials if the account requesting the elevation is not a member of the administrators group), start the process using the unrestricted token.[103]
 Leaked documents from 2013 to 2016 codenamed Vault 7 detail the capabilities of the CIA to perform electronic surveillance and cyber warfare,[104] such as the ability to compromise operating systems such as Windows.[105]
 In August 2019, computer experts reported that the BlueKeep security vulnerability, CVE-2019-0708, that potentially affects older unpatched Windows versions via the program's Remote Desktop Protocol, allowing for the possibility of remote code execution, may include related flaws, collectively named DejaBlue, affecting newer Windows versions (i.e., Windows 7 and all recent versions) as well.[106] In addition, experts reported a Microsoft security vulnerability, CVE-2019-1162, based on legacy code involving Microsoft CTF and ctfmon (ctfmon.exe), that affects all Windows versions from Windows XP to the then most recent Windows 10 versions; a patch to correct the flaw is available.[107]
 Microsoft releases security patches through its Windows Update service approximately once a month (usually the second Tuesday of the month), although critical updates are made available at shorter intervals when necessary.[108] Versions subsequent to Windows 2000 SP3 and Windows XP implemented automatic download and installation of updates, substantially increasing the number of users installing security updates.[109]
 Windows integrates the Windows Defender antivirus, which is seen as one of the best available.[110] Windows also implements Secure Boot, Control Flow Guard, ransomware protection, BitLocker disk encryption, a firewall, and Windows SmartScreen.
 All Windows versions from Windows NT 3 have been based on a file system permission system referred to as AGDLP (Accounts, Global, Domain Local, Permissions) in which file permissions are applied to the file/folder in the form of a 'local group' which then has other 'global groups' as members. These global groups then hold other groups or users depending on different Windows versions used. This system varies from other vendor products such as Linux and NetWare due to the 'static' allocation of permission being applied directly to the file or folder. However using this process of AGLP/AGDLP/AGUDLP allows a small number of static permissions to be applied and allows for easy changes to the account groups without reapplying the file permissions on the files and folders.
 Owing to the operating system's popularity, a number of applications have been released that aim to provide compatibility with Windows applications, either as a compatibility layer for another operating system, or as a standalone system that can run software written for Windows out of the box. These include:


Source: https://en.wikipedia.org/wiki/MacOS
Content: 
 macOS (/ˌmækoʊˈɛs/ MAK-oh-ESS[4]), originally Mac OS X, previously shortened as OS X, is an operating system developed and marketed by Apple since 2001. It is the primary operating system for Apple's Mac computers. Within the market of desktop and laptop computers, it is the second most widely used desktop OS, after Microsoft Windows and ahead of all Linux distributions, including ChromeOS.
 Mac OS X succeeded classic Mac OS, the primary Macintosh operating system from 1984 to 2001. Its underlying architecture came from NeXT's NeXTSTEP, as a result of Apple's acquisition of NeXT, which also brought Steve Jobs back to Apple.
 The first desktop version, Mac OS X 10.0, was released on March 24, 2001. All releases from Mac OS X Leopard onward (except for OS X Lion) are UNIX 03 certified.[5][6] The derivatives of macOS are Apple's other operating systems: iOS, iPadOS, watchOS, tvOS, and audioOS.
 A prominent part of macOS's original brand identity was the use of Roman numeral X, pronounced "ten", as well as code naming each release after species of big cats, and later, places within California.[7] Apple shortened the name to "OS X" in 2011 and then changed it to "macOS" in 2016 to align with the branding of Apple's other operating systems, iOS, watchOS, and tvOS.[8] After sixteen distinct versions of macOS 10, macOS Big Sur was presented as version 11 in 2020, and every subsequent version has also incremented the major version number, similarly to classic Mac OS and iOS.
 macOS has supported three major processor architectures, beginning with PowerPC-based Macs in 1999. In 2006, Apple transitioned to the Intel architecture with a line of Macs using Intel Core processors. In 2020, Apple began the Apple silicon transition, using self-designed, 64-bit Arm-based Apple M series processors on the latest Macintosh computers.[9] As of 2023[update], the most recent release of macOS is macOS 14 Sonoma.
 The heritage of what would become macOS had originated at NeXT, a company founded by Steve Jobs following his departure from Apple in 1985. There, the Unix-like NeXTSTEP operating system was developed, before being launched in 1989. The kernel of NeXTSTEP is based upon the Mach kernel, which was originally developed at Carnegie Mellon University, with additional kernel layers and low-level user space code derived from parts of FreeBSD[10] and other BSD operating systems.[11] Its graphical user interface was built on top of an object-oriented GUI toolkit using the Objective-C programming language.
 Throughout the 1990s, Apple had tried to create a "next-generation" OS to succeed its classic Mac OS through the Taligent, Copland and Gershwin projects, but all were eventually abandoned.[12] This led Apple to acquire NeXT in 1997, allowing NeXTSTEP, later called OPENSTEP, to serve as the basis for Apple's next generation operating system.[13]
This purchase also led to Steve Jobs returning to Apple as an interim, and then the permanent CEO, shepherding the transformation of the programmer-friendly OPENSTEP into a system that would be adopted by Apple's primary market of home users and creative professionals. The project was first codenamed "Rhapsody" before officially being named Mac OS X.[14][15]
 The letter "X" in Mac OS X's name refers to the number 10, a Roman numeral, and Apple has stated that it should be pronounced "ten" in this context. However, it is also commonly pronounced like the letter "X".[16][17] The iPhone X, iPhone XR and iPhone XS all later followed this convention.
 Previous Macintosh operating systems (versions of the classic Mac OS) were named using Arabic numerals, as with Mac OS 8 and Mac OS 9.[18][16] Until macOS 11 Big Sur, all versions of the operating system were given version numbers of the form 10.x, with this going from 10.0 up until 10.15; starting with macOS 11 Big Sur, Apple switched to numbering major releases with numbers that increase by 1 with every major release.
 The first version of Mac OS X, Mac OS X Server 1.0, was a transitional product, featuring an interface resembling the classic Mac OS, though it was not compatible with software designed for the older system. Consumer releases of Mac OS X included more backward compatibility. Mac OS applications could be rewritten to run natively via the Carbon API; many could also be run directly through the Classic Environment with a reduction in performance.
 The consumer version of Mac OS X was launched in 2001 with Mac OS X 10.0. Reviews were variable, with extensive praise for its sophisticated, glossy Aqua interface, but criticizing it for sluggish performance.[19] With Apple's popularity at a low, the maker of FrameMaker, Adobe Inc., declined to develop new versions of it for Mac OS X.[20] Ars Technica columnist John Siracusa, who reviewed every major OS X release up to 10.10, described the early releases in retrospect as "dog-slow, feature poor" and Aqua as "unbearably slow and a huge resource hog".[19][21][22]
 Apple rapidly developed several new releases of Mac OS X.[23] Siracusa's review of version 10.3, Panther, noted "It's strange to have gone from years of uncertainty and vaporware to a steady annual supply of major new operating system releases."[24] Version 10.4, Tiger, reportedly shocked executives at Microsoft by offering a number of features, such as fast file searching and improved graphics processing, that Microsoft had spent several years struggling to add to Windows Vista with acceptable performance.[25]
 As the operating system evolved, it moved away from the classic Mac OS, with applications being added and removed.[26] Considering music to be a key market, Apple developed the iPod music player and music software for the Mac, including iTunes and GarageBand.[27] Targeting the consumer and media markets, Apple emphasized its new "digital lifestyle" applications such as the iLife suite, integrated home entertainment through the Front Row media center and the Safari web browser. With increasing popularity of the internet, Apple offered additional online services, including the .Mac, MobileMe and most recently iCloud products. It later began selling third-party applications through the Mac App Store.
 Newer versions of Mac OS X also included modifications to the general interface, moving away from the striped gloss and transparency of the initial versions. Some applications began to use a brushed metal appearance, or non-pinstriped title bar appearance in version 10.4.[28] In Leopard, Apple announced a unification of the interface, with a standardized gray-gradient window style.[29][30]
 In 2006, the first Intel Macs were released with a specialized version of Mac OS X 10.4 Tiger.[31]
 A key development for the system was the announcement and release of the iPhone from 2007 onwards. While Apple's previous iPod media players used a minimal operating system, the iPhone used an operating system based on Mac OS X, which would later be called "iPhone OS" and then iOS. The simultaneous release of two operating systems based on the same frameworks placed tension on Apple, which cited the iPhone as forcing it to delay Mac OS X 10.5 Leopard.[32] However, after Apple opened the iPhone to third-party developers its commercial success drew attention to Mac OS X, with many iPhone software developers showing interest in Mac development.[33]
 In 2007, Mac OS X 10.5 Leopard was the sole release with universal binary components, allowing installation on both Intel Macs and select PowerPC Macs.[34] It is also the final release with PowerPC Mac support. Mac OS X 10.6 Snow Leopard was the first version of Mac OS X to be built exclusively for Intel Macs, and the final release with 32-bit Intel Mac support.[35] The name was intended to signal its status as an iteration of Leopard, focusing on technical and performance improvements rather than user-facing features; indeed it was explicitly branded to developers as being a 'no new features' release.[36] Since its release, several OS X or macOS releases (namely OS X Mountain Lion, OS X El Capitan, macOS High Sierra, and macOS Monterey) follow this pattern, with a name derived from its predecessor, similar to the 'tick–tock model' used by Intel.
 In two succeeding versions, Lion and Mountain Lion, Apple moved some applications to a highly skeuomorphic style of design inspired by contemporary versions of iOS while simplifying some elements by making controls such as scroll bars fade out when not in use.[21] This direction was, like brushed metal interfaces, unpopular with some users, although it continued a trend of greater animation and variety in the interface previously seen in design aspects such as the Time Machine backup utility, which presented past file versions against a swirling nebula, and the glossy translucent dock of Leopard and Snow Leopard.[37] In addition, with Mac OS X 10.7 Lion, Apple ceased to release separate server versions of Mac OS X, selling server tools as a separate downloadable application through the Mac App Store. A review described the trend in the server products as becoming "cheaper and simpler... shifting its focus from large businesses to small ones."[38]
 In 2012, with the release of OS X 10.8 Mountain Lion, the name of the system was officially shortened from Mac OS X to OS X, after the previous version shortened the system name in a similar fashion a year prior. That year, Apple removed the head of OS X development, Scott Forstall, and design was changed towards a more minimal direction.[39] Apple's new user interface design, using deep color saturation, text-only buttons and a minimal, 'flat' interface, was debuted with iOS 7 in 2013. With OS X engineers reportedly working on iOS 7, the version released in 2013, OS X 10.9 Mavericks, was something of a transitional release, with some of the skeuomorphic design removed, while most of the general interface of Mavericks remained unchanged.[40] The next version, OS X 10.10 Yosemite, adopted a design similar to iOS 7 but with greater complexity suitable for an interface controlled with a mouse.[41]
 From 2012 onwards, the system has shifted to an annual release schedule similar to that of iOS and Mac OS X releases prior to 10.4 Tiger[citation needed]. It also steadily cut the cost of updates from Snow Leopard onwards, before removing upgrade fees altogether in OS X Mavericks.[42] Some journalists and third-party software developers have suggested that this decision, while allowing more rapid feature release, meant less opportunity to focus on stability, with no version of OS X recommendable for users requiring stability and performance above new features.[43] Apple's 2015 update, OS X 10.11 El Capitan, was announced to focus specifically on stability and performance improvements.[44]
 In 2016, with the release of macOS 10.12 Sierra, the name was changed from OS X to macOS, in order to align it with the branding of Apple's other primary operating systems, iOS, watchOS, and tvOS.[45][46] macOS Sierra added Siri, iCloud Drive, picture-in-picture support, a Night Shift mode that switches the display to warmer colors at night, and two Continuity features: Universal Clipboard, which syncs a user's clipboard across their Apple devices, and Auto Unlock, which can unlock a user's Mac with their Apple Watch. macOS Sierra also adds support for the Apple File System (APFS), Apple's successor to the dated HFS+ file system.[47][48][49] macOS 10.13 High Sierra, released in 2017, included performance improvements, Metal 2 and HEVC support, and made APFS the default file system for SSD boot drives.[50]
 Its successor, macOS 10.14 Mojave, was released in 2018, adding a dark mode option and a dynamic wallpaper setting.[51] It was succeeded by macOS 10.15 Catalina in 2019, which replaces iTunes with separate apps for different types of media, and introduces the Catalyst system for porting iOS apps.[52]
 In 2020, Apple previewed macOS 11 Big Sur at the WWDC 2020. This was the first increment in the primary version number of macOS since the release of Mac OS X Public Beta in 2000; updates to macOS 11 were given 11.x numbers, matching the version numbering scheme used by Apple's other operating systems. Big Sur brought major changes to the UI and was the first version to run on the Arm instruction set.[53] The new numbering system was continued in 2021 with macOS 12 Monterey, 2022 with macOS 13 Ventura, and 2023 with macOS 14 Sonoma.
 
 At macOS's core is a POSIX-compliant operating system built on top of the XNU kernel,[76] (which incorporated large parts of FreeBSD kernel[10]) and FreeBSD userland[10] for the standard Unix facilities available from the command line interface. Apple has released this family of software as a free and open source operating system named Darwin. On top of Darwin, Apple layered a number of components, including the Aqua interface and the Finder, to complete the GUI-based operating system which is macOS.[77]
 With its original introduction as Mac OS X, the system brought a number of new capabilities to provide a more stable and reliable platform than its predecessor, the classic Mac OS. For example, pre-emptive multitasking and memory protection improved the system's ability to run multiple applications simultaneously without them interrupting or corrupting each other. Many aspects of macOS's architecture are derived from OPENSTEP, which was designed to be portable, to ease the transition from one platform to another. For example, NeXTSTEP was ported from the original 68k-based NeXT workstations to x86 and other architectures before NeXT was purchased by Apple,[78] and OPENSTEP was later ported to the PowerPC architecture as part of the Rhapsody project.
 Prior to macOS High Sierra, and on drives other than solid state drives (SSDs), the default file system is HFS+, which it inherited from the classic Mac OS. Operating system designer Linus Torvalds had criticized HFS+, saying it is "probably the worst file system ever", whose design is "actively corrupting user data". He criticized the case insensitivity of file names, a design made worse when Apple extended the file system to support Unicode.[79][80]
 The Darwin subsystem in macOS manages the file system, which includes the Unix permissions layer. In 2003 and 2005, two Macworld editors expressed criticism of the permission scheme; Ted Landau called misconfigured permissions "the most common frustration" in macOS, while Rob Griffiths suggested that some users may even have to reset permissions every day, a process which can take up to 15 minutes.[81] More recently, another Macworld editor, Dan Frakes, called the procedure of repairing permissions vastly overused.[82] He argues that macOS typically handles permissions properly without user interference, and resetting permissions should only be tried when problems emerge.[83]
 The architecture of macOS incorporates a layered design:[84]
the layered frameworks aid rapid development of applications by providing existing code for common tasks.[85] Apple provides its own software development tools, most prominently an integrated development environment called Xcode. Xcode provides interfaces to compilers that support several programming languages including C, C++, Objective-C, and Swift. For the Mac transition to Intel processors, it was modified so that developers could build their applications as a universal binary, which provides compatibility with both the Intel-based and PowerPC-based Macintosh lines.[86] First and third-party applications can be controlled programmatically using the AppleScript framework,[87] retained from the classic Mac OS,[88] or using the newer Automator application that offers pre-written tasks that do not require programming knowledge.[89]
 Apple offered two main APIs to develop software natively for macOS: Cocoa and Carbon. Cocoa was a descendant of APIs inherited from OPENSTEP with no ancestry from the classic Mac OS, while Carbon was an adaptation of classic Mac OS APIs, allowing Mac software to be minimally rewritten to run natively on Mac OS X.[15]
 The Cocoa API was created as the result of a 1993 collaboration between NeXT Computer and Sun Microsystems. This heritage is highly visible for Cocoa developers, since the "NS" prefix is ubiquitous in the framework, standing variously for NeXTSTEP or NeXT/Sun. The official OPENSTEP API, published in September 1994, was the first to split the API between Foundation and ApplicationKit and the first to use the "NS" prefix.[78] Traditionally, Cocoa programs have been mostly written in Objective-C, with Java as an alternative. However, on July 11, 2005, Apple announced that "features added to Cocoa in Mac OS X versions later than 10.4 will not be added to the Cocoa-Java programming interface."[99] macOS also used to support the Java Platform as a "preferred software package"—in practice this means that applications written in Java fit as neatly into the operating system as possible while still being cross-platform compatible, and that graphical user interfaces written in Swing look almost exactly like native Cocoa interfaces. Since 2014, Apple has promoted its new programming language Swift as the preferred language for software development on Apple platforms.
 Apple's original plan with macOS was to require all developers to rewrite their software into the Cocoa APIs. This caused much outcry among existing Mac developers, who threatened to abandon the platform rather than invest in a costly rewrite, and the idea was shelved.[15][100] To permit a smooth transition from Mac OS 9 to Mac OS X, the Carbon Application Programming Interface (API) was created.[15] Applications written with Carbon were initially able to run natively on both classic Mac OS and Mac OS X, although this ability was later dropped as Mac OS X developed. Carbon was not included in the first product sold as Mac OS X: the little-used original release of Mac OS X Server 1.0, which also did not include the Aqua interface.[101] Apple limited further development of Carbon from the release of Leopard onwards and announced that Carbon applications would not run at 64-bit.[100][15] A number of macOS applications continued to use Carbon for some time afterwards, especially ones with heritage dating back to the classic Mac OS and for which updates would be difficult, uneconomic or not necessary. This included Microsoft Office up to Office 2016, and Photoshop up to CS5.[102][100] Early versions of macOS could also run some classic Mac OS applications through the Classic Environment with performance limitations; this feature was removed from 10.5 onwards and all Macs using Intel processors.
 Because macOS is POSIX compliant, many software packages written for the other Unix-like systems including Linux can be recompiled to run on it, including much scientific and technical software.[103] Third-party projects such as Homebrew, Fink, MacPorts and pkgsrc provide pre-compiled or pre-formatted packages. Apple and others have provided versions of the X Window System graphical interface which can allow these applications to run with an approximation of the macOS look-and-feel.[104][105][106] The current Apple-endorsed method is the open-source XQuartz project; earlier versions could use the X11 application provided by Apple, or before that the XDarwin project.[107]
 Applications can be distributed to Macs and installed by the user from any source and by any method such as downloading (with or without code signing, available via an Apple developer account) or through the Mac App Store, a marketplace of software maintained by Apple through a process requiring the company's approval. Apps installed through the Mac App Store run within a sandbox, restricting their ability to exchange information with other applications or modify the core operating system and its features. This has been cited as an advantage, by allowing users to install apps with confidence that they should not be able to damage their system, but also as a disadvantage due to blocking the Mac App Store's use for professional applications that require elevated privileges.[108][109] Applications without any code signature cannot be run by default except from a computer's administrator account.[110][111]
 Apple produces macOS applications. Some are included with macOS and some sold separately. This includes iWork, Final Cut Pro, Logic Pro, iLife, and the database application FileMaker. Numerous other developers also offer software for macOS.
 In 2018, Apple introduced an application layer, codenamed Marzipan, to port iOS apps to macOS.[112][113] macOS Mojave included ports of four first-party iOS apps including Home and News, and it was announced that the API would be available for third-party developers to use from 2019.[114][115][116] In 2019, in macOS Catalina, the application layer was made available to third-party developers as Mac Catalyst.[117]
 List of macOS versions, the supported systems on which they run, and their RAM requirements
 Tools such as XPostFacto and patches applied to the installation media have been developed by third parties to enable installation of newer versions of macOS on systems not officially supported by Apple. This includes a number of pre-G3 Power Macintosh systems that can be made to run up to and including Mac OS X 10.2 Jaguar, all G3-based Macs which can run up to and including Tiger, and sub-867 MHz G4 Macs can run Leopard by removing the restriction from the installation DVD or entering a command in the Mac's Open Firmware interface to tell the Leopard Installer that it has a clock rate of 867 MHz or greater. Except for features requiring specific hardware such as graphics acceleration or DVD writing, the operating system offers the same functionality on all supported hardware.
 As most Mac hardware components, or components similar to those, since the Intel transition are available for purchase,[122] some technology-capable groups have developed software to install macOS on non-Apple computers. These are referred to as Hackintoshes, a portmanteau of the words "hack" and "Macintosh". This violates Apple's EULA (and is therefore unsupported by Apple technical support, warranties etc.), but communities that cater to personal users, who do not install for resale and profit, have generally been ignored by Apple.[123][124][125] These self-made computers allow more flexibility and customization of hardware, but at a cost of leaving the user more responsible for their own machine, such as on matter of data integrity or security.[126] Psystar, a business that attempted to profit from selling macOS on non-Apple certified hardware, was sued by Apple in 2008.[127]
 In April 2002, eWeek announced a rumor that Apple had a version of Mac OS X code-named Marklar, which ran on Intel x86 processors. The idea behind Marklar was to keep Mac OS X running on an alternative platform should Apple become dissatisfied with the progress of the PowerPC platform.[128] These rumors subsided until late in May 2005, when various media outlets, such as The Wall Street Journal[129] and CNET,[130] announced that Apple would unveil Marklar in the coming months.[131][132][133]
 On June 6, 2005, Steve Jobs announced in his keynote address at WWDC that Apple would be making the transition from PowerPC to Intel processors over the following two years, and that Mac OS X would support both platforms during the transition. Jobs also confirmed rumors that Apple had versions of Mac OS X running on Intel processors for most of its developmental life. Intel-based Macs would run a new recompiled version of OS X along with Rosetta, a binary translation layer which enables software compiled for PowerPC Mac OS X to run on Intel Mac OS X machines.[134] The system was included with Mac OS X versions up to version 10.6.8.[135] Apple dropped support for Classic mode on the new Intel Macs. Third party emulation software such as Mini vMac, Basilisk II and SheepShaver provided support for some early versions of Mac OS. A new version of Xcode and the underlying command-line compilers supported building universal binaries that would run on either architecture.[136]
 PowerPC-only software is supported with Apple's official binary translation software, Rosetta, though applications eventually had to be rewritten to run properly on the newer versions released for Intel processors. Apple initially encouraged developers to produce universal binaries with support for both PowerPC and Intel.[137] PowerPC binaries suffer a performance penalty when run on Intel Macs through Rosetta. Moreover, some PowerPC software, such as kernel extensions and System Preferences plugins, are not supported on Intel Macs at all. Plugins for Safari need to be compiled for the same platform as Safari, so when Safari is running on Intel, it requires plug-ins that have been compiled as Intel-only or universal binaries, so PowerPC-only plug-ins will not work.[138] While Intel Macs can run PowerPC, Intel, and universal binaries, PowerPC Macs support only universal and PowerPC builds.
 Support for the PowerPC platform was dropped following the transition. In 2009, Apple announced at WWDC that Mac OS X 10.6 Snow Leopard would drop support for PowerPC processors and be Intel-only.[139] Rosetta continued to be offered as an optional download or installation choice in Snow Leopard before it was discontinued with Mac OS X 10.7 Lion.[140] In addition, new versions of Mac OS X first- and third-party software increasingly required Intel processors, including new versions of iLife, iWork, Aperture and Logic Pro.
 Rumors of Apple shifting Macs to the Arm processors used by iOS devices began circulating as early as 2011,[141] and ebbed and flowed throughout the 2010s.[142] Rumors intensified in 2020, when numerous reports announced that the company would announce its shift to its custom processors at WWDC.[143]
 Apple officially announced its shift to processors designed in-house on June 22, 2020, at WWDC 2020, with the transition planned to last for two years.[144] The first release of macOS to support Arm is macOS Big Sur. Big Sur and later versions support Universal 2 binaries, which are applications consisting of both Intel (x86-64) and Apple silicon (AArch64) binaries; when launched, only the appropriate binary is run. Additionally, Intel binaries can be run on Apple silicon-based Macs using the Rosetta 2 binary translation software.
 The change in processor architecture allows Macs with Arm processors to be able to run iOS and iPadOS apps natively.[145]
 One of the major differences between the classic Mac OS and the current macOS was the addition of Aqua, a graphical user interface with water-like elements, in the first major release of Mac OS X. Every window element, text, graphic, or widget is drawn on-screen using spatial anti-aliasing technology.[146] ColorSync, a technology introduced many years before, was improved and built into the core drawing engine, to provide color matching for printing and multimedia professionals.[147] Also, drop shadows were added around windows and isolated text elements to provide a sense of depth. New interface elements were integrated, including sheets (dialog boxes attached to specific windows) and drawers, which would slide out and provide options.
 The use of soft edges, translucent colors, and pinstripes, similar to the hardware design of the first iMacs, brought more texture and color to the user interface when compared to what Mac OS 9 and Mac OS X Server 1.0's "Platinum" appearance had offered. According to Siracusa, the introduction of Aqua and its departure from the then conventional look "hit like a ton of bricks."[148]
Bruce Tognazzini (who founded the original Apple Human Interface Group) said that the Aqua interface in Mac OS X 10.0 represented a step backwards in usability compared with the original Mac OS interface.[149][150]
Third-party developers started producing skins for customizable applications and other operating systems which mimicked the Aqua appearance. To some extent, Apple has used the successful transition to this new design as leverage, at various times threatening legal action against people who make or distribute software with an interface the company says is derived from its copyrighted design.[151]
 Apple has continued to change aspects of the macOS appearance and design, particularly with tweaks to the appearance of windows and the menu bar. Since 2012, Apple has sold many of its Mac models with high-resolution Retina displays, and macOS and its APIs have extensive support for resolution-independent development on supporting high-resolution displays. Reviewers have described Apple's support for the technology as superior to that on Windows.[152][153][154]
 The human interface guidelines published by Apple for macOS are followed by many applications, giving them consistent user interface and keyboard shortcuts.[155] In addition, new services for applications are included, which include spelling and grammar checkers, special characters palette, color picker, font chooser and dictionary; these global features are present in every Cocoa application, adding consistency. The graphics system OpenGL composites windows onto the screen to allow hardware-accelerated drawing. This technology, introduced in version 10.2, is called Quartz Extreme, a component of Quartz. Quartz's internal imaging model correlates well with the Portable Document Format (PDF) imaging model, making it easy to output PDF to multiple devices.[147] As a side result, PDF viewing and creating PDF documents from any application are built-in features.[156] Reflecting its popularity with design users, macOS also has system support for a variety of professional video and image formats and includes an extensive pre-installed font library, featuring many prominent brand-name designs.[157]
 The Finder is a file browser allowing quick access to all areas of the computer, which has been modified throughout subsequent releases of macOS.[158][159] Quick Look has been part of the Finder since version 10.5. It allows for dynamic previews of files, including videos and multi-page documents without opening any other applications. Spotlight, a file searching technology which has been integrated into the Finder since version 10.4, allows rapid real-time searches of data files; mail messages; photos; and other information based on item properties (metadata) or content.[160][161] macOS makes use of a Dock, which holds file and folder shortcuts as well as minimized windows.
 Apple added Exposé in version 10.3 (called Mission Control since version 10.7), a feature which includes three functions to help accessibility between windows and desktop. Its functions are to instantly display all open windows as thumbnails for easy navigation to different tasks, display all open windows as thumbnails from the current application, and hide all windows to access the desktop.[162] FileVault is optional encryption of the user's files with the 128-bit Advanced Encryption Standard (AES-128).[163]
 Features introduced in version 10.4 include Automator, an application designed to create an automatic workflow for different tasks;[164] Dashboard, a full-screen group of small applications called desktop widgets that can be called up and dismissed in one keystroke;[165] and Front Row, a media viewer interface accessed by the Apple Remote.[166] Sync Services allows applications to access a centralized extensible database for various elements of user data, including calendar and contact items. The operating system then managed conflicting edits and data consistency.[167]
 All system icons are scalable up to 512×512 pixels as of version 10.5 to accommodate various places where they appear in larger size, including for example the Cover Flow view, a three-dimensional graphical user interface included with iTunes, the Finder, and other Apple products for visually skimming through files and digital media libraries via cover artwork. That version also introduced Spaces, a virtual desktop implementation which enables the user to have more than one desktop and display them in an Exposé-like interface;[168] an automatic backup technology called Time Machine, which allows users to view and restore previous versions of files and application data;[169] and Screen Sharing was built in for the first time.[170]
 In more recent releases, Apple has developed support for emoji characters by including the proprietary Apple Color Emoji font.[171][172] Apple has also connected macOS with social networks such as Twitter and Facebook through the addition of share buttons for content such as pictures and text.[173] Apple has brought several applications and features that originally debuted in iOS, its mobile operating system, to macOS in recent releases, notably the intelligent personal assistant Siri, which was introduced in version 10.12 of macOS.[174][175]
 There are 39 system languages available in macOS for the user at the moment of installation; the system language is used throughout the entire operating system environment.[176] Input methods for typing in dozens of scripts can be chosen independently of the system language.[177] Recent updates have added increased support for Chinese characters and interconnections with popular social networks in China.[178][179][180][181]
 
 macOS can be updated using the Software Update settings pane in System Settings or the softwareupdate command line utility. Until OS X 10.8 Mountain Lion, a separate Software Update application performed this functionality. In Mountain Lion and later, this was merged into the Mac App Store application, although the underlying update mechanism remains unchanged and is fundamentally different from the download mechanism used when purchasing an App Store application. In macOS 10.14 Mojave, the updating function was moved again to the Software Update settings pane.
 Most Macs receive six or seven years of macOS updates. After a new major release of macOS, the previous two releases still receive occasional updates, but many security vulnerabilities are only patched in the latest macOS release.[182]
 Mac OS X versions were named after big cats, with the exception of Mac OS X Server 1.0 and the original public beta, from Mac OS X 10.0 until OS X 10.9 Mavericks, when Apple switched to using California locations. Prior to its release, version 10.0 was code named internally at Apple as "Cheetah" , and Mac OS X 10.1 was code named internally as "Puma". After the immense buzz surrounding Mac OS X 10.2, codenamed "Jaguar", Apple's product marketing began openly using the code names to promote the operating system. Mac OS X 10.3 was marketed as "Panther", Mac OS X 10.4 as "Tiger", Mac OS X 10.5 as "Leopard", Mac OS X 10.6 as "Snow Leopard", Mac OS X 10.7 as "Lion", OS X 10.8 as "Mountain Lion", and OS X 10.9 as "Mavericks".
 "Panther", "Tiger" and "Leopard" are registered as trademarks of Apple,[183][184][185] but "Cheetah", "Puma" and "Jaguar" have never been registered. Apple has also registered "Lynx" and "Cougar" as trademarks, though these were allowed to lapse.[186][187] Computer retailer Tiger Direct sued Apple for its use of the name "Tiger". On May 16, 2005, a US federal court in the Southern District of Florida ruled that Apple's use did not infringe on Tiger Direct's trademark.[188]
 On September 13, 2000, Apple released a $29.95[189] "preview" version of Mac OS X, internally codenamed Kodiak, to gain feedback from users.
 The "PB", as it was known, marked the first public availability of the Aqua interface and Apple made many changes to the UI based on customer feedback. Mac OS X Public Beta expired and ceased to function in Spring 2001.[190]
 On March 24, 2001, Apple released Mac OS X 10.0 (internally codenamed Cheetah).[191]
The initial version was slow,[192] incomplete,[193] and had very few applications available at launch, mostly from independent developers.[194] While many critics suggested that the operating system was not ready for mainstream adoption, they recognized the importance of its initial launch as a base on which to improve.[193] Simply releasing Mac OS X was received by the Macintosh community as a great accomplishment,[193] for attempts to overhaul the Mac OS had been underway since 1996, and delayed by countless setbacks.
 Later that year on September 25, 2001, Mac OS X 10.1 (internally codenamed Puma) was released. It featured increased performance and provided missing features, such as DVD playback. Apple released 10.1 as a free upgrade CD for 10.0 users, in addition to the US$129 boxed version for people running Mac OS 9. It was discovered that the upgrade CDs were full install CDs that could be used with Mac OS 9 systems by removing a specific file; Apple later re-released the CDs in an actual stripped-down format that did not facilitate installation on such systems.[195] On January 7, 2002, Apple announced that Mac OS X was to be the default operating system for all Macintosh products by the end of that month.[196]
 On August 23, 2002,[197] Apple followed up with Mac OS X 10.2 Jaguar, the first release to use its code name as part of the branding.[198]
It brought great raw performance improvements, a sleeker look, and many powerful user-interface enhancements (over 150, according to Apple[199]), including Quartz Extreme for compositing graphics directly on an ATI Radeon or Nvidia GeForce2 MX AGP-based video card with at least 16 MB of VRAM, a system-wide repository for contact information in the new Address Book, and an instant messaging client named iChat.[200] The Happy Mac which had appeared during the Mac OS startup sequence for almost 18 years was replaced with a large grey Apple logo with the introduction of Mac OS X v10.2.[201]
 Mac OS X v10.3 Panther was released on October 24, 2003. It significantly improved performance and incorporated the most extensive update yet to the user interface. Panther included as many or more new features as Jaguar had the year before, including an updated Finder, incorporating a brushed-metal interface, Fast user switching, Exposé (Window manager), FileVault, Safari, iChat AV (which added video conferencing features to iChat), improved Portable Document Format (PDF) rendering and much greater Microsoft Windows interoperability.[202] Support for some early G3 computers such as "beige" Power Macs and "WallStreet" PowerBooks was discontinued.[203]
 Mac OS X 10.4 Tiger was released on April 29, 2005. Apple stated that Tiger contained more than 200 new features.[204] As with Panther, certain older machines were no longer supported; Tiger requires a Mac with 256 MB and a built-in FireWire port.[119] Among the new features, Tiger introduced Spotlight, Dashboard, Smart Folders, updated Mail program with Smart Mailboxes, QuickTime 7, Safari 2, Automator, VoiceOver, Core Image and Core Video. The initial release of the Apple TV used a modified version of Tiger with a different graphical interface and fewer applications and services.[205] On January 10, 2006, Apple released the first Intel-based Macs along with the 10.4.4 update to Tiger. This operating system functioned identically on the PowerPC-based Macs and the new Intel-based machines, with the exception of the Intel release lacking support for the Classic environment.[206]
 Mac OS X 10.5 Leopard was released on October 26, 2007. It was called by Apple "the largest update of Mac OS X". It brought more than 300 new features.[207] Leopard supports both PowerPC- and Intel x86-based Macintosh computers; support for the G3 processor was dropped and the G4 processor required a minimum clock rate of 867 MHz, and at least 512 MB of RAM to be installed. The single DVD works for all supported Macs (including 64-bit machines). New features include a new look, an updated Finder, Time Machine, Spaces, Boot Camp pre-installed,[208] full support for 64-bit applications (including graphical applications), new features in Mail and iChat, and a number of new security features. Leopard is an Open Brand UNIX 03 registered product on the Intel platform. It was also the first BSD-based OS to receive UNIX 03 certification.[209][210] Leopard dropped support for the Classic Environment and all Classic applications.[211] It was the final version of Mac OS X to support the PowerPC architecture.[212]
 Mac OS X 10.6 Snow Leopard was released on August 28, 2009. Rather than delivering big changes to the appearance and end user functionality like the previous releases of Mac OS X, Snow Leopard focused on "under the hood" changes, increasing the performance, efficiency, and stability of the operating system. For most users, the most noticeable changes were: the disk space that the operating system frees up after a clean install compared to Mac OS X 10.5 Leopard, a more responsive Finder rewritten in Cocoa, faster Time Machine backups, more reliable and user-friendly disk ejects, a more powerful version of the Preview application, as well as a faster Safari web browser. Snow Leopard only supported machines with Intel CPUs, required at least 1 GB of RAM, and dropped default support for applications built for the PowerPC architecture (Rosetta could be installed as an additional component to retain support for PowerPC-only applications).[213]
 Snow Leopard also featured new 64-bit technology capable of supporting greater amounts of RAM, improved support for multi-core processors through Grand Central Dispatch, and advanced GPU performance with OpenCL.[214]
 The 10.6.6 update introduced support for the Mac App Store, Apple's digital distribution platform for macOS applications.[215]
 OS X 10.7 Lion was released on July 20, 2011. It brought developments made in Apple's iOS, such as an easily navigable display of installed applications called Launchpad and a greater use of multi-touch gestures, to the Mac. This release removed Rosetta, making it incompatible with PowerPC applications.[140]
 Changes made to the GUI include auto-hiding scrollbars that only appear when they are used, and Mission Control which unifies Exposé, Spaces, Dashboard, and full-screen applications within a single interface.[216] Apple also made changes to applications: they resume in the same state as they were before they were closed, similar to iOS. Documents auto-save by default.[217]
 OS X 10.8 Mountain Lion was released on July 25, 2012.[69] Following the release of Lion the previous year, it was the first of the annual rather than two-yearly updates to OS X (and later macOS), which also closely aligned with the annual iOS operating system updates. It incorporates some features seen in iOS 5, which include Game Center, support for iMessage in the new Messages messaging application, and Reminders as a to-do list app separate from iCal (which is renamed as Calendar, like the iOS app). It also includes support for storing iWork documents in iCloud.[218] Notification Center, which makes its debut in Mountain Lion, is a desktop version similar to the one in iOS 5.0 and higher. Application pop-ups are now concentrated on the corner of the screen, and the Center itself is pulled from the right side of the screen. Mountain Lion also includes more Chinese features including support for Baidu as an option for Safari search engine, QQ, 163.com and 126.com services for Mail, Contacts and Calendar, Youku, Tudou and Sina Weibo are integrated into share sheets.[181]
 Starting with Mountain Lion, Apple software updates (including the OS) are distributed via the App Store.[219] This updating mechanism replaced the Apple Software Update utility.[220]
 OS X 10.9 Mavericks was released on October 22, 2013. It was a free upgrade to all users running Snow Leopard or later with a 64-bit Intel processor.[221] Its changes include the addition of the previously iOS-only Maps and iBooks applications, improvements to the Notification Center, enhancements to several applications, and many under-the-hood improvements.[222]
 OS X 10.10 Yosemite was released on October 16, 2014. It features a redesigned user interface similar to that of iOS 7, intended to feature a more minimal, text-based 'flat' design, with use of translucency effects and intensely saturated colors.[223] Apple's showcase new feature in Yosemite is Handoff, which enables users with iPhones running iOS 8.1 or later to answer phone calls, receive and send SMS messages, and complete unfinished iPhone emails on their Mac. As of OS X 10.10.3, Photos replaced iPhoto and Aperture.[224]
 OS X 10.11 El Capitan was released on September 30, 2015. Similar to Mac OS X 10.6 Snow Leopard, Apple described this release as emphasizing "refinements to the Mac experience" and "improvements to system performance".[225] Refinements include public transport built into the Maps application, GUI improvements to the Notes application, adopting San Francisco as the system font for clearer legibility, and the introduction of System Integrity Protection.
 The Metal API, first introduced in iOS 8, was also included in this operating system for "all Macs since 2012".[226] According to Apple, Metal accelerates system-level rendering by up to 50 percent, resulting in faster graphics performance for everyday apps. Metal also delivers up to 10 times faster draw call performance for more fluid experience in games and pro apps.[227]
 macOS 10.12 Sierra was released to the public on September 20, 2016. New features include the addition of Siri, Optimized Storage, and updates to Photos, Messages, and iTunes.[228][229]
 macOS 10.13 High Sierra was released to the public on September 25, 2017.[230] Like OS X El Capitan and OS X Mountain Lion, High Sierra is a refinement-based update having very few new features visible to the user, including updates to Safari, Photos, and Mail, among other changes.[231]
 The major change under the hood is the switch to the Apple File System, optimized for the solid-state storage used in most new Mac computers.[232]
 macOS 10.14 Mojave was released on September 24, 2018.[51] The update introduced a system-wide dark mode and several new apps lifted from iOS, such as Apple News. It was the first version to require a GPU that supports Metal. Mojave also changed the system software update mechanism from the App Store (where it had been since OS X Mountain Lion) to a new panel in System Preferences. App updates remain in the App Store.
 macOS 10.15 Catalina was released on October 7, 2019.[233] Updates included enhanced voice control, and bundled apps for music, video, and podcasts that together replace the functions of iTunes, and the ability to use an iPad as an external monitor. Catalina officially dropped support for 32-bit applications.[234]
 macOS Big Sur was announced during the WWDC keynote speech on June 22, 2020,[235] and it was made available to the general public on November 12, 2020. This is the first time the major version number of the operating system has been incremented since the Mac OS X Public Beta in 2000. It brings Arm support,[236] new icons, and aesthetic user interface changes to the system.[237]
 macOS Monterey was announced during the WWDC keynote speech on June 7, 2021, and released on October 25, 2021, introducing Universal Control (which allows input devices to be used with multiple devices simultaneously), Focus (which allows selectively limiting notifications and alerts depending on user-defined user/work modes), Shortcuts (a task automation framework previously only available on iOS and iPadOS expected to replace Automator), a redesigned Safari Web browser, and updates and improvements to FaceTime.[238]
 macOS Ventura was announced during the WWDC keynote speech on June 6, 2022[239] and released on October 24, 2022.[240] It came with the redesigned System Preferences to a more iOS-like settings, and now with the new Weather and Clock app for Mac. Users can use an iPhone as a webcam for video conferencing.
 macOS Sonoma was announced during the WWDC keynote speech on June 5, 2023. It was released on September 26, 2023.[241]
 Apple publishes Apple Platform Security documents to lay out the security protections built into macOS and Mac hardware.[242]
 macOS supports additional hardware-based security features on Apple silicon Macs:[243]
 macOS's optional Lockdown Mode enables additional protections, such as disabling just-in-time compilation for Safari's JavaScript engine, preventing some vulnerabilities.[245]
 Only the latest major release of macOS (currently macOS Sonoma) receives patches for all known security vulnerabilities. The previous two releases receive some security updates, but not for all vulnerabilities known to Apple. In 2021, Apple fixed a critical privilege escalation vulnerability in macOS Big Sur, but a fix remained unavailable for the previous release, macOS Catalina, for 234 days, until Apple was informed that the vulnerability was being used to infect the computers of people who visited Hong Kong pro-democracy websites.[246][247]
 macOS Ventura added support for Rapid Security Response (RSR) updates. These smaller updates may require a reboot, but take less than a minute to install.[248][249] In an analysis, Hackintosh developer Mykola Grymalyuk noted that RSR updates can only fix userland vulunerability, and cannot patch the macOS kernel.[250]
 In its earlier years, Mac OS X enjoyed a near-absence of the types of malware and spyware that have affected Microsoft Windows users.[251][252][253] macOS has a smaller usage share compared to Windows.[254] Worms, as well as potential vulnerabilities, were noted in 2006, which led some industry analysts and anti-virus companies to issue warnings that Apple's Mac OS X is not immune to malware.[255] Increasing market share coincided with additional reports of a variety of attacks.[256] In early 2011, Mac OS X experienced a large increase in malware attacks,[257] and malware such as Mac Defender, MacProtector, and MacGuard was seen as an increasing problem for Mac users. At first, the malware installer required the user to enter the administrative password, but later versions installed without user input.[258] Initially, Apple support staff were instructed not to assist in the removal of the malware or admit the existence of the malware issue, but as the malware spread, a support document was issued. Apple announced an OS X update to fix the problem. An estimated 100,000 users were affected.[259][260] Apple releases security updates for macOS regularly,[261] as well as signature files containing malware signatures for Xprotect, an anti-malware feature part of File Quarantine present since Mac OS X Snow Leopard.[262]
 As of January 2023[update], macOS is the second-most widely used general-purpose desktop operating system used on the World Wide Web following Microsoft Windows, with a 15.33% usage share according to statistics compiled by Statcounter GlobalStats.[263]
 As a device company, Apple has mostly promoted macOS to sell Macs, with promotion of macOS updates focused on existing users, promotion at Apple Store and other retail partners, or through events for developers. In larger scale advertising campaigns, Apple specifically promoted macOS as better for handling media and other home-user applications, and comparing Mac OS X (especially versions Tiger and Leopard) with the heavy criticism Microsoft received for the long-awaited Windows Vista operating system.[264][265]


Source: https://en.wikipedia.org/wiki/Multipurpose_Internet_Mail_Extensions
Content: Multipurpose Internet Mail Extensions (MIME) is an Internet standard that extends the format of email messages to support text in character sets other than ASCII, as well as attachments of audio, video, images, and application programs.  Message bodies may consist of multiple parts, and header information may be specified in non-ASCII character sets. Email messages with MIME formatting are typically transmitted with standard protocols, such as the  Simple Mail Transfer Protocol (SMTP), the Post Office Protocol (POP), and the Internet Message Access Protocol (IMAP).
 The MIME standard is specified in a series of requests for comments:  RFC 2045,
 RFC 2046,
 RFC 2047,
 RFC 4288,
 RFC 4289 and 
 RFC 2049. The integration with SMTP email is specified in 
 RFC 1521 and 
 RFC 1522.
 Although the MIME formalism was designed mainly for SMTP, its content types are also important in other communication protocols. In the HyperText Transfer Protocol (HTTP) for the World Wide Web, servers insert a MIME header field at the beginning of any Web transmission. Clients use the content type or media type header to select an appropriate viewer application for the type of data indicated.
 MIME originated from the Andrew Messaging System, which was part of Andrew Project developed at Carnegie Mellon University (CMU), as a cross-platform alternative to the Andrew-specific data format.[1]
 The presence of this header field indicates the message is MIME-formatted. The value is typically "1.0". The field appears as follows:
 According to MIME co-creator Nathaniel Borenstein, the version number was introduced to permit changes to the MIME protocol in subsequent versions. However, Borenstein admitted short-comings in the specification that hindered the implementation of this feature: "We did not adequately specify how to handle a future MIME version. ... So if you write something that knows 1.0, what should you do if you encounter 2.0 or 1.1? I sort of thought it was obvious but it turned out everyone implemented that in different ways. And the result is that it would be just about impossible for the Internet to ever define a 2.0 or a 1.1."[2]
 This header field indicates the media type of the message content, consisting of a type and subtype, for example
 Through the use of the multipart type, MIME allows mail messages to have parts arranged in a tree structure where the leaf nodes are any non-multipart content type and the non-leaf nodes are any of a variety of multipart types.
This mechanism supports:
 The original MIME specifications only described the structure of mail messages. They did not address the issue of presentation styles. The content-disposition header field was added in RFC 2183 to specify the presentation style. A MIME part can have:
 In addition to the presentation style, the field Content-Disposition also provides parameters for specifying the name of the file, the creation date and modification date, which can be used by the reader's mail user agent to store the attachment.
 The following example is taken from RFC 2183, where the header field is defined:
 The filename may be encoded as defined in RFC 2231.
 As of 2010, a majority of mail user agents did not follow this prescription fully. The widely used Mozilla Thunderbird mail client ignores the content-disposition fields in the messages and uses independent algorithms for selecting the MIME parts to display automatically. Thunderbird prior to version 3 also sends out newly composed messages with inline content disposition for all MIME parts. Most users are unaware of how to set the content disposition to attachment.[3] Many mail user agents also send messages with the file name in the name parameter of the content-type header instead of the filename parameter of the header field Content-Disposition. This practice is discouraged, as the file name should be specified either with the parameter filename, or with both the parameters filename and name.[4]
 In HTTP, the response header field Content-Disposition: attachment is usually used as a hint to the client to present the response body as a downloadable file. Typically, when receiving such a response, a Web browser prompts the user to save its content as a file, instead of displaying it as a page in a browser window, with filename suggesting the default file name.
 In June 1992, MIME (RFC 1341, since made obsolete by RFC 2045) defined a set of methods for representing binary data in formats other than ASCII text format. The content-transfer-encoding: MIME header field has 2-sided significance:
 The RFC and the IANA's list of transfer encodings define the values shown below, which are not case sensitive. '7bit', '8bit', and 'binary' mean that no binary-to-text encoding on top of the original encoding was used. In these cases, the header field is actually redundant for the email client to decode the message body, but it may still be useful as an indicator of what type of object is being sent. Values 'quoted-printable' and 'base64' tell the email client that a binary-to-text encoding scheme was used and that appropriate initial decoding is necessary before the message can be read with its original encoding (e.g. UTF-8).
 There is no encoding defined which is explicitly designed for sending arbitrary binary data through SMTP transports with the 8BITMIME extension. Thus, if BINARYMIME isn't supported, base64 or quoted-printable (with their associated inefficiency) are sometimes still useful. This restriction does not apply to other uses of MIME such as Web Services with MIME attachments or MTOM.
 Since RFC 2822, conforming message header field names and values use ASCII characters; values that contain non-ASCII data should use the MIME encoded-word syntax (RFC 2047) instead of a literal string. This syntax uses a string of ASCII characters indicating both the original character encoding (the "charset") and the content-transfer-encoding used to map the bytes of the charset into ASCII characters.
 The form is: "=?charset?encoding?encoded text?=".
 The ASCII codes for the question mark ("?") and equals sign ("=") may not be represented directly as they are used to delimit the encoded word. The ASCII code for space may not be represented directly because it could cause older parsers to split up the encoded word undesirably. To make the encoding smaller and easier to read the underscore is used to represent the ASCII code for space creating the side effect that underscore cannot be represented directly. The use of encoded words in certain parts of header fields imposes further restrictions on which characters may be represented directly.
 For example,
 Subject: =?iso-8859-1?Q?=A1Hola,_se=F1or!?=
 is interpreted as "Subject: ¡Hola, señor!".
 The encoded-word format is not used for the names of the headers fields (for example Subject). These names are usually English terms and always in ASCII in the raw message. When viewing a message with a non-English email client, the header field names might be translated by the client.
 The MIME multipart message contains a boundary in the header field Content-Type:; this boundary, which must not occur in any of the parts, is placed between the parts, and at the beginning and end of the body of the message, as follows:
 Each part consists of its own content header (zero or more Content- header fields) and a body. Multipart content can be nested. The Content-Transfer-Encoding of a multipart type must always be "7bit", "8bit" or "binary" to avoid the complications that would be posed by multiple levels of decoding. The multipart block as a whole does not have a charset; non-ASCII characters in the part headers are handled by the Encoded-Word system, and the part bodies can have charsets specified if appropriate for their content-type.
 Notes:
 The MIME standard defines various multipart-message subtypes, which specify the nature of the message parts and their relationship to one another. The subtype is specified in the Content-Type header field of the overall message. For example, a multipart MIME message using the digest subtype would have its Content-Type set as "multipart/digest".
 The RFC initially defined four subtypes: mixed, digest, alternative and parallel. A minimally compliant application must support mixed and digest; other subtypes are optional. Applications must treat unrecognized subtypes as "multipart/mixed". Additional subtypes, such as signed and form-data, have since been separately defined in other RFCs.
 multipart/mixed is used for sending files with different Content-Type header fields inline (or as attachments). If sending pictures or other easily readable files, most mail clients will display them inline (unless explicitly specified with Content-Disposition: attachment in which case offered as attachments). The default content-type for each part is "text/plain".
 The type is defined in RFC 2046.[5]
 multipart/digest is a simple way to send multiple text messages. The default content-type for each part is "message/rfc822".
 The MIME type is defined in RFC 2046.[6]
 The multipart/alternative subtype indicates that each part is an "alternative" version of the same (or similar) content, each in a different format denoted by its "Content-Type" header. The order of the parts is significant.  RFC1341 states: In general, user agents that compose multipart/alternative entities should place the body parts in increasing order of preference, that is, with the preferred format last.[7]
 Systems can then choose the "best" representation they are capable of processing; in general, this will be the last part that the system can understand, although other factors may affect this.
 Since a client is unlikely to want to send a version that is less faithful than the plain text version, this structure places the plain text version (if present) first. This makes life easier for users of clients that do not understand multipart messages.
 Most commonly, multipart/alternative is used for email with two parts, one plain text (text/plain) and one HTML (text/html). The plain text part provides backwards compatibility while the HTML part allows use of formatting and hyperlinks. Most email clients offer a user option to prefer plain text over HTML; this is an example of how local factors may affect how an application chooses which "best" part of the message to display.
 While it is intended that each part of the message represent the same content, the standard does not require this to be enforced in any way. At one time, anti-spam filters would only examine the text/plain part of a message,[8] because it is easier to parse than the text/html part. But spammers eventually took advantage of this, creating messages with an innocuous-looking text/plain part and advertising in the text/html part. Anti-spam software eventually caught up on this trick, penalizing messages with very different text in a multipart/alternative message.[8]
 The type is defined in RFC 2046.[9]
 A multipart/related is used to indicate that each message part is a component of an aggregate whole. It is for compound objects consisting of several inter-related components –  proper display cannot be achieved by individually displaying the constituent parts. The message consists of a root part (by default, the first) which reference other parts inline, which may in turn reference other parts. Message parts are commonly referenced by Content-ID. The syntax of a reference is unspecified and is instead dictated by the encoding or protocol used in the part.
 One common usage of this subtype is to send a web page complete with images in a single message. The root part would contain the HTML document, and use image tags to reference images stored in the latter parts.
 The type is defined in RFC 2387.
 multipart/report is a message type that contains data formatted for a mail server to read. It is split between a text/plain (or some other content/type easily readable) and a message/delivery-status, which contains the data formatted for the mail server to read.
 The type is defined in RFC 6522.
 A multipart/signed message is used to attach a digital signature to a message. It has exactly two body parts, a body part and a signature part. The whole of the body part, including mime fields, is used to create the signature part. Many signature types are possible, like "application/pgp-signature" (RFC 3156) and "application/pkcs7-signature" (S/MIME).
 The type is defined in RFC 1847.[10]
 A multipart/encrypted message has two parts. The first part has control information that is needed to decrypt the application/octet-stream second part. Similar to signed messages, there are different implementations which are identified by their separate content types for the control part. The most common types are "application/pgp-encrypted" (RFC 3156) and "application/pkcs7-mime" (S/MIME).
 The MIME type defined in RFC 1847.[11]
 The MIME type multipart/form-data is used to express values submitted through a form. Originally defined as part of HTML 4.0, it is most commonly used for submitting files with HTTP. It is specified in RFC 7578, superseding RFC 2388. example
 The content type multipart/x-mixed-replace was developed as part of a technology to emulate server push and streaming over HTTP.
 All parts of a mixed-replace message have the same semantic meaning. However, each part invalidates –  "replaces" –  the previous parts as soon as it is received completely. Clients should process the individual parts as soon as they arrive and should not wait for the whole message to finish.
 Originally developed by Netscape,[12] it is still supported by Mozilla, Firefox, Safari, and Opera. It is commonly used in IP cameras as the MIME type for MJPEG streams.[13] It was supported by Chrome for main resources until 2013 (images can still be displayed using this content type).[14]
 The multipart/byterange is used to represent noncontiguous byte ranges of a single message, it is used by HTTP when a server returns multiple byte ranges and is defined in RFC 2616.


Source: https://en.wikipedia.org/wiki/Hypertext_Transfer_Protocol
Content: 
 The Hypertext Transfer Protocol (HTTP) is an application layer protocol in the Internet protocol suite model for distributed, collaborative, hypermedia information systems.[1] HTTP is the foundation of data communication for the World Wide Web, where hypertext documents include hyperlinks to other resources that the user can easily access, for example by a mouse click or by tapping the screen in a web browser.
 Development of HTTP was initiated by Tim Berners-Lee at CERN in 1989 and summarized in a simple document describing the behavior of a client and a server using the first HTTP version, named 0.9.[2] That version was subsequently developed, eventually becoming the public 1.0.[3]
 Development of early HTTP Requests for Comments (RFCs) started a few years later in a coordinated effort by the Internet Engineering Task Force (IETF) and the World Wide Web Consortium (W3C), with work later moving to the IETF.
 HTTP/1 was finalized and fully documented (as version 1.0) in 1996.[4]  It evolved (as version 1.1) in 1997 and then its specifications were updated in 1999, 2014, and 2022.[5]
 Its secure variant named HTTPS is used by more than 85% of websites.[6]
HTTP/2, published in 2015, provides a more efficient expression of HTTP's semantics "on the wire". As of January 2024,[update] it is used by 36% of websites[7] and supported by almost all web browsers (over 98% of users).[8] It is also supported by major web servers over Transport Layer Security (TLS) using an Application-Layer Protocol Negotiation (ALPN) extension[9] where TLS 1.2 or newer is required.[10][11]
 HTTP/3, the successor to HTTP/2, was published in 2022.[12] As of February 2024,[update] it is now used on 29% of websites[13] and is supported by most web browsers, i.e. (at least partially) supported by 97% of users.[14] HTTP/3 uses QUIC instead of TCP for the underlying transport protocol. Like HTTP/2, it does not obsolesce previous major versions of the protocol. Support for HTTP/3 was added to Cloudflare and Google Chrome first,[15][16] and is also enabled in Firefox.[17] HTTP/3 has lower latency for real-world web pages, if enabled on the server, and loads faster than with HTTP/2, in some cases over three times faster than HTTP/1.1 (which is still commonly only enabled).[18]
 HTTP functions as a request–response protocol in the client–server model. A web browser, for example, may be the client whereas a process, named web server, running on a computer hosting one or more websites may be the server. The client submits an HTTP request message to the server. The server, which provides resources such as HTML files and other content or performs other functions on behalf of the client, returns a response message to the client. The response contains completion status information about the request and may also contain requested content in its message body.
 A web browser is an example of a user agent (UA). Other types of user agent include the indexing software used by search providers (web crawlers), voice browsers, mobile apps, and other software that accesses, consumes, or displays web content.
 HTTP is designed to permit intermediate network elements to improve or enable communications between clients and servers. High-traffic websites often benefit from web cache servers that deliver content on behalf of upstream servers to improve response time. Web browsers cache previously accessed web resources and reuse them, whenever possible, to reduce network traffic. HTTP proxy servers at private network boundaries can facilitate communication for clients without a globally routable address, by relaying messages with external servers.
 To allow intermediate HTTP nodes (proxy servers, web caches, etc.) to accomplish their functions, some of the HTTP headers (found in HTTP requests/responses) are managed hop-by-hop whereas other HTTP headers are managed end-to-end (managed only by the source client and by the target web server).
 HTTP is an application layer protocol designed within the framework of the Internet protocol suite. Its definition presumes an underlying and reliable transport layer protocol.[19] In the latest version HTTP/3, the Transmission Control Protocol (TCP) is no longer used, but the older versions are still more used and they most commonly use TCP. They have also been adapted to use unreliable protocols such as the User Datagram Protocol (UDP), which HTTP/3 also (indirectly) always builds on, for example in HTTPU and Simple Service Discovery Protocol (SSDP).
 HTTP resources are identified and located on the network by Uniform Resource Locators (URLs), using the Uniform Resource Identifiers (URI's) schemes http and https. As defined in RFC 3986, URIs are encoded as hyperlinks in HTML documents, so as to form interlinked hypertext documents.
 In HTTP/1.0 a separate TCP connection to the same server is made for every resource request.[20]
 In HTTP/1.1 instead a TCP connection can be reused to make multiple resource requests (i.e. of HTML pages, frames, images, scripts, stylesheets, etc.).[21][22]
 HTTP/1.1 communications therefore experience less latency as the establishment of TCP connections presents considerable overhead, especially under high traffic conditions.[23]
 HTTP/2 is a revision of previous HTTP/1.1 in order to maintain the same client–server model and the same protocol methods but with these differences in order:
 HTTP/2 communications therefore experience much less latency and, in most cases, even higher speeds than HTTP/1.1 communications.
 HTTP/3 is a revision of previous HTTP/2 in order to use QUIC + UDP transport protocols instead of TCP. Before that version, TCP/IP connections were used; but now, only the IP layer is used (which UDP, like TCP, builds on). This slightly improves the average speed of communications and to avoid the occasional (very rare) problem of TCP connection congestion that can temporarily block or slow down the data flow of all its streams (another form of "head of line blocking").
 The term hypertext was coined by Ted Nelson in 1965 in the Xanadu Project, which was in turn inspired by Vannevar Bush's 1930s vision of the microfilm-based information retrieval and management "memex" system described in his 1945 essay "As We May Think". Tim Berners-Lee and his team at CERN are credited with inventing the original HTTP, along with HTML and the associated technology for a web server and a client user interface called web browser. Berners-Lee designed HTTP in order to help with the adoption of his other idea: the "WorldWideWeb" project, which was first proposed in 1989, now known as the World Wide Web.
 The first web server went live in 1990.[25][26] The protocol used had only one method, namely GET, which would request a page from a server.[27] The response from the server was always an HTML page.[2]
 Since HTTP/0.9 did not support header fields in a request, there is no mechanism for it to support name-based virtual hosts (selection of resource by inspection of the Host header field).  Any server that implements name-based virtual hosts ought to disable support for HTTP/0.9.  Most requests that appear to be HTTP/0.9 are, in fact, badly constructed HTTP/1.x requests caused by a client failing to properly encode the request-target. HTTP is a stateless application-level protocol and it requires a reliable network transport connection to exchange data between client and server.[19] In HTTP implementations, TCP/IP connections are used using well-known ports (typically port 80 if the connection is unencrypted or port 443 if the connection is encrypted, see also List of TCP and UDP port numbers).[43][44] In HTTP/2, a TCP/IP connection plus multiple protocol channels are used. In HTTP/3, the application transport protocol QUIC over UDP is used.
 Data is exchanged through a sequence of request–response messages which are exchanged by a session layer transport connection.[19] An HTTP client initially tries to connect to a server establishing a connection (real or virtual). An HTTP(S) server listening on that port accepts the connection and then waits for a client's request message. The client sends its HTTP request message. Upon receiving the request the server sends back an HTTP response message, which includes header(s) plus a body if it is required. The body of this response message is typically the requested resource, although an error message or other information may also be returned. At any time (for many reasons) client or server can close the connection. Closing a connection is usually advertised in advance by using one or more HTTP headers in the last request/response message sent to server or client.[21]
 In HTTP/0.9, the TCP/IP connection is always closed after server response has been sent, so it is never persistent.
 In HTTP/1.0, as stated in RFC 1945, the TCP/IP connection should always be closed by server after a response has been sent.[note 3]
 In HTTP/1.1 a keep-alive-mechanism was officially introduced so that a connection could be reused for more than one request/response. Such persistent connections reduce request latency perceptibly because the client does not need to re-negotiate the TCP 3-Way-Handshake connection after the first request has been sent. Another positive side effect is that, in general, the connection becomes faster with time due to TCP's slow-start-mechanism.
 HTTP/1.1 added also HTTP pipelining in order to further reduce lag time when using persistent connections by allowing clients to send multiple requests before waiting for each response.  This optimization was never considered really safe because a few web servers and many proxy servers, specially transparent proxy servers placed in Internet / Intranets between clients and servers, did not handle pipelined requests properly (they served only the first request discarding the others, they closed the connection because they saw more data after the first request or some proxies even returned responses out of order etc.).  Besides this only HEAD and some GET requests (i.e. limited to real file requests and so with URLs without query string used as a command, etc.) could be pipelined in a safe and idempotent mode.  After many years of struggling with the problems introduced by enabling pipelining, this feature was first disabled and then removed from most browsers also because of the announced adoption of HTTP/2.
 HTTP/2 extended the usage of persistent connections by multiplexing many concurrent requests/responses through a single TCP/IP connection.
 HTTP/3 does not use TCP/IP connections but QUIC + UDP (see also: technical overview).
 HTTP provides multiple authentication schemes such as basic access authentication and digest access authentication which operate via a challenge–response mechanism whereby the server identifies and issues a challenge before serving the requested content.
 HTTP provides a general framework for access control and authentication, via an extensible set of challenge–response authentication schemes, which can be used by a server to challenge a client request and by a client to provide authentication information.[1]
 The authentication mechanisms described above belong to the HTTP protocol and are managed by client and server HTTP software (if configured to require authentication before allowing client access to one or more web resources), and not by the web applications using a web application session.
 The HTTP Authentication specification also provides an arbitrary, implementation-specific construct for further dividing resources common to a given root URI. The realm value string, if present, is combined with the canonical root URI to form the protection space component of the challenge. This in effect allows the server to define separate authentication scopes under one root URI.[1]
 HTTP is a stateless protocol. A stateless protocol does not require the web server to retain information or status about each user for the duration of multiple requests.
 Some web applications need to manage user sessions, so they implement states, or server side sessions, using for instance HTTP cookies[45] or hidden variables within web forms.
 To start an application user session, an interactive authentication via web application login must be performed. To stop a user session a logout operation must be requested by user.  These kind of operations do not use HTTP authentication but a custom managed web application authentication.
 Request messages are sent by a client to a target server.[note 4]
 A client sends request messages to the server, which consist of:[46]
 In the HTTP/1.1 protocol, all header fields except Host: hostname are optional.
 A request line containing only the path name is accepted by servers to maintain compatibility with HTTP clients before the HTTP/1.0 specification in RFC 1945.[47]
 HTTP defines methods (sometimes referred to as verbs, but nowhere in the specification does it mention verb) to indicate the desired action to be performed on the identified resource. What this resource represents, whether pre-existing data or data that is generated dynamically, depends on the implementation of the server. Often, the resource corresponds to a file or the output of an executable residing on the server. The HTTP/1.0 specification[48] defined the GET, HEAD, and POST methods as well as listing the PUT, DELETE, LINK and UNLINK methods under additional methods.  However, the HTTP/1.1 specification[49] formally defined and added five new methods: PUT, DELETE, CONNECT, OPTIONS, and TRACE. Any client can use any method and the server can be configured to support any combination of methods. If a method is unknown to an intermediate, it will be treated as an unsafe and non-idempotent method. There is no limit to the number of methods that can be defined, which allows for future methods to be specified without breaking existing infrastructure. For example, WebDAV defined seven new methods and RFC 5789 specified the PATCH method.
 Method names are case sensitive.[50][51] This is in contrast to HTTP header field names which are case-insensitive.[52]
 
 
 
 
 
 
 
 
 
 All general-purpose web servers are required to implement at least the GET and HEAD methods, and all other methods are considered optional by the specification.[51]
 
 A request method is safe if a request with that method has no intended effect on the server. The methods GET, HEAD, OPTIONS, and TRACE are defined as safe. In other words, safe methods are intended to be read-only. They do not exclude side effects though, such as appending request information to a log file or charging an advertising account, since they are not requested by the client, by definition.
 In contrast, the methods POST, PUT, DELETE, CONNECT, and PATCH are not safe. They may modify the state of the server or have other effects such as sending an email. Such methods are therefore not usually used by conforming web robots or web crawlers; some that do not conform tend to make requests without regard to context or consequences.
 Despite the prescribed safety of GET requests, in practice their handling by the server is not technically limited in any way. Careless or deliberately irregular programming can allow GET requests to cause non-trivial changes on the server. This is discouraged because of the problems which can occur when web caching, search engines, and other automated agents make unintended changes on the server. For example, a website might allow deletion of a resource through a URL such as https://example.com/article/1234/delete, which, if arbitrarily fetched, even using GET, would simply delete the article.[59] A properly coded website would require a DELETE or POST method for this action, which non-malicious bots would not make.
 One example of this occurring in practice was during the short-lived Google Web Accelerator beta, which prefetched arbitrary URLs on the page a user was viewing, causing records to be automatically altered or deleted en masse. The beta was suspended only weeks after its first release, following widespread criticism.[60][59]
 A request method is idempotent if multiple identical requests with that method have the same effect as a single such request. The methods PUT and DELETE, and safe methods are defined as idempotent. Safe methods are trivially idempotent, since they are intended to have no effect on the server whatsoever; the PUT and DELETE methods, meanwhile, are idempotent since successive identical requests will be ignored. A website might, for instance, set up a PUT endpoint to modify a user's recorded email address. If this endpoint is configured correctly, any requests which ask to change a user's email address to the same email address which is already recorded—e.g. duplicate requests following a successful request—will have no effect. Similarly, a request to DELETE a certain user will have no effect if that user has already been deleted.
 In contrast, the methods POST, CONNECT, and PATCH are not necessarily idempotent, and therefore sending an identical POST request multiple times may further modify the state of the server or have further effects, such as sending multiple emails. In some cases this is the desired effect, but in other cases it may occur accidentally. A user might, for example, inadvertently send multiple POST requests by clicking a button again if they were not given clear feedback that the first click was being processed. While web browsers may show alert dialog boxes to warn users in some cases where reloading a page may re-submit a POST request, it is generally up to the web application to handle cases where a POST request should not be submitted more than once.
 Note that whether or not a method is idempotent is not enforced by the protocol or web server. It is perfectly possible to write a web application in which (for example) a database insert or other non-idempotent action is triggered by a GET or other request. To do so against recommendations, however, may result in undesirable consequences, if a user agent assumes that repeating the same request is safe when it is not.
 A request method is cacheable if responses to requests with that method may be stored for future reuse. The methods GET, HEAD, and POST are defined as cacheable.
 In contrast, the methods PUT, DELETE, CONNECT, OPTIONS, TRACE, and PATCH are not cacheable.
 Request header fields allow the client to pass additional information beyond the request line, acting as request modifiers (similarly to the parameters of a procedure). They give information about the client, about the target resource, or about the expected handling of the request.
 A response message is sent by a server to a client as a reply to its former request message.[note 4]
 A server sends response messages to the client, which consist of:[46]
 In HTTP/1.0 and since, the first line of the HTTP response is called the status line and includes a numeric status code (such as "404") and a textual reason phrase (such as "Not Found"). The response status code is a three-digit integer code representing the result of the server's attempt to understand and satisfy the client's corresponding request. The way the client handles the response depends primarily on the status code, and secondarily on the other response header fields. Clients may not understand all registered status codes but they must understand their class (given by the first digit of the status code) and treat an unrecognized status code as being equivalent to the x00 status code of that class.
 The standard reason phrases are only recommendations, and can be replaced with "local equivalents" at the web developer's discretion. If the status code indicated a problem, the user agent might display the reason phrase to the user to provide further information about the nature of the problem. The standard also allows the user agent to attempt to interpret the reason phrase, though this might be unwise since the standard explicitly specifies that status codes are machine-readable and reason phrases are human-readable.
 The first digit of the status code defines its class:
 The response header fields allow the server to pass additional information beyond the status line, acting as response modifiers. They give information about the server or about further access to the target resource or related resources.
 Each response header field has a defined meaning which can be further refined by the semantics of the request method or response status code.
 Below is a sample HTTP transaction between an HTTP/1.1 client and an HTTP/1.1 server running on www.example.com, port 80.[note 5][note 6]
 A client request (consisting in this case of the request line and a few headers that can be reduced to only the "Host: hostname" header) is followed by a blank line, so that the request ends with a double end of line, each in the form of a carriage return followed by a line feed. The "Host: hostname" header value distinguishes between various DNS names sharing a single IP address, allowing name-based virtual hosting. While optional in HTTP/1.0, it is mandatory in HTTP/1.1. (A "/" (slash) will usually fetch a /index.html file if there is one.)
 The ETag (entity tag) header field is used to determine if a cached version of the requested resource is identical to the current version of the resource on the server. "Content-Type" specifies the Internet media type of the data conveyed by the HTTP message, while "Content-Length" indicates its length in bytes. The HTTP/1.1 webserver publishes its ability to respond to requests for certain byte ranges of the document by setting the field "Accept-Ranges: bytes". This is useful, if the client needs to have only certain portions[61] of a resource sent by the server, which is called byte serving. When "Connection: close" is sent, it means that the web server will close the TCP connection immediately after the end of the transfer of this response.[21]
 Most of the header lines are optional but some are mandatory. When header "Content-Length: number" is missing in a response with an entity body then this should be considered an error in HTTP/1.0 but it may not be an error in HTTP/1.1 if header "Transfer-Encoding: chunked" is present. Chunked transfer encoding uses a chunk size of 0 to mark the end of the content. Some old implementations of HTTP/1.0 omitted the header "Content-Length" when the length of the body entity was not known at the beginning of the response and so the transfer of data to client continued until server closed the socket.
 A "Content-Encoding: gzip" can be used to inform the client that the body entity part of the transmitted data is compressed by gzip algorithm.
 The most popular way of establishing an encrypted HTTP connection is HTTPS.[62] Two other methods for establishing an encrypted HTTP connection also exist: Secure Hypertext Transfer Protocol, and using the HTTP/1.1 Upgrade header to specify an upgrade to TLS. Browser support for these two is, however, nearly non-existent.[63][64][65]
 


Source: https://en.wikipedia.org/wiki/Relationale_Datenbank
Content: Other reasons this message may be displayed:


Source: https://en.wikipedia.org/wiki/Regul%C3%A4rer_Ausdruck
Content: Other reasons this message may be displayed:


Source: https://en.wikipedia.org/wiki/Tkinter
Content: Tkinter is a Python binding to the Tk GUI toolkit. It is the standard Python interface to the Tk GUI toolkit,[1] and is Python's de facto standard GUI.[2] Tkinter is included with standard Linux, Microsoft Windows and macOS installs of Python.
 The name Tkinter comes from Tk interface. Tkinter was written by Steen Lumholt and Guido van Rossum,[3] then later revised by Fredrik Lundh.[4]
 Tkinter is free software released under a Python license.[5]
 As with most other modern Tk bindings, Tkinter is implemented as a Python wrapper around a complete Tcl interpreter embedded in the Python interpreter. Tkinter calls are translated into Tcl commands, which are fed to this embedded interpreter, thus making it possible to mix Python and Tcl in a single application.
 There are several popular GUI library alternatives available, such as Kivy, Pygame, Pyglet, PyGObject, PyQt, PySide, and wxPython.
 This term has different meanings in different contexts, but in general it refers to a rectangular area somewhere on the user's display screen.
 A window which acts as a child of the primary window. It will be decorated with the standard frame and controls for the desktop manager. It can be moved around the desktop and can usually be resized.
 The generic term for any of the building blocks that make up an application in a graphical user interface.
 In Tkinter, the Frame widget is the basic unit of organization for complex layouts. A frame is a rectangular area that can contain other widgets.
 When any widget is created, a parent–child relationship is created. For example, if you place a text label inside a frame, the frame is the parent of the label.
 Here is a minimal Python 3 Tkinter application with one widget:[9]
 For Python 2, the only difference is the word "tkinter" in the import command will be capitalized to "Tkinter".[10]
 There are four stages to creating a widget[11]
 These are often compressed, and the order can vary.
 Using the object-oriented paradigm in Python, a simple program would be (requires Tcl version 8.6, which is not used by Python on MacOS by default):


Source: https://en.wikipedia.org/wiki/Tcl
Content: Tcl (pronounced "tickle" or as an initialism[8]) is a high-level, general-purpose, interpreted, dynamic programming language. It was designed with the goal of being very simple but powerful.[9] Tcl casts everything into the mold of a command, even programming constructs like variable assignment and procedure definition.[10] Tcl supports multiple programming paradigms, including object-oriented, imperative, functional, and procedural styles.
 It is commonly used embedded into C applications,[11] for rapid prototyping, scripted applications, GUIs, and testing.[12] Tcl interpreters are available for many operating systems, allowing Tcl code to run on a wide variety of systems. Because Tcl is a very compact language, it is used on embedded systems platforms, both in its full form and in several other small-footprint versions.[13]
 The popular combination of Tcl with the Tk extension is referred to as Tcl/Tk (pronounced "tickle teak" or as an initialism) and enables building a graphical user interface (GUI) natively in Tcl. Tcl/Tk is included in the standard Python installation in the form of Tkinter.
 The Tcl programming language was created in the spring of 1988 by John Ousterhout while he was working at the University of California, Berkeley.[14][15] Originally "born out of frustration",[11] according to the author, with programmers devising their own languages for extending electronic design automation (EDA) software and, more specifically, the VLSI design tool Magic, which was a professional focus for John at the time.[16] Later Tcl gained acceptance on its own. Ousterhout was awarded the ACM Software System Award in 1997 for Tcl/Tk.[17]
 The name originally comes from Tool Command Language, but is conventionally written Tcl rather than TCL.[18]
 Tcl conferences and workshops are held in both the United States and Europe.[28]
 Tcl's features include
 Safe-Tcl is a subset of Tcl that has restricted features so that Tcl scripts cannot harm their hosting machine or application.[31] File system access is limited and arbitrary system commands are prevented from execution. It uses a dual interpreter model with the untrusted interpreter running code in an untrusted script. It was designed by Nathaniel Borenstein and Marshall Rose to include active messages in e-mail. Safe-Tcl can be included in e-mail when the application/safe-tcl and multipart/enabled-mail are supported. The functionality of Safe-Tcl has since been incorporated as part of the standard Tcl/Tk releases.[32][33]
 The syntax and semantics of Tcl are covered by twelve rules[34] known as the Dodekalogue.[35]
 A Tcl script consists of a series of command invocations. A command invocation is a list of words separated by whitespace and terminated by a newline or semicolon. The first word is the name of a command, which may be built into the language, found in an available library, or defined in the script itself. The subsequent words serve as arguments to the command:
 The following example uses the puts (short for "put string") command to display a string of text on the host console:
 This sends the string "Hello, World!" to the standard output device along with an appended newline character.
 Variables and the results of other commands can be substituted into strings, such as in this example which uses the set and expr commands to store the result of a calculation in a variable (note that Tcl does not use = as an assignment operator), and then uses puts to print the result together with some explanatory text:
 The # character introduces a comment. Comments can appear anywhere the interpreter is expecting a command name.
 As seen in these examples, there is one basic construct in the language: the command. Quoting mechanisms and substitution rules determine how the arguments to each command are processed.
 One special substitution occurs before the parsing of any commands or arguments. If the final character on a line (i.e., immediately before a newline) is a backslash, then the backslash-newline combination (and any spaces or tabs immediately following the newline) are replaced by a single space. This provides a line continuation mechanism, whereby long lines in the source code can be wrapped to the next line for the convenience of readers.
 Continuing with normal argument processing, a word that begins with a double-quote character (") extends to the next double-quote character. Such a word can thus contain whitespace and semicolons without those characters being interpreted as having any special meaning (i.e., they are treated as normal text characters). A word that begins with an opening curly-brace character ({) extends to the next closing curly-brace character (}). Inside curly braces all forms of substitution are suppressed except the previously mentioned backslash-newline elimination. Words not enclosed in either construct are known as bare words.
 In bare and double-quoted words, three types of substitution may occur:
 Substitution proceeds left-to-right in a single scan through each word. Any substituted text will not be scanned again for possible further substitutions. However, any number of substitutions can appear in a single word.
 From Tcl 8.5 onwards, any word may be prefixed by {*}, which causes the word to be split apart into its constituent sub-words for the purposes of building the command invocation (similar to the ,@ sequence of Lisp's quasiquote feature).
 As a consequence of these rules, the result of any command may be used as an argument to any other command. Note that, unlike in Unix command shells, Tcl does not reparse any string unless explicitly directed to do so, which makes interactive use more cumbersome, but scripted use more predictable (e.g., the presence of spaces in filenames does not cause difficulties).
 The single equality sign (=) serves no special role in the language at all. The double equality sign (==) is the test for equality which is used in expression contexts such as the expr command and in the first argument to if. (Both commands are part of the standard library; they have no special place in the library and can be replaced if desired.)
 The majority of Tcl commands, especially in the standard library, are variadic, and the proc (the constructor for scripted command procedures) allows one to define default values for unspecified arguments and a catch-all argument to allow the code to process arbitrary numbers of arguments.
 Tcl is not statically typed: each variable may contain integers, floats, strings, lists, command names, dictionaries, or any other value; values are reinterpreted (subject to syntactic constraints) as other types on demand. However, values are immutable and operations that appear to change them actually just return a new value instead.
 The most important commands that refer to program execution and data operations are:
 The usual execution control commands are:
 Those above looping commands can be additionally controlled by the following commands:
 uplevel allows a command script to be executed in a scope other than the current innermost scope on the stack. Because the command script may itself call procedures that use the uplevel command, this has the net effect of transforming the call stack into a call tree.[36]
 It was originally implemented to permit Tcl procedures to reimplement built-in commands (like for, if or while) and still have the ability to manipulate local variables. For example, the following Tcl script is a reimplementation of the for command (omitting exception handling):
 upvar arranges for one or more local variables in the current procedure to refer to variables in an enclosing procedure call or to global variables. The upvar command simplifies the implementation of call-by-name procedure calling and also makes it easier to build new control constructs as Tcl procedures.[37]
 A decr command that works like the built-in incr command except it subtracts the value from the variable instead of adding it:
 Tcl 8.6 added a built-in dynamic object system, TclOO, in 2012.[29] It includes features such as:
 Tcl did not have object oriented (OO) syntax until 2012,[29] so various extension packages emerged to enable object-oriented programming. They are widespread in existing Tcl source code. Popular extensions include:
 TclOO was not only added to build a strong object oriented system, but also to enable extension packages to build object oriented abstractions using it as a foundation. After the release of TclOO, incr Tcl was updated to use TclOO as its foundation.[27]
 Tcl Web Server is a pure-Tcl implementation of an HTTP protocol server. It runs as a script on top of a vanilla Tcl interpreter.
 Apache Rivet is an open source programming system for Apache HTTP Server that allows developers to use Tcl as a scripting language for creating dynamic web applications. Rivet is similar to PHP, ASP, and JSP. Rivet was primarily developed by Damon Courtney, David Welton, Massimo Manghi, Harald Oehlmann and Karl Lehenbauer. Rivet can use any of the thousands of publicly available Tcl packages that offer countless features such as database interaction (Oracle, PostgreSQL, MySQL, SQLite, etc.), or interfaces to popular applications such as the GD Graphics Library.
 Tcl interfaces natively with the C language.[38] This is because it was originally written to be a framework for providing a syntactic front-end to commands written in C, and all commands in the language (including things that might otherwise be keywords, such as if or while) are implemented this way. Each command implementation function is passed an array of values that describe the (already substituted) arguments to the command, and is free to interpret those values as it sees fit.
 Digital logic simulators often include a Tcl scripting interface for simulating Verilog, VHDL and SystemVerilog hardware languages.
 Tools exist (e.g. SWIG, Ffidl) to automatically generate the necessary code to connect arbitrary C functions and the Tcl runtime, and Critcl does the reverse, allowing embedding of arbitrary C code inside a Tcl script and compiling it at runtime into a DLL.
 The Tcl language has always allowed for extension packages, which provide additional functionality, such as a GUI, terminal-based application automation, database access, and so on. Commonly used extensions include:


Source: https://en.wikipedia.org/wiki/Grafische_Benutzeroberfl%C3%A4che
Content: Other reasons this message may be displayed:


Source: https://en.wikipedia.org/wiki/Tk_(Toolkit)
Content: Other reasons this message may be displayed:


Source: https://en.wikipedia.org/wiki/Wrapper_(Software)
Content: Other reasons this message may be displayed:


Source: https://en.wikipedia.org/wiki/Englische_Sprache
Content: Other reasons this message may be displayed:


Source: https://en.wikipedia.org/wiki/GUI-Toolkit
Content: Other reasons this message may be displayed:


Source: https://en.wikipedia.org/wiki/PyGTK
Content: PyGTK is a set of Python wrappers for the GTK graphical user interface library. PyGTK is free software and licensed under the LGPL. It is analogous to PyQt/PySide and wxPython, the Python wrappers for Qt and wxWidgets, respectively. Its original author is GNOME developer James Henstridge. There are six people in the core development team, with various other people who have submitted patches and bug reports. PyGTK has been selected as the environment of choice for applications running on One Laptop Per Child systems.
 PyGTK will be phased out with the transition to GTK version 3 and be replaced with PyGObject,[4][5] which uses GObject Introspection to generate bindings for Python and other languages on the fly. This is expected to eliminate the delay between GTK updates and corresponding language binding updates, as well as reduce maintenance burden on the developers.[6]
 The Python code below will produce a 200x200 pixel window with the words "Hello World" inside.
 PyGTK has been used in a number of notable applications, some examples:
 PyGObject provides a wrapper for use in Python programs when accessing GObject libraries. GObject is an object system used by GTK, GLib, GIO, GStreamer and other libraries.
 Like the GObject library itself, PyGObject is licensed under the GNU LGPL, so it is suitable for use in both free software and proprietary applications. It is already in use in many applications ranging from small single-purpose scripts to large full-featured applications.
 PyGObject can dynamically access any GObject libraries that use GObject Introspection. It replaces the need for separate modules such as PyGTK, GIO and python-gnome to build a full GNOME 3.0 application.  Once new functionality is added to GObject library it is instantly available as a Python API without the need for intermediate Python glue.
 PyGObject has replaced PyGTK, but it has taken a considerable amount of time for many programs to be ported. Most of the software listed here has an older version which used PyGTK.


Source: https://en.wikipedia.org/wiki/PyQt
Content: PyQt is a Python binding of the cross-platform GUI toolkit Qt, implemented as a Python plug-in. PyQt is free software developed by the British firm Riverbank Computing. It is available under similar terms to Qt versions older than 4.5; this means a variety of licenses including GNU General Public License (GPL) and commercial license, but not the GNU Lesser General Public License (LGPL).[3] PyQt supports Microsoft Windows as well as various kinds of UNIX, including Linux and MacOS (or Darwin).[4]
 PyQt implements around 440 classes and over 6,000 functions and methods[5] including:
 To automatically generate these bindings, Phil Thompson developed the tool SIP, which is also used in other projects.
 PyQt was first released by Riverbank Computing in 1998.[8]
 In August 2009, Nokia sought for the Python binding to be available under the LGPL license. At the time, Nokia owned Qt Software, the developer of QT. After failing to reach an agreement with Riverbank Computing, Nokia released its binding, PySide, providing similar functionality.[9]
 PyQt4 contains the following Python modules.
 PyQt5 contains the following Python modules:
 PyQt version 4 works with both Qt 4 and Qt 5. PyQt version 5 only supports Qt version 5,[4] and drops support for features that are deprecated in Qt 5.[11]
 The below code written for PyQt6 shows a small window on the screen.


Source: https://en.wikipedia.org/wiki/WxPython
Content: wxPython is a wrapper for the cross-platform GUI API (often referred to as a "toolkit") wxWidgets (which is written in C++) for the Python programming language. It is one of the alternatives to Tkinter. It is implemented as a Python extension module (native code).
 In 1995, Robin Dunn needed a GUI application to be deployed on HP-UX systems but also run Windows 3.1 within short time frame. He needed a cross-platform solution. While evaluating free and commercial solutions, he ran across Python bindings on the wxWidgets toolkit webpage (known as wxWindows at the time). This was Dunn's introduction to Python. Together with Harri Pasanen and Edward Zimmerman he developed those initial bindings into wxPython 0.2.[2]
 In August 1998, version 0.3 of wxPython was released. It was built for wxWidgets 2.0 and ran on Win32, with a wxGTK version in the works.[3]
 The first versions of the wrapper were created by hand. However, the code became difficult to maintain and keep synchronized with wxWidgets releases. By 1997, versions were created with SWIG, greatly decreasing the amount of work to update the wrapper.[2]
 In 2010, the Project Phoenix began; an effort to clean up the wxPython implementation and in the process make it compatible with Python 3.[4] The project is a new implementation of wxPython, focused on improving speed, maintainability and extensibility. Like the previous version of wxPython, it wraps the wxWidgets C++ toolkit and provides access to the user interface portions of the wxWidgets API.[5]
 With the release of 4.0.0a1 wxPython in 2017, the Project Phoenix version became the official version.[6] wxPython 4.x is the current version being developed as of June 2022.[7]
 wxPython enables Python to be used for cross-platform GUI applications requiring very little, if any, platform-specific code.
 This is a simple "Hello world" module, depicting the creation of the two main objects in wxPython (the main window object and the application object), followed by passing the control to the event-driven system (by calling MainLoop()) which manages the user-interactive part of the program.
 
This is another example of the wxPython Close Button with wxPython GUI display show in Windows 10 operating system. Being a wrapper, wxPython uses the same free software license used by wxWidgets (wxWindows License)[8]—which is approved by Free Software Foundation and Open Source Initiative.


Source: https://en.wikipedia.org/wiki/PyObjC
Content: PyObjC is a bidirectional bridge between the Python and Objective-C programming languages, allowing programmers to use and extend existing Objective-C libraries, such as Apple's Cocoa framework, using Python.
 PyObjC is used to develop macOS applications in pure Python.
 There is also limited support for GNUstep, an open source, cross-platform implementation of Cocoa.
 The most important usage of PyObjC is enabling programmers to create GUI applications using Cocoa libraries in pure Python.[2] Moreover, as an effect of Objective-C's close relationship with the C programming language (it is a pure superset), developers are also able to incorporate any C-based API by wrapping it with an Objective-C wrapper and then using the wrapped code over the PyObjC bridge. Using Objective-C++, the same can be done with C++ libraries.
 Cocoa developers may also benefit, as tasks written in Python generally take fewer lines than the Objective-C equivalent. This can be used to their advantage as it enables faster prototyping.
 PyObjC's origins date back to 1996, when Lele Gaifax built the original module in September of that year.[3] Among the credited contributors were Guido van Rossum, creator of the Python programming language.
 PyObjC was rewritten in 2002. Notable additions include the ability to directly subclass Objective-C classes from Python and nearly complete support for the Foundation, App Kit and Address Book frameworks.
 Later the same year, support was added for non-framework Python builds, as well as subsequent support for the Python distribution included with Mac OS X. Along with these changes came project templates for standalone Cocoa applications for use with Project Builder, the predecessor to the current Apple platform IDE, Xcode.
 Apple incorporated PyObjC into Mac OS X in 2007, with the release of Mac OS X 10.5 Leopard.[4]
 In Objective-C, objects communicate with each other by sending messages, which is analogous to method calls in other object-oriented languages. When an object receives a message, it looks up the message's name, or selector, and matches it up with a method designated the same selector, which it then invokes.
 The syntax for these message expressions is inherited from Smalltalk, and appears as an object, called the receiver, placed to the left of the name of the message, or selector, and both are enclosed within a pair of square brackets (the square bracket syntax is not inherited from Smalltalk). Colons within a selector indicate that it accepts one or more arguments, one for each colon. Intended to improve code readability, colons are placed within the selector such that when the required arguments are in place, the expression's intent is unambiguous:
 This is distinct from the syntax used in Python, and in many other languages, where an equivalent expression would read:
 Translating Objective-C selectors to Python method names is accomplished by replacing each colon with a single underscore and listing the arguments within a pair of parentheses at the end, as demonstrated above.
 Objective-C classes are subclassed in the same manner as a normal Python class:


Source: https://en.wikipedia.org/wiki/Fast_Light_Toolkit
Content: Fast Light Toolkit (FLTK)[3] is a cross-platform widget (graphical control element) library for graphical user interfaces (GUIs), developed by Bill Spitzak and others. Made to accommodate 3D graphics programming, it has an interface to OpenGL, but it is also suitable for general GUI programming.
 Using its own widget, drawing and event systems abstracted from the underlying system-dependent code, it allows for writing programs which look the same on all supported operating systems.
 FLTK is free and open-source software, licensed under GNU Lesser General Public License (LGPL) with an added clause permitting static linking from applications with incompatible licenses.
 In contrast to user interface libraries like GTK, Qt, and wxWidgets, FLTK uses a more lightweight design and restricts itself to GUI functionality. Because of this, the library is very small (the FLTK "Hello World" program is around 100 KiB), and is usually statically linked. It also avoids complex macros, separate code preprocessors, and use of some advanced C++ features: templates, exceptions, and run-time type information (RTTI) or, for FLTK 1.x, namespaces. Combined with the modest size of the package, this makes it relatively easy to learn for new users.[4]
 These advantages come with corresponding disadvantages. FLTK offers fewer widgets than most GUI toolkits and, because of its use of non-native widgets, does not have native look-and-feel on any platform.
 FLTK was originally designed to be compatible with the Forms Library written for Silicon Graphics (SGI) machines (a derivative of this library called XForms is still used quite often). In that library, all functions and structures start with fl_. This naming was extended to all new methods and widgets in the C++ library, and this prefix FL was taken as the name of the library. After FL was released as open source, it was discovered that searching "FL" on the Internet was a problem, because it is also the abbreviation for Florida. After much debating and searching for a new name for the toolkit, which was already in use by several people, Bill Spitzak came up with Fast Light Tool Kit (FLTK).[5]
 FLTK is an object-oriented widget toolkit written in the programming language C++. While GTK is mainly optimized for the X Window System, FLTK works on other platforms, including Microsoft Windows (interfaced with the Windows API), and OS X (interfaced with Quartz). A Wayland back-end has been implemented and will be available in release 1.4.0.[6] FLTK2 has gained experimental support for optionally using the cairo graphics library.
 A library written in one programming language may be used in another language if language bindings are written. FLTK has a range of bindings for various languages.
 FLTK was mainly designed for, and is written in, the programming language C++. However, bindings exist for other languages, for example Lua,[7] Perl,[8] Python,[9] Ruby,[10] Rust[11] and Tcl.[12]
 For FLTK 1.x, this example creates a window with an Okay button:
 FLTK includes Fast Light User Interface Designer (FLUID), a graphical GUI designer that generates C++ source and header files.
 Many programs and projects use FLTK, including:
 This version history is an example of the sometimes tumultuous nature of open-source development.[34]
 This is a prior stable version, now unmaintained.
 This is a prior stable version, now unmaintained.
 This was a development branch, long thought to be the next step in FLTK's evolution, with many new features and a cleaner programming style. It never achieved stability, and development has largely ceased. The branch is inactive now.
 This was an attempt to take some of the best features of 2.0 and merge them back into the more popular 1.1 branch. It is no longer developed.
 Current stable release.[35] Provides UTF-8 support.
 Current development branch. Adds more features to 1.3.[35]
 This branch is mostly a conceptual model for future work. Now inactive.


Source: https://en.wikipedia.org/wiki/Turtle-Grafik
Content: Other reasons this message may be displayed:


Source: https://en.wikipedia.org/wiki/Datei:Tkinter_Hallo_Welt.png
Content: Other reasons this message may be displayed:


Source: https://en.wikipedia.org/wiki/Datei:Beispiel_f%C3%BCr_ein_Turtle-Muster.png
Content: Other reasons this message may be displayed:


Source: https://en.wikipedia.org/wiki/Datei:Python_Kreis-Muster.png
Content: Other reasons this message may be displayed:


Source: https://en.wikipedia.org/wiki/Datei:Python_Turtle_Muster_Stern.png
Content: Other reasons this message may be displayed:


Source: https://en.wikipedia.org/wiki/Datei:Turtle-Programmierung.gif
Content: Other reasons this message may be displayed:


Source: https://en.wikipedia.org/wiki/Quicksort
Content: 
 Quicksort is an efficient, general-purpose sorting algorithm. Quicksort was developed by British computer scientist Tony Hoare in 1959[1] and published in 1961.[2] It is still a commonly used algorithm for sorting. Overall, it is slightly faster than merge sort and heapsort for randomized data, particularly on larger distributions.[3]
 Quicksort is a divide-and-conquer algorithm. It works by selecting a 'pivot' element from the array and partitioning the other elements into two sub-arrays, according to whether they are less than or greater than the pivot. For this reason, it is sometimes called partition-exchange sort.[4] The sub-arrays are then sorted recursively. This can be done in-place, requiring small additional amounts of memory to perform the sorting.
 Quicksort is a comparison sort, meaning that it can sort items of any type for which a "less-than" relation (formally, a total order) is defined. It is a comparison-based sort since elements a and b are only swapped in case their relative order has been obtained in the transitive closure of prior comparison-outcomes. Most implementations of quicksort are not stable, meaning that the relative order of equal sort items is not preserved.
 Mathematical analysis of quicksort shows that, on average, the algorithm takes 



O
(
n
log
⁡

n

)


{\displaystyle O(n\log {n})}

 comparisons to sort n items. In the worst case, it makes 



O
(

n

2


)


{\displaystyle O(n^{2})}

 comparisons.
 The quicksort algorithm was developed in 1959 by Tony Hoare while he was a visiting student at Moscow State University. At that time, Hoare was working on a machine translation project for the National Physical Laboratory. As a part of the translation process, he needed to sort the words in Russian sentences before looking them up in a Russian-English dictionary, which was in alphabetical order on magnetic tape.[5] After recognizing that his first idea, insertion sort, would be slow, he came up with a new idea. He wrote the partition part in Mercury Autocode but had trouble dealing with the list of unsorted segments. On return to England, he was asked to write code for Shellsort. Hoare mentioned to his boss that he knew of a faster algorithm and his boss bet a sixpence that he did not. His boss ultimately accepted that he had lost the bet. Hoare published a paper about his algorithm in The Computer Journal Volume 5, Issue 1, 1962, Pages 10–16. Later, Hoare learned about ALGOL and its ability to do recursion that enabled him to publish an improved version of the algorithm in ALGOL in Communications of the Association for Computing Machinery, the premier computer science journal of the time.[2][6] The ALGOL code is published in Communications of the ACM (CACM), Volume 4, Issue 7 July 1961, pp 321 Algorithm 63: partition and Algorithm 64: Quicksort. 
 Quicksort gained widespread adoption, appearing, for example, in Unix as the default library sort subroutine. Hence, it lent its name to the C standard library subroutine qsort[7] and in the reference implementation of Java.
 Robert Sedgewick's PhD thesis in 1975 is considered a milestone in the study of Quicksort where he resolved many open problems related to the analysis of various pivot selection schemes including Samplesort, adaptive partitioning by Van Emden[8] as well as derivation of expected number of comparisons and swaps.[7] Jon Bentley and Doug McIlroy in 1993 incorporated various improvements for use in programming libraries, including a technique to deal with equal elements and a pivot scheme known as pseudomedian of nine, where a sample of nine elements is divided into groups of three and then the median of the three medians from three groups is chosen.[7] Bentley described another simpler and compact partitioning scheme in his book Programming Pearls that he attributed to Nico Lomuto. Later Bentley wrote that he used Hoare's version for years but never really understood it but Lomuto's version was simple enough to prove correct.[9] Bentley described Quicksort as the "most beautiful code I had ever written" in the same essay. Lomuto's partition scheme was also popularized by the textbook Introduction to Algorithms although it is inferior to Hoare's scheme because it does three times more swaps on average and degrades to O(n2) runtime when all elements are equal.[10][self-published source?] McIlroy would further produce an AntiQuicksort (aqsort) function in 1998, which consistently drives even his 1993 variant of Quicksort into quadratic behavior by producing adversarial data on-the-fly.[11]
 Quicksort is a type of divide-and-conquer algorithm for sorting an array, based on a partitioning routine; the details of this partitioning can vary somewhat, so that quicksort is really a family of closely related algorithms. Applied to a range of at least two elements, partitioning produces a division into two consecutive non empty sub-ranges, in such a way that no element of the first sub-range is greater than any element of the second sub-range. After applying this partition, quicksort then recursively sorts the sub-ranges, possibly after excluding from them an element at the point of division that is at this point known to be already in its final location. Due to its recursive nature, quicksort (like  the partition routine) has to be formulated so as to be callable for a range within a larger array, even if the ultimate goal is to sort a complete array. The steps for in-place quicksort are:
 The choice of partition routine (including the pivot selection) and other details not entirely specified above can affect the algorithm's performance, possibly to a great extent for specific input arrays. In discussing the efficiency of quicksort, it is therefore necessary to specify these choices first. Here we mention two specific partition methods.
 This scheme is attributed to Nico Lomuto and popularized by Bentley in his book Programming Pearls[12] and Cormen et al. in their book Introduction to Algorithms.[13] In most formulations this scheme chooses as the pivot the last element in the array. The algorithm maintains index i as it scans the array using another index j such that the elements at lo through i-1 (inclusive) are less than the pivot, and the elements at i through j (inclusive) are equal to or greater than the pivot. As this scheme is more compact and easy to understand, it is frequently used in introductory material, although it is less efficient than Hoare's original scheme e.g., when all elements are equal.[14] The complexity of Quicksort with this scheme degrades to O(n2) when the array is already in order, due to the partition being the worst possible one.[10] There have been various variants proposed to boost performance including various ways to select the pivot, deal with equal elements, use other sorting algorithms such as insertion sort for small arrays, and so on. In pseudocode, a quicksort that sorts elements at lo through hi (inclusive) of an array A can be expressed as:[13]
 Sorting the entire array is accomplished by quicksort(A, 0, length(A) - 1).
 The original partition scheme described by Tony Hoare uses two pointers (indices into the range) that start at both ends of the array being partitioned, then move toward each other, until they detect an inversion: a pair of elements, one greater than the bound (Hoare's terms for the pivot value) at the first pointer, and one less than the bound at the second pointer; if at this point the first pointer is still before the second, these elements are in the wrong order relative to each other, and they are then exchanged.[15] After this the pointers are moved inwards, and the search for an inversion is repeated; when eventually the pointers cross (the first points after the second), no exchange is performed; a valid partition is found, with the point of division between the crossed pointers (any entries that might be strictly between the crossed pointers are equal to the pivot and can be excluded from both sub-ranges formed). With this formulation it is possible that one sub-range turns out to be the whole original range, which would prevent the algorithm from advancing. Hoare therefore stipulates that at the end, the sub-range containing the pivot element (which still is at its original position) can be decreased in size by excluding that pivot, after (if necessary) exchanging it with the sub-range element closest to the separation; thus, termination of quicksort is ensured.
 With respect to this original description, implementations often make minor but important variations. Notably, the scheme as presented below includes elements equal to the pivot among the candidates for an inversion (so "greater than or equal" and "less than or equal" tests are used instead of "greater than" and "less than" respectively; since the formulation uses do...while rather than repeat...until which is actually reflected by the use of strict comparison operators[clarification needed]). While there is no reason to exchange elements equal to the bound, this change allows tests on the pointers themselves to be omitted, which are otherwise needed to ensure they do not run out of range. Indeed, since at least one instance of the pivot value is present in the range, the first advancement of either pointer cannot pass across this instance if an inclusive test is used; once an exchange is performed, these exchanged elements are now both strictly ahead of the pointer that found them, preventing that pointer from running off. (The latter is true independently of the test used, so it would be possible to use the inclusive test only when looking for the first inversion. However, using an inclusive test throughout also ensures that a division near the middle is found when all elements in the range are equal, which gives an important efficiency gain for sorting arrays with many equal elements.) The risk of producing a non-advancing separation is avoided in a different manner than described by Hoare. Such a separation can only result when no inversions are found, with both pointers advancing to the pivot element at the first iteration (they are then considered to have crossed, and no exchange takes place). The division returned is after the final position of the second pointer, so the case to avoid is where the pivot is the final element of the range and all others are smaller than it. Therefore, the pivot choice must avoid the final element (in Hoare's description it could be any element in the range); this is done here by rounding down the middle position, using the floor function.[16] This illustrates that the argument for correctness of an implementation of the Hoare partition scheme can be subtle, and it is easy to get it wrong.
 In pseudocode,[13]
 The entire array is sorted by quicksort(A, 0, length(A) - 1).
 Hoare's scheme is more efficient than Lomuto's partition scheme because it does three times fewer swaps on average. Also, as mentioned, the implementation given creates a balanced partition even when all values are equal.[10][self-published source?], which Lomuto's scheme does not. Like Lomuto's partition scheme, Hoare's partitioning also would cause Quicksort to degrade to O(n2) for already sorted input, if the pivot was chosen as the first or the last element. With the middle element as the pivot, however, sorted data results with (almost) no swaps in equally sized partitions leading to best case behavior of Quicksort, i.e. O(n log(n)). Like others, Hoare's partitioning doesn't produce a stable sort. In this scheme, the pivot's final location is not necessarily at the index that is returned, as the pivot and elements equal to the pivot can end up anywhere within the partition after a partition step, and may not be sorted until the base case of a partition with a single element is reached via recursion. Therefore, the next two segments that the main algorithm recurs on are (lo..p) (elements ≤ pivot) and (p+1..hi) (elements ≥ pivot) as opposed to (lo..p-1) and (p+1..hi) as in Lomuto's scheme.
 Subsequent recursions (expansion on previous paragraph)
 Let's expand a little bit on the next two segments that the main algorithm recurs on. Because we are using strict comparators (>, <) in the "do...while" loops to prevent ourselves from running out of range, there's a chance that the pivot itself gets swapped with other elements in the partition function. Therefore, the index returned in the partition function isn't necessarily where the actual pivot is. Consider the example of [5, 2, 3, 1, 0], following the scheme, after the first partition the array becomes [0, 2, 1, 3, 5], the "index" returned is 2, which is the number 1, when the real pivot, the one we chose to start the partition with was the number 3. With this example, we see how it is necessary to include the returned index of the partition function in our subsequent recursions. As a result, we are presented with the choices of either recursing on (lo..p) and (p+1..hi), or (lo..p - 1) and (p..hi). Which of the two options we choose depends on which index (i or j) we return in the partition function when the indices cross, and how we choose our pivot in the partition function (floor v.s. ceiling).
 Let's first examine the choice of recursing on (lo..p) and (p+1..hi), with the example of sorting an array where multiple identical elements exist [0, 0]. If index i (the "latter" index) is returned after indices cross in the partition function, the index 1 would be returned after the first partition. The subsequent recursion on (lo..p)would be on (0, 1), which corresponds to the exact same array [0, 0]. A non-advancing separation that causes infinite recursion is produced. It is therefore obvious that when recursing on (lo..p) and (p+1..hi), because the left half of the recursion includes the returned index, it is the partition function's job to exclude the "tail" in non-advancing scenarios. Which is to say, index j (the "former" index when indices cross) should be returned instead of i. Going with a similar logic, when considering the example of an already sorted array [0, 1], the choice of pivot needs to be "floor" to ensure that the pointers stop on the "former" instead of the "latter" (with "ceiling" as the pivot, the index 1 would be returned and included in (lo..p) causing infinite recursion). It is for the exact same reason why choice of the last element as pivot must be avoided.
 The choice of recursing on (lo..p - 1) and (p..hi) follows the exact same logic as above. Because the right half of the recursion includes the returned index, it is the partition function's job to exclude the "head" in non-advancing scenarios. The index i (the "latter" index after the indices cross) in the partition function needs to be returned, and "ceiling" needs to be chosen as the pivot. The two nuances are clear, again, when considering the examples of sorting an array where multiple identical elements exist ([0, 0]), and an already sorted array [0, 1] respectively. It is noteworthy that with version of recursion, for the same reason, choice of the first element as pivot must be avoided.
 In the very early versions of quicksort, the leftmost element of the partition would often be chosen as the pivot element. Unfortunately, this causes worst-case behavior on already sorted arrays, which is a rather common use-case.[17] The problem was easily solved by choosing either a random index for the pivot, choosing the middle index of the partition or (especially for longer partitions) choosing the median of the first, middle and last element of the partition for the pivot (as recommended by Sedgewick).[18] This "median-of-three" rule counters the case of sorted (or reverse-sorted) input, and gives a better estimate of the optimal pivot (the true median) than selecting any single element, when no information about the ordering of the input is known.
 Median-of-three code snippet for Lomuto partition:
 It puts a median into A[hi] first, then that new value of A[hi] is used for a pivot, as in a basic algorithm presented above.
 Specifically, the expected number of comparisons needed to sort n elements (see § Analysis of randomized quicksort) with random pivot selection is 1.386 n log n. Median-of-three pivoting brings this down to Cn, 2 ≈ 1.188 n log n, at the expense of a three-percent increase in the expected number of swaps.[7] An even stronger pivoting rule, for larger arrays, is to pick the ninther, a recursive median-of-three (Mo3), defined as[7]
 Selecting a pivot element is also complicated by the existence of integer overflow. If the boundary indices of the subarray being sorted are sufficiently large, the naïve expression for the middle index, (lo + hi)/2, will cause overflow and provide an invalid pivot index. This can be overcome by using, for example, lo + (hi−lo)/2 to index the middle element, at the cost of more complex arithmetic. Similar issues arise in some other methods of selecting the pivot element.
 With a partitioning algorithm such as the Lomuto partition scheme described above (even one that chooses good pivot values), quicksort exhibits poor performance for inputs that contain many repeated elements. The problem is clearly apparent when all the input elements are equal: at each recursion, the left partition is empty (no input values are less than the pivot), and the right partition has only decreased by one element (the pivot is removed). Consequently, the Lomuto partition scheme takes quadratic time to sort an array of equal values. However, with a partitioning algorithm such as the Hoare partition scheme, repeated elements generally results in better partitioning, and although needless swaps of elements equal to the pivot may occur, the running time generally decreases as the number of repeated elements increases (with memory cache reducing the swap overhead). In the case where all elements are equal, Hoare partition scheme needlessly swaps elements, but the partitioning itself is best case, as noted in the Hoare partition section above.
 To solve the Lomuto partition scheme problem (sometimes called the Dutch national flag problem[7]), an alternative linear-time partition routine can be used that separates the values into three groups: values less than the pivot, values equal to the pivot, and values greater than the pivot. (Bentley and McIlroy call this a "fat partition" and it was already implemented in the qsort of Version 7 Unix.[7]) The values equal to the pivot are already sorted, so only the less-than and greater-than partitions need to be recursively sorted. In pseudocode, the quicksort algorithm becomes:
 The partition algorithm returns indices to the first ('leftmost') and to the last ('rightmost') item of the middle partition. Every other item of the partition is equal to the pivot and is therefore sorted. Consequently, the items of the partition need not be included in the recursive calls to quicksort.
 The best case for the algorithm now occurs when all elements are equal (or are chosen from a small set of k ≪ n elements). In the case of all equal elements, the modified quicksort will perform only two recursive calls on empty subarrays and thus finish in linear time (assuming the partition subroutine takes no longer than linear time).
 Other important optimizations, also suggested by Sedgewick and widely used in practice, are:[19][20]
 Quicksort's divide-and-conquer formulation makes it amenable to parallelization using task parallelism. The partitioning step is accomplished through the use of a parallel prefix sum algorithm to compute an index for each array element in its section of the partitioned array.[23][24] Given an array of size n, the partitioning step performs O(n) work in O(log n) time and requires O(n) additional scratch space. After the array has been partitioned, the two partitions can be sorted recursively in parallel. Assuming an ideal choice of pivots, parallel quicksort sorts an array of size n in O(n log n) work in O(log2 n) time using O(n) additional space.
 Quicksort has some disadvantages when compared to alternative sorting algorithms, like merge sort, which complicate its efficient parallelization. The depth of quicksort's divide-and-conquer tree directly impacts the algorithm's scalability, and this depth is highly dependent on the algorithm's choice of pivot. Additionally, it is difficult to parallelize the partitioning step efficiently in-place. The use of scratch space simplifies the partitioning step, but increases the algorithm's memory footprint and constant overheads.
 Other more sophisticated parallel sorting algorithms can achieve even better time bounds.[25] For example, in 1991 David M W Powers described a parallelized quicksort (and a related radix sort) that can operate in O(log n) time on a CRCW (concurrent read and concurrent write) PRAM (parallel random-access machine) with n processors by performing partitioning implicitly.[26]
 The most unbalanced partition occurs when one of the sublists returned by the partitioning routine is of size n − 1.[27] This may occur if the pivot happens to be the smallest or largest element in the list, or in some implementations (e.g., the Lomuto partition scheme as described above) when all the elements are equal.
 If this happens repeatedly in every partition, then each recursive call processes a list of size one less than the previous list. Consequently, we can make n − 1 nested calls before we reach a list of size 1. This means that the call tree is a linear chain of n − 1 nested calls. The ith call does O(n − i) work to do the partition, and 





∑

i
=
0


n


(
n
−
i
)
=
O
(

n

2


)



{\displaystyle \textstyle \sum _{i=0}^{n}(n-i)=O(n^{2})}

, so in that case quicksort takes O(n2) time.
 In the most balanced case, each time we perform a partition we divide the list into two nearly equal pieces. This means each recursive call processes a list of half the size. Consequently, we can make only log2 n nested calls before we reach a list of size 1. This means that the depth of the call tree is log2 n. But no two calls at the same level of the call tree process the same part of the original list; thus, each level of calls needs only O(n) time all together (each call has some constant overhead, but since there are only O(n) calls at each level, this is subsumed in the O(n) factor). The result is that the algorithm uses only O(n log n) time.
 To sort an array of n distinct elements, quicksort takes O(n log n) time in expectation, averaged over all n! permutations of n elements with equal probability. Alternatively, if the algorithm selects the pivot uniformly at random from the input array, the same analysis can be used to bound the expected running time for any input sequence; the expectation is then taken over the random choices made by the algorithm (Cormen et al., Introduction to Algorithms,[13] Section 7.3).
 We list here three common proofs to this claim providing different insights into quicksort's workings.
 If each pivot has rank somewhere in the middle 50 percent, that is, between the 25th percentile and the 75th percentile, then it splits the elements with at least 25% and at most 75% on each side. If we could consistently choose such pivots, we would only have to split the list at most 




log

4

/

3


⁡
n


{\displaystyle \log _{4/3}n}

 times before reaching lists of size 1, yielding an O(n log n) algorithm.
 When the input is a random permutation, the pivot has a random rank, and so it is not guaranteed to be in the middle 50 percent. However, when we start from a random permutation, in each recursive call the pivot has a random rank in its list, and so it is in the middle 50 percent about half the time. That is good enough. Imagine that a coin is flipped: heads means that the rank of the pivot is in the middle 50 percent, tail means that it isn't. Now imagine that the coin is flipped over and over until it gets k heads. Although this could take a long time, on average only 2k flips are required, and the chance that the coin won't get k heads after 100k flips is highly improbable (this can be made rigorous using Chernoff bounds). By the same argument, Quicksort's recursion will terminate on average at a call depth of only 



2

log

4

/

3


⁡
n


{\displaystyle 2\log _{4/3}n}

. But if its average call depth is O(log n), and each level of the call tree processes at most n elements, the total amount of work done on average is the product, O(n log n). The algorithm does not have to verify that the pivot is in the middle half—if we hit it any constant fraction of the times, that is enough for the desired complexity.
 An alternative approach is to set up a recurrence relation for the T(n) factor, the time needed to sort a list of size n. In the most unbalanced case, a single quicksort call involves O(n) work plus two recursive calls on lists of size 0 and n−1, so the recurrence relation is
 This is the same relation as for insertion sort and selection sort, and it solves to worst case T(n) = O(n2).
 In the most balanced case, a single quicksort call involves O(n) work plus two recursive calls on lists of size n/2, so the recurrence relation is
 The master theorem for divide-and-conquer recurrences tells us that T(n) = O(n log n).
 The outline of a formal proof of the O(n log n) expected time complexity follows. Assume that there are no duplicates as duplicates could be handled with linear time pre- and post-processing, or considered cases easier than the analyzed. When the input is a random permutation, the rank of the pivot is uniform random from 0 to n − 1. Then the resulting parts of the partition have sizes i and n − i − 1, and i is uniform random from 0 to n − 1. So, averaging over all possible splits and noting that the number of comparisons for the partition is n − 1, the average number of comparisons over all permutations of the input sequence can be estimated accurately by solving the recurrence relation:
 Solving the recurrence gives C(n) = 2n ln n ≈ 1.39n log2 n.
 This means that, on average, quicksort performs only about 39% worse than in its best case. In this sense, it is closer to the best case than the worst case. A comparison sort cannot use less than log2(n!) comparisons on average to sort n items (as explained in the article Comparison sort) and in case of large n, Stirling's approximation yields log2(n!) ≈ n(log2 n − log2 e), so quicksort is not much worse than an ideal comparison sort. This fast average runtime is another reason for quicksort's practical dominance over other sorting algorithms.
 The following binary search tree (BST) corresponds to each execution of quicksort: the initial pivot is the root node; the pivot of the left half is the root of the left subtree, the pivot of the right half is the root of the right subtree, and so on. The number of comparisons of the execution of quicksort equals the number of comparisons during the construction of the BST by a sequence of insertions. So, the average number of comparisons for randomized quicksort equals the average cost of constructing a BST when the values inserted 



(

x

1


,

x

2


,
…
,

x

n


)


{\displaystyle (x_{1},x_{2},\ldots ,x_{n})}

 form a random permutation.
 Consider a BST created by insertion of a sequence 



(

x

1


,

x

2


,
…
,

x

n


)


{\displaystyle (x_{1},x_{2},\ldots ,x_{n})}

 of values forming a random permutation. Let C denote the cost of creation of the BST. We have 



C
=

∑

i



∑

j
<
i



c

i
,
j




{\displaystyle C=\sum _{i}\sum _{j<i}c_{i,j}}

, where 




c

i
,
j




{\displaystyle c_{i,j}}

 is a binary random variable expressing whether during the insertion of 




x

i




{\displaystyle x_{i}}

 there was a comparison to 




x

j




{\displaystyle x_{j}}

.
 By linearity of expectation, the expected value 



E
⁡
[
C
]


{\displaystyle \operatorname {E} [C]}

 of C is 



E
⁡
[
C
]
=

∑

i



∑

j
<
i


Pr
(

c

i
,
j


)


{\displaystyle \operatorname {E} [C]=\sum _{i}\sum _{j<i}\Pr(c_{i,j})}

.
 Fix i and j<i. The values 





x

1


,

x

2


,
…
,

x

j





{\displaystyle {x_{1},x_{2},\ldots ,x_{j}}}

, once sorted, define j+1 intervals. The core structural observation is that 




x

i




{\displaystyle x_{i}}

 is compared to 




x

j




{\displaystyle x_{j}}

 in the algorithm if and only if 




x

i




{\displaystyle x_{i}}

 falls inside one of the two intervals adjacent to 




x

j




{\displaystyle x_{j}}

.
 Observe that since 



(

x

1


,

x

2


,
…
,

x

n


)


{\displaystyle (x_{1},x_{2},\ldots ,x_{n})}

 is a random permutation, 



(

x

1


,

x

2


,
…
,

x

j


,

x

i


)


{\displaystyle (x_{1},x_{2},\ldots ,x_{j},x_{i})}

 is also a random permutation, so the probability that 




x

i




{\displaystyle x_{i}}

 is adjacent to 




x

j




{\displaystyle x_{j}}

 is exactly 





2

j
+
1





{\displaystyle {\frac {2}{j+1}}}

.
 We end with a short calculation:
 The space used by quicksort depends on the version used.
 The in-place version of quicksort has a space complexity of O(log n), even in the worst case, when it is carefully implemented using the following strategies.
 Quicksort with in-place and unstable partitioning uses only constant additional space before making any recursive call. Quicksort must store a constant amount of information for each nested recursive call. Since the best case makes at most O(log n) nested recursive calls, it uses O(log n) space. However, without Sedgewick's trick to limit the recursive calls, in the worst case quicksort could make O(n) nested recursive calls and need O(n) auxiliary space.
 From a bit complexity viewpoint, variables such as lo and hi do not use constant space; it takes O(log n) bits to index into a list of n items. Because there are such variables in every stack frame, quicksort using Sedgewick's trick requires O((log n)2) bits of space. This space requirement isn't too terrible, though, since if the list contained distinct elements, it would need at least O(n log n) bits of space.
 Another, less common, not-in-place, version of quicksort uses O(n) space for working storage and can implement a stable sort. The working storage allows the input array to be easily partitioned in a stable manner and then copied back to the input array for successive recursive calls. Sedgewick's optimization is still appropriate.
 Quicksort is a space-optimized version of the binary tree sort. Instead of inserting items sequentially into an explicit tree, quicksort organizes them concurrently into a tree that is implied by the recursive calls. The algorithms make exactly the same comparisons, but in a different order. An often desirable property of a sorting algorithm is stability – that is the order of elements that compare equal is not changed, allowing controlling order of multikey tables (e.g. directory or folder listings) in a natural way. This property is hard to maintain for in-place quicksort (that uses only constant additional space for pointers and buffers, and O(log n) additional space for the management of explicit or implicit recursion). For variant quicksorts involving extra memory due to representations using pointers (e.g. lists or trees) or files (effectively lists), it is trivial to maintain stability. The more complex, or disk-bound, data structures tend to increase time cost, in general making increasing use of virtual memory or disk.
 The most direct competitor of quicksort is heapsort. Heapsort has the advantages of simplicity, and a worst case run time of O(n log n), but heapsort's average running time is usually considered slower than in-place quicksort, primarily due to its worse locality of reference.[28] This result is debatable; some publications indicate the opposite.[29][30] The main disadvantage of quicksort is the implementation complexity required to avoid bad pivot choices and the resultant O(n2) performance.  Introsort is a variant of quicksort which solves this problem by switching to heapsort when a bad case is detected.  Major programming languages, such as C++ (in the GNU and LLVM implementations), use introsort.[31]
 Quicksort also competes with merge sort, another O(n log n) sorting algorithm.  Merge sort's main advantages are that it is a stable sort and has excellent worst-case performance.  The main disadvantage of merge sort is that it is an out-of-place algorithm, so when operating on arrays, efficient implementations require O(n) auxiliary space (vs. O(log n) for quicksort with in-place partitioning and tail recursion, or O(1) for heapsort).
 Merge sort works very well on linked lists, requiring only a small, constant amount of auxiliary storage.  Although quicksort can be implemented as a stable sort using linked lists, there is no reason to; it will often suffer from poor pivot choices without random access, and is essentially always inferior to merge sort.  Merge sort is also the algorithm of choice for external sorting of very large data sets stored on slow-to-access media such as disk storage or network-attached storage.
 Bucket sort with two buckets is very similar to quicksort; the pivot in this case is effectively the value in the middle of the value range, which does well on average for uniformly distributed inputs.
 A selection algorithm chooses the kth smallest of a list of numbers; this is an easier problem in general than sorting. One simple but effective selection algorithm works nearly in the same manner as quicksort, and is accordingly known as quickselect. The difference is that instead of making recursive calls on both sublists, it only makes a single tail-recursive call on the sublist that contains the desired element. This change lowers the average complexity to linear or O(n) time, which is optimal for selection, but the selection algorithm is still O(n2) in the worst case.
 A variant of quickselect, the median of medians algorithm, chooses pivots more carefully, ensuring that the pivots are near the middle of the data (between the 30th and 70th percentiles), and thus has guaranteed linear time – O(n). This same pivot strategy can be used to construct a variant of quicksort (median of medians quicksort) with O(n log n) time. However, the overhead of choosing the pivot is significant, so this is generally not used in practice.
 More abstractly, given an O(n) selection algorithm, one can use it to find the ideal pivot (the median) at every step of quicksort and thus produce a sorting algorithm with O(n log n) running time. Practical implementations of this variant are considerably slower on average, but they are of theoretical interest because they show an optimal selection algorithm can yield an optimal sorting algorithm.
 Instead of partitioning into two subarrays using a single pivot, multi-pivot quicksort (also multiquicksort[22]) partitions its input into some s number of subarrays using s − 1 pivots. While the dual-pivot case (s = 3) was considered by Sedgewick and others already in the mid-1970s, the resulting algorithms were not faster in practice than the "classical" quicksort.[32] A 1999 assessment of a multiquicksort with a variable number of pivots, tuned to make efficient use of processor caches, found it to increase the instruction count by some 20%, but simulation results suggested that it would be more efficient on very large inputs.[22] A version of dual-pivot quicksort developed by Yaroslavskiy in 2009[33] turned out to be fast enough[34] to warrant implementation in Java 7, as the standard algorithm to sort arrays of primitives (sorting arrays of objects is done using Timsort).[35] The performance benefit of this algorithm was subsequently found to be mostly related to cache performance,[36] and experimental results indicate that the three-pivot variant may perform even better on modern machines.[37][38]
 For disk files, an external sort based on partitioning similar to quicksort is possible. It is slower than external merge sort, but doesn't require extra disk space. 4 buffers are used, 2 for input, 2 for output. Let N = number of records in the file, B = the number of records per buffer, and M = N/B = the number of buffer segments in the file. Data is read (and written) from both ends of the file inwards. Let X represent the segments that start at the beginning of the file and Y represent segments that start at the end of the file. Data is read into the X and Y read buffers. A pivot record is chosen and the records in the X and Y buffers other than the pivot record are copied to the X write buffer in ascending order and Y write buffer in descending order based comparison with the pivot record. Once either X or Y buffer is filled, it is written to the file and the next X or Y buffer is read from the file. The process continues until all segments are read and one write buffer remains. If that buffer is an X write buffer, the pivot record is appended to it and the X buffer written. If that buffer is a Y write buffer, the pivot record is prepended to the Y buffer and the Y buffer written. This constitutes one partition step of the file, and the file is now composed of two subfiles. The start and end positions of each subfile are pushed/popped to a stand-alone stack or the main stack via recursion. To limit stack space to O(log2(n)), the smaller subfile is processed first. For a stand-alone stack, push the larger subfile parameters onto the stack, iterate on the smaller subfile. For recursion, recurse on the smaller subfile first, then iterate to handle the larger subfile. Once a sub-file is less than or equal to 4 B records, the subfile is sorted in-place via quicksort and written. That subfile is now sorted and in place in the file. The process is continued until all sub-files are sorted and in place. The average number of passes on the file is approximately 1 + ln(N+1)/(4 B), but worst case pattern is N passes (equivalent to O(n^2) for worst case internal sort).[39]
 This algorithm is a combination of radix sort and quicksort. Pick an element from the array (the pivot) and consider the first character (key) of the string (multikey). Partition the remaining elements into three sets: those whose corresponding character is less than, equal to, and greater than the pivot's character. Recursively sort the "less than" and "greater than" partitions on the same character. Recursively sort the "equal to" partition by the next character (key). Given we sort using bytes or words of length W bits, the best case is O(KN) and the worst case O(2KN) or at least O(N2) as for standard quicksort, given for unique keys N<2K, and K is a hidden constant in all standard comparison sort algorithms including quicksort. This is a kind of three-way quicksort in which the middle partition represents a (trivially) sorted subarray of elements that are exactly equal to the pivot.
 Also developed by Powers as an O(K) parallel PRAM algorithm. This is again a combination of radix sort and quicksort but the quicksort left/right partition decision is made on successive bits of the key, and is thus O(KN) for N K-bit keys. All comparison sort algorithms implicitly assume the transdichotomous model with K in Θ(log N), as if K is smaller we can sort in O(N) time using a hash table or integer sorting.  If K ≫ log N but elements are unique within O(log N) bits, the remaining bits will not be looked at by either quicksort or quick radix sort.  Failing that, all comparison sorting algorithms will also have the same overhead of looking through O(K) relatively useless bits but quick radix sort will avoid the worst case O(N2) behaviours of standard quicksort and radix quicksort, and will be faster even in the best case of those comparison algorithms under these conditions of uniqueprefix(K) ≫ log N. See Powers[40] for further discussion of the hidden overheads in comparison, radix and parallel sorting.
 In any comparison-based sorting algorithm, minimizing the number of comparisons requires maximizing the amount of information gained from each comparison, meaning that the comparison results are unpredictable.  This causes frequent branch mispredictions, limiting performance.[41] BlockQuicksort[42] rearranges the computations of quicksort to convert unpredictable branches to data dependencies.  When partitioning, the input is divided into moderate-sized blocks (which fit easily into the data cache), and two arrays are filled with the positions of elements to swap.  (To avoid conditional branches, the position is unconditionally stored at the end of the array, and the index of the end is incremented if a swap is needed.) A second pass exchanges the elements at the positions indicated in the arrays.  Both loops have only one conditional branch, a test for termination, which is usually taken.
 The BlockQuicksort technique is incorporated into LLVM's C++ STL implementation, libcxx, providing a 50% improvement on random integer sequences. Pattern-defeating quicksort (pdqsort), a version of introsort, also incorporates this technique.[31]
 Several variants of quicksort exist that separate the k smallest or largest elements from the rest of the input.
 Richard Cole and David C. Kandathil, in 2004, discovered a one-parameter family of sorting algorithms, called partition sorts, which on average (with all input orderings equally likely) perform at most 



n
log
⁡
n
+

O

(
n
)


{\displaystyle n\log n+{O}(n)}

 comparisons (close to the information theoretic lower bound) and 




Θ

(
n
log
⁡
n
)


{\displaystyle {\Theta }(n\log n)}

 operations; at worst they perform 




Θ

(
n

log

2


⁡
n
)


{\displaystyle {\Theta }(n\log ^{2}n)}

 comparisons (and also operations); these are in-place, requiring only additional 




O

(
log
⁡
n
)


{\displaystyle {O}(\log n)}

 space. Practical efficiency and smaller variance in performance were demonstrated against optimised quicksorts (of Sedgewick and Bentley-McIlroy).[43]


Source: https://en.wikipedia.org/wiki/Lisp
Content: A lisp is a  speech impairment in which a person misarticulates sibilants ([s], [z], [ts], [dz], [ʃ], [ʒ], [t͡ʃ], [d͡ʒ]).[1] These misarticulations often result in unclear speech.
 Successful treatments have shown that causes are functional rather than physical: that is, most lisps are caused by errors in tongue placement or density of the tongue within the mouth rather than caused by any injury or congenital or acquired deformity to the mouth. The most frequently discussed of these problems is tongue thrust in which the tongue protrudes beyond the front teeth.[3] This protrusion affects speech as well as swallowing and can lead to lisping. Ankyloglossia or tongue tie can also be responsible for lisps in children — however, it is unclear whether these deficiencies are caused by the tongue tie itself or the muscle weakness following the correction of the tongue tie.[4] Overbites and underbites may also contribute to non lingual lisping. Temporary lisps can be caused by dental work, dental appliances such as dentures or retainers or by swollen or bruised tongues.
 Lisps caused by tongue tie can be treated by a dentist or otolaryngologist (ENT) with a lingual frenectomy, or laser incision, which takes less than 10 to 15 minutes to complete.[5][6][7]
 With an interdental lisp, the therapist teaches the student how to keep the tongue behind the two front incisors.[8]
 One popular method of correcting articulation or lisp disorders is to isolate sounds and work on correcting the sound in isolation. The basic sound, or phoneme, is selected as a target for treatment. Typically the position of the sound within a word is considered and targeted. The sound appears in the beginning of the word, middle, or end of the word (initial, medial, or final).
 Take for example, correction of an "S" sound (lisp). Most likely, a speech language pathologist (SLP) would employ exercises to work on "Sssssss."[clarify] Starting practice words would most likely consist of "S-initial" words such as "say, sun, soap, sip, sick, said, sail." According to this protocol, the SLP slowly increases the complexity of tasks (context of pronunciations) as the production of the sound improves. Examples of increased complexity could include saying words in phrases and sentences, saying longer multi syllabic words, or increasing the tempo of pronunciation.
 Using this method, the SLP achieves success with their student by targeting a sound in a phonetically consistent manner. Phonetic consistency means that a target sound is isolated at the smallest possible level (phoneme, phone, or allophone) and that the context of production must be consistent. Consistency is critical, because factors such as the position within the word, grouping with other sounds (vowels or consonants), and the complexity all may affect production.
 Another popular method for treating a lisp is using specially designed devices that go in the mouth to provide a tactile cue of exactly where the tongue should be positioned when saying the "S" sound. This tactile feedback has been shown to correct lisp errors twice as fast as traditional therapy. 
 Using either or both methods, the repetition of consistent contexts allows the student to align all the necessary processes required to properly produce language; language skills (ability to formulate correct sounds in the brain: What sounds do I need to make?), motor planning (voicing and jaw and tongue movements: How do I produce the sound?), and auditory processing (receptive feedback: Was the sound produced correctly? Do I need to correct?).
 A student with an articulation or lisp disorder has a deficiency in one or more of these areas. To correct the deficiency, adjustments have to be made in one or more of these processes. The process to correct it is more often than not, trial and error. With so many factors, however, isolating the variables (the sound) is imperative to getting to the result faster. 
 A phonetically consistent treatment strategy means practicing the same thing over and over. What is practiced is consistent and does not change. The words might change, but the phoneme and its positioning is the same (say, sip, sill, soap, ...). Thus, successful correction of the disorder is found in manipulating or changing the other factors involved with speech production (tongue positioning, cerebral processing, etc.). Once a successful result (speech) is achieved, then consistent practice becomes essential to reinforcing correct productions.
 When the difficult sound is mastered, the student will then learn to say the sound in syllables, then words, then phrases and then sentences. When a student can speak a whole sentence without lisping, attention is then focused on making correct sounds throughout natural conversation. Towards the end of the course of therapy, the student will be taught how to monitor his or her own speech, and how to correct as necessary. Speech therapy can sometimes fix the problem, but in some cases speech therapy fails to work.


Source: https://en.wikipedia.org/wiki/Ruby_(Programmiersprache)
Content: Other reasons this message may be displayed:


Source: https://en.wikipedia.org/wiki/Groovy
Content: Groovy (or, less commonly, groovie or groovey) is a slang colloquialism popular during the 1950s, '60s and '70s. It is roughly synonymous with words such as "excellent", "fashionable", or "amazing", depending on context.
 The word originated in the jazz culture of the 1920s, in which it referred to the “groove” of a piece of music (its rhythm and “feel”), plus the response felt by its listeners.[1] It can also reference the physical groove of a record in which the pick-up needle runs. Radio disc jockeys would announce playing “good grooves, hot grooves, cool grooves, etc.” when introducing a record about to play.
 Recorded use of the word in its slang context has been found dating back to September 30, 1941, when it was used on the Fibber McGee and Molly radio show; band leader Billy Mills used it to describe his summer vacation. In the 1941 song “Let Me Off Uptown” by Gene Krupa, Anita O’Day invites Roy Eldridge to “… come here Roy and get groovy”. The 1942 film Miss Annie Rooney features a teenage Shirley Temple using the term as she impresses Dickie Moore with her jitterbug moves and knowledge of jive. In the 1945 film A Thousand and One Nights, Phil Silvers uses the term to describe an ostentatiously bejeweled turban.
 It has been found in print as early as 1946, in Really the Blues, the autobiography of jazz saxophonist Mezz Mezzrow.[2] The word appears in advertising spots for the 1947 film Miracle on 34th Street, and in the same year the phrase “Everything’s groovy” was included on a 78 rpm recording of “Open The Door, Richard” sung by Walter Brown with Tiny Grimes Sextet.
 Starting in the 1940s, variations of the word were used in the titles of many popular songs, including:
 An early use of the word is in the trailer to the 1947 film Miracle on 34th Street, which depicts various viewers reactions to the films, wherein a few of the younger viewers use the word “groovy” to describe the film. 
 The term was also part of the title of a TV program called The Groovy Show, which ran from 1967 to 1970.  There was also an American TV cartoon show called Groovie Goolies, which ran from 1970 to 1972.
 By the early 1970s, the word was commonplace in American TV advertisements aimed at young audiences, as exemplified by the slogan "Feeling groovy, just had my Cheerios."
 An early ironic use of the term appears in the title of the 1974 film The Groove Tube, which satirized the American counterculture of the time.
 The term was later used jokingly in films such as Evil Dead II, Army of Darkness, and the Austin Powers films, as well as in the Duke Nukem 3D video game.
 It later made its way into the titles of albums, such as Groovy Decay, a 1982 album by Robyn Hitchcock, and Groovy, Laidback and Nasty, a 1990 album by Cabaret Voltaire.  Examples of band names include Groovy Aardvark from Canada, The Groovy Little Numbers from Scotland, and Groovy Rednecks and the Flamin' Groovies from the US. There was also a band called Groovy Ruby.
 E. B. White used the term in the 1970 novel The Trumpet of the Swan, which takes place in 1968. "'This is real groovy!' cried a boy in the front seat. 'That bird is as good as Louis Armstrong, the famous trumpet player.'"
 Marvel Comics produced a Silver Age comic book entitled Groovy, subtitled "Cartoons, gags, jokes". Only three issues were published, dated March, May and July 1967.


Source: https://en.wikipedia.org/wiki/Perl_(Programmiersprache)
Content: Other reasons this message may be displayed:


Source: https://en.wikipedia.org/wiki/Kommandozeileninterpreter
Content: Other reasons this message may be displayed:


Source: https://en.wikipedia.org/wiki/Unixoid
Content: 
 A Unix-like (sometimes referred to as UN*X or *nix) operating system is one that behaves in a manner similar to a Unix system, although not necessarily conforming to or being certified to any version of the Single UNIX Specification. A Unix-like application is one that behaves like the corresponding Unix command or shell. Although there are general philosophies for Unix design, there is no technical standard defining the term, and opinions can differ about the degree to which a particular operating system or application is Unix-like.
 Some well-known examples of Unix-like operating systems include Linux and BSD. These systems are often used on servers as well as on personal computers and other devices. Many popular applications, such as the Apache web server and the Bash shell, are also designed to be used on Unix-like systems.
 One of the key features of Unix-like systems is their ability to support multiple users and processes simultaneously. This allows users to run multiple programs at the same time and to share resources such as memory and disk space. This is in contrast to many older operating systems, which were designed to only support a single user or process at a time. Another important feature of Unix-like systems is their modularity. This means that the operating system is made up of many small, interchangeable components that can be added or removed as needed. This makes it easy to customize the operating system to suit the needs of different users or environments.
 The Open Group owns the UNIX trademark and administers the Single UNIX Specification, with the "UNIX" name being used as a certification mark. They do not approve of the construction "Unix-like", and consider it a misuse of their trademark. Their guidelines require "UNIX" to be presented in uppercase or otherwise distinguished from the surrounding text, strongly encourage using it as a branding adjective for a generic word such as "system", and discourage its use in hyphenated phrases.[1]
 Other parties frequently treat "Unix" as a genericized trademark. Some add a wildcard character to the name to make an abbreviation like "Un*x"[2] or "*nix", since Unix-like systems often have Unix-like names such as AIX, A/UX, HP-UX, IRIX, Linux, Minix, Ultrix, Xenix, and XNU. These patterns do not literally match many system names, but are still generally recognized to refer to any UNIX system, descendant, or work-alike, even those with completely dissimilar names such as Darwin/macOS, illumos/Solaris or FreeBSD.
 In 2007, Wayne R. Gray sued to dispute the status of UNIX as a trademark, but lost his case, and lost again on appeal, with the court upholding the trademark and its ownership.[3][4]
 "Unix-like" systems started to appear in the late 1970s and early 1980s. Many proprietary versions, such as Idris (1978), UNOS (1982), Coherent (1983), and UniFlex (1985), aimed to provide businesses with the functionality available to academic users of UNIX.
 When AT&T allowed relatively inexpensive commercial binary sublicensing of UNIX in 1979, a variety of proprietary systems were developed based on it, including AIX, HP-UX, IRIX, SunOS, Tru64, Ultrix, and Xenix. These largely displaced the proprietary clones. Growing incompatibility among these systems led to the creation of interoperability standards, including POSIX and the Single UNIX Specification.
 Various free, low-cost, and unrestricted substitutes for UNIX emerged in the 1980s and 1990s, including 4.4BSD, Linux, and Minix. Some of these have in turn been the basis for commercial "Unix-like" systems, such as BSD/OS and macOS. Several versions of (Mac) OS X/macOS running on Intel-based Mac computers have been certified under the Single UNIX Specification.[5][6][7][8][9][10][11] The BSD variants are descendants of UNIX developed by the University of California at Berkeley, with UNIX source code from Bell Labs. However, the BSD code base has evolved since then, replacing all the AT&T code. Since the BSD variants are not certified as compliant with the Single UNIX Specification, they are referred to as "UNIX-like" rather than "UNIX".
 Dennis Ritchie, one of the original creators of Unix, expressed his opinion that Unix-like systems such as Linux are de facto Unix systems.[12] Eric S. Raymond and Rob Landley have suggested that there are three kinds of Unix-like systems:[13]
 Those systems with a historical connection to the AT&T codebase. Most commercial UNIX systems fall into this category. So do the BSD systems, which are descendants of work done at the University of California, Berkeley in the late 1970s and early 1980s. Some of these systems have no original AT&T code but can still trace their ancestry to AT&T designs.
 These systems‍—‌largely commercial in nature‍—‌have been determined by the Open Group to meet the Single UNIX Specification and are allowed to carry the UNIX name. Most such systems are commercial derivatives of the System V code base in one form or another, although Apple macOS 10.5 and later is a BSD variant that has been certified, and EulerOS and Inspur K-UX are Linux distributions that have been certified. A few other systems (such as IBM z/OS) earned the trademark through a POSIX compatibility layer and are not otherwise inherently Unix systems. Many ancient UNIX systems no longer meet this definition.
 Broadly, any Unix-like system that behaves in a manner roughly consistent with the UNIX specification, including having a "program which manages your login and command line sessions";[14] more specifically, this can refer to systems such as Linux or Minix that behave similarly to a UNIX system but have no genetic or trademark connection to the AT&T code base. Most free/open-source implementations of the UNIX design, whether genetic UNIX or not, fall into the restricted definition of this third category due to the expense of obtaining Open Group certification, which costs thousands of dollars.[15]
 Around 2001 Linux was given the opportunity to get a certification including free help from the POSIX chair Andrew Josey for the symbolic price of one dollar.[citation needed] There have been some activities to make Linux POSIX-compliant, with Josey having prepared a list of differences between the POSIX standard and the Linux Standard Base specification,[16] but in August 2005, this project was shut down because of missing interest at the LSB work group.[citation needed]
 Some non-Unix-like operating systems provide a Unix-like compatibility layer, with varying degrees of Unix-like functionality.
 Other means of Windows-Unix interoperability include:


Source: https://en.wikipedia.org/wiki/Betriebssystem
Content: Other reasons this message may be displayed:


Source: https://en.wikipedia.org/wiki/Kommandozeile
Content: Other reasons this message may be displayed:


Source: https://en.wikipedia.org/wiki/IPython
Content: 
 IPython (Interactive Python) is a command shell for interactive computing in multiple programming languages, originally developed for the Python programming language, that offers introspection, rich media, shell syntax, tab completion, and history. IPython provides the following features:
 IPython is a NumFOCUS fiscally sponsored project.[3]
 IPython is based on an architecture that provides parallel and distributed computing. IPython enables parallel applications to be developed, executed, debugged and monitored interactively, hence the I (Interactive) in IPython.[4] This architecture abstracts out parallelism, enabling IPython to support many different styles of parallelism[5] including:
 With the release of IPython 4.0, the parallel computing capabilities were made optional and released under the ipyparallel python package. And most of the capabilities of ipyparallel are now covered by more mature libraries like Dask.
 IPython frequently draws from SciPy stack[6] libraries like NumPy and SciPy, often installed alongside one of many Scientific Python distributions.[6] IPython provides integration with some libraries of the SciPy stack, notably matplotlib, producing inline graphs when used with the Jupyter notebook. Python libraries can implement IPython specific hooks to customize rich object display. SymPy for example implements rendering of mathematical expressions as rendered LaTeX when used within IPython context, and Pandas dataframe use an HTML representation.[7]
 IPython allows non-blocking interaction with Tkinter, PyGTK, PyQt/PySide and wxPython (the standard Python shell only allows interaction with Tkinter). IPython can interactively manage parallel computing clusters using asynchronous status callbacks and/or MPI. IPython can also be used as a system shell replacement.[8] Its default behavior is largely similar to Unix shells, but it allows customization and the flexibility of executing code in a live Python environment.
 IPython 5.x (Long Time Support) series is the last version of IPython to support Python 2. The IPython project pledged to not support Python 2 beyond 2020[9] by being one of the first projects to join the Python 3 Statement, the 6.x series is only compatible with Python 3 and above. It is still possible though to run an IPython kernel and a Jupyter Notebook server on different Python versions allowing users to still access Python 2 on newer version of Jupyter.
 In 2014, IPython creator Fernando Pérez announced a spin-off project from IPython called Project Jupyter.[10] IPython continued to exist as a Python shell and kernel for Jupyter, but the notebook interface and other language-agnostic parts of IPython were moved under the Jupyter name.[11][12] Jupyter is language agnostic and its name is a reference to core programming languages supported by Jupyter, which are Julia, Python, and R.[13]
 Jupyter Notebook (formerly IPython Notebook) is a web-based interactive computational environment for creating, executing, and visualizing Jupyter notebooks. It is similar to the notebook interface of other programs such as Maple, Mathematica, and SageMath, a computational interface style that originated with Mathematica in the 1980s.[14] It supports execution environments (aka kernels) in dozens of languages. By default Jupyter Notebook ships with the IPython kernel, but there are over 100 Jupyter kernels  as of May 2018.
 IPython has been mentioned in the popular computing press and other popular media,[15][14] and it has a presence at scientific conferences.[16] For scientific and engineering work, it is often presented as a companion tool to matplotlib.[17]
 Beginning 1 January 2013, the Alfred P. Sloan Foundation announced that it would support IPython development for two years.[18]
 On 23 March 2013, Fernando Perez was awarded the Free Software Foundation Advancement of Free Software award for IPython.
 In August 2013, Microsoft made a donation of $100,000 to sponsor IPython's continued development.[19]
 In January 2014, it won the Jolt Productivity Award[20] from Dr. Dobb's in the category of coding tools. In July 2015, the project won a funding of $6 million from Gordon and Betty Moore Foundation, Alfred P. Sloan Foundation and  Leona M. and Harry B. Helmsley Charitable Trust.[21]
 In May 2018, it was awarded the 2017 ACM Software System Award.[22] It is the largest team to have won the award.[23]


Source: https://en.wikipedia.org/wiki/Interpreter
Content: Interpreting is a translational activity in which one produces a first and final target-language output on the basis of a one-time exposure to an expression in a source language.
 The most common two modes of interpreting are simultaneous interpreting, which is done at the time of the exposure to the source language, and consecutive interpreting, which is done at breaks to this exposure.
 Interpreting is an ancient human activity which predates the invention of writing.[1] However, the origins of the profession of interpreting date back to less than a century ago.[when?][2]
 Research into the various aspects of the history of interpreting is quite new.[3] For as long as most scholarly interest was given to professional conference interpreting, very little academic work was done on the practice of interpreting in history, and until the 1990s, only a few dozen publications were done on it.[4]
 Considering the amount of interpreting activities that is assumed to have occurred for thousands of years, historical records are limited.[5] Moreover, interpreters and their work have usually not found their way into the history books.[6] One of the reasons for that is the dominance of the written text over the spoken word (in the sense that those who have left written texts are more likely to be recorded by historians).[3][4] Another problem is the tendency to view it as an ordinary support activity which does not require any special attention,[4] and the social status of interpreters, who were sometimes treated unfairly by scribes, chroniclers and historians.[note 1][3]
 Our knowledge of the past of interpreting tends to come from letters, chronicles, biographies, diaries and memoirs, along with a variety of other documents and literary works, many of which (and with few exceptions) were only incidentally or marginally related to interpreting.[6][4]
 Many Indo-European languages have words for interpreting and interpreter.[1] Expressions in Germanic, Scandinavian and Slavic languages denoting an interpreter can be traced back to Akkadian, around 1900 BCE.[1] The Akkadian root targumânu/turgumânu also gave rise to the term dragoman via an etymological sideline from Arabic.[7]
 The English word interpreter, however, is derived from Latin interpres (meaning 'expounder', 'person explaining what is obscure'), whose semantic roots are not clear.[8] Some scholars take the second part of the word to be derived from partes or pretium (meaning 'price', which fits the meaning of a 'middleman', 'intermediary' or 'commercial go-between'), but others have suggested a Sanskrit root.[8]
 In consecutive interpreting (CI), the interpreter starts to interpret after the speaker pauses; thus much more time (perhaps double) is needed. Customarily, such an interpreter will sit or stand near the speaker.[9]
 Consecutive interpretation can be conducted in a pattern of short or long segments according to the interpreter's preference. In short CI, the interpreter relies mostly on memory whereas, in long CI, most interpreters will rely on note-taking. The notes must be clear and legible in order to not waste time on reading them.[10] Consecutive interpreting of whole thoughts, rather than in small pieces, is desirable so that the interpreter has the whole meaning before rendering it in the target language. This affords a truer, more accurate, and more accessible interpretation than where short CI or simultaneous interpretation is used.
 An attempt at consensus about lengths of segments may be reached prior to commencement, depending upon complexity of the subject matter and purpose of the interpretation, though speakers generally face difficulty adjusting to unnatural speech patterns.[citation needed]
 On occasion, document sight translation is required of the interpreter during consecutive interpretation work. Sight translation combines interpretation and translation; the interpreter must render the source-language document to the target-language as if it were written in the target language. Sight translation occurs usually, but not exclusively, in judicial and medical work.
 Consecutive interpretation may be the chosen mode when bilingual listeners are present who wish to hear both the original and interpreted speech or where, as in a court setting, a record must be kept of both.[citation needed]
 When no interpreter is available to interpret directly from source to target, an intermediate interpreter will be inserted in a relay mode, e.g. a Greek source language could be interpreted into English and then from English to another language. This is also commonly known as double-interpretation. Triple-interpretation may even be needed, particularly where rare languages or dialects are involved. Such interpretation can only be effectively conducted using consecutive interpretation.
 Simultaneous interpretation (SI) has the disadvantage that if a person is performing the service the interpreter must do the best they can within the time permitted by the pace of source speech. However they also have the advantages of saving time and not disturbing the natural flow of the speaker.  SI can also be accomplished by software where the program can simultaneously listen to incoming speech and speak the associated interpretation. The most common form is extempore SI, where the interpreter does not know the message until they hear it.
 Simultaneous interpretation using electronic equipment where the interpreter can hear the speaker's voice as well as the interpreter's own voice was introduced at the Nuremberg trials in 1945.[11] The equipment facilitated large numbers of listeners, and interpretation was offered in French, Russian, German and English.[12] The technology arose in the 1920s and 1930s when American businessman Edward Filene and British engineer Alan Gordon Finlay developed simultaneous interpretation equipment with IBM.[13] Yvonne Kapp attended a conference with simultaneous interpretation in 1935 in the Soviet Union.[14] As it proved successful, IBM was able to sell the equipment to the United Nations, where it is now widely used in the United Nations Interpretation Service.
 In the ideal setting for oral language, the interpreter sits in a sound-proof booth and speaks into a microphone, while clearly seeing and hearing the source-language speaker via earphones. The simultaneous interpretation is rendered to the target-language listeners via their earphones.
 Pavel Palazchenko's My Years with Gorbachev and Shevardnadze: The Memoir of a Soviet Interpreter gives a short history of modern interpretation and of the transition from its consecutive to simultaneous forms. He explains that during the nineteenth century interpreters were rarely needed during European diplomatic discussions; these were routinely conducted in French, and all government diplomats were required to be fluent in this language.  Most European government leaders and heads of state could also speak French.[15] Historian Harold Nicolson attributes the growing need for interpretation after World War I to the fact that U.S. President Woodrow Wilson and British Prime Minister David Lloyd George "were no linguists".[16] At the time, the concept and special equipment needed for simultaneous interpretation, later patented by Alan Gordon Finlay, had not been developed, so consecutive interpretation was used.[15]
 Consecutive interpreters, in order be accurate, used a specialized system of note-taking which included symbols abbreviations and acronyms.  Because they waited until the speaker was finished to provide interpretation, the interpreters then had the difficult task of creating from these notes as much as half an hour of free-flowing sentences closely matching the speaker's meaning. Palazchenko cites Anton Velleman [de], Jean Herbert and the Kaminker brothers as skilled interpreters, and notes one unusual case in which André Kaminker interpreted a speech by a French diplomat who spoke for two and a half hours without stopping.[15]
 After World War II, simultaneous interpretation came into use at the Nuremberg trial, and began to be more accepted. Experienced consecutive interpreters asserted that the difficulties of listening and speaking at the same time, adjusting for differences in sentence structure between languages, and interpreting the beginning of a sentence before hearing its end, would produce an inferior result.  As well, these interpreters, who to that point had been prominent speakers, would now be speaking invisibly from booths.[15]
 In 1951, when the United Nations expanded its number of working languages to five (English, French, Russian, Chinese and Spanish), consecutive interpretation became impractical in most cases, and simultaneous interpretation became the most common process for the organization's large meetings.[17] Consecutive interpretation, which provides a more fluent result without the need for specialized equipment, continued to be used for smaller discussions.[15]
 Stemming from the field of computer-assisted translation, the field of computer-assisted interpretation has emerged, with dedicated tools integrating glossaries and automated speech recognition.[18][19]
 Whispered interpretation is known in the trade by the French term chuchotage. To avoid disturbing the participants using the original language, the interpreter speaks to a few people at close proximity with normal voiced delivery at a very low volume, or through electronic equipment without the benefit of a soundproof booth. Typically, no actual whispering is involved as this is difficult to decipher, causes postural fatigue while parties lean in to one another, and straining to be heard at a whisper "can be as bad for your voice as shouting."[20]
 Conference interpreting refers to interpretation at a conference or large meeting, either simultaneously or consecutively. The advent of multi-lingual meetings has reduced the amount of consecutive interpretation in the last 20 years.
 Conference interpretation is divided between two markets: institutional and private. International institutions (EU, UN, EPO, et cetera), which hold multilingual meetings, often favor interpreting several foreign languages into the interpreters' mother tongues. Local private markets tend to have bilingual meetings (the local language plus another), and the interpreters work both into and out of their mother tongues. These markets are not mutually exclusive. The International Association of Conference Interpreters (AIIC) is the only worldwide association of conference interpreters. Founded in 1953, its membership includes more than 2,800 professional conference interpreters, in more than 90 countries.
 Judicial, legal, or court interpreting occurs in courts of justice, administrative tribunals, and wherever a legal proceeding is held (i.e., a police station for an interrogation, a conference room for a deposition, or the locale for taking a sworn statement). Legal interpreting can be the consecutive interpretation of witnesses' testimony, for example, or the simultaneous interpretation of entire proceedings, by electronic means, for one person, or all of the people attending. In a legal context, where ramifications of misinterpretation may be dire, accuracy is paramount. Teams of two or more interpreters, with one actively interpreting and the second monitoring for greater accuracy, may be deployed.
 The right to a competent interpreter for anyone who does not understand the language of the court (especially for the accused in a criminal trial) is usually considered a fundamental rule of justice. Therefore, this right is often guaranteed in national constitutions, declarations of rights, fundamental laws establishing the justice system or by precedents set by the highest courts. However, it is not a constitutionally required procedure (in the United States) that a certified interpreter be present at police interrogation.[21] This has been especially controversial in cases where illegal immigrants with no English skills are accused of crimes.
 In the US, depending upon the regulations and standards adhered to per state and venue, court interpreters usually work alone when interpreting consecutively, or as a team, when interpreting simultaneously. In addition to practical mastery of the source and target languages, thorough knowledge of law and legal and court procedures is required of court interpreters. They are often required to have formal authorization from the state to work in the courts – and then are called certified court interpreters.[note 2] In many jurisdictions, the interpretation is considered an essential part of the evidence. Incompetent interpretation, or simply failure to swear in the interpreter, can lead to a mistrial.
 In escort interpreting, an interpreter accompanies a person or a delegation on a tour, on a visit, or to a business meeting or interview. An interpreter in this role is called an escort interpreter or an escorting interpreter. An escort interpreter's work session may run for days, weeks, or even months, depending on the period of the client's visit. This type of interpreting is often needed in business contexts, during presentations, investor meetings, and business negotiations. As such, an escort interpreter needs to be equipped with some business and financial knowledge in order to best understand and convey messages back and forth.
 Signed language interpreters typically refer to this role as a "designated interpreter."[22] It is not a new practice; since the 1960s, deaf professionals and academics such as Robert Sanderson[23] increasingly sought out and trained specific interpreters to work with on a regular, if not exclusive basis.
 Also known as community interpreting, is the type of interpreting occurring in fields such as legal, health, and federal and local government, social, housing, environmental health, education, and welfare services. In community interpreting, factors exist which determine and affect language and communication production, such as speech's emotional content, hostile or polarized social surroundings, its created stress, the power relationships among participants, and the interpreter's degree of responsibility – in many cases more than extreme; in some cases, even the life of the other person depends upon the interpreter's work.
 Medical interpreting is a subset of public service interpreting, consisting of communication among healthcare personnel and the patient and their family or among Healthcare personnel speaking different languages, facilitated by an interpreter, usually formally educated and qualified to provide such interpretation services. In some situations, medical employees who are multilingual may participate part-time as members of internal language banks.[24] Depending on country/state-specific requirements, the interpreter is often required to have some knowledge of medical terminology, common procedures, the patient interview and exam process. Medical interpreters are often cultural liaisons for people (regardless of language) who are unfamiliar with or uncomfortable in hospital, clinical, or medical settings.
 For example, in China, there is no mandatory certificate for medical interpreters as of 2012. Most interpretation in hospitals in China is done by doctors, who are proficient in both Chinese and English (mostly) in his/her specialty. They interpret more in academic settings than for communications between doctors and patients. When a patient needs English language service in a Chinese hospital, more often than not the patient will be directed to a staff member in the hospital, who is recognized by his/her colleagues as proficient in English. The actual quality of such service for patients or medical interpretation for communications between doctors speaking different languages is unknown by the interpreting community as interpreters who lack Healthcare background rarely receive accreditation for medical interpretation in the medical community. Interpreters working in the Healthcare setting may be considered Allied Health Professionals.
 In the United States, language access is a socioeconomic disparity, and language access to federally-funded health services is required by law. Title VI of the Civil Rights Act of 1964 prohibits discrimination on the basis of race, color, or national origin in any program or activity that receives Federal funds or other Federal financial assistance.[25] Hospital systems and clinics that are funded by federal programs, such as Medicare, are required by this law to take reasonable steps towards ensuring equitable access to health services for limited English proficient patients.
 Interpreters are often used in a military context, carrying out interpretation usually either during active military combat or during noncombat operations. Interpretation is one of the main factors in multi-national and multi-lingual cooperation and military cohesion of the military and civilian populations.
 During inactive military operations, the most common goal of military interpreters is to increase overall cohesion in the military unit, and with the civilian population. One of the primary forces behind the feeling of an occupation is a lack of mutual intelligibility. During the War in Afghanistan, the use of American soldiers that did not speak the languages of Afghanistan, and the primary recruitment from northern Afghanistan, primarily Tajiks, led to a feeling of the United States and Tajik forces as an occupying force.[26] This feeling was most common in majority Pashtun areas of the country, which in turn was one of the main causes of the Taliban's resurgence. If interpreters are not present inside war zones, it becomes extremely common for misunderstandings from the civilian population and a military force to spiral into an open conflict, or to produce animosity and distrust, forming the basis of a conflict or an insurgency.[27]
 Military interpreters are commonly found in Iraq and have been largely effective, particularly in the Kurdish held regions (Kurdistan Regional Government), during the fighting against ISIS. Military interpreters were the primary drivers in cooperation between the coalition and the Iraqi population and military. Likewise managing to produce stability in areas held by the coalition, Kurdish interpreters were known for being a primary aid in this endeavour.[28]
 The fundamental act of interpreting during active combat is extremely stressful and dangerous. It is, however, necessary when different-language battalions are fighting together with no common intermediate language. Misunderstandings in this context are most often fatal, the most common misinterpretations are positioning and attempted break outs. In the chaos of combat, however, it can be very easy to make a mistake in interpreting, particularly with the immense noise and changing locations.[29]
 Military interpreters are also used within single armies instead of multi-lingual cooperation. In this context, a military interpreter is usually a given job in each unit. Common examples include Bosnia, Pakistan, Switzerland, and South Africa. This use of assigning soldiers with different languages to a single battalion helps reinforce a feeling of unity in the military force.[30][31]
 For an historical example, see also Linguistics and translations in the Austro-Hungarian Army.
 A sign language interpreter conveys messages between combinations of spoken and signed languages and manual systems. This may be between deaf signers and hearing nonsigners, or among users of different signed languages and manual systems.[32][33] This may be done in simultaneous or consecutive modes, or as sight translation from printed text.
 Interpreters may be hearing, hard of hearing, or deaf, and work in teams of any combination, depending upon the circumstance or audience. Historically, deaf interpreters or DIs work with DeafBlind people who use either close vision or Protactile signing, deaf people with nonstandard, emerging, or idiolect language varieties, affinity or cultural groups within the Deaf community, minors, immigrants of a different signed language, users of a minority signed language, participants in medical, carceral, or legal matters, and persons with cognitive or intellectual disabilities.[34] DIs may work in relay teams with hearing interpreters, from a teleprompter, or with another DI to access the source language. DIs are commonly the member of the team visible on camera or on stage at televised, recorded, or public events.
 Interpreters can be formally trained in postsecondary programs and receive a certificate, associates, bachelors, masters, or doctoral degree.[35][36] In some circumstances, lay interpreters take an experiential route through churches, families, and social networks. Formal interpreter education practices are largely the product of 20th century developments.[37]
 In the United States, Sign Language interpreters have National and some states have a State level certifications. The Registry of Interpreters for the Deaf (RID), a non-profit organization, is known for its national recognition and certification process. In addition to training requirements and stringent certification testing, RID members must abide by a Code of Professional Conduct, Grievance Process and Continuing Education Requirement. There are many interpreter-training programs in the U.S. The Collegiate Commission on Interpreter Education is the body that accredits Interpreter Preparation Programs. A list of accredited programs can be found on the CCIE web site.[38]
 Some countries have more than one national association due to regional or language differences.[39] National associations can become members of the umbrella organizations, the World Association of Sign Language Interpreters[40] or the European Forum of Sign Language Interpreters (efsli).[41] In Canada, the professional association that recognizes and nationally certifies sign language interpreters is the Association of Visual Language Interpreters of Canada (AVLIC). Under AVLIC holds several affiliate chapters representing a specified region of Canada.[42]
 Sign language interpreters encounter a number of linguistic, environmental, interpersonal and intrapersonal factors that can have an effect on their ability to provide accurate interpretation. Studies have found that most interpreter training programs do not sufficiently prepare students for the highly variable day-to-day stresses that an interpreter must manage, and there is an ongoing conversation in the interpreting field as to how to appropriately prepare students for the challenges of the job. Proposed changes include having a more robust definition of what a qualified interpreter should know, as well as a post-graduate internship structure that would allow new interpreters to work with the benefit of supervision from more experienced interpreters, much like the programs in place in medicine, law enforcement, etc.[43]
 In Israel, Naama Weiss, a board member of Malach, the Organization of the Israeli Sign Language Interpreters,[44] advertised a video which she produced. It was her paraphrase of the video So-Low,[45] and showed her viewpoint upon the Israeli Sign Language interpreters' jobs.[46] A study which was made in Finland found that, in comparison to the foreign language teachers and non-linguistic experts, a high cooperativeness was found to be more characteristic to simultaneous and consecutive interpreters,[47] and Weiss showed it in her video, although she claimed to be comic.[48]
 The World Federation of the Deaf asserts that computer-generated signing avatars "do not surpass the natural quality and skill provided by appropriately trained and qualified interpreters," and approves their application only "for pre-recorded static customer information, for example, in hotels or train stations".[49] The WFD statement concedes to such a project only if "deaf people have been involved in advising," and it does not intend to replace human interpreters. Quality and naturalness of movements are closely critiqued by sign-fluent viewers, particularly those who began signing at a younger age.[50]
 By its very nature, media interpreting has to be conducted in the simultaneous mode. It is provided particularly for live television coverages such as press conferences, live or taped interviews with political figures, musicians, artists, sportsmen or people from the business circle. In this type of interpreting, the interpreter has to sit in a sound-proof booth where ideally he/she can see the speakers on a monitor and the set. All equipment should be checked before recording begins. In particular, satellite connections have to be double-checked to ensure that the interpreter's voice is not sent back and the interpreter gets to hear only one channel at a time. In the case of interviews recorded outside the studio and some current affairs program, the interpreter interprets what they hear on a TV monitor. Background noise can be a serious problem. The interpreter working for the media has to sound as slick and confident as a television presenter.
 Media interpreting has gained more visibility and presence especially after the Gulf War. Television channels have begun to hire staff simultaneous interpreters. The interpreter renders the press conferences, telephone beepers, interviews and similar live coverage for the viewers. It is more stressful than other types of interpreting as the interpreter has to deal with a wide range of technical problems coupled with the control room's hassle and wrangling during live coverage.
 Interpreting services can be delivered in multiple modalities. The most common modality through which interpreting services are provided is on-site interpreting.
 Also called "in-person" and "face-to-face" or "F2F" interpreting, this traditional method requires the interpreter be physically present. With the growth of remote settings, having interpreters on-site remains crucial in high-stakes medical, legal, and diplomatic situations, and with socially, intellectually, or emotionally vulnerable clients.[51]
 Also referred to as "over-the-phone interpreting", "telephonic interpreting", and "tele-interpreting", telephone interpreting enables interpretation via telephone. Telephone interpreting can be used in community settings as well as conference settings. Telephone interpreting may be used in place of on-site interpreting when no on-site interpreter is readily available at the location where services are needed. However, it is more commonly used for situations in which all parties who wish to communicate are already speaking to one another via telephone (e.g. telephone applications for insurance or credit cards, or telephone inquiries from consumers to businesses).
 Interpretation services via Video Remote Interpreting (VRI) or a Video Relay Service (VRS) are useful for spoken language barriers where visual-cultural recognition is relevant, and even more applicable where one of the parties is deaf, hard-of-hearing or speech-impaired (mute). In such cases the direction of interpretation is normally within the same principal language, such as French Sign Language (FSL) to spoken French and Spanish Sign Language (SSL) to spoken Spanish. Multilingual sign language interpreters, who can also interpret as well across principal languages (such as to and from SSL, to and from spoken English), are also available, albeit less frequently. Such activities involve considerable effort on the part of the interpreter, since sign languages are distinct natural languages with their own construction and syntax, different from the aural version of the same principal language.
 With video interpreting, sign language interpreters work remotely with live video and audio feeds, so that the interpreter can see the deaf or mute party, converse with the hearing party and vice versa. Much like telephone interpreting, video interpreting can be used for situations in which no on-site interpreters are available. However, video interpreting cannot be used for situations in which all parties are speaking via telephone alone. VRI and VRS interpretation requires all parties to have the necessary equipment. Some advanced equipment enables interpreters to control the video camera, in order to zoom in and out, and to point the camera toward the party that is signing.
 The majority of professional full-time conference interpreters work for phone interpreting agencies, health care institutions, courts, school systems and international organizations like the United Nations (for the United Nations Interpretation Service), the European Union, or the African Union.
 The world's largest employer of interpreters is currently the European Commission,[52] which employs hundreds of staff and freelance interpreters working into the official languages of the European Union and some others in DG Interpretation. In 2016, Florika Fink-Hooijer was appointed as Director General and the first ever Knowledge Centre on Interpretation was created.[53] She had spoken about the need to "futureproof" services by strengthening the skills of colleagues to work with new technologies.' as well as how Artificial Intelligence may be an (un)desired revolution in linguistic services.[54][55] Subsequently, she drove forward the digitalization of the service by introducing features like automatic speech recognition and other support services to interpreters.[56] During the COVID-19 pandemic, she scaled up multilingual interpretation in hybrid meetings via new digital platforms and technologies, which was a "watershed moment" for the interpretation profession.[57]
 The European Union's other institutions (the European Parliament and the European Court of Justice) have smaller interpreting services.
 The United Nations employs interpreters at almost all its sites throughout the world. Because it has only six official languages, however, it is a smaller employer than the European Union.
 Interpreters may also work as freelance operators in their local, regional and national communities, or may take on contract work under an interpreting business or service. They would typically take on work as described above.
 Militaries often use interpreters to better communicate with the local population. One notable example is the US military during the war in Iraq and Afghanistan.
 There are a number of interpreting and translation associations around the world, including NAATI (National Accreditation Authority for Translators and Interpreters), AIIC (The International Association of Conference Interpreters), CATTI (China Accreditation Test for Translators and Interpreters), CTTIC (Canadian Translators, Terminologists and Interpreters Council), the Institute of Translation & Interpreting, ADICA (Argentinian International Association of Conference and Interpreters) and TAALS (The American Association of Language Specialists).
 No worldwide testing or certification agency exists for all types of interpreters. For conference interpretation, there is the International Association of Conference Interpreters, or AIIC.
 Specific regions, countries, or even cities will have their own certification standards. In many cases, graduates of a certain caliber university program acts as a de facto certification for conference interpretation.
 The most recognized interpretation and translation certificate in P.R.C. is China Accreditation Test for Translation and Interpretation, or CATTI.
It is entrusted by the Ministry of Human Resources and Social Security of P.R.C. It is a translation and interpretation professional qualification accreditation test which is implemented throughout the country according to uniform standards, in order to assess examinees' bilingual translation or interpretation capability.
CATTI was introduced in 2003. In later 2013, translation and interpreting tests of different levels in English, French, Japanese, Russian, German, Spanish and Arabic were held across the nation.
 Those examinees who pass CATTI and obtain translation and interpretation certificates acquire corresponding translation and interpretation professional titles.
 Relevant institutions from Australia, France, Japan, the Republic of Korea, Singapore and other countries as well as Hong Kong Special Administrative Region and Taiwan have established work ties with CATTI.
 In Germany, anyone can become and call themselves an interpreter; access to this profession is not regulated, but court interpreters must be sworn in and prove their qualifications, e.g. through a recognized certificate or professional experience of several years.[58]
 In order to learn and practice the necessary skills, colleges and universities offer studies in Translation and/or Interpretation Studies, primarily to/from English, but there are also Sign Language Interpretation studies.[58]
Admission to higher education, however, is highly restricted.
 Some states offer a State Examination title Staatlich geprüfter Dolmetscher.
Unlike a bachelor's or master's degree, this certificate merely certifies professional skills.
Access to the exam is far easier, but requires proof of the necessary skills.
For that, there are private schools that offer preparatory courses.
Attending these schools is usually sufficient to prove someone's aptitude.[58]
Of course, a university or college degree is accepted, too.
 Furthermore, the State Examination is offered in many more languages, including German Sign Language, yet primarily to/from German.


Source: https://en.wikipedia.org/wiki/Java_(Programmiersprache)
Content: Other reasons this message may be displayed:


Source: https://en.wikipedia.org/wiki/Jython
Content: 
 Jython is an implementation of the Python programming language designed to run on the Java platform. It was known as JPython until 1999.[3]
 Jython programs can import and use any Java class. Except for some standard modules, Jython programs use Java classes instead of Python modules. Jython includes almost all of the modules in the standard Python programming language distribution, lacking only some of the modules implemented originally in C. For example, a user interface in Jython could be written with Swing, AWT or SWT. Jython compiles Python source code to Java bytecode (an intermediate language) either on demand or statically.
 Jython was initially created in late 1997 to replace C with Java for performance-intensive code accessed by Python programs, moving to SourceForge in October 2000. The Python Software Foundation awarded a grant in January 2005.  Jython 2.5 was released in June 2009.[4]
 The most recent release is Jython 2.7.3. It was released on September 10, 2022 and is compatible with Python 2.7.[5]
 Python 3 compatible changes are planned in Jython 3 Roadmap.[6]
 Although Jython implements the Python language specification, it has some differences and incompatibilities with CPython, which is the reference implementation of Python.[7][8]
 From version 2.2 on, Jython (including the standard library) is released under the Python Software Foundation License (v2). Older versions are covered by the Jython 2.0, 2.1 license and the JPython 1.1.x Software License.[9]
 The command-line interpreter is available under the Apache Software License.


Source: https://en.wikipedia.org/wiki/Java-Laufzeitumgebung
Content: Other reasons this message may be displayed:


Source: https://en.wikipedia.org/wiki/Compiler
Content: 
 In computing, a compiler is a computer program that translates computer code written in one programming language (the source language) into another language (the target language). The name "compiler" is primarily used for programs that translate source code from a high-level programming language to a low-level programming language (e.g. assembly language, object code, or machine code) to create an executable program.[1][2]: p1 [3]
 There are many different types of compilers which produce output in different useful forms. A cross-compiler  produces code for a different CPU or operating system than the one on which the cross-compiler itself runs.  A bootstrap compiler is often a temporary compiler, used for compiling a more permanent or better optimised compiler for a language.
 Related software include decompilers, programs that translate from low-level languages to higher level ones; programs that translate between high-level languages, usually called source-to-source compilers or transpilers; language rewriters, usually programs that translate the form of expressions without a change of language; and compiler-compilers, compilers that produce compilers (or parts of them), often in a generic and reusable way so as to be able to produce many differing compilers.
 A compiler is likely to perform some or all of the following operations, often called phases: preprocessing, lexical analysis, parsing, semantic analysis (syntax-directed translation), conversion of input programs to an intermediate representation, code optimization and machine specific code generation. Compilers generally implement these phases as modular components, promoting efficient design and correctness of transformations of source input to target output. Program faults caused by incorrect compiler behavior can be very difficult to track down and work around; therefore, compiler implementers invest significant effort to ensure compiler correctness.[4]
 Compilers are not the only language processor used to transform source programs. An interpreter is computer software that transforms and then executes the indicated operations.[2]: p2  The translation process influences the design of computer languages, which leads to a preference of compilation or interpretation. In theory, a programming language can have both a compiler and an interpreter. In practice, programming languages tend to be associated with just one (a compiler or an interpreter).
 Theoretical computing concepts developed by scientists, mathematicians, and engineers formed the basis of digital modern computing development during World War II. Primitive binary languages evolved because digital devices only understand ones and zeros and the circuit patterns in the underlying machine architecture. In the late 1940s, assembly languages were created to offer a more workable abstraction of the computer architectures. Limited memory capacity of early computers led to substantial technical challenges when the first compilers were designed. Therefore, the compilation process needed to be divided into several small programs. The front end programs produce the analysis products used by the back end programs to generate target code. As computer technology provided more resources, compiler designs could align better with the compilation process.
 It is usually more productive for a programmer to use a high-level language, so the development of high-level languages followed naturally from the capabilities offered by digital computers. High-level languages are formal languages that are strictly defined by their syntax and semantics which form the high-level language architecture. Elements of these formal languages include:
 The sentences in a language may be defined by a set of rules called a grammar.[5]
 Backus–Naur form (BNF) describes the syntax of "sentences" of a language. It was developed by John Backus and used for the syntax of Algol 60.[6] The ideas derive from the context-free grammar concepts by linguist Noam Chomsky.[7] "BNF and its extensions have become standard tools for describing the syntax of programming notations. In many cases, parts of compilers are generated automatically from a BNF description."[8]
 Between 1942 and 1945, Konrad Zuse designed the first (algorithmic) programming language for computers called Plankalkül ("Plan Calculus").  Zuse also envisioned a Planfertigungsgerät ("Plan assembly device") to automatically translate the mathematical formulation of a program into machine-readable punched film stock.[9] While no actual implementation occurred until the 1970s, it presented concepts later seen in APL designed by Ken Iverson in the late 1950s.[10] APL is a language for mathematical computations.
 Between 1949 and 1951, Heinz Rutishauser proposed Superplan, a high-level language and automatic translator.[11] His ideas were later refined by Friedrich L. Bauer and Klaus Samelson.[12]
 High-level language design during the formative years of digital computing provided useful programming tools for a variety of applications:
 Compiler technology evolved from the need for a strictly defined transformation of the high-level source program into a low-level target program for the digital computer. The compiler could be viewed as a front end to deal with the analysis of the source code and a back end to synthesize the analysis into the target code. Optimization between the front end and back end could produce more efficient target code.[16]
 Some early milestones in the development of compiler technology:
 Early operating systems and software were written in assembly language. In the 1960s and early 1970s, the use of high-level languages for system programming was still controversial due to resource limitations. However, several research and industry efforts began the shift toward high-level systems programming languages, for example, BCPL, BLISS, B, and C.
 BCPL (Basic Combined Programming Language) designed in 1966 by Martin Richards at the University of Cambridge was originally developed as a compiler writing tool.[28] Several compilers have been implemented, Richards' book provides insights to the language and its compiler.[29] BCPL was not only an influential systems programming language that is still used in research[30] but also provided a basis for the design of B and C languages.
 BLISS (Basic Language for Implementation of System Software) was developed for a Digital Equipment Corporation (DEC) PDP-10 computer by W. A. Wulf's Carnegie Mellon University (CMU) research team. The CMU team went on to develop BLISS-11 compiler one year later in 1970.
 Multics (Multiplexed Information and Computing Service), a time-sharing operating system project, involved MIT, Bell Labs, General Electric (later Honeywell) and was led by Fernando Corbató from MIT.[31] Multics was written in the PL/I language developed by IBM and IBM User Group.[32] IBM's goal was to satisfy business, scientific, and systems programming requirements. There were other languages that could have been considered but PL/I offered the most complete solution even though it had not been implemented.[33] For the first few years of the Multics project, a subset of the language could be compiled to assembly language with the Early PL/I (EPL) compiler by Doug McIlory and Bob Morris from Bell Labs.[34] EPL supported the project until a boot-strapping compiler for the full PL/I could be developed.[35]
 Bell Labs left the Multics project in 1969, and developed a system programming language B based on BCPL concepts, written by Dennis Ritchie and Ken Thompson. Ritchie created a boot-strapping compiler for B and wrote Unics (Uniplexed Information and Computing Service) operating system for a PDP-7 in B. Unics eventually became spelled Unix.
 Bell Labs started the development and expansion of C based on B and BCPL. The BCPL compiler had been transported to Multics by Bell Labs and BCPL was a preferred language at Bell Labs.[36] Initially, a front-end program to Bell Labs' B compiler was used while a C compiler was developed. In 1971, a new PDP-11 provided the resource to define extensions to B and rewrite the compiler. By 1973 the design of C language was essentially complete and the Unix kernel for a PDP-11 was rewritten in C. Steve Johnson started development of Portable C Compiler (PCC) to support retargeting of C compilers to new machines.[37][38]
 Object-oriented programming (OOP) offered some interesting possibilities for application development and maintenance. OOP concepts go further back but were part of LISP and Simula language science.[39] Bell Labs became interested in OOP with the development of C++.[40] C++ was first used in 1980 for systems programming. The initial design leveraged C language systems programming capabilities with Simula concepts. Object-oriented facilities were added in 1983.[41] The Cfront program implemented a C++ front-end for C84 language compiler. In subsequent years several C++ compilers were developed as C++ popularity grew.
 In many application domains, the idea of using a higher-level language quickly caught on. Because of the expanding functionality supported by newer programming languages and the increasing complexity of computer architectures, compilers became more complex.
 DARPA (Defense Advanced Research Projects Agency) sponsored a compiler project with Wulf's CMU research team in 1970. The Production Quality Compiler-Compiler PQCC design would produce a Production Quality Compiler (PQC) from formal definitions of source language and the target.[42] PQCC tried to extend the term compiler-compiler beyond the traditional meaning as a parser generator (e.g., Yacc) without much success. PQCC might more properly be referred to as a compiler generator.
 PQCC research into code generation process sought to build a truly automatic compiler-writing system. The effort discovered and designed the phase structure of the PQC. The BLISS-11 compiler provided the initial structure.[43] The phases included analyses (front end), intermediate translation to virtual machine (middle end), and translation to the target (back end). TCOL was developed for the PQCC research to handle language specific constructs in the intermediate representation.[44] Variations of TCOL supported various languages. The PQCC project investigated techniques of automated compiler construction. The design concepts proved useful in optimizing compilers and compilers for the (since 1995, object-oriented) programming language Ada.
 The Ada STONEMAN document[a] formalized the program support environment (APSE) along with the kernel (KAPSE) and minimal (MAPSE). An Ada interpreter NYU/ED supported development and standardization efforts with the American National Standards Institute (ANSI) and the International Standards Organization (ISO). Initial Ada compiler development by the U.S. Military Services included the compilers in a complete integrated design environment along the lines of the STONEMAN document. Army and Navy worked on the Ada Language System (ALS) project targeted to DEC/VAX architecture while the Air Force started on the Ada Integrated Environment (AIE) targeted to IBM 370 series. While the projects did not provide the desired results, they did contribute to the overall effort on Ada development.[45]
 Other Ada compiler efforts got underway in Britain at the University of York and in Germany at the University of Karlsruhe. In the U. S., Verdix (later acquired by Rational) delivered the Verdix Ada Development System (VADS) to the Army. VADS provided a set of development tools including a compiler. Unix/VADS could be hosted on a variety of Unix platforms such as DEC Ultrix and the Sun 3/60 Solaris targeted to Motorola 68020 in an Army CECOM evaluation.[46] There were soon many Ada compilers available that passed the Ada Validation tests. The Free Software Foundation GNU project developed the GNU Compiler Collection (GCC) which provides a core capability to support multiple languages and targets. The Ada version GNAT is one of the most widely used Ada compilers. GNAT is free but there is also commercial support, for example, AdaCore, was founded in 1994 to provide commercial software solutions for Ada. GNAT Pro includes the GNU GCC based GNAT with a tool suite to provide an integrated development environment.
 High-level languages continued to drive compiler research and development. Focus areas included optimization and automatic code generation. Trends in programming languages and development environments influenced compiler technology. More compilers became included in language distributions (PERL, Java Development Kit) and as a component of an IDE (VADS, Eclipse, Ada Pro). The interrelationship and interdependence of technologies grew. The advent of web services promoted growth of web languages and scripting languages. Scripts trace back to the early days of Command Line Interfaces (CLI) where the user could enter commands to be executed by the system. User Shell concepts developed with languages to write shell programs. Early Windows designs offered a simple batch programming capability. The conventional transformation of these language used an interpreter. While not widely used, Bash and Batch compilers have been written. More recently sophisticated interpreted languages became part of the developers tool kit. Modern scripting languages include PHP, Python, Ruby and Lua. (Lua is widely used in game development.) All of these have interpreter and compiler support.[47]
 "When the field of compiling began in the late 50s, its focus was limited to the translation of high-level language programs into machine code ... The compiler field is increasingly intertwined with other disciplines including computer architecture, programming languages, formal methods, software engineering, and computer security."[48] The "Compiler Research: The Next 50 Years" article noted the importance of object-oriented languages and Java. Security and parallel computing were cited among the future research targets.
 A compiler implements a formal transformation from a high-level source program to a low-level target program. Compiler design can define an end-to-end solution or tackle a defined subset that interfaces with other compilation tools e.g. preprocessors, assemblers, linkers. Design requirements include rigorously defined interfaces both internally between compiler components and externally between supporting toolsets.
 In the early days, the approach taken to compiler design was directly affected by the complexity of the computer language to be processed, the experience of the person(s) designing it, and the resources available. Resource limitations led to the need to pass through the source code more than once.
 A compiler for a relatively simple language written by one person might be a single, monolithic piece of software. However, as the source language grows in complexity the design may be split into a number of interdependent phases. Separate phases provide design improvements that focus development on the functions in the compilation process.
 Classifying compilers by number of passes has its background in the hardware resource limitations of computers. Compiling involves performing much work and early computers did not have enough memory to contain one program that did all of this work. As a result, compilers were split up into smaller programs which each made a pass over the source (or some representation of it) performing some of the required analysis and translations.
 The ability to compile in a single pass has classically been seen as a benefit because it simplifies the job of writing a compiler and one-pass compilers generally perform compilations faster than multi-pass compilers. Thus, partly driven by the resource limitations of early systems, many early languages were specifically designed so that they could be compiled in a single pass (e.g., Pascal).
 In some cases, the design of a language feature may require a compiler to perform more than one pass over the source. For instance, consider a declaration appearing on line 20 of the source which affects the translation of a statement appearing on line 10. In this case, the first pass needs to gather information about declarations appearing after statements that they affect, with the actual translation happening during a subsequent pass.
 The disadvantage of compiling in a single pass is that it is not possible to perform many of the sophisticated optimizations needed to generate high quality code. It can be difficult to count exactly how many passes an optimizing compiler makes. For instance, different phases of optimization may analyse one expression many times but only analyse another expression once.
 Splitting a compiler up into small programs is a technique used by researchers interested in producing provably correct compilers. Proving the correctness of a set of small programs often requires less effort than proving the correctness of a larger, single, equivalent program.
 Regardless of the exact number of phases in the compiler design, the phases can be assigned to one of three stages. The stages include a front end, a middle end, and a back end.
 This front/middle/back-end approach makes it possible to combine front ends for different languages with back ends for different CPUs while sharing the optimizations of the middle end.[49] Practical examples of this approach are the GNU Compiler Collection, Clang (LLVM-based C/C++ compiler),[50] and the Amsterdam Compiler Kit, which have multiple front-ends, shared optimizations and multiple back-ends.
 The front end analyzes the source code to build an internal representation of the program, called the intermediate representation (IR). It also manages the symbol table, a data structure mapping each symbol in the source code to associated information such as location, type and scope.
 While the frontend can be a single monolithic function or program, as in a scannerless parser, it was traditionally implemented and analyzed as several phases, which may execute sequentially or concurrently. This method is favored due to its modularity and separation of concerns. Most commonly, the frontend is broken into three phases: lexical analysis (also known as lexing or scanning), syntax analysis (also known as scanning or parsing), and semantic analysis. Lexing and parsing comprise the syntactic analysis (word syntax and phrase syntax, respectively), and in simple cases, these modules (the lexer and parser) can be automatically generated from a grammar for the language, though in more complex cases these require manual modification. The lexical grammar and phrase grammar are usually context-free grammars, which simplifies analysis significantly, with context-sensitivity handled at the semantic analysis phase. The semantic analysis phase is generally more complex and written by hand, but can be partially or fully automated using attribute grammars. These phases themselves can be further broken down: lexing as scanning and evaluating, and parsing as building a concrete syntax tree (CST, parse tree) and then transforming it into an abstract syntax tree (AST, syntax tree). In some cases additional phases are used, notably line reconstruction and preprocessing, but these are rare.
 The main phases of the front end include the following:
 The middle end, also known as optimizer, performs optimizations on the intermediate representation in order to improve the performance and the quality of the produced machine code.[54] The middle end contains those optimizations that are independent of the CPU architecture being targeted.
 The main phases of the middle end include the following:
 Compiler analysis is the prerequisite for any compiler optimization, and they tightly work together. For example, dependence analysis is crucial for loop transformation.
 The scope of compiler analysis and optimizations vary greatly; their scope may range from operating within a basic block, to whole procedures, or even the whole program.  There is a trade-off between the granularity of the optimizations and the cost of compilation.  For example, peephole optimizations are fast to perform during compilation but only affect a small local fragment of the code, and can be performed independently of the context in which the code fragment appears.  In contrast, interprocedural optimization requires more compilation time and memory space, but enable optimizations that are only possible by considering the behavior of multiple functions simultaneously.
 Interprocedural analysis and optimizations are common in modern commercial compilers from HP, IBM, SGI, Intel, Microsoft, and Sun Microsystems. The free software GCC was criticized for a long time for lacking powerful interprocedural optimizations, but it is changing in this respect. Another open source compiler with full analysis and optimization infrastructure is Open64, which is used by many organizations for research and commercial purposes.
 Due to the extra time and space needed for compiler analysis and optimizations, some compilers skip them by default. Users have to use compilation options to explicitly tell the compiler which optimizations should be enabled.
 The back end is responsible for the CPU architecture specific optimizations and for code generation[54].
 The main phases of the back end include the following:
 Compiler correctness is the branch of software engineering that deals with trying to show that a compiler behaves according to its language specification.[56] Techniques include developing the compiler using formal methods and using rigorous testing (often called compiler validation) on an existing compiler.
 Higher-level programming languages usually appear with a type of translation in mind: either designed as compiled language or interpreted language. However, in practice there is rarely anything about a language that requires it to be exclusively compiled or exclusively interpreted, although it is possible to design languages that rely on re-interpretation at run time. The categorization usually reflects the most popular or widespread implementations of a language – for instance, BASIC is sometimes called an interpreted language, and C a compiled one, despite the existence of BASIC compilers and C interpreters.
 Interpretation does not replace compilation completely. It only hides it from the user and makes it gradual. Even though an interpreter can itself be interpreted, a set of directly executed machine instructions is needed somewhere at the bottom of the execution stack (see machine language).
 Furthermore, for optimization compilers can contain interpreter functionality, and interpreters may include ahead of time compilation techniques. For example, where an expression can be executed during compilation and the results inserted into the output program, then it prevents it having to be recalculated each time the program runs, which can greatly speed up the final program. Modern trends toward just-in-time compilation and bytecode interpretation at times blur the traditional categorizations of compilers and interpreters even further.
 Some language specifications spell out that implementations must include a compilation facility; for example, Common Lisp. However, there is nothing inherent in the definition of Common Lisp that stops it from being interpreted. Other languages have features that are very easy to implement in an interpreter, but make writing a compiler much harder; for example, APL, SNOBOL4, and many scripting languages allow programs to construct arbitrary source code at runtime with regular string operations, and then execute that code by passing it to a special evaluation function. To implement these features in a compiled language, programs must usually be shipped with a runtime library that includes a version of the compiler itself.
 One classification of compilers is by the platform on which their generated code executes. This is known as the target platform.
 A native or hosted compiler is one whose output is intended to directly run on the same type of computer and operating system that the compiler itself runs on. The output of a cross compiler is designed to run on a different platform. Cross compilers are often used when developing software for embedded systems that are not intended to support a software development environment.
 The output of a compiler that produces code for a virtual machine (VM) may or may not be executed on the same platform as the compiler that produced it. For this reason, such compilers are not usually classified as native or cross compilers.
 The lower level language that is the target of a compiler may itself be a high-level programming language. C, viewed by some as a sort of portable assembly language, is frequently the target language of such compilers. For example, Cfront, the original compiler for C++, used C as its target language. The C code generated by such a compiler is usually not intended to be readable and maintained by humans, so indent style and creating pretty C intermediate code are ignored. Some of the features of C that make it a good target language include the #line directive, which can be generated by the compiler to support debugging of the original source, and the wide platform support available with C compilers.
 While a common compiler type outputs machine code, there are many other types:
 Assemblers, which translate human readable assembly language to the machine code instructions executed by hardware, are not considered compilers.[64][b] (The inverse program that translates machine code to assembly language is called a disassembler.)


Source: https://en.wikipedia.org/wiki/Cython
Content: 
 Cython (/ˈsaɪθɒn/) is a superset of the programming language Python, which allows developers to write Python code (with optional, C-inspired syntax extensions) that yields performance comparable to that of C.[5][6]
 Cython is a compiled language that is typically used to generate CPython extension modules. Annotated Python-like code is compiled to C (also usable from e.g. C++) and then automatically wrapped in interface code, producing extension modules that can be loaded and used by regular Python code using the import statement, but with significantly less computational overhead at run time. Cython also facilitates wrapping independent C or C++ code into python-importable modules.
 Cython is written in Python and C and works on Windows, macOS, and Linux, producing C source files compatible with CPython 2.6, 2.7, and 3.3 and later versions. The Cython source code that Cython compiles (to C) can use both Python 2 and Python 3 syntax, defaulting to Python 2 syntax in Cython 0.x (and Python 3 syntax in Cython 3.x). The default can be overridden (e.g. in source code comment) to Python 3 (or 2) syntax. Since Python 3 syntax has changed in recent versions, Cython may not be up to date with latest addition. Cython has "native support for most of the C++ language" and "compiles almost all existing Python code".[7]
 Cython 3.0.0 was released on 17 July 2023.[8]
 Cython works by producing a standard Python module. However, the behavior differs from standard Python in that the module code, originally written in Python, is translated into C. While the resulting code is fast, it makes many calls into the CPython interpreter and CPython standard libraries to perform actual work. Choosing this arrangement saved considerably on Cython's development time, but modules have a dependency on the Python interpreter and standard library.
 Although most of the code is C-based, a small stub loader written in interpreted Python is usually required (unless the goal is to create a loader written entirely in C, which may involve work with the undocumented internals of CPython). However, this is not a major problem due to the presence of the Python interpreter.[9]
 Cython has a foreign function interface for invoking C/C++ routines and the ability to declare the static type of subroutine parameters and results, local variables, and class attributes.
 A Cython program that implements the same algorithm as a corresponding Python program may consume fewer computing resources such as core memory and processing cycles due to differences between the CPython and Cython execution models. A basic Python program is loaded and executed by the CPython virtual machine, so both the runtime and the program itself consume computing resources. A Cython program is compiled to C code, which is further compiled to machine code, so the virtual machine is used only briefly when the program is loaded.[10][11][12][13]
 Cython employs:
 Performance depends both on what C code is generated by Cython and how that code is compiled by the C compiler.[16]
 Cython is a derivative of the Pyrex language, and supports more features and optimizations than Pyrex.[17][18] Cython was forked from Pyrex in 2007 by developers of the Sage computer algebra package, because they were unhappy with Pyrex's limitations and could not get patches accepted by Pyrex's maintainer Greg Ewing, who envisioned a much smaller scope for his tool than the Sage developers had in mind. They then forked Pyrex as SageX. When they found people were downloading Sage just to get SageX, and developers of other packages (including Stefan Behnel, who maintains the XML library LXML) were also maintaining forks of Pyrex, SageX was split off the Sage project and merged with cython-lxml to become Cython.[19]
 Cython files have a .pyx extension. At its most basic, Cython code looks exactly like Python code. However, whereas standard Python is dynamically typed, in Cython, types can optionally be provided, allowing for improved performance, allowing loops to be converted into C loops where possible. For example:
 A sample hello world program for Cython is more complex than in most languages because it interfaces with the Python C API and setuptools or other PEP517-compliant extension building facilities. At least three files are required for a basic project:
 The following code listings demonstrate the build and launch process:
 These commands build and launch the program:
 A more straightforward way to start with Cython is through command-line IPython (or through in-browser python console called Jupyter notebook):
 which gives a 95 times improvement over the pure-python version. More details on the subject in the official quickstart page.[20]
 Cython is particularly popular among scientific users of Python,[12][21][22] where it has "the perfect audience" according to Python creator Guido van Rossum.[23] Of particular note:
 Cython's domain is not limited to just numerical computing. For example, the lxml XML toolkit is written mostly in Cython, and like its predecessor Pyrex, Cython is used to provide Python bindings for many C and C++ libraries such as the messaging library ZeroMQ.[28] Cython can also be used to develop parallel programs for multi-core processor machines; this feature makes use of the OpenMP library.


Source: https://en.wikipedia.org/wiki/IronPython
Content: IronPython is an implementation of the Python programming language targeting the .NET and Mono frameworks. The project is currently maintained by a group of volunteers at GitHub. It is free and open-source software, and can be implemented with Python Tools for Visual Studio, which is a free and open-source extension for Microsoft's Visual Studio IDE.[2][3]
 IronPython is written entirely in C#, although some of its code is automatically generated by a code generator written in Python.
 IronPython is implemented on top of the Dynamic Language Runtime (DLR), a library running on top of the Common Language Infrastructure that provides dynamic typing and dynamic method dispatch, among other things, for dynamic languages.[4] The DLR is part of the .NET Framework 4.0 and is also a part of Mono since version 2.4 from 2009.[5] The DLR can also be used as a library on older CLI implementations.
 Jim Hugunin created the project and actively contributed to it up until Version 1.0 which was released on September 5, 2006.[6] IronPython 2.0 was released on December 10, 2008.[7] After version 1.0 it was maintained by a small team at Microsoft until the 2.7 Beta 1 release. Microsoft abandoned IronPython (and its sister project IronRuby) in late 2010, after which Hugunin left to work at Google.[8] The project is currently maintained by a group of volunteers at GitHub.
 There are some differences between the Python reference implementation CPython and IronPython.[23] Some projects built on top of IronPython are known not to work under CPython.[24] Conversely, CPython applications that depend on extensions to the language that are implemented in C are not compatible with IronPython
,[25] unless they are implemented in a .NET interop. For example, NumPy was wrapped by Microsoft in 2011, allowing code and libraries dependent on it to be run directly from .NET Framework.[26]
 IronPython is supported on Silverlight (which is deprecated by Microsoft and already has lost support in most web browsers[27]). It can be used as a scripting engine in the browser just like the JavaScript engine.[28] IronPython scripts are passed like simple client-side JavaScript scripts in <script>-tags. It is then also possible to modify embedded XAML markup.
 The technology behind this is called Gestalt.[citation needed]
 The same works for IronRuby.
 Until version 0.6, IronPython was released under the terms of Common Public License.[29] Following recruitment of the project lead in August 2004, IronPython was made available as part of Microsoft's Shared Source initiative. This license is not OSI-approved but the authors claim it meets the open-source definition.[30] With the 2.0 alpha release, the license was changed to the Microsoft Public License,[31] which the OSI has approved. The latest versions are released under the terms of the Apache License 2.0.
 One of IronPython's key advantages is in its function as an extensibility layer to application frameworks written in a .NET language. It is relatively simple to integrate an IronPython interpreter into an existing .NET application framework. Once in place, downstream developers can use scripts written in IronPython that interact with .NET objects in the framework, thereby extending the functionality in the framework's interface, without having to change any of the framework's code base.[32]
 IronPython makes extensive use of reflection. When passed in a reference to a .NET object, it will automatically import the types and methods available to that object. This results in a highly intuitive experience when working with .NET objects from within an IronPython script.
 The following IronPython script manipulates .NET Framework objects. This script can be supplied by a third-party client-side application developer and passed into the server-side framework through an interface. Note that neither the interface, nor the server-side code is modified to support the analytics required by the client application.
 In this case, assume that the .NET Framework implements a class, BookDictionary, in a module called BookService, and publishes an interface into which IronPython scripts can be sent and executed.
 This script, when sent to that interface, will iterate over the entire list of books maintained by the framework, and pick out those written by Booker Prize-winning authors.
 What's interesting is that the responsibility for writing the actual analytics reside with the client-side developer. The demands on the server-side developer are minimal, essentially just providing access to the data maintained by the server. This design pattern greatly simplifies the deployment and maintenance of complex application frameworks.
 The following script uses the .NET Framework to create a simple Hello World message.
 The performance characteristics of IronPython compared to CPython, the reference implementation of Python, depends on the exact benchmark used. IronPython performs worse than CPython on most benchmarks taken with the PyStone script but better on other benchmarks.[33]
IronPython may perform better in Python programs that use threads or multiple cores, as it has a JIT compiler, and also because it doesn't have the Global Interpreter Lock.[34][35]


Source: https://en.wikipedia.org/wiki/.Net-Framework
Content: Other reasons this message may be displayed:


Source: https://en.wikipedia.org/wiki/Mono_(Software)
Content: Other reasons this message may be displayed:


Source: https://en.wikipedia.org/wiki/C%2B%2B
Content: 
 C++ (/ˈsiː plʌs plʌs/, pronounced "C plus plus" and sometimes abbreviated as CPP) is a high-level, general-purpose programming language created by Danish computer scientist Bjarne Stroustrup. First released in 1985 as an extension of the C programming language, it has since expanded significantly over time; as of 1997[update], C++ has object-oriented, generic, and functional features, in addition to facilities for low-level memory manipulation for making things like microcomputers or to make operating systems like Linux or Windows. It is almost always implemented as a compiled language, and many vendors provide C++ compilers, including the Free Software Foundation, LLVM, Microsoft, Intel, Embarcadero, Oracle, and IBM.[14]
 C++ was designed with systems programming and embedded, resource-constrained software and large systems in mind, with performance, efficiency, and flexibility of use as its design highlights.[15] C++ has also been found useful in many other contexts, with key strengths being software infrastructure and resource-constrained applications,[15] including desktop applications, video games, servers (e.g., e-commerce, web search, or databases), and performance-critical applications (e.g., telephone switches or space probes).[16]
 C++ is standardized by the International Organization for Standardization (ISO), with the latest standard version ratified and published by ISO in December 2020 as ISO/IEC 14882:2020 (informally known as C++20).[17] The C++ programming language was initially standardized in 1998 as ISO/IEC 14882:1998, which was then amended by the C++03, C++11, C++14, and C++17 standards. The current C++20 standard supersedes these with new features and an enlarged standard library. Before the initial standardization in 1998, C++ was developed by Stroustrup at Bell Labs since 1979 as an extension of the C language; he wanted an efficient and flexible language similar to C that also provided high-level features for program organization.[18] Since 2012, C++ has been on a three-year release schedule[19] with C++23 as the next planned standard.[20]
 In 1979, Bjarne Stroustrup, a Danish computer scientist, began work on "C with Classes", the predecessor to C++.[21] The motivation for creating a new language originated from Stroustrup's experience in programming for his PhD thesis. Stroustrup found that Simula had features that were very helpful for large software development, but the language was too slow for practical use, while BCPL was fast but too low-level to be suitable for large software development. When Stroustrup started working in AT&T Bell Labs, he had the problem of analyzing the UNIX kernel with respect to distributed computing. Remembering his PhD experience, Stroustrup set out to enhance the C language with Simula-like features.[22] C was chosen because it was general-purpose, fast, portable, and widely used. In addition to C and Simula's influences, other languages influenced this new language, including ALGOL 68, Ada, CLU, and ML.[citation needed]
 Initially, Stroustrup's "C with Classes" added features to the C compiler, Cpre, including classes, derived classes, strong typing, inlining, and default arguments.[23]
 In 1982, Stroustrup started to develop a successor to C with Classes, which he named "C++" (++ being the increment operator in C) after going through several other names. New features were added, including virtual functions, function name and operator overloading, references, constants, type-safe free-store memory allocation (new/delete), improved type checking, and BCPL-style single-line comments with two forward slashes (//). Furthermore, Stroustrup developed a new, standalone compiler for C++, Cfront.
 In 1984, Stroustrup implemented the first stream input/output library. The idea of providing an output operator rather than a named output function was suggested by Doug McIlroy[2] (who had previously suggested Unix pipes).
 In 1985, the first edition of The C++ Programming Language was released, which became the definitive reference for the language, as there was not yet an official standard.[24] The first commercial implementation of C++ was released in October of the same year.[21]
 In 1989, C++ 2.0 was released, followed by the updated second edition of The C++ Programming Language in 1991.[25] New features in 2.0 included multiple inheritance, abstract classes, static member functions, const member functions, and protected members. In 1990, The Annotated C++ Reference Manual was published. This work became the basis for the future standard. Later feature additions included templates, exceptions, namespaces, new casts, and a Boolean type.
 In 1998, C++98 was released, standardizing the language, and a minor update (C++03) was released in 2003.
 After C++98, C++ evolved relatively slowly until, in 2011, the C++11 standard was released, adding numerous new features, enlarging the standard library further, and providing more facilities to C++ programmers. After a minor C++14 update released in December 2014, various new additions were introduced in C++17.[26] After becoming finalized in February 2020,[27] a draft of the C++20 standard was approved on 4 September 2020, and officially published on 15 December 2020.[28][29]
 On January 3, 2018, Stroustrup was announced as the 2018 winner of the Charles Stark Draper Prize for Engineering, "for conceptualizing and developing the C++ programming language".[30]
 As of December 2022[update], C++ ranked third on the TIOBE index, surpassing Java for the first time in the history of the index. It ranks third, after Python and C.[31]
 According to Stroustrup, "the name signifies the evolutionary nature of the changes from C."[32] This name is credited to Rick Mascitti (mid-1983)[23] and was first used in December 1983. When Mascitti was questioned informally in 1992 about the naming, he indicated that it was given in a tongue-in-cheek spirit. The name comes from C's ++ operator (which increments the value of a variable) and a common naming convention of using "+" to indicate an enhanced computer program.
 During C++'s development period, the language had been referred to as "new C" and "C with Classes"[23][33] before acquiring its final name.
 Throughout C++'s life, its development and evolution has been guided by a set of principles:[22]
 C++ is standardized by an ISO working group known as JTC1/SC22/WG21. So far, it has published six revisions of the C++ standard and is currently working on the next revision, C++23.
 In 1998, the ISO working group standardized C++ for the first time as ISO/IEC 14882:1998, which is informally known as C++98. In 2003, it published a new version of the C++ standard called ISO/IEC 14882:2003, which fixed problems identified in C++98.
 The next major revision of the standard was informally referred to as "C++0x", but it was not released until 2011.[40] C++11 (14882:2011) included many additions to both the core language and the standard library.[37]
 In 2014, C++14 (also known as C++1y) was released as a small extension to C++11, featuring mainly bug fixes and small improvements.[41]  The Draft International Standard ballot procedures completed in mid-August 2014.[42]
 After C++14, a major revision C++17, informally known as C++1z, was completed by the ISO C++ committee in mid July 2017 and was approved and published in December 2017.[43]
 As part of the standardization process, ISO also publishes technical reports and specifications:
 More technical specifications are in development and pending approval, including new set of concurrency extensions.[61]
 The C++ language has two main components: a direct mapping of hardware features provided primarily by the C subset, and zero-overhead abstractions based on those mappings. Stroustrup describes C++ as "a light-weight abstraction programming language [designed] for building and using efficient and elegant abstractions";[15] and "offering both hardware access and abstraction is the basis of C++. Doing it efficiently is what distinguishes it from other languages."[62]
 C++ inherits most of C's syntax. The following is Bjarne Stroustrup's version of the Hello world program that uses the C++ Standard Library stream facility to write a message to standard output:[63][64][note 2]
 As in C, C++ supports four types of memory management: static storage duration objects, thread storage duration objects, automatic storage duration objects, and dynamic storage duration objects.[65]
 Static storage duration objects are created before main() is entered (see exceptions below) and destroyed in reverse order of creation after main() exits. The exact order of creation is not specified by the standard (though there are some rules defined below) to allow implementations some freedom in how to organize their implementation. More formally, objects of this type have a lifespan that "shall last for the duration of the program".[66]
 Static storage duration objects are initialized in two phases. First, "static initialization" is performed, and only after all static initialization is performed, "dynamic initialization" is performed.  In static initialization, all objects are first initialized with zeros; after that, all objects that have a constant initialization phase are initialized with the constant expression (i.e. variables initialized with a literal or constexpr). Though it is not specified in the standard, the static initialization phase can be completed at compile time and saved in the data partition of the executable.  Dynamic initialization involves all object initialization done via a constructor or function call (unless the function is marked with constexpr, in C++11). The dynamic initialization order is defined as the order of declaration within the compilation unit (i.e. the same file). No guarantees are provided about the order of initialization between compilation units.
 Variables of this type are very similar to static storage duration objects. The main difference is the creation time is just prior to thread creation and destruction is done after the thread has been joined.[67]
 The most common variable types in C++ are local variables inside a function or block, and temporary variables.[68] The common feature about automatic variables is that they have a lifetime that is limited to the scope of the variable. They are created and potentially initialized at the point of declaration (see below for details) and destroyed in the reverse order of creation when the scope is left. This is implemented by allocation on the stack.
 Local variables are created as the point of execution passes the declaration point. If the variable has a constructor or initializer this is used to define the initial state of the object. Local variables are destroyed when the local block or function that they are declared in is closed. C++ destructors for local variables are called at the end of the object lifetime, allowing a discipline for automatic resource management termed RAII, which is widely used in C++.
 Member variables are created when the parent object is created. Array members are initialized from 0 to the last member of the array in order. Member variables are destroyed when the parent object is destroyed in the reverse order of creation. i.e. If the parent is an "automatic object" then it will be destroyed when it goes out of scope which triggers the destruction of all its members.
 Temporary variables are created as the result of expression evaluation and are destroyed when the statement containing the expression has been fully evaluated (usually at the ; at the end of a statement).
 These objects have a dynamic lifespan and can be created directly with a call to new and destroyed explicitly with a call to delete.[69] C++ also supports malloc and free, from C, but these are not compatible with new and delete. Use of new returns an address to the allocated memory. The C++ Core Guidelines advise against using new directly for creating dynamic objects in favor of smart pointers through make_unique<T> for single ownership and make_shared<T> for reference-counted multiple ownership,[70] which were introduced in C++11.
 C++ templates enable generic programming. C++ supports function, class, alias, and variable templates. Templates may be parameterized by types, compile-time constants, and other templates. Templates are implemented by instantiation at compile-time. To instantiate a template, compilers substitute specific arguments for a template's parameters to generate a concrete function or class instance. Some substitutions are not possible; these are eliminated by an overload resolution policy described by the phrase "Substitution failure is not an error" (SFINAE). Templates are a powerful tool that can be used for generic programming, template metaprogramming, and code optimization, but this power implies a cost. Template use may increase object code size, because each template instantiation produces a copy of the template code: one for each set of template arguments, however, this is the same or smaller amount of code that would be generated if the code were written by hand.[71] This is in contrast to run-time generics seen in other languages (e.g., Java) where at compile-time the type is erased and a single template body is preserved.
 Templates are different from macros: while both of these compile-time language features enable conditional compilation, templates are not restricted to lexical substitution. Templates are aware of the semantics and type system of their companion language, as well as all compile-time type definitions, and can perform high-level operations including programmatic flow control based on evaluation of strictly type-checked parameters. Macros are capable of conditional control over compilation based on predetermined criteria, but cannot instantiate new types, recurse, or perform type evaluation and in effect are limited to pre-compilation text-substitution and text-inclusion/exclusion. In other words, macros can control compilation flow based on pre-defined symbols but cannot, unlike templates, independently instantiate new symbols. Templates are a tool for static polymorphism (see below) and generic programming.
 In addition, templates are a compile-time mechanism in C++ that is Turing-complete, meaning that any computation expressible by a computer program can be computed, in some form, by a template metaprogram prior to runtime.
 In summary, a template is a compile-time parameterized function or class written without knowledge of the specific arguments used to instantiate it. After instantiation, the resulting code is equivalent to code written specifically for the passed arguments. In this manner, templates provide a way to decouple generic, broadly applicable aspects of functions and classes (encoded in templates) from specific aspects (encoded in template parameters) without sacrificing performance due to abstraction.
 C++ introduces object-oriented programming (OOP) features to C. It offers classes, which provide the four features commonly present in OOP (and some non-OOP) languages: abstraction, encapsulation, inheritance, and polymorphism. One distinguishing feature of C++ classes compared to classes in other programming languages is support for deterministic destructors, which in turn provide support for the Resource Acquisition is Initialization (RAII) concept.
 Encapsulation is the hiding of information to ensure that data structures and operators are used as intended and to make the usage model more obvious to the developer. C++ provides the ability to define classes and functions as its primary encapsulation mechanisms. Within a class, members can be declared as either public, protected, or private to explicitly enforce encapsulation. A public member of the class is accessible to any function. A private member is accessible only to functions that are members of that class and to functions and classes explicitly granted access permission by the class ("friends"). A protected member is accessible to members of classes that inherit from the class in addition to the class itself and any friends.
 The object-oriented principle ensures the encapsulation of all and only the functions that access the internal representation of a type. C++ supports this principle via member functions and friend functions, but it does not enforce it. Programmers can declare parts or all of the representation of a type to be public, and they are allowed to make public entities not part of the representation of a type. Therefore, C++ supports not just object-oriented programming, but other decomposition paradigms such as modular programming.
 It is generally considered good practice to make all data private or protected, and to make public only those functions that are part of a minimal interface for users of the class. This can hide the details of data implementation, allowing the designer to later fundamentally change the implementation without changing the interface in any way.[72][73]
 Inheritance allows one data type to acquire properties of other data types. Inheritance from a base class may be declared as public, protected, or private. This access specifier determines whether unrelated and derived classes can access the inherited public and protected members of the base class. Only public inheritance corresponds to what is usually meant by "inheritance". The other two forms are much less frequently used. If the access specifier is omitted, a "class" inherits privately, while a "struct" inherits publicly. Base classes may be declared as virtual; this is called virtual inheritance. Virtual inheritance ensures that only one instance of a base class exists in the inheritance graph, avoiding some of the ambiguity problems of multiple inheritance.
 Multiple inheritance is a C++ feature allowing a class to be derived from more than one base class; this allows for more elaborate inheritance relationships. For example, a "Flying Cat" class can inherit from both "Cat" and "Flying Mammal". Some other languages, such as C# or Java, accomplish something similar (although more limited) by allowing inheritance of multiple interfaces while restricting the number of base classes to one (interfaces, unlike classes, provide only declarations of member functions, no implementation or member data). An interface as in C# and Java can be defined in C++ as a class containing only pure virtual functions, often known as an abstract base class or "ABC". The member functions of such an abstract base class are normally explicitly defined in the derived class, not inherited implicitly. C++ virtual inheritance exhibits an ambiguity resolution feature called dominance.
 C++ provides more than 35 operators, covering basic arithmetic, bit manipulation, indirection, comparisons, logical operations and others. Almost all operators can be overloaded for user-defined types, with a few notable exceptions such as member access (. and .*) as well as the conditional operator. The rich set of overloadable operators is central to making user-defined types in C++ seem like built-in types.
 Overloadable operators are also an essential part of many advanced C++ programming techniques, such as smart pointers. Overloading an operator does not change the precedence of calculations involving the operator, nor does it change the number of operands that the operator uses (any operand may however be ignored by the operator, though it will be evaluated prior to execution). Overloaded "&&" and "||" operators lose their short-circuit evaluation property.
 Polymorphism enables one common interface for many implementations, and for objects to act differently under different circumstances.
 C++ supports several kinds of static (resolved at compile-time) and dynamic (resolved at run-time) polymorphisms, supported by the language features described above. Compile-time polymorphism does not allow for certain run-time decisions, while runtime polymorphism typically incurs a performance penalty.
 Function overloading allows programs to declare multiple functions having the same name but with different arguments (i.e. ad hoc polymorphism). The functions are distinguished by the number or types of their formal parameters. Thus, the same function name can refer to different functions depending on the context in which it is used. The type returned by the function is not used to distinguish overloaded functions and differing return types would result in a compile-time error message.
 When declaring a function, a programmer can specify for one or more parameters a default value. Doing so allows the parameters with defaults to optionally be omitted when the function is called, in which case the default arguments will be used. When a function is called with fewer arguments than there are declared parameters, explicit arguments are matched to parameters in left-to-right order, with any unmatched parameters at the end of the parameter list being assigned their default arguments. In many cases, specifying default arguments in a single function declaration is preferable to providing overloaded function definitions with different numbers of parameters.
 Templates in C++ provide a sophisticated mechanism for writing generic, polymorphic code (i.e. parametric polymorphism). In particular, through the curiously recurring template pattern, it is possible to implement a form of static polymorphism that closely mimics the syntax for overriding virtual functions. Because C++ templates are type-aware and Turing-complete, they can also be used to let the compiler resolve recursive conditionals and generate substantial programs through template metaprogramming. Contrary to some opinion, template code will not generate a bulk code after compilation with the proper compiler settings.[71]
 Variable pointers and references to a base class type in C++ can also refer to objects of any derived classes of that type. This allows arrays and other kinds of containers to hold pointers to objects of differing types (references cannot be directly held in containers). This enables dynamic (run-time) polymorphism, where the referred objects can behave differently, depending on their (actual, derived) types.
 C++ also provides the dynamic_cast operator, which allows code to safely attempt conversion of an object, via a base reference/pointer, to a more derived type: downcasting. The attempt is necessary as often one does not know which derived type is referenced. (Upcasting, conversion to a more general type, can always be checked/performed at compile-time via static_cast, as ancestral classes are specified in the derived class's interface, visible to all callers.) dynamic_cast relies on run-time type information (RTTI), metadata in the program that enables differentiating types and their relationships. If a dynamic_cast to a pointer fails, the result is the nullptr constant, whereas if the destination is a reference (which cannot be null), the cast throws an exception. Objects known to be of a certain derived type can be cast to that with static_cast, bypassing RTTI and the safe runtime type-checking of dynamic_cast, so this should be used only if the programmer is very confident the cast is, and will always be, valid.
 Ordinarily, when a function in a derived class overrides a function in a base class, the function to call is determined by the type of the object. A given function is overridden when there exists no difference in the number or type of parameters between two or more definitions of that function. Hence, at compile time, it may not be possible to determine the type of the object and therefore the correct function to call, given only a base class pointer; the decision is therefore put off until runtime. This is called dynamic dispatch. Virtual member functions or methods[74] allow the most specific implementation of the function to be called, according to the actual run-time type of the object. In C++ implementations, this is commonly done using virtual function tables. If the object type is known, this may be bypassed by prepending a fully qualified class name before the function call, but in general calls to virtual functions are resolved at run time.
 In addition to standard member functions, operator overloads and destructors can be virtual. An inexact rule based on practical experience states that if any function in the class is virtual, the destructor should be as well. As the type of an object at its creation is known at compile time, constructors, and by extension copy constructors, cannot be virtual. Nonetheless, a situation may arise where a copy of an object needs to be created when a pointer to a derived object is passed as a pointer to a base object. In such a case, a common solution is to create a clone() (or similar) virtual function that creates and returns a copy of the derived class when called.
 A member function can also be made "pure virtual" by appending it with = 0 after the closing parenthesis and before the semicolon. A class containing a pure virtual function is called an abstract class. Objects cannot be created from an abstract class; they can only be derived from. Any derived class inherits the virtual function as pure and must provide a non-pure definition of it (and all other pure virtual functions) before objects of the derived class can be created. A program that attempts to create an object of a class with a pure virtual member function or inherited pure virtual member function is ill-formed.
 C++ provides support for anonymous functions, also known as lambda expressions, with the following form:[75]
 Since C++20, the keyword template is optional for template parameters of lambda expressions:
 If the lambda takes no parameters, and no return type or other specifiers are used, the () can be omitted; that is,
 The return type of a lambda expression can be automatically inferred, if possible; e.g.:
 The [capture] list supports the definition of closures. Such lambda expressions are defined in the standard as syntactic sugar for an unnamed function object.
 Exception handling is used to communicate the existence of a runtime problem or error from where it was detected to where the issue can be handled.[76] It permits this to be done in a uniform manner and separately from the main code, while detecting all errors.[77] Should an error occur, an exception is thrown (raised), which is then caught by the nearest suitable exception handler. The exception causes the current scope to be exited, and also each outer scope (propagation) until a suitable handler is found, calling in turn the destructors of any objects in these exited scopes.[78] At the same time, an exception is presented as an object carrying the data about the detected problem.[79]
 Some C++ style guides, such as Google's,[80] LLVM's,[81] and Qt's,[82] forbid the usage of exceptions.
 The exception-causing code is placed inside a try block. The exceptions are handled in separate catch blocks (the handlers); each try block can have multiple exception handlers, as it is visible in the example below.[83]
 It is also possible to raise exceptions purposefully, using the throw keyword; these exceptions are handled in the usual way. In some cases, exceptions cannot be used due to technical reasons. One such example is a critical component of an embedded system, where every operation must be guaranteed to complete within a specified amount of time. This cannot be determined with exceptions as no tools exist to determine the maximum time required for an exception to be handled.[84]
 Unlike signal handling, in which the handling function is called from the point of failure, exception handling exits the current scope before the catch block is entered, which may be located in the current function or any of the previous function calls currently on the stack.
 C++ has enumeration types that are directly inherited from C's and work mostly like these, except that an enumeration is a real type in C++, giving added compile-time checking. Also (as with structs), the C++ enum keyword is combined with a typedef, so that instead of naming the type enum name, simply name it name. This can be simulated in C using a typedef: typedef enum {Value1, Value2} name;
 C++11 also provides a second kind of enumeration, called a scoped enumeration. These are type-safe: the enumerators are not implicitly converted to an integer type. Among other things, this allows I/O streaming to be defined for the enumeration type. Another feature of scoped enumerations is that the enumerators do not leak, so usage requires prefixing with the name of the enumeration (e.g., Color::Red for the first enumerator in the example below), unless a using enum declaration (introduced in C++20) has been used to bring the enumerators into the current scope. A scoped enumeration is specified by the phrase enum class (or enum struct). For example:
 The underlying type of an enumeration is an implementation-defined integral type that is large enough to hold all enumerated values; it does not have to be the smallest possible type. The underlying type can be specified directly, which allows "forward declarations" of enumerations:
 The C++ standard consists of two parts: the core language and the standard library. C++ programmers expect the latter on every major implementation of C++; it includes aggregate types (vectors, lists, maps, sets, queues, stacks, arrays, tuples), algorithms (find, for_each, binary_search, random_shuffle, etc.), input/output facilities (iostream, for reading from and writing to the console and files), filesystem library, localisation support, smart pointers for automatic memory management, regular expression support, multi-threading library, atomics support (allowing a variable to be read or written to by at most one thread at a time without any external synchronisation), time utilities (measurement, getting current time, etc.), a system for converting error reporting that does not use C++ exceptions into C++ exceptions, a random number generator, and a slightly modified version of the C standard library (to make it comply with the C++ type system).
 A large part of the C++ library is based on the Standard Template Library (STL). Useful tools provided by the STL include containers as the collections of objects (such as vectors and lists), iterators that provide array-like access to containers, and algorithms that perform operations such as searching and sorting.
 Furthermore, (multi)maps (associative arrays) and (multi)sets are provided, all of which export compatible interfaces. Therefore, using templates it is possible to write generic algorithms that work with any container or on any sequence defined by iterators. As in C, the features of the library are accessed by using the #include directive to include a standard header. The C++ Standard Library provides 105 standard headers, of which 27 are deprecated.
 The standard incorporates the STL that was originally designed by Alexander Stepanov, who experimented with generic algorithms and containers for many years. When he started with C++, he finally found a language where it was possible to create generic algorithms (e.g., STL sort) that perform even better than, for example, the C standard library qsort, thanks to C++ features like using inlining and compile-time binding instead of function pointers. The standard does not refer to it as "STL", as it is merely a part of the standard library, but the term is still widely used to distinguish it from the rest of the standard library (input/output streams, internationalization, diagnostics, the C library subset, etc.).[85]
 Most C++ compilers, and all major ones, provide a standards-conforming implementation of the C++ standard library.
 The C++ Core Guidelines[86] are an initiative led by Bjarne Stroustrup, the inventor of C++, and Herb Sutter, the convener and chair of the C++ ISO Working Group, to help programmers write 'Modern C++' by using best practices for the language standards C++11 and newer, and to help developers of compilers and static checking tools to create rules for catching bad programming practices.
 The main aim is to efficiently and consistently write type and resource safe C++.
 The Core Guidelines were announced[87] in the opening keynote at CPPCon 2015.
 The Guidelines are accompanied by the Guideline Support Library (GSL),[88] a header only library of types and functions to implement the Core Guidelines and static checker tools for enforcing Guideline rules.[89]
 To give compiler vendors greater freedom, the C++ standards committee decided not to dictate the implementation of name mangling, exception handling, and other implementation-specific features. The downside of this decision is that object code produced by different compilers is expected to be incompatible. There are, however, attempts to standardize compilers for particular machines or operating systems. For example, the Itanium C++ ABI is processor-independent (despite its name) and is implemented by GCC and Clang.[90]
 C++ is often considered to be a superset of C but this is not strictly true.[91] Most C code can easily be made to compile correctly in C++ but there are a few differences that cause some valid C code to be invalid or behave differently in C++. For example, C allows implicit conversion from void* to other pointer types but C++ does not (for type safety reasons). Also, C++ defines many new keywords, such as new and class, which may be used as identifiers (for example, variable names) in a C program.
 Some incompatibilities have been removed by the 1999 revision of the C standard (C99), which now supports C++ features such as line comments (//) and declarations mixed with code. On the other hand, C99 introduced a number of new features that C++ did not support that were incompatible or redundant in C++, such as variable-length arrays, native complex-number types (however, the std::complex class in the C++ standard library provides similar functionality, although not code-compatible), designated initializers, compound literals, and the restrict keyword.[92] Some of the C99-introduced features were included in the subsequent version of the C++ standard, C++11 (out of those which were not redundant).[93][94][95] However, the C++11 standard introduces new incompatibilities, such as disallowing assignment of a string literal to a character pointer, which remains valid C.
 To intermix C and C++ code, any function declaration or definition that is to be called from/used both in C and C++ must be declared with C linkage by placing it within an extern "C" {/*...*/} block. Such a function may not rely on features depending on name mangling (i.e., function overloading).
 Despite its widespread adoption, some notable programmers have criticized the C++ language, including Linus Torvalds,[96] Richard Stallman,[97] Joshua Bloch, Ken Thompson,[98][99][100] and Donald Knuth.[101][102]
 
One of the most often criticised points of C++ is its perceived complexity as a language, with the criticism that a large number of non-orthogonal features in practice necessitates restricting code to a subset of C++, thus eschewing the readability benefits of common style and idioms. As expressed by Joshua Bloch:   I think C++ was pushed well beyond its complexity threshold, and yet there are a lot of people programming it. But what you do is you force people to subset it. So almost every shop that I know of that uses C++ says, "Yes, we're using C++ but we're not doing multiple-implementation inheritance and we're not using operator overloading." There are just a bunch of features that you're not going to use because the complexity of the resulting code is too high. And I don't think it's good when you have to start doing that. You lose this programmer portability where everyone can read everyone else's code, which I think is such a good thing.  Donald Knuth (1993, commenting on pre-standardized C++), who said of Edsger Dijkstra that "to think of programming in C++" "would make him physically ill":[101][102]   The problem that I have with them today is that... C++ is too complicated. At the moment, it's impossible for me to write portable code that I believe would work on lots of different systems, unless I avoid all exotic features. Whenever the C++ language designers had two competing ideas as to how they should solve some problem, they said "OK, we'll do them both". So the language is too baroque for my taste.  Ken Thompson, who was a colleague of Stroustrup at Bell Labs, gives his assessment:[99][100]   It certainly has its good points. But by and large I think it's a bad language. It does a lot of things half well and it's just a garbage heap of ideas that are mutually exclusive. Everybody I know, whether it's personal or corporate, selects a subset and these subsets are different. So it's not a good language to transport an algorithm—to say, "I wrote it; here, take it." It's way too big, way too complex. And it's obviously built by a committee.
Stroustrup campaigned for years and years and years, way beyond any sort of technical contributions he made to the language, to get it adopted and used. And he sort of ran all the standards committees with a whip and a chair. And he said "no" to no one. He put every feature in that language that ever existed. It wasn't cleanly designed—it was just the union of everything that came along. And I think it suffered drastically from that.  
However Brian Kernighan, also a colleague at Bell Labs, disputes this assessment:[103]  C++ has been enormously influential. ... Lots of people say C++ is too big and too complicated etc. etc. but in fact it is a very powerful language and pretty much everything that is in there is there for a really sound reason: it is not somebody doing random invention, it is actually people trying to solve real world problems. Now a lot of the programs that we take for granted today, that we just use, are C++ programs.  Stroustrup himself comments that C++ semantics are much cleaner than its syntax: "within C++, there is a much smaller and cleaner language struggling to get out."[104]
 Other complaints may include a lack of reflection or garbage collection, long compilation times, perceived feature creep,[105] and verbose error messages, particularly from template metaprogramming.[106]


Source: https://en.wikipedia.org/wiki/Boost_(C%2B%2B-Bibliothek)
Content: Other reasons this message may be displayed:


Source: https://en.wikipedia.org/wiki/Parser
Content: Parsing, syntax analysis, or syntactic analysis is the process of analyzing a string of symbols, either in natural language, computer languages or data structures, conforming to the rules of a formal grammar.  The term parsing comes from Latin pars (orationis), meaning part (of speech).[1]
 The term has slightly different meanings in different branches of linguistics and computer science. Traditional sentence parsing is often performed as a method of understanding the exact meaning of a sentence or word, sometimes with the aid of devices such as sentence diagrams. It usually emphasizes the importance of grammatical divisions such as subject and predicate.
 Within computational linguistics the term is used to refer to the formal analysis by a computer of a sentence or other string of words into its constituents, resulting in a parse tree showing their syntactic relation to each other, which may also contain semantic information.[citation needed] Some parsing algorithms generate a parse forest or list of parse trees from a string that is syntactically ambiguous.[2]
 The term is also used in psycholinguistics when describing language comprehension. In this context, parsing refers to the way that human beings analyze a sentence or phrase (in spoken language or text) "in terms of grammatical constituents, identifying the parts of speech, syntactic relations, etc."[1] This term is especially common when discussing which linguistic cues help speakers interpret garden-path sentences.
 Within computer science, the term is used in the analysis of computer languages, referring to the syntactic analysis of the input code into its component parts in order to facilitate the writing of compilers and interpreters. The term may also be used to describe a split or separation.
 The traditional grammatical exercise of parsing, sometimes known as clause analysis, involves breaking down a text into its component parts of speech with an explanation of the form, function, and syntactic relationship of each part.[3] This is determined in large part from study of the language's conjugations and declensions, which can be quite intricate for heavily inflected languages. To parse a phrase such as "man bites dog" involves noting that the singular noun "man" is the subject of the sentence, the verb "bites" is the third person singular of the present tense of the verb "to bite", and the singular noun "dog" is the object of the sentence. Techniques such as sentence diagrams are sometimes used to indicate relation between elements in the sentence.
 Parsing was formerly central to the teaching of grammar throughout the English-speaking world, and widely regarded as basic to the use and understanding of written language.  However, the general teaching of such techniques is no longer current.[citation needed]
 In some machine translation and natural language processing systems, written texts in human languages are parsed by computer programs.[4] Human sentences are not easily parsed by programs, as there is substantial ambiguity in the structure of human language, whose usage is to convey meaning (or semantics) amongst a potentially unlimited range of possibilities, but only some of which are germane to the particular case.[5] So an utterance "Man bites dog" versus "Dog bites man" is definite on one detail but in another language might appear as "Man dog bites" with a reliance on the larger context to distinguish between those two possibilities, if indeed that difference was of concern. It is difficult to prepare formal rules to describe informal behaviour even though it is clear that some rules are being followed.[citation needed]
 In order to parse natural language data, researchers must first agree on the grammar to be used. The choice of syntax is affected by both linguistic and computational concerns; for instance some parsing systems use lexical functional grammar, but in general, parsing for grammars of this type is known to be NP-complete. Head-driven phrase structure grammar is another linguistic formalism which has been popular in the parsing community, but other research efforts have focused on less complex formalisms such as the one used in the Penn Treebank. Shallow parsing aims to find only the boundaries of major constituents such as noun phrases. Another popular strategy for avoiding linguistic controversy is dependency grammar parsing.
 Most modern parsers are at least partly statistical; that is, they rely on a corpus of training data which has already been annotated (parsed by hand). This approach allows the system to gather information about the frequency with which various constructions occur in specific contexts. (See machine learning.) Approaches which have been used include straightforward PCFGs (probabilistic context-free grammars),[6] maximum entropy,[7] and neural nets.[8] Most of the more successful systems use lexical statistics (that is, they consider the identities of the words involved, as well as their part of speech). However such systems are vulnerable to overfitting and require some kind of smoothing to be effective.[citation needed]
 Parsing algorithms for natural language cannot rely on the grammar having 'nice' properties as with manually designed grammars for programming languages. As mentioned earlier some grammar formalisms are very difficult to parse computationally; in general, even if the desired structure is not context-free, some kind of context-free approximation to the grammar is used to perform a first pass. Algorithms which use context-free grammars often rely on some variant of the CYK algorithm, usually with some heuristic to prune away unlikely analyses to save time. (See chart parsing.) However some systems trade speed for accuracy using, e.g., linear-time versions of the shift-reduce algorithm. A somewhat recent development has been parse reranking in which the parser proposes some large number of analyses, and a more complex system selects the best option.[citation needed] In natural language understanding applications, semantic parsers convert the text into a representation of its meaning.[9]
 In psycholinguistics, parsing involves not just the assignment of words to categories (formation of ontological insights), but the evaluation of the meaning of a sentence according to the rules of syntax drawn by inferences made from each word in the sentence (known as connotation). This normally occurs as words are being heard or read. 
 Neurolinguistics generally understands parsing to be a function of working memory, meaning that parsing is used to keep several parts of one sentence at play in the mind at one time, all readily accessible to be analyzed as needed. Because the human working memory has limitations, so does the function of sentence parsing. [10]This is evidenced by several different types of syntactically complex sentences that propose potentially issues for mental parsing of sentences. 
 The first, and perhaps most well-known, type of sentence that challenges parsing ability is the garden-path sentence. These sentences are designed so that the most common interpretation of the sentence appears grammatically faulty, but upon further inspection, these sentences are grammatically sound. Garden-path sentences are difficult to parse because they contain a phrase or a word with more than one meaning, often their most typical meaning being a different part of speech.[11] For example, in the sentence, "the horse raced past the barn fell", raced is initially interpreted as a past tense verb, but in this sentence, it functions as part of an adjective phrase.[12] Since parsing is used to identify parts of speech, these sentences challenge the parsing ability of the reader.
 Another type of sentence that is difficult to parse is an attachment ambiguity, which includes a phrase that could potentially modify different parts of a sentence, and therefore presents a challenge in identifying syntactic relationship (i.e. "The boy saw the lady with the telescope", in which the ambiguous phrase with the telescope could modify the boy saw or the lady.) [13]
 A third type of sentence that challenges parsing ability is center embedding, in which phrases are placed in the center of other similarly formed phrases (i.e. "The rat the cat the man hit chased ran into the trap".) Sentences with 2 or in the most extreme cases 3 center embeddings are challenging for mental parsing, again because of ambiguity of syntactic relationship. [14]
 Within neurolinguistics there are multiple theories that aim to describe how parsing takes place in the brain. One such model is a more traditional generative model of sentence processing, which theorizes that within the brain there is a distinct module designed for sentence parsing, which is preceded by access to lexical recognition and retrieval, and then followed by syntactic processing that considers a single syntactic result of the parsing, only returning to revise that syntactic interpretation if a potential problem is detected.[15] The opposing, more contemporary model theorizes that within the mind, the processing of a sentence is not modular, or happening in strict sequence. Rather, it poses that several different syntactic possibilities can be considered at the same time, because lexical access, syntactic processing, and determination of meaning occur in parallel in the brain. In this way these processes are integrated. [16]
 Although there is still much to learn about the neurology of parsing, studies have shown evidence that several areas of the brain might play a role in parsing. These include the left anterior temporal pole, the left inferior frontal gyrus, the left superior temporal gyrus, the left superior frontal gyrus, the right posterior cingulate cortex, and the left angular gyrus. Although it has not been absolutely proven, it has been suggested that these different structures might favor either phrase-structure parsing or dependency-structure parsing, meaning different types of parsing could be processed in different ways which have yet to be understood. [17]
 Discourse analysis examines ways to analyze language use and semiotic events. Persuasive language may be called rhetoric.
 A parser is a software component that takes input data (typically text) and builds a data structure – often some kind of parse tree, abstract syntax tree or other hierarchical structure, giving a structural representation of the input while checking for correct syntax. The parsing may be preceded or followed by other steps, or these may be combined into a single step. The parser is often preceded by a separate lexical analyser, which creates tokens from the sequence of input characters; alternatively, these can be combined in scannerless parsing. Parsers may be programmed by hand or may be automatically or semi-automatically generated by a parser generator. Parsing is complementary to templating, which produces formatted output. These may be applied to different domains, but often appear together, such as the scanf/printf pair, or the input (front end parsing) and output (back end code generation) stages of a compiler.
 The input to a parser is typically text in some computer language, but may also be text in a natural language or less structured textual data, in which case generally only certain parts of the text are extracted, rather than a parse tree being constructed. Parsers range from very simple functions such as scanf, to complex programs such as the frontend of a C++ compiler or the HTML parser of a web browser. An important class of simple parsing is done using regular expressions, in which a group of regular expressions defines a regular language and a regular expression engine automatically generating a parser for that language, allowing pattern matching and extraction of text. In other contexts regular expressions are instead used prior to parsing, as the lexing step whose output is then used by the parser.
 The use of parsers varies by input. In the case of data languages, a parser is often found as the file reading facility of a program, such as reading in HTML or XML text; these examples are markup languages. In the case of programming languages, a parser is a component of a compiler or interpreter, which parses the source code of a computer programming language to create some form of internal representation; the parser is a key step in the compiler frontend. Programming languages tend to be specified in terms of a deterministic context-free grammar because fast and efficient parsers can be written for them. For compilers, the parsing itself can be done in one pass or multiple passes – see one-pass compiler and multi-pass compiler.
 The implied disadvantages of a one-pass compiler can largely be overcome by adding fix-ups, where provision is made for code relocation during the forward pass, and the fix-ups are applied backwards when the current program segment has been recognized as having been completed. An example where such a fix-up mechanism would be useful would be a forward GOTO statement, where the target of the GOTO is unknown until the program segment is completed. In this case, the application of the fix-up would be delayed until the target of the GOTO was recognized. Conversely, a backward GOTO does not require a fix-up, as the location will already be known.
 Context-free grammars are limited in the extent to which they can express all of the requirements of a language. Informally, the reason is that the memory of such a language is limited. The grammar cannot remember the presence of a construct over an arbitrarily long input; this is necessary for a language in which, for example, a name must be declared before it may be referenced. More powerful grammars that can express this constraint, however, cannot be parsed efficiently. Thus, it is a common strategy to create a relaxed parser for a context-free grammar which accepts a superset of the desired language constructs (that is, it accepts some invalid constructs); later, the unwanted constructs can be filtered out at the semantic analysis (contextual analysis) step.
 For example, in Python the following is syntactically valid code:
 The following code, however, is syntactically valid in terms of the context-free grammar, yielding a syntax tree with the same structure as the previous, but violates the semantic  rule requiring variables to be initialized before use:
 The following example demonstrates the common case of parsing a computer language with two levels of grammar: lexical and syntactic.
 The first stage is the token generation, or lexical analysis, by which the input character stream is split into meaningful symbols defined by a grammar of regular expressions. For example, a calculator program would look at an input such as "12 * (3 + 4)^2" and split it into the tokens 12, *, (, 3, +, 4, ), ^, 2, each of which is a meaningful symbol in the context of an arithmetic expression. The lexer would contain rules to tell it that the characters *, +, ^, ( and ) mark the start of a new token, so meaningless tokens like "12*" or "(3" will not be generated.
 The next stage is parsing or syntactic analysis, which is checking that the tokens form an allowable expression. This is usually done with reference to a context-free grammar which recursively defines components that can make up an expression and the order in which they must appear. However, not all rules defining programming languages can be expressed by context-free grammars alone, for example type validity and proper declaration of identifiers. These rules can be formally expressed with attribute grammars.
 The final phase is semantic parsing or analysis, which is working out the implications of the expression just validated and taking the appropriate action.[18] In the case of a calculator or interpreter, the action is to evaluate the expression or program; a compiler, on the other hand, would generate some kind of code. Attribute grammars can also be used to define these actions.
 The task of the parser is essentially to determine if and how the input can be derived from the start symbol of the grammar. This can be done in essentially two ways:
 LL parsers and recursive-descent parser  are examples of top-down parsers that cannot accommodate left recursive production rules. Although it has been believed that simple implementations of top-down parsing cannot accommodate direct and indirect left-recursion and may require exponential time and space complexity while parsing ambiguous context-free grammars, more sophisticated algorithms for top-down parsing have been created by Frost, Hafiz, and Callaghan[21][22] which accommodate ambiguity and left recursion in polynomial time and which generate polynomial-size representations of the potentially exponential number of parse trees. Their algorithm is able to produce both left-most and right-most derivations of an input with regard to a given context-free grammar.
 An important distinction with regard to parsers is whether a parser generates a leftmost derivation or a rightmost derivation (see context-free grammar). LL parsers will generate a leftmost derivation and LR parsers will generate a rightmost derivation (although usually in reverse).[19]
 Some graphical parsing algorithms have been designed for visual programming languages.[23][24] Parsers for visual languages are sometimes based on graph grammars.[25]
 Adaptive parsing algorithms have been used to construct "self-extending" natural language user interfaces.[26]
 A simple parser implementation reads the entire input file, performs an intermediate computation or translation, and then writes the entire output file,
such as in-memory multi-pass compilers.
 Alternative parser implementation approaches:
 Some of the well known parser development tools include the following:
 Lookahead establishes the maximum incoming tokens that a parser can use to decide which rule it should use. Lookahead is especially relevant to LL, LR, and LALR parsers, where it is often explicitly indicated by affixing the lookahead to the algorithm name in parentheses, such as LALR(1).
 Most programming languages, the primary target of parsers, are carefully defined in such a way that a parser with limited lookahead, typically one, can parse them, because parsers with limited lookahead are often more efficient. One important change[citation needed] to this trend came in 1990 when Terence Parr created ANTLR for his Ph.D. thesis, a parser generator for efficient LL(k) parsers, where k is any fixed value.
 LR parsers typically have only a few actions after seeing each token. They are shift (add this token to the stack for later reduction), reduce (pop tokens from the stack and form a syntactic construct), end, error (no known rule applies) or conflict (does not know whether to shift or reduce).
 Lookahead has two advantages.[clarification needed]
 Example: Parsing the Expression  1 + 2 * 3[dubious  – discuss]
 Most programming languages (except for a few such as APL and Smalltalk) and algebraic formulas give higher precedence to multiplication than addition, in which case the correct interpretation of the example above is 1 + (2 * 3).
Note that Rule4 above is a semantic rule. It is possible to rewrite the grammar to incorporate this into the syntax. However, not all such rules can be translated into syntax.
 Initially Input = [1, +, 2, *, 3]
 The parse tree and resulting code from it is not correct according to language semantics.
 To correctly parse without lookahead, there are three solutions:
 The parse tree generated is correct and simply more efficient[clarify][citation needed] than non-lookahead parsers. This is the strategy followed in LALR parsers.


Source: https://en.wikipedia.org/wiki/Parrot
Content: 
 Parrots (Psittaciformes), also known as psittacines (/ˈsɪtəsaɪnz/),[1][2] are birds with a strong curved beak, upright stance, and clawed feet.[a] They are conformed by four families that contain roughly 410 species in 101 genera, found mostly in tropical and subtropical regions.  The four families are the Psittaculidae (Old World parrots), Psittacidae (African and New World parrots), Cacatuoidea (cockatoos), and Strigopidae (New Zealand parrots). One-third of all parrot species are threatened by extinction, with a higher aggregate extinction risk (IUCN Red List Index) than any other comparable bird group.[3] Parrots have a generally pantropical distribution with several species inhabiting temperate regions as well. The greatest diversity of parrots is in South America[4] and Australasia.[5]
 Parrots—along with ravens, crows, jays, and magpies—are among the most intelligent birds, and the ability of some species to imitate human speech enhances their popularity as pets. They form the most variably sized bird order in terms of length; many are vividly coloured and some, multi-coloured. Most parrots exhibit little or no sexual dimorphism in the visual spectrum.
 The most important components of most parrots' diets are seeds, nuts, fruit, buds, and other plant material. A few species sometimes eat animals and carrion, while the lories and lorikeets are specialised for feeding on floral nectar and soft fruits. Almost all parrots nest in tree hollows (or nest boxes in captivity), and lay white eggs from which hatch altricial (helpless) young.
 Trapping wild parrots for the pet trade, as well as hunting, habitat loss, and competition from invasive species, has diminished wild populations, with parrots being subjected to more exploitation than any other group of wild birds. As of 2021, about 50 million parrots (half of all parrots) live in captivity, with the vast majority of these living as pets in people's homes.[6] Measures taken to conserve the habitats of some high-profile charismatic species have also protected many of the less charismatic species living in the same ecosystems.
 Parrots are the only creatures that display true tripedalism, using their necks and beaks as limbs with propulsive forces equal to or greater than those forces generated by the forelimbs of primates when climbing vertical surfaces. They can travel with cyclical tripedal gaits when climbing.[7]
 Psittaciform diversity in South America and Australasia suggests that the order may have evolved in Gondwana, centred in Australasia.[8] The scarcity of parrots in the fossil record, however, presents difficulties in confirming the hypothesis. There is currently a higher number of fossil remains from the northern hemisphere in the early Cenozoic.[9] Molecular studies suggest that parrots evolved approximately 59 million years ago (Mya) (range 66–51 Mya) in Gondwana. The Neotropical Parrots are monophyletic, and the three major clades originated about 50 Mya (range 57–41 Mya).[10]
 A single 15 mm (0.6 in) fragment from a large lower bill (UCMP 143274), found in deposits from the Lance Creek Formation in Niobrara County, Wyoming, had been thought to be the oldest parrot fossil and is presumed to have originated from the Late Cretaceous period, which makes it about 70 million years old.[11] However, other studies suggest that this fossil is not from a bird, but from a caenagnathid oviraptorosaur (a non-avian dinosaur with a birdlike beak), as several details of the fossil used to support its identity as a parrot are not actually exclusive to parrots, and it is dissimilar to the earliest-known unequivocal parrot fossils.[12][13]
 It is generally assumed that the Psittaciformes were present during the Cretaceous–Paleogene extinction event (K-Pg extinction), 66 mya. They were probably generalised arboreal birds, and did not have the specialised crushing bills of modern species.[9][14] Genomic analysis provides strong evidence that parrots are the sister group of passerines, forming the clade Psittacopasserae, which is the sister group of the falcons.[15]
 The first uncontroversial parrot fossils date to tropical Eocene Europe around 50 mya. Initially, a neoavian named Mopsitta tanta, uncovered in Denmark's Early Eocene Fur Formation and dated to 54 mya, was assigned to the Psittaciformes. However, the rather nondescript bone is not unequivocally psittaciform, and it may rather belong to the ibis genus Rhynchaeites, whose fossil legs were found in the same deposits.[16]
 Several fairly complete skeletons of parrot-like birds have been found in England and Germany.[17] These are probably not transitional fossils between ancestral and modern parrots, but rather lineages that evolved parallel to true parrots and cockatoos:[18]
 The earliest records of modern parrots date to around 23–20 mya.[20] The fossil record—mainly from Europe—consists of bones clearly recognisable as belonging to anatomically modern parrots.[21] The Southern Hemisphere contains no known parrot-like remains earlier than the Early Miocene around 20 mya.[20]
 The name 'Psittaciformes' comes from the ancient Greek for parrot, ψιττακός ('Psittacus'), whose origin is unclear. Ctesias (5th century BCE) recorded the name Psittacus after the Indian name for a bird, most likely a parakeet (now placed in the genus Psittacula). Pliny the Elder (23/24–79 CE) in his Natural History (book 10, chapter 58) noted that the Indians called the bird as "siptaces"; however, no matching Indian name has been traced.[22][23] Popinjay is an older term for parrots, first used in English in the 1500s.[24]
 Molecular phylogenetic studies have shown that Psittaciformes form a monophyletic clade that is sister to the Passeriformes:[25][26] The time calibrated phylogeny indicates that the Australaves diverged around 65 Ma (million years ago) and the Psittaciformes diverged from the Passeriformes around 62 Ma.[26]
 Cariamiformes – seriemas
 Falconiformes – falcons
 Passeriformes – songbirds
 Psittaciformes – parrots
 Most taxonomists now divide Psittaciformes into four families: Strigopidae (New Zealand parrots), Cacatuidae (Cockatoos), Psittacidae (African and New World parrots) and Psittaculidae (Old World parrots).[27] In 2012 Leo Joseph and collaborators proposed that the parrots should be divided into six families. The New Zealand parrots in the genus Nestor were placed in a separate family Nestoridae and the two basal genera in the family Psittaculidae (Psittrichas and Coracopsis) were placed in a separate family Psittrichasidae.[28] The two additional families have not been recognised by taxonomists involved in curating lists of world birds and instead only four families are recognised.[27][29][30][31]
 The following cladogram shows the phylogenetic relationships between the four families. The species numbers are taken from the list maintained by Frank Gill, Pamela Rasmussen and David Donsker on behalf of the International Ornithological Committee (IOC), now the International Ornithologists' Union.[27][32]
 Strigopidae – New Zealand parrots (4 species)
 Cacatuidae – Cockatoos (22 species)
 Psittacidae – African and New World parrots (179 species)
 Psittaculidae – Old World parrots (203 species)
 The Psittaciformes comprise three main lineages: Strigopoidea, Psittacoidea and Cacatuoidea.[28] The Strigopoidea were considered part of the Psittacoidea, but the former is now placed at the base of the parrot tree next to the remaining members of the Psittacoidea, as well as all members of the Cacatuoidea.[8][33][34] The Cacatuoidea are quite distinct, having a movable head crest, a different arrangement of the carotid arteries, a gall bladder, differences in the skull bones, and lack the Dyck texture feathers that—in the Psittacidae—scatter light to produce the vibrant colours of so many parrots. Colourful feathers with high levels of psittacofulvin resist the feather-degrading bacterium Bacillus licheniformis better than white ones.[35] Lorikeets were previously regarded as a third family, Loriidae,[36]: 45   but are now considered a tribe (Loriini) within the subfamily Loriinae, family Psittaculidae. The two other tribes in the subfamily are the closely related fig parrots (two genera in the tribe Cyclopsittini) and budgerigar (tribe Melopsittacini).[8][33][34]
 Strigopidae 
 Cacatuidae
 Neotropical parrots
 Psittacinae
 Psittrichadinae
 Coracopseinae
 Psittaculinae
 Broad-tailed parrots
 Fig parrots
 Budgerigar
 Lories and Lorikeets
 Bolbopsittacus
 Hanging parrots
 Lovebirds
 Psittacella
 The order Psittaciformes consists of four families containing roughly 410 species belonging to 101 genera.[27][28]
 Superfamily Strigopoidea: New Zealand parrots
 Superfamily Cacatuoidea: cockatoos
 Superfamily Psittacoidea: true parrots
 Living species range in size from the buff-faced pygmy parrot, at under 10 g (0.4 oz) in weight and 8 cm (3.1 in) in length,[36]: 149  to the hyacinth macaw, at 1 m (3.3 ft) in length,[37] and the kākāpō, at 4.0 kg (8.8 lb) in weight.[38] Among the superfamilies, the three extant Strigopoidea species are all large parrots, and the cockatoos tend to be large birds, as well. The Psittacoidea parrots are far more variable, ranging the full spectrum of sizes shown by the family.[38]
 The most obvious physical characteristic is the strong, curved, broad bill.  The upper mandible is prominent, curves downward, and comes to a point.  It is not fused to the skull, which allows it to move independently, and contributes to the tremendous biting pressure the birds are able to exert. A large macaw, for example, has a bite force of 35 kg/cm2 (500 lb/sq in), close to that of a large dog.[39] The lower mandible is shorter, with a sharp, upward-facing cutting edge, which moves against the flat part of the upper mandible in an anvil-like fashion.  Touch receptors occur along the inner edges of the keratinised bill, which are collectively known as the "bill tip organ", allowing for highly dexterous manipulations.  Seed-eating parrots have a strong tongue (containing similar touch receptors to those in the bill tip organ), which helps to manipulate seeds or position nuts in the bill so that the mandibles can apply an appropriate cracking force. The head is large, with eyes positioned high and laterally in the skull, so the visual field of parrots is unlike any other birds.  Without turning its head, a parrot can see from just below its bill tip, all above its head, and quite far behind its head.  Parrots also have quite a wide frontal binocular field for a bird, although this is nowhere near as large as primate binocular visual fields.[40] Unlike humans, the vision of parrots is also sensitive to ultraviolet light.[41]
 Parrots have strong zygodactyl feet (two toes facing forward and two back) with sharp, elongated claws, which are used for climbing and swinging. Most species are capable of using their feet to manipulate food and other objects with a high degree of dexterity, in a similar manner to a human using their hands. A study conducted with Australian parrots has demonstrated that they exhibit "handedness", a distinct preference with regards to the foot used to pick up food, with adult parrots being almost exclusively "left-footed" or "right-footed", and with the prevalence of each preference within the population varying by species.[42]
 Cockatoo species have a mobile crest of feathers on the top of their heads, which they can raise for display, and retract.[43] No other parrots can do so, but the Pacific lorikeets in the genera Vini and Phigys can ruffle the feathers of the crown and nape, and the red-fan parrot (or hawk-headed parrot) has a prominent feather neck frill that it can raise and lower at will. The predominant colour of plumage in parrots is green, though most species have some red or another colour in small quantities. Cockatoos, however, are predominately black or white with some red, pink, or yellow.[44] Strong sexual dimorphism in plumage is not typical among parrots, with some notable exceptions, the most striking being the eclectus parrot.[36]: 202–207  However, it has been shown that some parrot species exhibit sexually dimorphic plumage in the ultraviolet spectrum, normally invisible to humans.[45][46]
 Parrots are found on all tropical and subtropical continents and regions including Australia and Oceania,[5] South Asia, Southeast Asia, Central America, South America,[4] and Africa.[47] Some Caribbean and Pacific islands are home to endemic species.[48] By far the greatest number of parrot species come from Australasia and South America.[49] The lories and lorikeets range from Sulawesi and the Philippines in the north to Australia and across the Pacific as far as French Polynesia, with the greatest diversity being found in and around New Guinea.[48] The subfamily Arinae encompasses all the neotropical parrots, including the amazons, macaws, and conures, and ranges from northern Mexico and the Bahamas to Tierra del Fuego in the southern tip of South America.[50] The pygmy parrots, tribe Micropsittini, form a small genus restricted to New Guinea and the Solomon Islands.[51] The superfamily Strigopoidea contains three living species of aberrant parrots from New Zealand.[52] The broad-tailed parrots, subfamily Platycercinae, are restricted to Australia, New Zealand, and the Pacific islands as far eastwards as Fiji.[53] The true parrot superfamily, Psittacoidea, includes a range of species from Australia and New Guinea to South Asia and Africa.[48] The centre of cockatoo biodiversity is Australia and New Guinea, although some species reach the Solomon Islands (and one formerly occurred in New Caledonia),[54] Wallacea and the Philippines.[55]
 Several parrots inhabit the cool, temperate regions of South America and New Zealand. Three species—the thick-billed parrot, the green parakeet, and the now-extinct Carolina parakeet—have lived as far north as the southern United States. Many parrots, especially monk parakeets, have been introduced to areas with temperate climates, and have established stable populations in parts of the United States (including New York City),[56] the United Kingdom,[57] Belgium,[58] Spain,[59][60] Italy, Greece,[61] and Turkey. These birds can be quite successful in introduced areas, such as the non-native population of red-crowned amazons in the U.S. which may rival that of their native Mexico.[62] The only parrot to inhabit alpine climates is the kea, which is endemic to the Southern Alps mountain range on New Zealand's South Island.[63]
 Few parrots are wholly sedentary or fully migratory. Most fall somewhere between the two extremes, making poorly understood regional movements, with some adopting an entirely nomadic lifestyle.[64] Only three species are migratory – the orange-bellied, blue-winged and swift parrots.[65]
 Numerous challenges are found in studying wild parrots, as they are difficult to catch and once caught, they are difficult to mark. Most wild bird studies rely on banding or wing tagging, but parrots chew off such attachments.[64] Parrots also tend to range widely, and consequently many gaps occur in knowledge of their behaviour. Some parrots have a strong, direct flight.  Most species spend much of their time perched or climbing in tree canopies.  They often use their bills for climbing by gripping or hooking on branches and other supports. On the ground, parrots often walk with a rolling gait.[40]
 The diet of parrots consists of seeds, fruit, nectar, pollen, buds, and sometimes arthropods and other animal prey. The most important of these for most true parrots and cockatoos are seeds; the large and powerful bill has evolved to open and consume tough seeds. All true parrots, except the Pesquet's parrot, employ the same method to obtain the seed from the husk; the seed is held between the mandibles and the lower mandible crushes the husk, whereupon the seed is rotated in the bill and the remaining husk is removed.[64] They may use their foot sometimes to hold large seeds in place. Parrots are granivores rather than seed dispersers, and in many cases where they are seen consuming fruit, they are only eating the fruit to get at the seed. As seeds often have poisons that protect them, parrots carefully remove seed coats and other chemically defended fruit parts prior to ingestion. Many species in the Americas, Africa, and Papua New Guinea consume clay, which releases minerals and absorbs toxic compounds from the gut.[66]
  Geographical range and body size predominantly explains the diet composition of Neotropical parrots rather than phylogeny.[67]
 Lories, lorikeets, hanging parrots, and swift parrots are primarily nectar and pollen consumers, and have tongues with brush tips to collect it, as well as some specialised gut adaptations. Many other species also consume nectar when it becomes available.[68][69]
 Some parrot species prey on animals, especially invertebrate larvae. Golden-winged parakeets prey on water snails,[70] the New Zealand kea can, though uncommonly, hunt adult sheep,[71] and the Antipodes parakeet, another New Zealand parrot, enters the burrows of nesting grey-backed storm petrels and kills the incubating adults.[72] Some cockatoos and the New Zealand kākā excavate branches and wood to feed on grubs; the bulk of the yellow-tailed black cockatoo's diet is made up of insects.[73]
 Some extinct parrots had carnivorous diets. Pseudasturids were probably cuckoo- or puffbird-like insectivores, while messelasturids were raptor-like carnivores.[19]
 With few exceptions, parrots are monogamous breeders who nest in cavities and hold no territories other than their nesting sites.[64][74] The pair bonds of the parrots and cockatoos are strong and a pair remains close during the nonbreeding season, even if they join larger flocks. As with many birds, pair bond formation is preceded by courtship displays; these are relatively simple in the case of cockatoos. In Psittacidae parrots' common breeding displays, usually undertaken by the male, include slow, deliberate steps known as a "parade" or "stately walk" and the "eye-blaze", where the pupil of the eye constricts to reveal the edge of the iris.[64] Allopreening is used by the pair to help maintain the bond. Cooperative breeding, where birds other than the breeding pair help raise the young and is common in some bird families, is extremely rare in parrots, and has only unambiguously been demonstrated in the El Oro parakeet and the golden parakeet (which may also exhibit polygamous, or group breeding, behaviour with multiple females contributing to the clutch).[75]
 Only the monk parakeet and five species of lovebirds build nests in trees,[76] and three Australian and New Zealand ground parrots nest on the ground. All other parrots and cockatoos nest in cavities, either tree hollows or cavities dug into cliffs, banks, or the ground. The use of holes in cliffs is more common in the Americas. Many species use termite nests, possibly to reduce the conspicuousness of the nesting site or to create a favourable microclimate.[77] In most cases, both parents participate in nest excavation. The length of the burrow varies with species, but is usually between 0.5 and 2 m (1.6 and 6.6 ft) in length. The nests of cockatoos are often lined with sticks, wood chips, and other plant material. In the larger species of parrots and cockatoos, the availability of nesting hollows may be limited, leading to intense competition for them both within the species and between species, as well as with other bird families. The intensity of this competition can limit breeding success in some cases.[78][79] Hollows created artificially by arborists have proven successful in boosting breeding rates in these areas.[80] Some species are colonial, with the burrowing parrot nesting in colonies up to 70,000 strong.[81] Coloniality is not as common in parrots as might be expected, possibly because most species adopt old cavities rather than excavate their own.[82]
 The eggs of parrots are white. In most species, the female undertakes all the incubation, although incubation is shared in cockatoos, the blue lorikeet, and the vernal hanging parrot. The female remains in the nest for almost all of the incubation period and is fed both by the male and during short breaks.  Incubation varies from 17 to 35 days, with larger species having longer incubation periods. The newly born young are altricial, either lacking feathers or with sparse white down. The young spend three weeks to four months in the nest, depending on species, and may receive parental care for several months thereafter.[83]
 As typical of K-selected species, the macaws and other larger parrot species have low reproductive rates. They require several years to reach maturity, produce one or very few young per year, and do not necessarily breed every year.[84]: 125 
 Some grey parrots have shown an ability to associate words with their meanings and form simple sentences. Along with crows, ravens, and jays (family Corvidae), parrots are considered the most intelligent of birds. The brain-to-body size ratio of psittacines and corvines is comparable to that of higher primates.[85] Instead of using the cerebral cortex like mammals, birds use the mediorostral HVC for cognition.[86][failed verification] Not only have parrots demonstrated intelligence through scientific testing of their language-using ability, but also some species of parrots, such as the kea, are also highly skilled at using tools and solving puzzles.[87]
 Learning in early life is apparently important to all parrots, and much of that learning is social learning. Social interactions are often practised with siblings, and in several species, crèches are formed with several broods. Foraging behaviour is generally learnt from parents, and can be a very protracted affair. Generalists and specialists generally become independent of their parents much quicker than partly specialised species who may have to learn skills over long periods as various resources become seasonally available. Play forms a large part of learning in parrots; play can be solitary or social. Species may engage in play fights or wild flights to practice predator evasion. An absence of stimuli can delay the development of young birds, as demonstrated by a group of vasa parrots kept in tiny cages with domesticated chickens from the age of three months; at nine months, these birds still behaved in the same way as 3-month-olds, but had adopted some chicken behaviour.[64] In a similar fashion, captive birds in zoo collections or pets can, if deprived of stimuli, develop stereotyped and harmful behaviours like self-plucking. Aviculturists working with parrots have identified the need for environmental enrichment to keep parrots stimulated.[88]
 Many parrots can imitate human speech or other sounds. A study by scientist Irene Pepperberg suggested a high learning ability in a grey parrot named Alex. Alex was trained to use words to identify objects, describe them, count them, and even answer complex questions such as "How many red squares?" with over 80% accuracy.[89] N'kisi, another grey parrot, has been shown to have a vocabulary of around a thousand words, and has displayed an ability to invent and use words in context in correct tenses.[90]
 Parrots do not have vocal cords, so sound is accomplished by expelling air across the mouth of the trachea in the organ called the syrinx. Different sounds are produced by changing the depth and shape of the trachea.[91] Grey parrots are known for their superior ability to imitate sounds and human speech, which has made them popular pets since ancient times.[92]
 Although most parrot species are able to imitate, some of the amazon parrots are generally regarded as the next-best imitators and speakers of the parrot world. The question of why birds imitate remains open, but those that do often score very high on tests designed to measure problem-solving ability. Wild grey parrots have been observed imitating other birds.[93]
 Parrots are unusual among birds due to their learned vocalizations, a trait they share with only hummingbirds and songbirds.[94] The syrinx (vocal organ) of parrots, which aids in their ability to produce song, is located at the base of the trachea and consists of two complex syringeal muscles that allow for the production of sound vibrations, and a pair of lateral tympaniform membranes that control sound frequency.[95] The position of the syrinx in birds allows for directed air flow into the interclavicular air sacs according to air sac pressure, which in turn creates a higher and louder tone in birds' singing.[94]
 A 2011 study stated that some African grey parrots preferred to work alone, while others like to work together.[96] With two parrots, they know the order of tasks or when they should do something together at once, but they have trouble exchanging roles. With three parrots, one parrot usually prefers to cooperate with one of the other two, but all of them are cooperating to solve the task.[97]
 Parrots may not make good pets for most people because of their natural wild instincts such as screaming and chewing. Although parrots can be very affectionate and cute when immature, they often become aggressive when mature (partly due to mishandling and poor training) and may bite, causing serious injury.[98] For this reason, parrot rescue groups estimate that most parrots are surrendered and rehomed through at least five homes before reaching their permanent destinations or before dying prematurely from unintentional or intentional neglect and abuse. The parrots' ability to mimic human words and their bright colours and beauty prompt impulse buying from unsuspecting consumers. The domesticated budgerigar, a small parrot, is the most popular of all pet bird species.[99] In 1992, the newspaper USA Today published that 11 million pet birds were in the United States alone,[100] many of them parrots. Europeans kept birds matching the description of the rose-ringed parakeet (or called the ring-necked parrot), documented particularly in a first-century account by Pliny the Elder.[101] As they have been prized for thousands of years for their beauty and ability to talk, they have also often been misunderstood. For example, author Wolfgang de Grahl says in his 1987 book The Grey Parrot that some importers had parrots drink only coffee while they were shipped by boat, believing that pure water was detrimental and that their actions would increase survival rates during shipping.[102] Nowadays, it is commonly accepted that the caffeine in coffee is toxic to birds.[103]
 Pet parrots may be kept in a cage or aviary; though generally, tame parrots should be allowed out regularly on a stand or gym. Depending on locality, parrots may be either wild-caught or be captive-bred, though in most areas without native parrots, pet parrots are captive-bred.  Parrot species that are commonly kept as pets include conures, macaws, amazon parrots, cockatoos, greys, lovebirds, cockatiels, budgerigars, caiques, parakeets, and  Eclectus, Pionus, and Poicephalus species. Temperaments and personalities vary even within a species, just as with dog breeds. Grey parrots are thought to be excellent talkers, but not all grey parrots want to talk, though they have the capability to do so. Noise level, talking ability, cuddliness with people, and care needs can sometimes depend on how the bird is cared for and the attention he/she regularly receives.[104]
 Parrots invariably require an enormous amount of attention, care, and intellectual stimulation to thrive, akin to that required by a three-year-old child, which many people find themselves unable to provide in the long term.[105] Parrots that are bred for pets may be hand-fed or otherwise accustomed to interacting with people from a young age to help ensure they become tame and trusting.  However, even when hand fed, parrots revert to biting and aggression during hormonal surges and if mishandled or neglected.[106] Parrots are not low-maintenance pets; they require feeding, grooming, veterinary care, training, and environmental enrichment through the provision of toys, exercise, and social interaction (with other parrots or humans) for good health.[107]
 Some large parrot species, including large cockatoos, amazons, and macaws, have very long lifespans, with 80 years being reported,[108] and record ages of over 100.[109] Small parrots, such as lovebirds, hanging parrots, and budgies, have shorter lifespans up to 15–20 years.[110] Some parrot species can be quite loud, and many of the larger parrots can be destructive and require a very large cage, and a regular supply of new toys, branches, or other items to chew up.[104] The intelligence of parrots means they are quick to learn tricks and other behaviours—both good and bad—that get them what they want, such as attention or treats.[107]
 The popularity, longevity, and intelligence of many of the larger kinds of pet parrots and their wild traits such as screaming, has led to many birds needing to be rehomed during the course of their long lifespans. A common problem is that large parrots that are cuddly and gentle as juveniles mature into intelligent, complex, often demanding adults who can outlive their owners, and can also become aggressive or even dangerous. Due to an increasing number of homeless parrots, they are being euthanised like dogs and cats, and parrot adoption centres and sanctuaries are becoming more common.[84]: 77–78  Parrots do not often do well in captivity, causing some parrots to go insane and develop repetitive behaviours, such as swaying and screaming, or they become riddled with intense fear. Feather destruction and self-mutilation, although not commonly seen in the wild, occur often in captivity.[111][112]
 The popularity of parrots as pets has led to a thriving—and often illegal—trade in the birds, and some species are now threatened with extinction. A combination of trapping of wild birds and damage to parrot habitats makes survival difficult or even impossible for some species of parrot. Importation of wild-caught parrots into the US and Europe is illegal after the Wild Bird Population Act was passed in 1992.[115]
 The scale of the problem can be seen in the Tony Silva case of 1996, in which a parrot expert and former director at Tenerife's Loro Parque (Europe's largest parrot park) was jailed in the United States for 82 months and fined $100,000 for smuggling hyacinth macaws (such birds command a very high price.)[116]
 Different nations have different methods of handling internal and international trade. Australia has banned the export of its native birds since 1960.[117] In July 2007, following years of campaigning by NGOs and outbreaks of avian flu, the European Union (EU) halted the importation of all wild birds with a permanent ban on their import.[118] Prior to an earlier temporary ban started in late October 2005, the EU was importing about two million live birds a year, about 90% of the international market: hundreds of thousands of these were parrots.[119] No national laws protect feral parrot populations in the U.S.[120]
 Mexico has a licensing system for capturing and selling native birds.[121] According to a 2007 report, 65,000 to 78,500 parrots are captured annually, but the mortality rate before reaching a buyer is over 75%, meaning around 50,000 to 60,000 will die.[122]
 Parrots have featured in human writings, story, art, humor, religion, and music for thousands of years, such as Aesop's fable "The parrot and the cat",[123] the mention "The parrot can speak, and yet is nothing more than a bird" in The Book of Rites of Ancient China,[124] the Masnavi by Rumi of Persia in 1250 "The Merchant and the Parrot".[125] Recent books about parrots in human culture include Parrot Culture.[126]
 In ancient times and current, parrot feathers have been used in ceremonies and for decoration.[127] They also have a long history as pets, stretching back thousands of years, and were often kept as a symbol of royalty or wealth.[128]
 Parrots are used as symbols of nations and nationalism. A parrot is found on the flag of Dominica and two parrots on their coat of arms.[129] The St. Vincent parrot is the national bird of St. Vincent and the Grenadines, a Caribbean nation.[130]
 Sayings about parrots colour the modern English language.  The verb "parrot" in the dictionary means "to repeat by rote".  Also clichés such as the British expression "sick as a parrot" are given; although this refers to extreme disappointment rather than illness, it may originate from the disease of psittacosis, which can be passed to humans.[131][132] The first occurrence of a related expression is in Aphra Behn's 1681 play The False Count.[133] Fans of Jimmy Buffett are known as parrotheads.[134] Parrots feature in many media. Magazines are devoted to parrots as pets, and to the conservation of parrots.[135] Fictional media include Monty Python's "Dead Parrot sketch",[136] Home Alone 3[137] and Rio;[138] and documentaries include The Wild Parrots of Telegraph Hill.[139]
 As early as the ancient Chinese Shang dynasty (c. 1600 BCE ~ 1045 BCE), jade artifacts are found crafted in the shape of parrots and were subjected to burning over wood along with other jade objects and livestock, likely as a part of ritual sacrifices known as 'Liao' sacrifices (燎祭), generating smoke offerings to the heavens, gods and ancestors. This ritual is believed to have been inherited from previous worship practices and continued into the Zhou Dynasty. A jade parrot, among other artifacts, recovered from the tomb of Fu Hao at Yinxu provides significant evidence of this practice.[140]
 In Polynesian legend as current in the Marquesas Islands, the hero Laka/Aka is mentioned as having undertaken a long and dangerous voyage to Aotona in what are now the Cook Islands, to obtain the highly prized feathers of a red parrot as gifts for his son and daughter. On the voyage, 100 of his 140 rowers died of hunger on their way, but the survivors reached Aotona and captured enough parrots to fill 140 bags with their feathers.[141][142]
 Parrots have also been considered sacred. The Moche people of ancient Peru worshipped birds and often depicted parrots in their art.[143] Parrots are popular in Buddhist scripture and many writings about them exist. For example, Amitābha once changed himself into a parrot to aid in converting people. Another old story tells how after a forest caught fire, the parrot was so concerned, it carried water to try to put out the flames. The ruler of heaven was so moved upon seeing the parrot's act, he sent rain to put out the fire.[144] In Chinese Buddhist iconography, a parrot is sometimes depicted hovering on the upper right side Guan Yin clasping a pearl or prayer beads in its beak.[145]
 In Hindu mythology, the parrot is the mount of the god of love, Kamadeva.[146] The bird is also associated with the goddess Meenakshi and the poet-saint Andal.[147]
 Escaped parrots of several species have become established in the wild outside their natural ranges and in some cases outside the natural range of parrots. Among the earliest instances were pet red shining-parrots from Fiji, which established a population on the islands of southern Tonga. These introductions were prehistoric and red-shining parrots were recorded in Tonga by Captain Cook in the 1770s.[54] Escapees first began breeding in cities in California, Texas, and Florida in the 1950s (with unproven earlier claims dating to the 1920s in Texas and Florida).[59] They have proved surprisingly hardy in adapting to conditions in Europe and North America. They sometimes even multiply to the point of becoming a nuisance or pest, and a threat to local ecosystems, and control measures have been used on some feral populations.[148]
 Feral parrot flocks can be formed after mass escapes of newly imported, wild-caught parrots from airports or quarantine facilities. Large groups of escapees have the protection of a flock and possess the skills to survive and breed in the wild.[149] Some feral parakeets may have descended from escaped zoo birds. Escaped or released pets rarely contribute to establishing feral populations, as they usually result in only a few escapees, and most captive-born birds do not possess the necessary survival skills to find food or avoid predators and often do not survive long without human caretakers. However, in areas where there are existing feral parrot populations, escaped pets may sometimes successfully join these flocks.[149][150] The most common years that feral parrots were released to non-native environments was from the 1890s to the 1940s, during the wild-caught parrot era.[150] In the "parrot fever" panic of 1930, a city health commissioner urged everyone who owned a parrot to put them down, but some owners abandoned their parrots on the streets.[151]
 The principal threats of parrots are habitat loss and degradation, hunting, and, for certain species, the wild-bird trade.[3] Parrots are persecuted because, in some areas, they are (or have been) hunted for food and feathers, and as agricultural pests. For a time, Argentina offered a bounty on monk parakeets for that reason, resulting in hundreds of thousands of birds being killed, though apparently this did not greatly affect the overall population.[153]
 Parrots, being cavity nesters, are vulnerable to the loss of nesting sites and to competition with introduced species for those sites. The loss of old trees is a particular problem in some areas, particularly in Australia, where suitable nesting trees must be centuries old. Many parrots occur only on islands and are vulnerable to introduced species such as rats and feral cat, as they lack the appropriate antipredator behaviours needed to deal with predators.[154] Island species, such as the Puerto Rican amazon, which have small populations in restricted habitats, are also vulnerable to natural events, such as hurricanes.[155] Due to deforestation, the Puerto Rican amazon is one of the world's rarest birds despite conservation efforts.[156]
 One of the largest parrot conservation groups is the World Parrot Trust,[157] an international organisation. The group gives assistance to worthwhile projects, as well as producing a magazine (PsittaScene)[158] and raising funds through donations and memberships, often from pet parrot owners. On a smaller scale, local parrot clubs raise money to donate to a conservation cause. Zoo and wildlife centres usually provide public education, to change habits that cause damage to wild populations. Conservation measures to conserve the habitats of some of the high-profile charismatic parrot species has also protected many of the less charismatic species living in the ecosystem.[159]: 12  A popular attraction that many zoos employ is a feeding station for lories and lorikeets, where visitors feed them with cups of liquid food. This is usually done in association with educational signs and lectures.[160] Birdwatching-based ecotourism can be beneficial to economies.[161]
 Several projects aimed specifically at parrot conservation have met with success. Translocation of vulnerable kākāpō, followed by intensive management and supplementary feeding, has increased the population from 50 individuals to 123 in 2010.[162] In New Caledonia, the Ouvea parakeet was threatened by trapping for the pet trade and loss of habitat. Community-based conservation, which eliminated the threat of poaching, has allowed the population to increase from around 600 birds in 1993 to over 2000 birds in 2009.[163]
 As of 2009, the IUCN recognises 19 species of parrot as extinct since 1500 (the date used to denote modern extinctions).[164] This does not include species like the New Caledonian lorikeet, which has not been officially seen for 100 years, yet is still listed as critically endangered.[165]
 Trade, export, and import of all wild-caught parrots is regulated and only permitted under special licensed circumstances in countries party to the Convention on the International Trade in Endangered Species (CITES) which came into force in 1975 to regulate the international trade of all endangered, wild-caught animal and plant species. In 1975, 24 parrot species were included in Appendix I, thus prohibiting commercial international trade in these birds. Since that initial listing, continuing threats from international trade led it to add an additional 32 parrot varieties to Appendix I.[166] All other parrot species, aside from the rosy-faced lovebird, budgerigar, cockatiel and rose-ringed parakeet (which are not included in the appendices) are protected on Appendix II of CITES.[167][168] In addition, individual countries may have laws to regulate trade in certain species; for example, the EU has banned parrot trade,[119] whereas Mexico has a licensing system for capturing parrots.[121]
 Every year on 31 May, World Parrot Day is celebrated.[169]


Source: https://en.wikipedia.org/wiki/Just-in-time-Kompilierung
Content: Other reasons this message may be displayed:


Source: https://en.wikipedia.org/wiki/PyPy
Content: PyPy (/ˈpaɪpaɪ/) is an implementation of the Python programming language.[2] PyPy often runs faster than the standard implementation CPython because PyPy uses a just-in-time compiler.[3] Most Python code runs well on PyPy except for code that depends on CPython extensions, which either does not work or incurs some overhead when run in PyPy.
 PyPy itself is built using a technique known as meta-tracing, which is a mostly automatic transformation that takes an interpreter as input and produces a tracing just-in-time compiler as output. Since interpreters are usually easier to write than compilers, but run slower, this technique can make it easier to produce efficient implementations of programming languages. PyPy's meta-tracing toolchain is called RPython.
 PyPy does not have full compatibility with more recent versions of the CPython ecosystem. While it claims compatibility with Python 2.7, 3.7, 3.8 and 3.9 ("a drop-in replacement for CPython"), it lacks some of the newer features and syntax in Python 3.10, such as syntax for pattern matching.[4]
 PyPy aims to provide a common translation and support framework for producing implementations of dynamic languages, emphasizing a clean separation between language specification and implementation aspects. It also aims to provide a compliant, flexible and fast implementation of the Python programming language using the above framework to enable new advanced features without having to encode low-level details into it.[5][6]
 The PyPy interpreter itself is written in a restricted subset of Python called RPython (Restricted Python).[7] RPython puts some constraints on the Python language such that a variable's type can be inferred at compile time.[8]
 The PyPy project has developed a toolchain that analyzes RPython code and translates it into a form of byte code, which can be lowered into C. There used to be other backends in addition to C (Java, C#, and Javascript), but those suffered from bitrot and have been removed. Thus, the recursive logo of PyPy is a snake swallowing itself since the RPython is translated by a Python interpreter. The code can also be run untranslated for testing and analysis, which provides a nice test-bed for research into dynamic languages.
 It allows for pluggable garbage collectors, as well as optionally enabling Stackless Python features. Finally, it includes a just-in-time (JIT) generator that builds a just-in-time compiler into the interpreter, given a few annotations in the interpreter source code. The generated JIT compiler is a tracing JIT.[9]
 RPython is now also used to write non-Python language implementations, such as Pixie.[10]
 PyPy as of version 7.3.7 is compatible with three CPython versions: 2.7, 3.7 and 3.8.[11][12] The first PyPy version compatible with CPython v3 is PyPy v2.3.1 (2014).[13] The PyPy interpreter compatible with CPython v3 is also known as PyPy3.
 PyPy has JIT compilation support on 32-bit/64-bit x86 and 32-bit/64-bit ARM processors.[14] It is tested nightly on Windows, Linux, OpenBSD and Mac OS X. PyPy is able to run pure Python software that does not rely on implementation-specific features.[15]
 There is a compatibility layer for CPython C API extensions called CPyExt, but it is incomplete and experimental. The preferred way of interfacing with C shared libraries is through the built-in C foreign function interface (CFFI) or ctypes libraries.
 PyPy is a followup to the Psyco project, a just-in-time specializing compiler for Python, developed by Armin Rigo between 2002 and 2010. PyPy's aim is to have a just-in-time specializing compiler with scope, which was not available for Psyco.[clarification needed] Initially, the RPython could also be compiled into Java bytecode, CIL and JavaScript, but these backends were removed due to lack of interest.
 PyPy was initially a research and development-oriented project. Reaching a mature state of development and an official 1.0 release in mid-2007, its next focus was on releasing a production-ready version with more CPython compatibility. Many of PyPy's changes have been made during coding sprints.
 PyPy was funded by the European Union being a Specific Targeted Research Project[31] between December 2004 and March 2007. In June 2008, PyPy announced funding being part of the Google Open Source programs and has agreed to focus on making PyPy more compatible with CPython. In 2009 Eurostars, a European Union funding agency specially focused on SMEs,[32] accepted a proposal from PyPy project members titled "PYJIT – a fast and flexible toolkit for dynamic programming languages based on PyPy". Eurostars funding lasted until August 2011.[33]
At PyCon US 2011, the Python Software Foundation provided a $10,000 grant for PyPy to continue work on performance and compatibility with newer versions of the language.[34]
The port to ARM architecture was sponsored in part by the Raspberry Pi Foundation.[22]
 The PyPy project also accepts donations through its status blog pages.[35] As of 2013, a variety of sub-projects had funding: Python 3 version compatibility, built-in optimized NumPy support for numerical calculations and software transactional memory support to allow better parallelism.[22]


Source: https://en.wikipedia.org/wiki/Mikrocontroller
Content: Other reasons this message may be displayed:


Source: https://en.wikipedia.org/wiki/MicroPython
Content: 
 MicroPython is a software implementation of a programming language largely compatible with Python 3, written in C, that is optimized to run on a microcontroller.[2][3]
 MicroPython consists of a Python compiler to bytecode and a runtime interpreter of that bytecode. The user is presented with an interactive prompt (the REPL) to execute supported commands immediately. Included are a selection of core Python libraries; MicroPython includes modules which give the programmer access to low-level hardware.[4]
 MicroPython does have an inline assembler, which lets the code run at full speed, but it is not portable across different microcontrollers.
 The source code for the project is available on GitHub under the MIT License.[5]
 MicroPython was originally created by the Australian programmer Damien George, after a successful Kickstarter-backed campaign in 2013.[6] While the original Kickstarter campaign released MicroPython with an STM32F4-powered development board "pyboard", MicroPython supports a number of ARM based architectures.[7] The ports supported in the mainline are ARM Cortex-M (many STM32[8] boards, RP2040 boards, TI CC3200/WiPy, Teensy boards, Nordic nRF series, SAMD21 and SAMD51), ESP8266, ESP32,[9] 16-bit PIC, Unix, Windows, Zephyr, and JavaScript.[10] Also, there are many forks for a variety of systems and hardware platforms not supported in the mainline.[11]
 In 2016, a version of MicroPython for the BBC Micro Bit was created as part of the Python Software Foundation's contribution to the Micro Bit partnership with the BBC.[12]
 In July 2017, MicroPython was forked to create CircuitPython, a version of MicroPython with emphasis on education and ease of use. MicroPython and CircuitPython support somewhat different sets of hardware (e.g. CircuitPython supports Atmel SAM D21 and D51 boards, but dropped support for ESP8266). As of version 4.0, CircuitPython is based on MicroPython version 1.9.4.[13]
 In 2017, Microsemi made a MicroPython port for RISC-V (RV32 and RV64) architecture.[14]
 In April 2019, a version of MicroPython for the Lego Mindstorms EV3 was created.[15]
 In January 2021, a MicroPython port for the RP2040 (ARM Cortex-M0+, on Raspberry Pi Pico and others) was created.[16]
 MicroPython has the ability to run Python, allowing users to create simple and easy-to-understand programs.[17]  MicroPython supports many standard Python libraries, supporting more than 80% of the features of Python's most used libraries.[17] MicroPython was designed specifically to support the typical performance gap between microcontrollers and Python.[18] Python code is able to directly access and interact with hardware, with increased hardware possibilities that are not available using a normal Python application that is run on an operating system.[19]
 MicroPython's utilisation of hardware abstraction layer (HAL) technology allows developed code to be portable among different microcontrollers within the same family or platform and on devices that support and can download MicroPython. Programs are often developed and tested on high-performance microcontrollers and distributed with the final application used on lower-performance microcontrollers.[20]
 MicroPython offers functionality, once new code has been written, to create a frozen module and use it as a library which can be a part of developed firmware. This feature assists with avoiding repetitive downloading of the same, already error-free, tested code into a MicroPython environment. This type of module will be saved to a microcontroller's modules directory for compiling and uploading to the microcontroller where the library will be available using Python's import command to be used repeatedly.[20]
 The read–eval–print loop (REPL) allows a developer to enter individual lines of code and have them run immediately on a terminal.[21] Linux-based and macOS systems have terminal emulators that can be used to create a direct connection to a MicroPython device's REPL using a serial USB connection. The REPL assists with the immediate testing of parts of an application as you can run each part of the code and visually see the results. Once different parts of your code is loaded into the REPL you can use additional REPL features to experiment with your code's functionality.[17]
 Helpful REPL commands (once connected to a serial console):[21]
 Although MicroPython fully implements Python language version 3.4 and much of 3.5, it does not implement all language features introduced from 3.5 onwards,[22] though some new syntax from 3.6 and more recent features from later versions, e.g. from 3.8 (assignment expressions) and 3.9. It includes a subset of the standard library.[23]
 MicroPython has more limited hardware support in the microcontroller market than other popular platforms, like Arduino with a smaller number of microcontroller choices that support the language.[18] MicroPython does not include an integrated development environment (IDE) or specific editor unlike other platforms.[18]
 MicroPython's syntax is adopted from Python, due to its clear and easy-to-understand style and power.[24] Unlike most other programming languages less punctuation is used with fewer syntactical machinations in order to prioritise readability.[17]
 MicroPython adopts Python's code block style, with code specific to a particular function, condition or loop being indented.[17] This differs from most other languages which typically use symbols or keywords to delimit blocks.[17]  This assists with the readability of MicroPython code as the visual structure mirrors the semantic structure. This key feature is simple but important as misused indentation can result in code executing under a wrong condition or an overall error from the interpreter.[17]
 A colon (:) is the key symbol used to indicate the ending of a condition statement.[17] The indent size is equivalent to one tab or 4 spaces.
 MicroPython has the ability to perform various mathematical operations using primitive and logical operations.[19]
 MicroPython is a lean and efficient implementation of Python with libraries similar to those in Python.[25] Some standard Python libraries have an equivalent library in MicroPython renamed to distinguish between the two. MicroPython libraries are smaller with less popular features removed or modified to save memory.[19]
 The three types of libraries in MicroPython:[19]
 MicroPython is highly customisable and configurable, with language differing between each board (microcontroller) and the availability of libraries may differ. Some functions and classes in a module or the entire module may be unavailable or altered.[19]
 When developers begin to create a new application, standard MicroPython libraries and drivers may not meet the requirements, with insufficient operations or calculations. Similar to Python, there is the possibility of extending MicroPython's functionality with custom libraries which extend the ability of the existing libraries and firmware.[20]
 In MicroPython, files ending with .py take preference over other library aliases which allows users to extend the use and implementation of the existing libraries.[19]
 As MicroPython's implementation and popularity continues to grow, more boards have the ability to run MicroPython. Many developers are building processor specific versions that can be downloaded onto different microcontrollers.[19] Installing MicroPython on microcontrollers is well documented and user-friendly.[20] MicroPython allows interactions between microcontroller hardware and applications to be simple, allowing access to a range of functionality while working in a resource constrained environment, with a strong level of responsiveness.[17]
 The two types of boards used to run MicroPython:[19]
 To move a program onto a MicroPython board, create a file and copy it onto the microcontroller in order to execute. With the hardware connected to a device, such as a computer, the board's flash drive will appear on the device allowing files to be moved to the flash drive. There will be two existing python files, boot.py and main.py that are typically not modified, main.py may be modified if you wish to run the program every time the microcontroller is booted, otherwise, programs will be run using the REPL console.[19]
 The pyboard is the official MicroPython microcontroller board which fully supports MicroPython's software features. The pyboard's hardware features include:[4]
 The pyboard contains an internal drive (filesystem) named /flash which is stored within the board's flash memory, additionally, a microSD card can be inserted into a slot and is accessible through /sd. When booted up, a pyboard must select a filesystem to boot from either /flash or /sd with the current directory being set to either /flash or /sd. By default, if an SD card is inserted, /sd will be used, if not, /flash is used. If needed, the use of the SD card for the booting process can be avoided by creating an empty file called /flash/SKIPSD which will remain on the board and exist when the pyboard is booted up and will skip the SD card for the booting process.[4]
 When the pyboard is powered up normally or the reset button is pressed then the pyboard is booted in a standard mode, meaning that the boot.py file is executed, then the USB configured and finally the python program will run.[4]
 There is an ability to override the standard boot sequence through holding down the user switch whilst the board is in the booting process and then pressing reset as you continue to hold the user switch. The pyboard's LEDs will flick between modes and once the LEDs have reached the mode wanted by the user, they can let go of the user switch and the board will boot in the specific mode.[4]
 the boot modes are:[4]
 Hello world program:
 Importing + turning on a LED:
 Reading a file + loop:
 MicroPython includes a cross compiler which generates MicroPython bytecode (file extension .mpy). The Python code can be compiled into the bytecode either directly on a microcontroller or it can be precompiled elsewhere.
 MicroPython firmware can be built without the compiler, leaving only the virtual machine which can run the precompiled mpy programs.
 MicroPython is utilised through firmware being loaded by standard software onto a particular microcontroller into flash memory, communicating using a terminal application loaded onto a computer that emulates a serial interface.[20]
 The main uses of MicroPython can be generalised into 3 categories:[20]
 Implementation of MicroPython can differ depending on the availability of standard and supporting libraries and the microcontroller's flash memory and RAM size.[20]


Source: https://en.wikipedia.org/wiki/IDLE
Content: IDLE (short for Integrated Development and Learning Environment)[2][3] is an integrated development environment for Python, which has been bundled with the default implementation of the language since 1.5.2b1.[4][5] It is packaged as an optional part of the Python packaging with many Linux distributions. It is completely written in Python and the Tkinter GUI toolkit (wrapper functions for Tcl/Tk).
 IDLE is intended to be a simple IDE and suitable for beginners, especially in an educational environment. To that end, it is cross-platform, and avoids feature clutter.
 According to the included README, its main features are:
 Author Guido van Rossum says IDLE stands for "Integrated Development and Learning Environment",[6] and since Van Rossum named the language Python after the British comedy group Monty Python, the name IDLE was probably also chosen partly to honor Eric Idle, one of Monty Python's founding members.[7][8]


Source: https://en.wikipedia.org/wiki/Shell_(Betriebssystem)
Content: Other reasons this message may be displayed:


Source: https://en.wikipedia.org/wiki/Integrierte_Entwicklungsumgebung
Content: Other reasons this message may be displayed:


Source: https://en.wikipedia.org/wiki/Eric_Python_IDE
Content: eric is a free integrated development environment (IDE) used for computer programming. Since it is a full featured IDE, it provides by default all necessary tools needed for the writing of code and for the professional management of a software project.
 eric is written in the programming language Python and its primary use is for developing software written in Python. It is usable for development of any combination of Python 3 or Python 2, Qt 5 or Qt 4 and PyQt 5 or PyQt 4 projects, on Linux, macOS and Microsoft Windows platforms.
 eric is licensed under the GNU General Public License version 3 or later and is thereby Free Software. This means in general terms that the source code of eric can be studied, changed and improved by anyone, that eric can be run for any purpose by anyone and that eric - and any changes or improvements that may have been made to it - can be redistributed by anyone to anyone as long as the license is not changed (copyleft).
 eric can be downloaded at SourceForge and installed manually with a python installer script.[3]
Most major Linux distributions include eric in their software repositories, so when using such Linux distributions eric can be obtained and installed automatically by using the package manager of the particular distribution.[4]
Additionally, the author offers access to the source code via a public Mercurial repository.[5]
 eric is written in Python and uses the PyQt Python bindings for the Qt GUI toolkit.[6] By design, eric acts as a front end for several programs, for example the QScintilla editor widget.[7]
 The key features of eric 6 are:[8]
 Prior to the release of eric version 5.5.0, eric version 4 and eric version 5 coexisted and were maintained simultaneously, while eric 4 was the variant for writing software in Python version 2 and eric version 5 was the variant for writing software in Python version 3.
 With the release of eric version 5.5.0 both variants had been merged into one, so that all versions as of eric version 5.5.0 support writing software in Python 2 as well as in Python 3, making the separate development lanes of eric version 4 and 5 obsolete. Those two separate development lanes are no longer maintained, and the last versions prior to merging them both to 5.5.0 were versions 4.5.25 and 5.4.7.[9]
 Until 2016, eric used a software versioning scheme with a three-sequence identifier, e.g. 5.0.1. The first sequence represents the major version number which is increased when there are significant jumps in functionality, the second sequence represents the minor number, which is incremented when only some features or significant fixes have been added, and the third sequence is the revision number, which is incremented when minor bugs are fixed or minor features have been added.
 From late 2016, the version numbers show the year and month of release, e.g. 16.11 for November 2016.[10]
 eric follows the development philosophy of Release early, release often, following loosely a time-based release schedule. Currently a revision version is released around the first weekend of every month, a minor version is released annually, in most cases approximately between December and February.
 The following table shows the version history of eric, starting from version 4.0.0.
Only major (e.g. 6.0.0) and minor (e.g. 6.1.0) releases are listed; revision releases (e.g. 6.0.1) are omitted.
 Several allusions are made to the British comedy group Monty Python, which the Python programming language is named after. Eric alludes to Eric Idle, a member of the group, as does IDLE, the standard python IDE shipped with most distributions.[17]


Source: https://en.wikipedia.org/wiki/Spyder_(Software)
Content: Other reasons this message may be displayed:


Source: https://en.wikipedia.org/wiki/PyCharm
Content: PyCharm is an integrated development environment (IDE) used for programming in Python. It provides code analysis, a graphical debugger, an integrated unit tester, integration with version control systems, and supports web development with Django. PyCharm is developed by the Czech company JetBrains.[4]
 It is cross-platform, working on Microsoft Windows, macOS and Linux. PyCharm has a Professional Edition, released under a proprietary license  and a Community Edition released under the Apache License.[5] PyCharm Community Edition is less extensive than the Professional Edition.
 PyCharm was released to the market of the Python-focused IDEs to compete with PyDev (for Eclipse)  or the more broadly focused Komodo IDE by ActiveState.
 The beta version of the product was released in July 2010, with the 1.0 arriving 3 months later. Version 2.0 was released on 13 December 2011, version 3.0 was released on 24 September 2013, and version 4.0 was released on November 19, 2014.[8]
 PyCharm became Open Source on 22 October 2013. The Open Source variant is released under the name Community Edition – while the commercial variant, Professional Edition, contains closed-source modules.[5]


Source: https://en.wikipedia.org/wiki/Plug-in
Content: Plug-in, plug in or plugin may refer to:


Source: https://en.wikipedia.org/wiki/Eclipse_(IDE)
Content: 
 Eclipse is an integrated development environment (IDE) used in computer programming.[5] It contains a base workspace and an extensible plug-in system for customizing the environment. It is the second-most-popular IDE for Java development, and, until 2016, was the most popular.[6] Eclipse is written mostly in Java and its primary use is for developing Java applications,[7] but it may also be used to develop applications in other programming languages via plug-ins, including Ada, ABAP, C, C++, C#, Clojure, COBOL, D, Erlang, Fortran, Groovy, Haskell, JavaScript, Julia,[8] Lasso, Lua, NATURAL, Perl, PHP, Prolog, Python, R, Ruby (including Ruby on Rails framework), Rust, Scala, and Scheme. It can also be used to develop documents with LaTeX (via a TeXlipse plug-in) and packages for the software Mathematica. Development environments include the Eclipse Java development tools (JDT) for Java and Scala, Eclipse CDT for C/C++, and Eclipse PDT for PHP, among others.[9]
 The initial codebase originated from IBM VisualAge.[10] The Eclipse software development kit (SDK), which includes the Java development tools, is meant for Java developers. Users can extend its abilities by installing plug-ins written for the Eclipse Platform, such as development toolkits for other programming languages, and can write and contribute their own plug-ins. Since Eclipse 3.0 (released in 2004), plug-ins are installed and managed as "bundles" using Equinox, an implementation of OSGi.[11]
 The Eclipse SDK is free and open-source software, released under the terms of the Eclipse Public License, although it is incompatible with the GNU General Public License.[12] It was one of the first IDEs to run under GNU Classpath and it runs without problems under IcedTea.
 Eclipse was inspired by the Smalltalk-based VisualAge family of integrated development environment (IDE) products.[10] Although fairly successful, a major drawback of the VisualAge products was that developed code was not in a component-based software engineering model. Instead, all code for a project was held in a compressed database using SCID techniques (somewhat like a zip file but in .dat). Individual classes could not be easily accessed, certainly not outside the tool. A team primarily at the IBM Cary, NC lab developed the new product as a Java-based replacement.[13][failed verification]
In November 2001, a consortium was formed with a board of stewards to further the development of Eclipse as open-source software. It is estimated that IBM had already invested nearly $40 million by that time.[14] The original members were Borland, IBM, Merant, QNX Software Systems, Rational Software, Red Hat, SuSE, TogetherSoft, and WebGain.[15] The number of stewards increased to over 80 by the end of 2003. In January 2004, the Eclipse Foundation was created.[16]
 Eclipse 3.0 (released on 21 June 2004) selected the OSGi Service Platform specifications as the runtime architecture.[17]
 The Association for Computing Machinery recognized Eclipse with the 2011 ACM Software System Award on 26 April 2012.[18]
 The Eclipse Public License (EPL) is the fundamental license under which Eclipse projects are released.[19] Some projects require dual licensing, for which the Eclipse Distribution License (EDL) is available, although use of this license must be applied for and is considered on a case-by-case basis.
 Eclipse was originally released under the Common Public License, but was later re-licensed under the Eclipse Public License. The Free Software Foundation has said that both licenses are free software licenses, but are incompatible with the GNU General Public License (GPL).[20]
 According to Lee Nackman, Chief Technology Officer of IBM's Rational division (originating in 2003) at that time, the name "Eclipse" (dating from at least 2001) was not a wordplay on Sun Microsystems, as the product's primary competition at the time of naming was Microsoft Visual Studio, which Eclipse was to eclipse.[21]
 Different versions of Eclipse have been given different science-related names. The versions named after Callisto, Europa, and Ganymede, which are moons of Jupiter, were followed by a version named after Galileo, the discoverer of those moons. These were followed by two sun-themed names, Helios of Greek mythology, and Indigo, one of the seven colors of a rainbow (which is produced by the sun). The version after that, Juno, has a triple meaning: a Roman mythological figure, an asteroid, and a spacecraft to Jupiter.[22] Kepler, Luna, and Mars continued the astronomy theme, and then Neon and Oxygen constituted a theme of chemical elements. Photon represented a return to sun-themed names.
 As of 2018[update], the alphabetic scheme was abandoned in order to better align with the new Simultaneous Release strategy.[23] Releases are named in the format YYYY-MM to reflect the quarterly releases, starting with version 4.9 named 2018-09.[24]
 Since 2006, the Foundation has coordinated an annual Simultaneous Release.[25] Each release includes the Eclipse Platform and several other Eclipse projects.
 From 2008 through 2018, each Simultaneous Release had occurred on the 4th Wednesday of June. In 2018 the project switched to quarterly (13 week) YYYY-MM releases without intermediate service releases.[26]
 [Notes 1]
 A Java 7 JRE/JDK is required to run most of the packages based on this version.[46]
 Dropped support for the following Unix based platforms: AIX, Solaris, HP-UX and s390.[53]
From this version on, a Java 8 or newer JRE/JDK is required to run Eclipse.[46]
 Eclipse uses plug-ins to provide all the functionality within and on top of the run-time system. Its run-time system is based on Equinox, an implementation of the OSGi core framework specification.[80]
 In addition to allowing the Eclipse Platform to be extended using other programming languages, such as C and Python, the plug-in framework allows the Eclipse Platform to work with typesetting languages like LaTeX[81] and networking applications such as telnet and database management systems. The plug-in architecture supports writing any desired extension to the environment, such as for configuration management. Java and CVS support is provided in the Eclipse SDK, with support for other version control systems provided by third-party plug-ins.
 With the exception of a small run-time kernel, everything in Eclipse is a plug-in. Thus, every plug-in developed integrates with Eclipse in the same way as other plug-ins; in this respect, all features are "created equal".[82] Eclipse provides plug-ins for a wide variety of features, some of which are from third parties using both free and commercial models. Examples of plug-ins include for Unified Modeling Language (UML), for Sequence and other UML diagrams, a plug-in for DB Explorer, and many more.
 The Eclipse SDK includes the Eclipse Java development tools (JDT), offering an IDE with a built-in Java incremental compiler and a full model of the Java source files. This allows for advanced refactoring techniques and code analysis. The IDE also makes use of a workspace, in this case a set of metadata over a flat filespace allowing external file modifications as long as the corresponding workspace resource is refreshed afterward.
 Eclipse implements the graphical control elements of the Java toolkit called Standard Widget Toolkit (SWT), whereas most Java applications use the Java standard Abstract Window Toolkit (AWT), Swing, or JavaFX. Eclipse's user interface also uses an intermediate graphical user interface layer called JFace, which simplifies the construction of applications based on SWT. Eclipse was made to run on Wayland during a Google Summer of Code (GSoC) Project in 2014.[83]
 As of 2017[update], language packs being developed by the Babel Project provide translations into over 40 natural languages.[4]
 Eclipse provides the rich client platform (RCP) for developing general-purpose applications.
 The following components constitute the rich client platform:
 Examples of rich client applications based on Eclipse are:
 Eclipse supports development for Tomcat, GlassFish and many other servers and is often capable of installing the required server (for development) directly from the IDE. It supports remote debugging, allowing a user to watch variables and step through the code of an application that is running on the attached server.
 The Eclipse Web Tools Platform (WTP) project is an extension of the Eclipse platform with tools for developing Web and Java EE applications. It includes source and graphical editors for a variety of languages, wizards and built-in applications to simplify development, and tools and APIs to support deploying, running, and testing apps.[85]
 The Modeling project contains all the official projects of the Eclipse Foundation focusing on model-based development technologies. All are compatible with the Eclipse Modeling Framework created by IBM. Those projects are separated in several categories: Model Transformation, Model Development Tools, Concrete Syntax Development, Abstract Syntax Development, Technology and Research, and Amalgam.
 Model Transformation projects uses Eclipse Modeling Framework (EMF) based models as an input and produce either a model or text as an output. Model to model transformation projects includes ATLAS Transformation Language (ATL), an open source transformation language and toolkit used to transform a given model or to generate a new model from a given EMF model. Model to text transformation projects contains Acceleo, an implementation of MOFM2T, a standard model to text language from the Object Management Group (OMG). The Acceleo code generator can generate any textual language (Java, PHP, Python, etc.) from EMF based models defined with any metamodel (Unified Modeling Language (UML), Systems Modeling Language (SysML), etc.). It is open-source.
 Model Development Tools projects are implementations of various modeling standards used in the industry, and their toolkits. Among those projects can be found implementations of several standards:
 The Concrete Syntax Development project contains the Graphical Modeling Framework, an Eclipse-based framework dedicated to the graphical representation of EMF based models.[citation needed]
 The Abstract Syntax Development project hosts the Eclipse Modeling Framework, core of most of the modeling project of the Eclipse Foundation and the framework available for EMF like Connected Data Objects (CDO), EMF query or EMF validation.
 Technology and Research projects are prototypes of Modeling project; this project is used to host all the modeling projects of the Eclipse Foundation during their incubation phase.[citation needed]
 Amalgam provides the packaging and integration between all the available modeling tools for the Eclipse package dedicated to modeling tools.[86]
 Application lifecycle management (ALM) and task management in Eclipse need an optional component called Mylyn (/ˈmaɪlɪn/), an open-source implementation of the task-focused interface.  It provides an API for tools embedding the task-focused interface.  For software developers, it helps a developer work efficiently with many different tasks (such as bugs, problem reports or new features). Tasks are integrated into Mylyn. For all tasks that have been integrated, Mylyn monitors user activity and tries to identify information relevant to the task at hand. It uses this task context to focus the Eclipse UI on the related information.  Mylyn can integrate with repositories such as Bugzilla, Trac, Redmine, Mantis,[87] JIRA, Unfuddle,[88] and GitHub.[89] It focuses on improving productivity by reducing searching, scrolling, and navigation. By making task context explicit, Mylyn is also meant to facilitate multitasking, planning, reusing past efforts, and sharing expertise.
 The project name comes from myelin, an electrically insulating layer that surrounds neurons' axons.[90] The original name of this project, "Mylar", replicated a trademark of a boPET film company, so the Eclipse Foundation changed the project name.[91]
 Eclipse IDE features include text editor with syntax coloring, coding assistance, code completion, code refactoring, code analysis with "Quick fix" suggestions along with code debugging.[92]
 Along with native support for OSGi, JPMS support has been added as of Java 9.[92]
 Eclipse supports a rich selection of extensions, adding support for Python via PyDev, Android development via Google's ADT (superseded by Android Studio since 2015), JavaFX via e(fx)clipse, JavaScript, jQuery, and many others at the Eclipse Marketplace. Valable is a Vala plug-in for Eclipse.[93]
 In addition to the built in Java compiler warnings, additional plug-ins are available for linting to improve code quality and consistency such as SpotBugs and Sonar.[94][95]
 Support for build tools such as Ant, Maven, Make, and CMake includes the capability to replace Eclipse native project file format with Maven pom.xml directly.[96]
 Several alternative distributions exist in the Eclipse project.
 The PHP Hypertext Preprocessor (PHP) Development Tools project provides a framework for the Eclipse platform. The project encompasses all development components, including code-completion, develop PHP and facilitate extensibility. It leverages the existing Eclipse Web Tools Platform (WTP) and Dynamic Languages Toolkit (DLTK).[97]
 Android Development Tools (ADT) was superseded in 2015 by the Eclipse foundation's own plugin, called Andmore: Development Tools for Android,[98] after Google discontinued development of their plug-in for the Eclipse IDE, that is designed to provide an integrated environment in which to build Android applications. ADT/Andmore extends the abilities of Eclipse to let developers set up new Android projects, create an application UI, add packages based on the Android Framework API, debug their applications using the Android SDK tools, and export signed (or unsigned) .apk files in order to distribute their applications. It is freely available to download. Googles' ADT was the official IDE for Android until 2015 but was replaced by Eclipse's Andmore and the official Android Studio.[99][100]


Source: https://en.wikipedia.org/wiki/Visual_Studio
Content: 
 Visual Studio is an integrated development environment (IDE) developed by Microsoft. It is used to develop computer programs including websites, web apps, web services and mobile apps. Visual Studio uses Microsoft software development platforms including Windows API, Windows Forms, Windows Presentation Foundation (WPF), Windows Store and Microsoft Silverlight. It can produce both native code and managed code.
 Visual Studio includes a code editor supporting IntelliSense (the code completion component) as well as code refactoring. The integrated debugger works as both a source-level debugger and as a machine-level debugger. Other built-in tools include a code profiler, designer for building GUI applications, web designer, class designer, and database schema designer. It accepts plug-ins that expand the functionality at almost every level—including adding support for source control systems (like Subversion and Git) and adding new toolsets like editors and visual designers for domain-specific languages or toolsets for other aspects of the software development lifecycle (like the Azure DevOps client: Team Explorer).
 Visual Studio supports 36 different programming languages[citation needed] and allows the code editor and debugger to support (to varying degrees) nearly any programming language, provided a language-specific service exists. Built-in languages include C,[6] C++, C++/CLI, Visual Basic .NET, C#, F#,[7] JavaScript, TypeScript, XML, XSLT, HTML, and CSS. Support for other languages such as Python,[8] Ruby, Node.js, and M among others is available via plug-ins. Java (and J#) were supported in the past.
 The most basic edition of Visual Studio, the Community edition, is available free of charge. The slogan for Visual Studio Community edition is "Free, fully-featured IDE for students, open-source and individual developers". As of February 19, 2024[update], Visual Studio 2022 is a current production-ready version. Visual Studio 2013, 2015 and 2017 are on Extended Support, while 2019 is on Mainstream Support.[9]
 Visual Studio does not support any programming language, solution or tool intrinsically; instead, it allows the plugging of functionality coded as a VSPackage. When installed, the functionality is available as a Service. The IDE provides three services: SVsSolution, which provides the ability to enumerate projects and solutions; SVsUIShell, which provides windowing and UI functionality (including tabs, toolbars, and tool windows); and SVsShell, which deals with registration of VSPackages. In addition, the IDE is also responsible for coordinating and enabling communication between services.[10] All editors, designers, project types and other tools are implemented as VSPackages. Visual Studio uses COM to access the VSPackages. The Visual Studio SDK also includes the Managed Package Framework (MPF), which is a set of managed wrappers around the COM-interfaces that allow the Packages to be written in any CLI compliant language.[11] However, MPF does not provide all the functionality exposed by the Visual Studio COM interfaces.[12]
The services can then be consumed for creation of other packages, which add functionality to the Visual Studio IDE.
 Support for programming languages is added by using a specific VSPackage called a Language Service. A language service defines various interfaces which the VSPackage implementation can implement to add support for various functionalities.[13] Functionalities that can be added this way include syntax coloring, statement completion, brace matching, parameter information tooltips, member lists, and error markers for background compilation.[13] If the interface is implemented, the functionality will be available for the language. Language services are implemented on a per-language basis. The implementations can reuse code from the parser or the compiler for the language.[13] Language services can be implemented either in native code or managed code. For native code, either the native COM interfaces or the Babel Framework (part of Visual Studio SDK) can be used.[14] For managed code, the MPF includes wrappers for writing managed language services.[15]
 Visual Studio does not include any source control support built in but it defines two alternative ways for source control systems to integrate with the IDE.[16] A Source Control VSPackage can provide its own customised user interface. In contrast, a source control plugin using the MSSCCI (Microsoft Source Code Control Interface) provides a set of functions that are used to implement various source control functionality, with a standard Visual Studio user interface.[17][18] MSSCCI was first used to integrate Visual SourceSafe with Visual Studio 6.0 but was later opened up via the Visual Studio SDK. Visual Studio .NET 2002 used MSSCCI 1.1, and Visual Studio .NET 2003 used MSSCCI 1.2. Visual Studio 2005, 2008, and 2010 use MSSCCI Version 1.3, which adds support for rename and delete propagation, as well as asynchronous opening.[18]
 Visual Studio supports running multiple instances of the environment (each with its own set of VSPackages). The instances use different registry hives (see MSDN's definition of the term "registry hive" in the sense used here) to store their configuration state and are differentiated by their AppId (Application ID). The instances are launched by an AppId-specific .exe that selects the AppId, sets the root hive, and launches the IDE. VSPackages registered for one AppId are integrated with other VSPackages for that AppId. The various product editions of Visual Studio are created using the different AppIds. The Visual Studio Express edition products are installed with their own AppIds, but the Standard, Professional, and Team Suite products share the same AppId. Consequently, one can install the Express editions side-by-side with other editions, unlike the other editions which update the same installation. The professional edition includes a superset of the VSPackages in the standard edition, and the team suite includes a superset of the VSPackages in both other editions. The AppId system is leveraged by the Visual Studio Shell in Visual Studio 2008.[19]
 Visual Studio includes a code editor that supports syntax highlighting and code completion using IntelliSense for variables, functions, methods, loops, and LINQ queries.[20] IntelliSense is supported for the included languages, as well as for XML, Cascading Style Sheets, and JavaScript when developing web sites and web applications.[21][22] Autocomplete suggestions appear in a modeless list box over the code editor window, in proximity of the editing cursor. In Visual Studio 2008 onwards, it can be made temporarily semi-transparent to see the code obstructed by it.[20] The code editor is used for all supported languages.
 The code editor in Visual Studio also supports setting bookmarks in code for quick navigation. Other navigational aids include collapsing code blocks and incremental search, in addition to normal text search and regex search.[23] The code editor also includes a multi-item clipboard and a task list.[23] The code editor supports code snippets, which are saved templates for repetitive code and can be inserted into code and customized for the project being worked on. A management tool for code snippets is built in as well. These tools are surfaced as floating windows which can be set to automatically hide when unused or docked to the side of the screen. The code editor in Visual Studio also supports code refactoring including parameter reordering, variable and method renaming, interface extraction, and encapsulation of class members inside properties, among others.
 Visual Studio includes a debugger that works both as a source-level debugger and as a machine-level debugger. It works with both managed code as well as native code and can be used for debugging applications written in any language supported by Visual Studio. In addition, it can also attach to running processes, monitor, and debug those processes.[24] If source code for the running process is available, it displays the code as it is being run. If source code is not available, it can show the disassembly. The Visual Studio debugger can also create memory dumps as well as load them later for debugging.[25] Multi-threaded programs are also supported. The debugger can be configured to be launched when an application running outside the Visual Studio environment crashes.
 The Visual Studio Debugger allows setting breakpoints (which allow execution to be stopped temporarily at a certain position) and watches (which monitor the values of variables as the execution progresses).[26] Breakpoints can be conditional, meaning they get triggered when the condition is met. Code can be stepped over, i.e., run one line (of source code) at a time.[27] It can either step into functions to debug inside it, or step over it, i.e., the execution of the function body isn't available for manual inspection.[27] The debugger supports Edit and Continue, i.e., it allows code to be edited as it is being debugged. When debugging, if the mouse pointer hovers over any variable, its current value is displayed in a tooltip ("data tooltips"), where it can also be modified if desired. During coding, the Visual Studio debugger lets certain functions be invoked manually from the Immediate tool window. The parameters to the method are supplied at the Immediate window.[28]
 Visual Studio includes many visual designers to aid in the development of applications. These tools include:
 Microsoft Visual Studio can write high-quality code with comprehensive testing tools to aid in the development of applications. These tools include:[37]
 Unit testing, IntelliTest, Live Unit Testing, Test Explorer, CodeLens test indicators, Code coverage analysis, Fakes.[37]
 Visual Studio allows developers to write extensions for Visual Studio to extend its capabilities. These extensions "plug into" Visual Studio and extend its functionality. Extensions come in the form of macros, add-ins, and packages. Macros represent repeatable tasks and actions that developers can record programmatically for saving, replaying, and distributing. Macros, however, cannot implement new commands or create tool windows. They are written using Visual Basic and are not compiled.[12] Add-Ins provide access to the Visual Studio object model and can interact with the IDE tools. Add-Ins can be used to implement new functionality and can add new tool windows. Add-Ins are plugged into the IDE via COM and can be created in any COM-compliant languages.[12] Packages are created using the Visual Studio SDK and provide the highest level of extensibility. They can create designers and other tools, as well as integrate other programming languages. The Visual Studio SDK provides unmanaged APIs as well as a managed API to accomplish these tasks. However, the managed API isn't as comprehensive as the unmanaged one.[12] Extensions are supported in the Standard (and higher) versions of Visual Studio 2005. Express Editions do not support hosting extensions.
 Visual Studio 2008 introduced the Visual Studio Shell that allows for development of a customized version of the IDE. The Visual Studio Shell defines a set of VSPackages that provide the functionality required in any IDE. On top of that, other packages can be added to customize the installation. The Isolated mode of the shell creates a new AppId where the packages are installed. These are to be started with a different executable. It is aimed for development of custom development environments, either for a specific language or a specific scenario. The Integrated mode installs the packages into the AppId of the Professional/Standard/Team System editions, so that the tools integrate into these editions.[19] The Visual Studio Shell is available as a free download.
 After the release of Visual Studio 2008, Microsoft created the Visual Studio Gallery. It serves as the central location for posting information about extensions to Visual Studio. Community developers as well as commercial developers can upload information about their extensions to Visual Studio .NET 2002 through Visual Studio 2010. Users of the site can rate and review the extensions to help assess the quality of extensions being posted. An extension is stored in a VSIX file. Internally a VSIX file is a ZIP file that contains some XML files, and possibly one or more DLL's. One of the main advantages of these extensions is that they do not require Administrator rights to be installed. RSS feeds to notify users on updates to the site and tagging features are also planned.[38]
 Microsoft Visual Studio is available in the following editions or SKUs:[52]
 The Community edition was announced on November 12, 2014, as a new free version, with similar functionality to Visual Studio Professional. Prior to this date, the only free editions of Visual Studio were the feature-limited Express variants. Unlike the Express variants, Visual Studio Community supports multiple languages, and provides support for extensions. Individual developers have no restrictions on their use of the Community edition. The following uses also allow unlimited usage: contributing to Open Source projects, academic research, in a classroom learning environment and for developing and testing device drivers for the Windows operating system. All other use by an organization depends on its classification as an Enterprise (more than 250 employees or more than 1 million USD in annual revenue, per Microsoft).[53] Non-Enterprises may use up to 5 copies without restriction, user number 6 and higher require a commercial license; Enterprise organizations require a commercial license for use outside of the noted exceptions.[53][54] Visual Studio Community is oriented towards individual developers and small teams.[55][56]
 As of Visual Studio 2010, the Professional edition is the entry level commercial edition of Visual Studio. (Previously, a more feature restricted Standard edition was available.)[57] It provides an IDE for all supported development languages. MSDN support is available as MSDN Essentials or the full MSDN library depending on licensing. It supports XML and XSLT editing, and can create deployment packages that only use ClickOnce and MSI. It includes tools like Server Explorer and integration with Microsoft SQL Server also. Windows Mobile development support was included in Visual Studio 2005 Standard, however, with Visual Studio 2008, it is only available in Professional and higher editions. Windows Phone 7 development support was added to all editions in Visual Studio 2010. Development for Windows Mobile is no longer supported in Visual Studio 2010. It is superseded by Windows Phone 7.
 In addition to the features provided by the Professional edition, the Enterprise edition provides a new set of software development, database development, collaboration, metrics, architecture, testing and reporting tools.
 The first version of Visual Studio was Visual Studio 97.[58] Before that, Visual Basic, Visual C++, Visual FoxPro and Visual SourceSafe were sold as separate products.
 Microsoft first released Visual Studio (codenamed Boston,[59] for the city of the same name, thus beginning the VS codenames related to places)[59] in 1997, bundling many of its programming tools together for the first time. Visual Studio 97 came in two editions: Visual Studio Professional and Visual Studio Enterprise, the professional edition has three CDs, and the enterprise four CDs. It included Visual J++ 1.1 for Java programming and introduced Visual InterDev for creating dynamically generated web sites using Active Server Pages.[citation needed] There was a single companion CD that contained the Microsoft Developer Network library. Visual Studio 97 is only compatible with Windows 95 and Windows NT 4.0. It is the last version to support Windows NT 4.0 before SP3.
 Visual Studio 97 was an attempt at using the same development environment for multiple languages. Visual J++, InterDev, and the MSDN Library had all been using the same 'environment', called Developer Studio.[118]
 Visual Studio was also sold as a bundle with the separate IDEs used for Visual C++, Visual Basic and Visual FoxPro.[19]
 The next version, version 6.0 (codenamed Aspen, after the ski resort in Colorado),[citation needed] was released in June 1998 and is the last version to support the Windows 9x platform, as well as Windows NT 4.0 before SP6, but after SP2.[119] Each version of each language in part also settled to v6.0, including Visual J++ which was prior v1.1, and Visual InterDev at the first release. The v6 edition of Microsoft was the core environment for the next four releases to provide programmers with an integrated look-alike platform. This led Microsoft to transition the development on the platform independent .NET Framework.
 Visual Studio 6.0 was the last version to include Visual J++,[120][121] which Microsoft removed as part of a settlement with Sun Microsystems that required Microsoft Internet Explorer not to provide support for the Java Virtual Machine.
 Visual Studio 6.0 came in two editions: Professional and Enterprise.[122] The Enterprise edition contained extra features not found in Professional edition, including:
 Visual Studio was also sold as a bundle with the separate IDEs used for Visual C++, Visual Basic and Visual FoxPro.[19]
 Microsoft released Visual Studio .NET (VS.NET), codenamed Rainier (for Washington's Mount Rainier),[citation needed] in February 2002 (the beta version was released via Microsoft Developer Network in 2001). The biggest change was the introduction of a managed code development environment using the .NET Framework. Programs developed using .NET are not compiled to machine language (like C++ is, for example) but instead to a format called Microsoft Intermediate Language (MSIL) or Common Intermediate Language (CIL). When a CIL application executes, it is compiled while being executed into the appropriate machine language for the platform it is being executed on, thereby making code portable across several platforms. Programs compiled into CIL can be executed only on platforms which have an implementation of Common Language Infrastructure. It is possible to run CIL programs in Linux or Mac OS X using non-Microsoft .NET implementations like Mono and DotGNU.
 This was the first version of Visual Studio to require an NT-based Windows platform.[123] The installer enforces this requirement, and is the last version to support Windows NT 4.0 SP6 or later and Windows 2000 before SP3.
 Visual Studio .NET 2002 shipped in four editions: Academic, Professional, Enterprise Developer, and Enterprise Architect. Microsoft introduced C# (C-sharp), a new programming language, that targets .NET. It also introduced the successor to Visual J++ called Visual J#. Visual J# programs use Java's language-syntax. However, unlike Visual J++ programs, Visual J# programs can only target the .NET Framework, not the Java Virtual Machine that all other Java tools target.
 Visual Basic changed drastically to fit the new framework, and the new version was called Visual Basic .NET. Microsoft also added extensions to C++, called Managed Extensions for C++, so .NET programs could be created in C++.
 Visual Studio .NET can produce applications targeting Windows (using the Windows Forms part of the .NET Framework), the Web (using ASP.NET and Web Services) and, with an add-in, portable devices (using the .NET Compact Framework).
 The internal version number of Visual Studio .NET 2002 is version 7.0. Microsoft released Service Pack 1 for Visual Studio .NET 2002 in March 2005.[124]
 In April 2003, Microsoft introduced a minor upgrade to Visual Studio .NET called Visual Studio .NET 2003, codenamed Everett (for the city of the same name).[citation needed] It includes an upgrade to the .NET Framework, version 1.1, and is the first release to support developing programs for mobile devices, using ASP.NET or the .NET Compact Framework. The Visual C++ compiler's standards-compliance improved, especially in the area of partial template specialization. Visual C++ Toolkit 2003 is a version of the same C++ compiler shipped with Visual Studio .NET 2003 without the IDE that Microsoft made freely available. As of 2010[update] it is no longer available and the Express Editions have superseded it. Visual Studio .NET 2003 also supports Managed C++, which is the predecessor of C++/CLI. The internal version number of Visual Studio .NET 2003 is version 7.1 while the file format version is 8.0.[125] Visual Studio .NET 2003 drops support for Windows NT 4.0, and is the last version to support Windows 2000 SP3 and Windows XP before SP2 and the only version to support Windows Server 2003 before SP1.
 Visual Studio .NET 2003 shipped in five editions: Academic, Standard, Professional, Enterprise Developer, and Enterprise Architect. The Visual Studio .NET 2003 Enterprise Architect edition includes an implementation of Microsoft Visio 2002's modeling technologies, including tools for creating Unified Modeling Language-based visual representations of an application's architecture, and an object-role modeling (ORM) and logical database-modeling solution. "Enterprise Templates" were also introduced, to help larger development teams standardize coding styles and enforce policies around component usage and property settings.
 Service Pack 1 was released September 13, 2006.[126]
 Visual Studio 2005, codenamed Whidbey (a reference to Whidbey Island in Puget Sound region),[citation needed] was released online in October 2005 and to retail stores a few weeks later. Microsoft removed the ".NET" moniker from Visual Studio 2005 (as well as every other product with .NET in its name), but it still primarily targets the .NET Framework, which was upgraded to version 2.0. It requires Windows 2000 with Service Pack 4, Windows XP with at least Service Pack 2 or Windows Server 2003 with at least Service Pack 1. It is the last version to run on Windows 2000 and also the last version able to target Windows 98 and Windows Me for C++ applications.[127][128]
 Visual Studio 2005's internal version number is 8.0 while the file format version is 9.0.[125] Microsoft released Service Pack 1 for Visual Studio 2005 on December 14, 2006.[129] An additional update for Service Pack 1 that offers Windows Vista compatibility was made available on June 3, 2007.[130]
 Visual Studio 2005 was upgraded to support all the new features introduced in .NET Framework 2.0, including generics and ASP.NET 2.0. The IntelliSense feature in Visual Studio was upgraded for generics and new project types were added to support ASP.NET web services. Visual Studio 2005 additionally introduces support for a new task-based build platform called Microsoft Build Engine (MSBuild) which employs a new XML-based project file format.[131] Visual Studio 2005 also includes a local web server, separate from IIS, that can host ASP.NET applications during development and testing. It also supports all SQL Server 2005 databases. Database designers were upgraded to support the ADO.NET 2.0, which is included with .NET Framework 2.0. C++ also got a similar upgrade with the addition of C++/CLI which is slated to replace the use of Managed C++.[132] Other new features of Visual Studio 2005 include the "Deployment Designer" which allows application designs to be validated before deployments, an improved environment for web publishing when combined with ASP.NET 2.0 and load testing to see application performance under various sorts of user loads. Starting with the 2005 edition, Visual Studio also added extensive 64-bit support. While the host development environment itself is only available as a 32-bit application, Visual C++ 2005 supports compiling for x86-64 (AMD64 and Intel 64) as well as IA-64 (Itanium).[133] The Platform SDK included 64-bit compilers and 64-bit versions of the libraries.
 Microsoft also announced Visual Studio Tools for Applications as the successor to Visual Basic for Applications (VBA) and VSA (Visual Studio for Applications). VSTA 1.0 was released to manufacturing along with Office 2007. It is included with Office 2007 and is also part of the Visual Studio 2005 SDK. VSTA consists of a customized IDE, based on the Visual Studio 2005 IDE, and a runtime that can be embedded in applications to expose its features via the .NET object model. Office 2007 applications continue to integrate with VBA, except for InfoPath 2007 which integrates with VSTA. Version 2.0 of VSTA (based on Visual Studio 2008) was released in April 2008.[134] It is significantly different from the first version, including features such as dynamic programming and support for WPF, WCF, WF, LINQ, and .NET 3.5 Framework.
 Visual Studio 2008,[135] and Visual Studio Team System 2008[136][137] codenamed Orcas (a reference to Orcas Island, also an island in Puget Sound region, like Whidbey for the previous 2005 release), were released to MSDN subscribers on November 19, 2007, alongside .NET Framework 3.5. The source code for the Visual Studio 2008 IDE is available under a shared source license to some of Microsoft's partners and ISVs.[84] Microsoft released Service Pack 1 for Visual Studio 2008 on August 11, 2008.[138] The internal version number of Visual Studio 2008 is version 9.0 while the file format version is 10.0. Visual Studio 2008 requires Windows XP Service Pack 2 plus Windows Installer 3.1, Windows Server 2003 Service Pack 1 or later. It is the last version available for Windows XP SP2, Windows Server 2003 SP1, as well as the only version to support Windows Vista before SP2 and Windows Server 2008 before SP2 and the last version to support targeting Windows 2000 for C++ applications.[139]
 Visual Studio 2008 is focused on development of Windows Vista, 2007 Office system, and Web applications. For visual design, a new Windows Presentation Foundation visual designer and a new HTML/CSS editor influenced by Microsoft Expression Web are included. J# is not included. Visual Studio 2008 requires .NET 3.5 Framework and by default configures compiled assemblies to run on .NET Framework 3.5, but it also supports multi-targeting which lets the developers choose which version of the .NET Framework (out of 2.0, 3.0, 3.5, Silverlight CoreCLR or .NET Compact Framework) the assembly runs on. Visual Studio 2008 also includes new code analysis tools, including the new Code Metrics tool (only in Team Edition and Team Suite Edition).[140] For Visual C++, Visual Studio adds a new version of Microsoft Foundation Classes (MFC 9.0) that adds support for the visual styles and UI controls introduced with Windows Vista.[141] For native and managed code interoperability, Visual C++ introduces the STL/CLR, which is a port of the C++ Standard Template Library (STL) containers and algorithms to managed code. STL/CLR defines STL-like containers, iterators and algorithms that work on C++/CLI managed objects.[142][143]
 Visual Studio 2008 features include an XAML-based designer (codenamed Cider), workflow designer, LINQ to SQL designer (for defining the type mappings and object encapsulation for SQL Server data), XSLT debugger, JavaScript Intellisense support, JavaScript Debugging support, support for UAC manifests, a concurrent build system, among others.[144] It ships with an enhanced set of UI widgets, both for Windows Forms and WPF. It also includes a multithreaded build engine (MSBuild) to compile multiple source files (and build the executable file) in a project across multiple threads simultaneously. It also includes support for compiling icon resources in PNG format, introduced in Windows Vista. An updated XML Schema designer was released separately some time after the release of Visual Studio 2008.[145]
 Visual Studio Debugger includes features targeting easier debugging of multi-threaded applications. In debugging mode, in the Threads window, which lists all the threads, hovering over a thread displays the stack trace of that thread in tooltips.[146] The threads can directly be named and flagged for easier identification from that window itself.[147] In addition, in the code window, along with indicating the location of the currently executing instruction in the current thread, the currently executing instructions in other threads are also pointed out.[147][148] The Visual Studio debugger supports integrated debugging of the .NET 3.5 Framework Base Class Library (BCL) which can dynamically download the BCL source code and debug symbols and allow stepping into the BCL source during debugging.[149] As of 2010[update] a limited subset of the BCL source is available, with more library support planned for later.
 On April 12, 2010, Microsoft released Visual Studio 2010, codenamed Dev10,[89] and .NET Framework 4.[150][151] It is available for Windows Server 2003 SP2, Windows XP SP3, Windows Vista SP2 and Windows Server 2008 SP2 and has support for Windows Server 2008 R2, as well as for Windows 7. It is the last version to support Windows XP SP3, Windows Server 2003 SP2, Windows Server 2003 R2, Windows Vista SP2 and Windows Server 2008 SP2, and the only version to support Windows 7 before SP1 and Windows Server 2008 R2 before SP1.
 The Visual Studio 2010 IDE was redesigned which, according to Microsoft, clears the UI organization and "reduces clutter and complexity."[152] The new IDE better supports multiple document windows and floating tool windows,[152] while offering better multi-monitor support. The IDE shell has been rewritten using the Windows Presentation Foundation (WPF), whereas the internals have been redesigned using Managed Extensibility Framework (MEF) that offers more extensibility points than previous versions of the IDE that enabled add-ins to modify the behavior of the IDE.[153]
 The new multi-paradigm ML-variant F# forms part of Visual Studio 2010.[154]
 Visual Studio 2010 comes with .NET Framework 4 and supports developing applications targeting Windows 7.[152] It supports IBM Db2 and Oracle databases, in addition to Microsoft SQL Server.[152] It has integrated support for developing Microsoft Silverlight applications, including an interactive designer.[152] Visual Studio 2010 offers several tools to make parallel programming simpler: in addition to the Parallel Extensions for the .NET Framework and the Parallel Patterns Library for native code, Visual Studio 2010 includes tools for debugging parallel applications. The new tools allow the visualization of parallel Tasks and their runtime stacks.[155] Tools for profiling parallel applications can be used for visualization of thread wait-times and thread migrations across processor cores.[156] Intel and Microsoft have jointly pledged support for a new Concurrency Runtime in Visual Studio 2010[157]
and Intel has launched parallelism support in Parallel Studio as an add-on for Visual Studio.[158]
 The Visual Studio 2010 code editor now highlights references; whenever a symbol is selected, all other usages of the symbol are highlighted.[159] It also offers a Quick Search feature to incrementally search across all symbols in C++, C# and VB.NET projects. Quick Search supports substring matches and camelCase searches.[159] The Call Hierarchy feature allows the developer to see all the methods that are called from a current method as well as the methods that call the current one.[159] IntelliSense in Visual Studio supports a consume-first mode which developers can opt into. In this mode, IntelliSense does not auto-complete identifiers; this allows the developer to use undefined identifiers (like variable or method names) and define those later. Visual Studio 2010 can also help in this by automatically defining them, if it can infer their types from usage.[159] Current versions of Visual Studio have a known bug which makes IntelliSense unusable for projects using pure C (not C++).[160]
 Visual Studio 2010 features a new Help System replacing the MSDN Library viewer. The Help System is no longer based on Microsoft Help 2 and does not use Microsoft Document Explorer. Dynamic help containing links to help items based on what the developer was doing at the time was removed in the final release,[161] but can be added back using a download from Microsoft.[162]
 Visual Studio 2010 no longer supports development for Windows Mobile prior to Windows Phone 7. Visual Studio 2010 Service Pack 1 was released in March 2011.[163]
 Visual Studio Ultimate 2010 replaces Visual Studio 2008 Team Suite.[164] It includes new modeling tools,[165] such as the Architecture Explorer, which graphically displays projects and classes and the relationships between them.[166][167]
It supports UML activity diagram, component diagram, (logical) class diagram, sequence diagram, and use case diagram.[167] Visual Studio Ultimate 2010 also includes Test Impact Analysis which provides hints on which test cases are impacted by modifications to the source code, without actually running the test cases.[168] This speeds up testing by avoiding running unnecessary test cases.
 Visual Studio Ultimate 2010 also includes a historical debugger for managed code called IntelliTrace. Unlike a traditional debugger that records only the currently active stack, IntelliTrace records all events, such as prior function calls, method parameters, events and exceptions. This allows the code execution to be rewound in case a breakpoint was not set where the error occurred.[169] Debugging with IntelliTrace causes the application to run more slowly than debugging without it, and uses more memory as additional data needs to be recorded. Microsoft allows configuration of how much data should be recorded, in effect, allowing developers to balance the speed of execution and resource usage. The Lab Management component of Visual Studio Ultimate 2010 uses virtualization to create a similar execution environment for testers and developers. The virtual machines are tagged with checkpoints which can later be investigated for issues, as well as to reproduce the issue.[170] Visual Studio Ultimate 2010 also includes the capability to record test runs that capture the specific state of the operating environment as well as the precise steps used to run the test. These steps can then be played back to reproduce issues.[171]
 The final build of Visual Studio 2012 was announced on August 1, 2012, and the official launch event was held on September 12, 2012.[172]
 Unlike prior versions, Visual Studio 2012 cannot record and play macros and the macro editor has been removed.[173] Also unlike prior versions, Visual Studio 2012 require Windows 7 SP1 and Windows Server 2008 R2 SP1.
 New features include support for WinRT and C++/CX (Component Extensions) and C++ AMP (GPGPU programming) Semantic Colorization.[174]
 On September 16, 2011, a complete 'Developer Preview' of Visual Studio 11 was published on Microsoft's website. Visual Studio 11 Developer Preview requires Windows 7, Windows Server 2008 R2, Windows 8, or later operating systems.[175] Versions of Microsoft Foundation Class Library (MFC) and C runtime (CRT) included with this release cannot produce software that is compatible with Windows XP or Windows Server 2003 except by using native multi-targeting and foregoing the newest libraries, compilers, and headers.[176] However, on June 15, 2012, a blog post on the VC++ Team blog announced that based on customer feedback, Microsoft would re-introduce native support for Windows XP targets (though not for XP as a development platform) in a version of Visual C++ to be released later in the fall of 2012.[177] "Visual Studio 2012 Update 1" (Visual Studio 2012.1) was released in November 2012. This update added support for Windows XP targets and also added other new tools and features (e.g. improved diagnostics and testing support for Windows Store apps).[178]
 On August 24, 2011, a blog post by Sumit Kumar, a Program Manager on the Visual C++ team, listed some of the features of the upcoming version of the Visual Studio C++ IDE:[179]
 The source code of Visual Studio 2012 consists of approximately 50 million lines of code.[180]
 During Visual Studio 11 beta, Microsoft eliminated the use of color within tools except in cases where color is used for notification or status change purposes. However, the use of color was returned after feedback demanding more contrast, differentiation, clarity and "energy" in the user interface.[181][182]
 In the Visual Studio 2012 release candidate (RC), a major change to the interface is the use of all-caps menu bar, as part of the campaign to keep Visual Studio consistent with the direction of other Microsoft user interfaces, and to provide added structure to the top menu bar area.[183] The redesign was criticized for being hard to read, and going against the trends started by developers to use CamelCase to make words stand out better.[184] Some speculated that the root cause of the redesign was to incorporate the simplistic look and feel of Metro programs.[185] However, there exists a Windows Registry option to allow users to disable the all-caps interface.[186]
 The preview for Visual Studio 2013 was announced at the Build 2013 conference and made available on June 26, 2013.[187] The Visual Studio 2013 RC (Release Candidate) was made available to developers on MSDN on September 9, 2013.[188]
 The final release of Visual Studio 2013 became available for download on October 17, 2013, along with .NET 4.5.1.[189] Visual Studio 2013 officially launched on November 13, 2013, at a virtual launch event keynoted by S. Somasegar and hosted on events.visualstudio.com.[190] "Visual Studio 2013 Update 1" (Visual Studio 2013.1) was released on January 20, 2014.[191]
Visual Studio 2013.1 is a targeted update that addresses some key areas of customer feedback.[192]
"Visual Studio 2013 Update 2" (Visual Studio 2013.2) was released on May 12, 2014.[193]
Visual Studio 2013 Update 3 was released on August 4, 2014. With this update, Visual Studio provides an option to disable the all-caps menus, which was introduced in VS2012.[194]
"Visual Studio 2013 Update 4" (Visual Studio 2013.4) was released on November 12, 2014.[195]
"Visual Studio 2013 Update 5" (Visual Studio 2013.5) was released on July 20, 2015.[196]
 Visual Studio 2013 also adds support for Windows 8.1 and Windows Server 2012 R2.
 Initially referred to as Visual Studio "14", the first Community Technology Preview (CTP) was released on June 3, 2014[197] and the Release Candidate was released on April 29, 2015; Visual Studio 2015 was officially announced as the final name on November 12, 2014.[198]
 Visual Studio 2015 RTM was released on July 20, 2015.[196] Visual Studio 2015 Update 1 was released on November 30, 2015.[196] Visual Studio 2015 Update 2 was released on March 30, 2016.[196] Visual Studio 2015 Update 3 was released on June 27, 2016.[196] Visual Studio 2015 is the first version to support Windows 10 and the last version to support Windows 8, Windows Server 2008 R2 SP1 and Windows Server 2012; it's also the last version to support targeting Windows XP SP3, Windows Server 2003 SP2, Windows Vista SP2 and Windows Server 2008 SP2 for C++ applications.
 Initially referred to as Visual Studio "15", it was released on March 7, 2017.[199] The first Preview was released on March 30, 2016.[200] Visual Studio "15" Preview 2 was released May 10, 2016.[201][202] Visual Studio "15" Preview 3 was released on July 7, 2016.[203][204] Visual Studio "15" Preview 4 was released on August 22, 2016.[205][206] Visual Studio "15" Preview 5 was released on October 5, 2016.[207]
 On November 14, 2016, for a brief period of time, Microsoft released a blog post revealing Visual Studio 2017 product name version alongside upcoming features.[208]
 On November 16, 2016, "Visual Studio 2017" was announced as the final name,[209] and Visual Studio 2017 RC was released.[210]
 On March 7, 2017, Visual Studio 2017 was released for general availability.[210] It requires Windows 7 SP1, Windows 8.1 with KB2919355 or Windows Server 2012 R2 with KB2919355 at the minimum, and also added support for Windows Server 2016.
 On March 14, 2017, first fix was released for Visual Studio 2017 due to failures during installation or opening solutions in the first release.[210]
 On April 5, 2017, Visual Studio 2017 15.1 was released and added support for targeting the .NET Framework 4.7.
 On May 10, 2017, Visual Studio 2017 15.2 was released and added a new workload, "Data Science and Analytical Applications Workload". An update to fix the dark color theme was released on May 12, 2017.
 On August 14, 2017, Visual Studio 2017 15.3 was released and added support for targeting .NET Core 2.0. An update (15.3.1) was released four days later to address a Git vulnerability with submodules (CVE 2017-1000117).
 On October 10, 2017, Visual Studio 15.4 was released.[211]
 On December 4, 2017, Visual Studio 15.5 was released. This update contained major performance improvements, new features, as well as bug fixes.[212]
 On March 6, 2018, Visual Studio 15.6 was released. It includes updates to unit testing and performance.[213]
 On May 7, 2018, Visual Studio 15.7 was released. It included updates across the board including, the installer, editor, debugger among others. Almost all point releases, the latest of which is 15.7.6 released August 2, 2018, include security updates. With the release of Visual Studio 2017 15.7, Visual C++ now conforms to the C++17 standard.[39]
 On September 20, 2018, Visual Studio 15.8.5 was released. Tools for Xamarin now supports Xcode 10.[214]
 Visual Studio 2017 offers new features like support for EditorConfig (a coding style enforcement framework), NGen support, .NET Core and Docker toolset (Preview), and Xamarin 4.3 (Preview).[210] It also has a XAML Editor, improved IntelliSense, live unit testing, debugging enhancement and better IDE experience and productivity. Additionally, it is the last version of Visual Studio to support maintaining Windows 10 Mobile projects.[215]
 On June 6, 2018, Microsoft announced Visual Studio 2019 (version 16).[216]
 On December 4, 2018, Visual Studio 2019 Preview 1 was released.[217]
 On January 24, 2019, Visual Studio 2019 Preview 2 was released.[218]
 On February 13, 2019, Visual Studio 2019 Preview 3 was released.[219]
 On February 27, 2019, Visual Studio 2019 RC was released while setting April 2, 2019 for its general availability.[220]
 It is generally available (GA) since April 2, 2019 and available for download.[108] It is the first version of Visual Studio to support Windows 11, and also requires Windows 7 SP1, Windows 8.1 with KB2919355, Windows Server 2012 R2 with KB2919355 or Windows 10, version 1703 at the minimum. It is the last 32-bit version of Visual Studio as later versions are only 64-bit. It is also the last version to support Windows 7 SP1, Windows 8.1 and Windows Server 2012 R2, with later versions requiring at least Windows 10 and Windows Server 2016.
 On April 19, 2021, Microsoft announced Visual Studio 2022 (version 17).[221][222] It is the first version to run as a 64-bit process allowing Visual Studio main process to access more than 4 GB of memory, preventing out-of-memory exceptions which could occur with large projects.
 On June 17, 2021, Visual Studio 2022 Preview 1 was released.[223]
 On July 14, 2021, Visual Studio 2022 Preview 2 was released.[224]
 On August 10, 2021, Visual Studio 2022 Preview 3 was released.[225]
 On September 14, 2021, Visual Studio 2022 Preview 4 was released.[226]
 On October 12, 2021, Visual Studio 2022 RC and Preview 5 was released while setting November 8, 2021 for its general availability.[227]
 It is generally available (GA) since November 8, 2021 and available for download.[228]
 It is available only for Windows 10 and Windows Server 2016 or later, and also supports Windows Server 2022.
 
 On November 13, 2013, Microsoft announced the release of a software as a service offering of Visual Studio on Microsoft Azure platform; at the time, Microsoft called it Visual Studio Online. Previously announced as Team Foundation Services, it expanded over the on-premises Team Foundation Server (TFS; now known as Azure DevOps Server) by making it available on the Internet and implementing a rolling release model.[229][230] Customers could use Azure portal to subscribe to Visual Studio Online. Subscribers receive a hosted Git-compatible version control system, a load-testing service, a telemetry service and an in-browser code editor codenamed "Monaco".[231] During the Connect(); 2015 developer event on November 18, 2015, Microsoft announced that the service was rebranded as "Visual Studio Team Services (VSTS)".[232] On September 10, 2018, Microsoft announced another rebranding of the service, this time to "Azure DevOps Services".[45]
 Microsoft offers Stakeholder, Basic, and Visual Studio subscriber access levels for Azure DevOps Services. The Basic plan is free of charge for up to five users. Users with a Visual Studio subscription can be added to a plan with no additional charge.[233]
 
Visual Studio Application Lifecycle Management (ALM) is a collection of integrated software development tools developed by Microsoft. These tools currently consist of the IDE (Visual Studio 2015 Community and greater editions), server (Team Foundation Server), and cloud services (Visual Studio Team Services).[234] Visual Studio ALM supports team-based development and collaboration, Agile project management, DevOps, source control, packaging, continuous development, automated testing, release management, continuous delivery, and reporting tools for apps and services.[235]
 In Visual Studio 2005 and Visual Studio 2008, the brand was known as Microsoft Visual Studio Team System (VSTS). In October 2009, the Team System brand was renamed[165][236] Visual Studio ALM with the Visual Studio 2010 (codenamed 'Rosario') release.[237]
 Visual Studio Team Services debuted as Visual Studio Online in 2013 and was renamed in 2015.[238]
 Visual Studio Lab Management is a software development tool developed by Microsoft for software testers to create and manage virtual environments. Lab Management extends the existing Visual Studio Application Lifecycle Management platform to enable an integrated Hyper-V based test lab. Since Visual Studio 2012, it is already shipped as a part of it; and, can be set up after Azure DevOps and SCVMM are integrated.[239]
 Microsoft Visual Studio LightSwitch is an extension and framework specifically tailored for creating line-of-business applications built on existing .NET technologies and Microsoft platforms. The applications produced are architecturally 3-tier: the user interface runs on either Microsoft Silverlight or HTML 5 client,[240] or as a SharePoint 2013 app;[241] the logic and data-access tier is built on WCF Data Services and exposed as an OData feed hosted[242] in ASP.NET; and the primary data storage supports Microsoft SQL Server Express, Microsoft SQL Server and Microsoft SQL Azure. LightSwitch also supports other data sources including Microsoft SharePoint, OData and WCF RIA Services.
 LightSwitch includes graphical designers for designing entities and entity relationships, entity queries, and UI screens. Business logic may be written in either Visual Basic or Visual C#. LightSwitch is included with Visual Studio 2012 Professional and higher. Visual Studio 2015 is the last release of Visual Studio that includes the LightSwitch tooling.[243]
 The user interface layer is now an optional component when deploying a LightSwitch solution, allowing a service-only deployment.[244]
 The first version of Visual Studio LightSwitch, released July 26, 2011,[245] had many differences from the current[when?] release of LightSwitch. Notably the tool was purchased and installed as a stand-alone product. If Visual Studio 2010 Professional or higher was already installed on the machine, LightSwitch would integrate into that.[246] The second major difference was the middle tier was built and exposed using WCF RIA Services.
 As of October 14, 2016, Microsoft no longer recommends LightSwitch for new application development.[247]
 Visual Studio Code is a freeware source code editor, along with other features, for Linux, Mac OS, and Windows.[248] It also includes support for debugging and embedded Git Control. It is built on open-source,[249] and on April 14, 2016, version 1.0 was released.[250]
 Visual Studio Team System Profiler (VSTS Profiler) is a tool to analyze the performance of .NET projects that analyzes the space and time complexity of the program.[251] It analyzes the code and prepares a report that includes CPU sampling, instrumentation, .NET memory allocation and resource contention.


Source: https://en.wikipedia.org/wiki/IntelliJ_IDEA
Content: IntelliJ IDEA is an integrated development environment (IDE) written in Java for developing computer software written in Java, Kotlin, Groovy, and other JVM-based languages. It is developed by JetBrains (formerly known as IntelliJ) and is available as an Apache 2 Licensed community edition,[2] and in a proprietary commercial edition. Both can be used for commercial development.[3][4]
 The first version of IntelliJ IDEA was released in January 2001 and was one of the first available Java IDEs with advanced code navigation and code refactoring capabilities integrated.[5][6]
 In 2009, JetBrains released the source code for IntelliJ IDEA under the open-source Apache License 2.0.[7][8] JetBrains also began distributing a limited version of IntelliJ IDEA consisting of open-source features under the moniker Community Edition. The commercial Ultimate Edition provides additional features and remains available for a fee.
 In a 2010 InfoWorld report, IntelliJ received the highest test center score out of the four top Java programming tools: Eclipse, IntelliJ IDEA, NetBeans and JDeveloper.[9]
 In December 2014, Google announced version 1.0 of Android Studio, an open-source IDE for Android apps, based on the open source community edition.[10] Other development environments based on IntelliJ's framework include AppCode, CLion, DataGrip, GoLand, PhpStorm, PyCharm, Rider, RubyMine, WebStorm, and MPS.[11]
 In September 2020, Huawei announced and released version 1.0 of DevEco Studio, an open-source IDE for HarmonyOS apps development, based on Jetbrains IntelliJ IDEA with Huawei's SmartAssist for Windows and macOS.[12]
 The IDE provides certain features[15] like code completion by analyzing the context, code navigation which allows jumping to a class or declaration in the code directly, code refactoring, code debugging[16]
, linting and options to fix inconsistencies via suggestions.
 The IDE provides[15] integration with build/packaging tools like Grunt, bower, Gradle, and sbt. It supports databases like Microsoft SQL Server, Oracle, PostgreSQL, SQLite, and MySQL can be accessed directly from the IDE in the Ultimate edition, through an embedded version of DataGrip, another IDE developed by JetBrains.
 IntelliJ supports plugins through which one can add additional functionality to the IDE. Plugins can be downloaded and installed either from IntelliJ's plugin repository website or through the IDE's inbuilt plugin search and install feature. Each edition has separate plugin repositories, with both the Community and Ultimate editions totaling over 3000 plugins each as of 2019.[17]
 The Community and Ultimate editions differ in their support for various programming languages as shown in the following table.[18]
 Supported in both Community and Ultimate Edition:
 
 Supported in both Community and Ultimate Edition via plugins:
 
 Supported only in Ultimate Edition:
 
 Supported only in Ultimate Edition via plugins:
 
 Source:[18]
 Supported in both Community and Ultimate Edition:
 
 Supported only in Ultimate Edition:
 
 There was a free plugin from Atlassian for IntelliJ available to integrate with JIRA,[31] Bamboo, Crucible and FishEye. However, the software, called IDE-Connector, was discontinued on June 1, 2015.[32]
 The two editions also differ in their support[18] for software versioning and revision control systems.
 Supported in both Community and Ultimate Edition:
 Supported only in Ultimate Edition:
 


Source: https://en.wikipedia.org/wiki/NetBeans_IDE
Content: 
 NetBeans is an integrated development environment (IDE) for Java. NetBeans allows applications to be developed from a set of modular software components called modules. NetBeans runs on Windows, macOS, Linux and Solaris. In addition to Java development, it has extensions for other languages like PHP, C, C++, HTML5,[3] and JavaScript. Applications based on NetBeans, including the NetBeans IDE, can be extended by third party developers.[4]
 NetBeans began in 1996 as Xelfi (word play on Delphi),[5][6] a Java IDE student project under the guidance of the Faculty of Engineering and Technology at Charles University in Prague. In 1997, Roman Staněk formed a company around the project and produced commercial versions of the NetBeans IDE until it was bought by Sun Microsystems in 1999. Sun open-sourced the NetBeans IDE in June of the following year. Since then, the NetBeans community has continued to grow.[7] In 2010, Sun (and thus NetBeans) was acquired by Oracle Corporation. Under Oracle, NetBeans had to find some synergy with JDeveloper, a freeware IDE that has historically been a product of the company, by 2012 both IDEs were rebuilt around a shared codebase - the NetBeans Platform. In September 2016, Oracle submitted a proposal to donate the NetBeans project to The Apache Software Foundation, stating that it was "opening up the NetBeans governance model to give NetBeans constituents a greater voice in the project's direction and future success through the upcoming release of Java 9 and NetBeans 9 and beyond". The move was endorsed by Java creator James Gosling.[8] The project entered the Apache Incubator in October 2016.[9]
 NetBeans IDE is an open-source integrated development environment. NetBeans IDE supports development of all Java application types (Java SE (including JavaFX), Java ME, web, EJB and mobile applications) out of the box. Among other features are an Ant-based project system, Maven support, refactorings, version control (supporting CVS, Subversion, Git, Mercurial and Clearcase).
 Modularity: All the functions of the IDE are provided by modules. Each module provides a well-defined function, such as support for the Java language, editing, or support for the CVS versioning system, and SVN. NetBeans contains all the modules needed for Java development in a single download, allowing the user to start working immediately. Modules also allow NetBeans to be extended. New features, such as support for other programming languages, can be added by installing additional modules. For instance, Sun Studio, Sun Java Studio Enterprise, and Sun Java Studio Creator from Sun Microsystems are all based on the NetBeans IDE.
 License: The IDE is licensed under the Apache License 2.0. Previously, from July 2006 through 2007, NetBeans IDE was licensed under Sun's Common Development and Distribution License (CDDL), a license based on the Mozilla Public License (MPL). In October 2007, Sun announced that NetBeans would henceforth be offered under a dual license of the CDDL and the GPL version 2 licenses, with the GPL linking exception for GNU Classpath.[10] Oracle has donated NetBeans Platform and IDE to the Apache Foundation where it underwent incubation and graduated as a top level project in April 2019.[11]
 In an October 2016 interview with Gabriela Motroc, Oracle Vice President Bill Pataky stated that Oracle has a number of products that depend on NetBeans.[12]
 These modules are part of the NetBeans IDE:
 The NetBeans Profiler[13] is a tool for the monitoring of Java applications: It helps developers find memory leaks and optimize speed. Formerly downloaded separately, it is integrated into the core IDE since version 6.0.
The Profiler is based on a Sun Laboratories research project that was named JFluid. That research uncovered specific techniques that can be used to lower the overhead of profiling a Java application. One of those techniques is dynamic bytecode instrumentation, which is particularly useful for profiling large Java applications. Using dynamic bytecode instrumentation and additional algorithms, the NetBeans Profiler is able to obtain runtime information on applications that are too large or complex for other profilers. NetBeans also support Profiling Points that let you profile precise points of execution and measure execution time.
 Formerly known as project Matisse, the GUI design-tool enables developers to prototype and design Swing GUIs by dragging and positioning GUI components.[14]
 The GUI builder has built-in support for JSR 295 (Beans Binding technology), but the support for JSR 296 (Swing Application Framework) was removed in 7.1.
 The NetBeans JavaScript editor provides extended support for JavaScript, Ajax, and CSS.[15][16]
 JavaScript editor features comprise syntax highlighting, refactoring, code completion for native objects and functions, generation of JavaScript class skeletons, generation of Ajax callbacks from a template; and automatic browser compatibility checks.
 CSS editor features comprise code completion for styles names, quick navigation through the navigator panel, displaying the CSS rule declaration in a List View and file structure in a Tree View, sorting the outline view by name, type or declaration order (List & Tree), creating rule declarations (Tree only), refactoring a part of a rule name (Tree only).
 The NetBeans 7.4 and later uses the new Nashorn JavaScript engine developed by Oracle.
 Users can choose to download NetBeans IDE bundles tailored to specific development needs. Users can also download and install all other features at a later date directly through the NetBeans IDE.
 The NetBeans IDE Bundle for Web & Java EE[17] provides complete tools for all the latest Java EE 6 standards, including the new Java EE 6 Web Profile, Enterprise Java Beans (EJBs), servlets, Java Persistence API, web services, and annotations.
NetBeans also supports the JSF 2.0 (Facelets), JavaServer Pages (JSP), Hibernate, Spring, and Struts frameworks, and the Java EE 5 and J2EE 1.4 platforms. It includes GlassFish and Apache Tomcat.
 Some of its features with Java EE include:
 NetBeans supports PHP since version 5.6. The bundle for PHP includes:
 Oracle also releases a version of NetBeans that includes all of the features of the above bundles. This bundle includes:
 Official Ruby support was removed with the release of 7.0.
 NetBeans IDE is translated into the following languages:
 Community translations of the IDE are also available in the following languages:
 


Source: https://en.wikipedia.org/wiki/Vim
Content: Vim or VIM most commonly refer to:
 Vim or VIM may also refer to:


Source: https://en.wikipedia.org/wiki/Emacs
Content: Emacs /ˈiːmæks/, originally named EMACS (an acronym for "Editor Macros"),[1][2][3] is a family of text editors that are characterized by their extensibility.[4] The manual for the most widely used variant,[5] GNU Emacs, describes it as "the extensible, customizable, self-documenting, real-time display editor".[6] Development of the first Emacs began in the mid-1970s,[7] and work on GNU Emacs, directly descended from the original, is ongoing; its latest version is 29.2, released January 2024.[8]
 Emacs has over 10,000 built-in commands and its user interface allows the user to combine these commands into macros to automate work. Implementations of Emacs typically feature a dialect of the Lisp programming language, allowing users and developers to write new commands and applications for the editor. Extensions have been written to, among other things, manage files, remote access,[9] e-mail, outlines, multimedia, Git integration, RSS feeds,[10] and collaborative editing,[11] as well as implementations of ELIZA, Pong, Conway's Life, Snake, Dunnet, and Tetris.[12]
 The original EMACS was written in 1976 by David A. Moon and Guy L. Steele Jr. as a set of macros for the TECO editor.[13][1][2][3][14] It was inspired by the ideas of the TECO-macro editors TECMAC and TMACS.[15]
 The most popular, and most ported, version of Emacs is GNU Emacs, which was created by Richard Stallman for the GNU Project.[16] XEmacs is a variant that branched from GNU Emacs in 1991. GNU Emacs and XEmacs use similar Lisp dialects and are, for the most part, compatible with each other. XEmacs development is inactive.
 GNU Emacs is, along with vi, one of the two main contenders in the traditional editor wars of Unix culture. GNU Emacs is among the oldest free and open source projects still under development.[17]
 Emacs development began during the 1970s at the MIT AI Lab, whose PDP-6 and PDP-10 computers used the Incompatible Timesharing System (ITS) operating system that featured a default line editor known as Tape Editor and Corrector (TECO). Unlike most modern text editors, TECO used separate modes in which the user would either add text, edit existing text, or display the document. One could not place characters directly into a document by typing them into TECO, but would instead enter a character ('i') in the TECO command language telling it to switch to input mode, enter the required characters, during which time the edited text was not displayed on the screen, and finally enter a character (<esc>) to switch the editor back to command mode. (A similar technique was used to allow overtyping.) This behavior is similar to that of the program ed.
 By the 1970s, TECO was already an old program, initially released in 1962. Richard Stallman visited the Stanford AI Lab in 1976[19] and saw the lab's E editor, written by Fred Wright.[20] He was impressed by the editor's intuitive WYSIWYG (What You See Is What You Get) behavior, which has since become the default behavior of most modern text editors. He returned to MIT where Carl Mikkelsen, a hacker at the AI Lab, had added to TECO a combined display/editing mode called Control-R that allowed the screen display to be updated each time the user entered a keystroke. Stallman reimplemented this mode to run efficiently and then added a macro feature to the TECO display-editing mode that allowed the user to redefine any keystroke to run a TECO program.[3]
 E had another feature that TECO lacked: random-access editing. TECO was a page-sequential editor that was designed for editing paper tape on the PDP-1 at a time when computer memory was generally small due to cost, and it was a feature of TECO that allowed editing on only one page at a time sequentially in the order of the pages in the file. Instead of adopting E's approach of structuring the file for page-random access on disk, Stallman modified TECO to handle large buffers more efficiently and changed its file-management method to read, edit, and write the entire file as a single buffer. Almost all modern editors use this approach.
 The new version of TECO quickly became popular at the AI Lab and soon accumulated a large collection of custom macros whose names often ended in MAC or MACS, which stood for macro. Two years later, Guy Steele took on the project of unifying the diverse macros into a single set.[21] Steele and Stallman's finished implementation included facilities for extending and documenting the new macro set.[3] The resulting system was called EMACS, which stood for Editing MACroS or, alternatively, E with MACroS. Stallman picked the name Emacs "because <E> was not in use as an abbreviation on ITS at the time."[22] An apocryphal hacker koan alleges that the program was named after Emack & Bolio's, a popular Boston ice cream store.[23] The first operational EMACS system existed in late 1976.[24]
 Stallman saw a problem in too much customization and de facto forking and set certain conditions for usage.[citation needed] He later wrote:[24]
 EMACS was distributed on a basis of communal sharing, which means all improvements must be given back to me to be incorporated and distributed. The original Emacs, like TECO, ran only on the PDP-10 running ITS. Its behavior was sufficiently different from that of TECO that it could be considered a text editor in its own right, and it quickly became the standard editing program on ITS. Mike McMahon ported Emacs from ITS to the TENEX and TOPS-20 operating systems. Other contributors to early versions of Emacs include Kent Pitman, Earl Killian, and Eugene Ciccarelli. By 1979, Emacs was the main editor used in MIT's AI lab and its Laboratory for Computer Science.[25]
 In the following years, programmers wrote a variety of Emacs-like editors for other computer systems. These included EINE (EINE Is Not EMACS) and ZWEI[26] (ZWEI Was EINE Initially), which were written for the Lisp machine by Mike McMahon and Daniel Weinreb, and Sine (Sine Is Not Eine),[27] which was written by Owen Theodore Anderson. Weinreb's EINE was the first Emacs written in Lisp. In 1978, Bernard Greenberg wrote Multics Emacs almost entirely in Multics Lisp at Honeywell's Cambridge Information Systems Lab. Multics Emacs was later maintained by Richard Soley, who went on to develop the NILE Emacs-like editor for the NIL Project, and by Barry Margolin. Many versions of Emacs, including GNU Emacs, would later adopt Lisp as an extension language.
 James Gosling, who would later invent NeWS and the Java programming language, wrote Gosling Emacs in 1981. The first Emacs-like editor to run on Unix[citation needed], Gosling Emacs was written in C and used Mocklisp, a language with Lisp-like syntax, as an extension language.
 Early Ads for Computer Corporation of America's CCA EMACS (Steve Zimmerman)[28] appeared in 1984.[29] 1985 comparisons to GNU Emacs, when it came out, mentioned free vs. $2,400.[30][irrelevant citation]
 Richard Stallman began work on GNU Emacs in 1984 to produce a free software alternative to the proprietary Gosling Emacs. GNU Emacs was initially based on Gosling Emacs, but Stallman's replacement of its Mocklisp interpreter with a true Lisp interpreter required that nearly all of its code be rewritten. This became the first program released by the nascent GNU Project. GNU Emacs is written in C and provides Emacs Lisp, also implemented in C, as an extension language. Version 13, the first public release, was made on March 20, 1985. The first widely distributed version of GNU Emacs was version 15.34, released later in 1985. Early versions of GNU Emacs were numbered as 1.x.x, with the initial digit denoting the version of the C core. The 1 was dropped after version 1.12, as it was thought that the major number would never change, and thus the numbering skipped from 1 to 13.[31] In September 2014, it was announced on the GNU emacs-devel mailing list that GNU Emacs would adopt a rapid release strategy and version numbers would increment more quickly in the future.[32]
 GNU Emacs offered more features than Gosling Emacs, in particular a full-featured Lisp as its extension language, and soon replaced Gosling Emacs as the de facto Unix Emacs editor. Markus Hess exploited a security flaw in GNU Emacs' email subsystem in his 1986 cracking spree in which he gained superuser access to Unix computers.[33]
 Most of GNU Emacs functionality is implemented through a scripting language called Emacs Lisp. Because about 70% of GNU Emacs is written in the Emacs Lisp extension language,[34] one only needs to port the C core which implements the Emacs Lisp interpreter. This makes porting Emacs to a new platform considerably less difficult than porting an equivalent project consisting of native code only.
 GNU Emacs development was relatively closed until 1999 and was used as an example of the Cathedral development style in The Cathedral and the Bazaar. The project has since adopted a public development mailing list and anonymous CVS access. Development took place in a single CVS trunk until 2008 and was then switched to the Bazaar DVCS. On November 11, 2014, development was moved to Git.[35]
 Richard Stallman has remained the principal maintainer of GNU Emacs, but he has stepped back from the role at times. Stefan Monnier and Chong Yidong were maintainers from 2008 to 2015.[36][37] John Wiegley was named maintainer in 2015 after a meeting with Stallman at MIT.[38] As of early 2014, GNU Emacs has had 579 individual committers throughout its history.[39]
 Lucid Emacs, based on an early alpha version of GNU Emacs 19, was developed beginning in 1991 by Jamie Zawinski and others at Lucid Inc. One of the best-known early forks in free software development occurred when the codebases of the two Emacs versions diverged and the separate development teams ceased efforts to merge them back into a single program.[40] Lucid Emacs has since been renamed XEmacs. Its development is currently inactive, with the most recent stable version 21.4.22 released in January 2009 (while a beta was released in 2013), while GNU Emacs has implemented many formerly XEmacs-only features.[41][better source needed]
 Other notable forks include:
 In the past, projects aimed at producing small versions of Emacs proliferated. GNU Emacs was initially targeted at computers with a 32-bit flat address space and at least 1 MiB of RAM.[43] Such computers were high end workstations and minicomputers in the 1980s, and this left a need for smaller reimplementations that would run on common personal computer hardware. Today's computers have more than enough power and capacity to eliminate these restrictions, but small clones have more recently been designed to fit on software installation disks or for use on less capable hardware.[44]
 Other projects aim to implement Emacs in a different dialect of Lisp or a different programming language altogether. Although not all are still actively maintained, these clones include:
 Emacs is primarily a text editor and is designed for manipulating pieces of text, although it is capable of formatting and printing documents like a word processor by interfacing with external programs such as LaTeX, Ghostscript or a web browser. Emacs provides commands to manipulate and differentially display semantic units of text such as words, sentences, paragraphs and source code constructs such as functions. It also features keyboard macros for performing user-defined batches of editing commands.
 GNU Emacs is a real-time display editor, as its edits are displayed onscreen as they occur. This is standard behavior for modern text editors but EMACS was among the earliest to implement this. The alternative is having to issue a distinct command to display text, (e.g. before or after modifying it). This was common in earlier (or merely simpler) line and context editors, such as QED (BTS, CTSS, Multics), ed (Unix), ED (CP/M), and Edlin (DOS).
 Almost all of the functionality in Emacs, including basic editing operations such as the insertion of characters into a file, is achieved through functions written in a dialect of the Lisp programming language. The dialect used in GNU Emacs is known as Emacs Lisp (Elisp), and was developed expressly to port Emacs to GNU and Unix. The Emacs Lisp layer sits atop a stable core of basic services and platform abstraction written in the C programming language, which enables GNU Emacs to be ported to a wide variety of operating systems and architectures without modifying the implementation semantics of the Lisp system where most of the editor lives. In this Lisp environment, variables and functions can be modified with no need to rebuild or restart Emacs, with even newly redefined versions of core editor features being asynchronously compiled and loaded into the live environment to replace existing definitions. Modern GNU Emacs features both bytecode and native code compilation for Emacs Lisp.
 All configuration is stored in variables, classes, and data structures, and changed by simply updating these live. The use of a Lisp dialect in this case is a key advantage, as Lisp syntax consists of so-called symbolic expressions (or sexprs), which can act as both evaluatable code expressions and as a data serialisation format akin to, but simpler and more general than, well known ones such as XML, JSON, and YAML. In this way there is little difference in practice between customising existing features and writing new ones, both of which are accomplished in the same basic way. This is operatively different from most modern extensible editors, for instance such as VS Code, in which separate languages are used to implement the interface and features of the editor and to encode its user-defined configuration and options. The goal of Emacs' open design is to transparently expose Emacs' internals to the Emacs user during normal use in the same way that they would be exposed to the Emacs developer working on the git tree, and to collapse as much as possible of the distinction between using Emacs and programming Emacs, while still providing a stable, practical, and responsive editing environment for novice users.
 The main text editing data structure is the buffer, a memory region containing data (usually text) with associated attributes. The most important of these are:
 Modes, in particular, are an important concept in Emacs, providing a mechanism to disaggregate Emacs' functionality into sets of behaviours and keybinds relevant to specific buffers' data. Major modes provide a general package of functions and commands relevant to a buffer's data and the way users might be interacting with it (e.g. editing source code in a specific language, editing hex, viewing the filesystem, interacting with git, etc.), and minor modes define subsidiary collections of functionality applicable across many major modes (such as auto-save-mode). Minor modes can be toggled on or off both locally to each buffer as well as globally across all buffers, while major modes can only be toggled per-buffer. Any other data relevant to a buffer but not bundled into a mode can be handled by simply focussing that buffer and live modifying the relevant data directly.
 Any interaction with the editor (like key presses or clicking a mouse button) is realized by evaluating Emacs Lisp code, typically a command, which is a function explicitly designed for interactive use. Keys can be arbitrarily redefined and commands can also be accessed by name; some commands evaluate arbitrary Emacs Lisp code provided by the user in various ways (e.g. a family of eval- functions, operating on the buffer, region, or individual expression). Even the simplest user inputs (such a printable characters) are effectuated as Emacs Lisp functions, such as the self-insert-command , bound by default to most keyboard keys in a typical text editing buffer, which parameterises itself with the locale-defined character associated with the key used to call it.
 For example, pressing the f key in a buffer that accepts text input evaluates the code (self-insert-command 1 ?f), which inserts one copy of the character constant ?f at point. The 1, in this case, is determined by what Emacs terms the universal argument: all Emacs command code accepts a numeric value which, in its simplest usage, indicates repetition of an action, but in more complex cases (where repetition doesn't make sense) can yield other behaviours. These arguments may be supplied via command prefices, such as Control+u 7 f, or more compactly Meta+7 f, which expands to (self-insert-command 7 ?f). When no prefix is supplied, the universal argument is 1: every command implicitly runs once, but may be called multiply, or in a different way, when supplied with such a prefix. Such arguments may also be non-positive where it makes sense for them to be so - it is up to the function accepting the argument to determine, according to its own semantics, what a given number means to it. One common usage is for functions to perform actions in reverse simply by checking the sign of the universal argument, such as a sort command which sorts in obverse by default and in reverse when called with a negative argument, using the absolute value of its argument as the sorting key (e.g. -7 sorting in reverse by column index (or delimiter) 7), or undo/redo, which are simply negatives of each other (traversing forward and backward through a recursive history of diffs by some number of steps at a time).
 Because of its relatively large vocabulary of commands, Emacs features a long-established command language, to concisely express the keystrokes necessary to perform an action. This command language recognises the following shift and modifier keys: Ctrl, Alt, ⇧ Shift, Meta, Super, and Hyper. Not all of these may be present on an IBM-style keyboard, though they can usually be configured as desired. These are represented in command language as the respective prefices: C-, A-, S-, M-, s-, and H-. Keys whose names are only printable with more than one character are enclosed in angle brackets. Thus, a keyboard shortcut such as Ctrl+Alt+⇧ Shift+F9 (check dependent formulas and calculate all cells in all open workbooks in Excel) would be rendered in Emacs command language as C-A-S-<f9>, while an Emacs command like Meta+s f Ctrl+Meta+s (incremental file search by filename-matching regexp), would be expressed as M-s f C-M-s. Command language is also used to express the actions needed to invoke commands with no assigned shortcut: for example, the command scratch-buffer (which initialises a buffer in memory for temporary text storage and manipulation), when invoked by the user, will be reported back as M-x scra <return>, with Emacs scanning the namespace of contextually available commands to return the shortest sequence of keystrokes which uniquely lexicate it.
 Because Emacs predates modern standard terminology for graphical user interfaces, it uses somewhat divergent names for familiar interface elements. Buffers, the data that Emacs users interact with, are displayed to the user inside windows, which are tiled portions of the terminal screen or the GUI window, which Emacs refers to as frames; in modern terminology, an Emacs frame would be a window and an Emacs window would be a split. Depending on configuration, windows can include their own scroll bars, line numbers, sometimes a 'header line' typically to ease navigation, and a mode line at the bottom (usually displaying buffer name, the active modes and point position of the buffer among others). The bottom of every frame is used for output messages (then called 'echo area') and text input for commands (then called 'minibuffer').
 In general, Emacs display elements (windows, frames, etc.) do not belong to any specific data or process. Buffers are not associated with windows, and multiple windows can be opened onto the same buffer, for example to track different parts of a long text side-by-side without scrolling back and forth, and multiple buffers can share the same text, for example to take advantage of different major modes in a mixed-language file. Similarly, Emacs instances are not associated with particular frames, and multiple frames can be opened displaying a single running Emacs process, e.g. a frame per screen in a multi-monitor setup, or a terminal frame connected via ssh from a remote system and a graphical frame displaying the same Emacs process via the local system's monitor.
 Just as buffers don't require windows, running Emacs processes do not require any frames, and one common usage pattern is to deploy Emacs as an editing server: running it as a headless daemon and connecting to it via a frame-spawning client. This server can then be made available in any situation where an editor is required, simply by declaring the client program to be the user's EDITOR or VISUAL variable. Such a server continues to run in the background, managing any child processes, accumulating stdin from open pipes, ports, or fifos, performing periodic or pre-programmed actions, and remembering buffer undo history, saved text snippets, command history, and other user state between editing sessions. In this mode of operation, Emacs overlaps the functionality of programs like screen and tmux.
 Because of its separation of display concerns from editing functionality, Emacs can display roughly similarly on any device more complex than a dumb terminal, including providing typical graphical WIMP elements on sufficiently featureful text terminals - though graphical frames are the preferred mode of display, providing a strict superset of the features of text terminal frames.
 The first Emacs contained a help library that included documentation for every command, variable and internal function. Because of this, Emacs proponents described the software as self-documenting in that it presents the user with information on its normal features and its current state. Each function includes a documentation string that is displayed to the user on request, a practice that subsequently spread to programming languages including Lisp, Java, Perl, and Python. This help system can take users to the actual code for each function, whether from a built-in library or an added third-party library.
 Emacs also has a built-in tutorial. Emacs displays instructions for performing simple editing commands and invoking the tutorial when it is launched with no file to edit. The tutorial is by Stuart Cracraft and Richard Stallman.
 The Church of Emacs, formed by Richard Stallman, is a parody religion created for Emacs users.[51] While it refers to vi as the editor of the beast (vi-vi-vi being 6-6-6 in Roman numerals), it does not oppose the use of vi; rather, it calls it proprietary software anathema. ("Using a free version of vi is not a sin but a penance."[52]) The Church of Emacs has its own newsgroup, alt.religion.emacs,[53] that has posts purporting to support this parody religion. Supporters of vi have created an opposing Cult of vi.
 Stallman has jokingly referred to himself as St I GNU cius, a saint in the Church of Emacs.[54] This is in reference to Ignatius of Antioch, an early Church father venerated in Christianity.
 There is folklore attributing a repetitive strain injury colloquially called Emacs pinky to Emacs' strong dependence on modifier keys,[55] although there have not been any studies done to show Emacs causes more such problems than other keyboard-heavy computer programs.
 Users have addressed this through various approaches. Some users recommend simply using the two Control keys on typical PC keyboards like Shift keys while touch typing to avoid overly straining the left pinky, a proper use of the keyboard will reduce the RSI.[56] Software-side methods include:[57]
 Hardware solutions include special keyboards such as Kinesis's Contoured Keyboard, which places the modifier keys where they can easily be operated by the thumb, or the Microsoft Natural keyboard, whose large modifier keys are placed symmetrically on both sides of the keyboard and can be pressed with the palm of the hand.[55] Foot pedals can also be used.
 The Emacs pinky is a relatively recent development. The Space-cadet keyboard on which Emacs was developed had oversized Control keys that were adjacent to the space bar and were easy to reach with the thumb.[66]
 The word emacs is sometimes pluralized as emacsen, by phonetic analogy with boxen and VAXen, referring to different varieties of Emacs.[67]


Source: https://en.wikipedia.org/wiki/GUI-Toolkit
Content: Other reasons this message may be displayed:


Source: https://en.wikipedia.org/wiki/Tkinter
Content: Tkinter is a Python binding to the Tk GUI toolkit. It is the standard Python interface to the Tk GUI toolkit,[1] and is Python's de facto standard GUI.[2] Tkinter is included with standard Linux, Microsoft Windows and macOS installs of Python.
 The name Tkinter comes from Tk interface. Tkinter was written by Steen Lumholt and Guido van Rossum,[3] then later revised by Fredrik Lundh.[4]
 Tkinter is free software released under a Python license.[5]
 As with most other modern Tk bindings, Tkinter is implemented as a Python wrapper around a complete Tcl interpreter embedded in the Python interpreter. Tkinter calls are translated into Tcl commands, which are fed to this embedded interpreter, thus making it possible to mix Python and Tcl in a single application.
 There are several popular GUI library alternatives available, such as Kivy, Pygame, Pyglet, PyGObject, PyQt, PySide, and wxPython.
 This term has different meanings in different contexts, but in general it refers to a rectangular area somewhere on the user's display screen.
 A window which acts as a child of the primary window. It will be decorated with the standard frame and controls for the desktop manager. It can be moved around the desktop and can usually be resized.
 The generic term for any of the building blocks that make up an application in a graphical user interface.
 In Tkinter, the Frame widget is the basic unit of organization for complex layouts. A frame is a rectangular area that can contain other widgets.
 When any widget is created, a parent–child relationship is created. For example, if you place a text label inside a frame, the frame is the parent of the label.
 Here is a minimal Python 3 Tkinter application with one widget:[9]
 For Python 2, the only difference is the word "tkinter" in the import command will be capitalized to "Tkinter".[10]
 There are four stages to creating a widget[11]
 These are often compressed, and the order can vary.
 Using the object-oriented paradigm in Python, a simple program would be (requires Tcl version 8.6, which is not used by Python on MacOS by default):


Source: https://en.wikipedia.org/wiki/GUI-Builder
Content: Other reasons this message may be displayed:


Source: https://en.wikipedia.org/wiki/WxPython
Content: wxPython is a wrapper for the cross-platform GUI API (often referred to as a "toolkit") wxWidgets (which is written in C++) for the Python programming language. It is one of the alternatives to Tkinter. It is implemented as a Python extension module (native code).
 In 1995, Robin Dunn needed a GUI application to be deployed on HP-UX systems but also run Windows 3.1 within short time frame. He needed a cross-platform solution. While evaluating free and commercial solutions, he ran across Python bindings on the wxWidgets toolkit webpage (known as wxWindows at the time). This was Dunn's introduction to Python. Together with Harri Pasanen and Edward Zimmerman he developed those initial bindings into wxPython 0.2.[2]
 In August 1998, version 0.3 of wxPython was released. It was built for wxWidgets 2.0 and ran on Win32, with a wxGTK version in the works.[3]
 The first versions of the wrapper were created by hand. However, the code became difficult to maintain and keep synchronized with wxWidgets releases. By 1997, versions were created with SWIG, greatly decreasing the amount of work to update the wrapper.[2]
 In 2010, the Project Phoenix began; an effort to clean up the wxPython implementation and in the process make it compatible with Python 3.[4] The project is a new implementation of wxPython, focused on improving speed, maintainability and extensibility. Like the previous version of wxPython, it wraps the wxWidgets C++ toolkit and provides access to the user interface portions of the wxWidgets API.[5]
 With the release of 4.0.0a1 wxPython in 2017, the Project Phoenix version became the official version.[6] wxPython 4.x is the current version being developed as of June 2022.[7]
 wxPython enables Python to be used for cross-platform GUI applications requiring very little, if any, platform-specific code.
 This is a simple "Hello world" module, depicting the creation of the two main objects in wxPython (the main window object and the application object), followed by passing the control to the event-driven system (by calling MainLoop()) which manages the user-interactive part of the program.
 
This is another example of the wxPython Close Button with wxPython GUI display show in Windows 10 operating system. Being a wrapper, wxPython uses the same free software license used by wxWidgets (wxWindows License)[8]—which is approved by Free Software Foundation and Open Source Initiative.


Source: https://en.wikipedia.org/wiki/PyQt
Content: PyQt is a Python binding of the cross-platform GUI toolkit Qt, implemented as a Python plug-in. PyQt is free software developed by the British firm Riverbank Computing. It is available under similar terms to Qt versions older than 4.5; this means a variety of licenses including GNU General Public License (GPL) and commercial license, but not the GNU Lesser General Public License (LGPL).[3] PyQt supports Microsoft Windows as well as various kinds of UNIX, including Linux and MacOS (or Darwin).[4]
 PyQt implements around 440 classes and over 6,000 functions and methods[5] including:
 To automatically generate these bindings, Phil Thompson developed the tool SIP, which is also used in other projects.
 PyQt was first released by Riverbank Computing in 1998.[8]
 In August 2009, Nokia sought for the Python binding to be available under the LGPL license. At the time, Nokia owned Qt Software, the developer of QT. After failing to reach an agreement with Riverbank Computing, Nokia released its binding, PySide, providing similar functionality.[9]
 PyQt4 contains the following Python modules.
 PyQt5 contains the following Python modules:
 PyQt version 4 works with both Qt 4 and Qt 5. PyQt version 5 only supports Qt version 5,[4] and drops support for features that are deprecated in Qt 5.[11]
 The below code written for PyQt6 shows a small window on the screen.


Source: https://en.wikipedia.org/wiki/Qt_Designer
Content: 
 Qt Creator is a cross-platform C++, JavaScript, Python and QML integrated development environment (IDE) which simplifies GUI application development. It is part of the SDK for the Qt GUI application development framework and uses the Qt API, which encapsulates host OS GUI function calls.[3] It includes a visual debugger and an integrated WYSIWYG GUI layout and forms designer. The editor has features such as syntax highlighting and autocompletion. Qt Creator uses the C++ compiler from the GNU Compiler Collection on Linux. On Windows it can use MinGW or MSVC with the default install and can also use Microsoft Console Debugger when compiled from source code. Clang is also supported.
 Development of what would eventually become Qt Creator had begun by 2007 or earlier under transitional names Workbench and later Project Greenhouse.[4] It debuted during the later part of the Qt 4 era, starting with the release of Qt Creator, version 1.0 in March 2009[5] and subsequently bundled with Qt 4.5 in SDK 2009.3.[6]
 This was at a time when the standalone Qt Designer application was still the widget layout tool of choice for developers. There is no indication that Creator had layout capability at this stage. The record is somewhat muddied on this point (perhaps due to changes in ownership or the emphasis on Qt Quick), but the integration of Qt Designer under Qt Creator is first mentioned at least as early as Qt 4.7 (ca. late 2011).[7] In the Qt 5 era, it is simply stated that "[Qt Designer's] functionality is now included as part of [sic] Qt Creator IDE."[8]
 Qt Creator includes a project manager that can use a variety of project formats such as .pro, CMake, Autotools and others. A project file can contain information such as what files are included into the project, custom build steps and settings for running the applications.
 Qt Creator includes a code editor and integrates Qt Designer for designing and building graphical user interfaces (GUIs) from Qt widgets.
 The code editor in Qt Creator supports syntax highlighting for various languages. In addition to that, the code editor can parse code in C++ and QML languages and as a result code completion, context-sensitive help, semantic navigation are provided.[9]
 Qt Designer is a tool for designing and building graphical user interfaces (GUIs) from Qt widgets. It is possible to compose and customize the widgets or dialogs and test them using different styles and resolutions directly in the editor. Widgets and forms created with Qt Designer are integrated with programmed code, using the Qt signals and slots mechanism.[10]
 Qt Quick Designer is a tool for developing animations by using a declarative programming language QML.
 Qt Creator provides support for building and running Qt applications for desktop environments (Windows, Linux, FreeBSD and macOS), mobile devices (Android, BlackBerry, iOS, Maemo, and MeeGo) and embedded Linux devices. Build settings allow the user to switch between build targets, different Qt versions and build configurations. For mobile device targets, Qt Creator can generate an installation package, install it to a mobile device that is attached to the development computer and run it there.
 Qt Creator is integrated with a set of tools, such as version control systems and Qt Simulator.
 The following version control systems are supported:
 Qt Simulator is a tool for testing Qt applications that are intended for mobile devices in an environment similar to that of the device.
 Qt Creator uses external native debuggers to debug the C++ language.[11] Qt Creator displays the raw information provided by the native debuggers in a simplified manner.
 Debuggers supported are:


Source: https://en.wikipedia.org/wiki/PyGTK
Content: PyGTK is a set of Python wrappers for the GTK graphical user interface library. PyGTK is free software and licensed under the LGPL. It is analogous to PyQt/PySide and wxPython, the Python wrappers for Qt and wxWidgets, respectively. Its original author is GNOME developer James Henstridge. There are six people in the core development team, with various other people who have submitted patches and bug reports. PyGTK has been selected as the environment of choice for applications running on One Laptop Per Child systems.
 PyGTK will be phased out with the transition to GTK version 3 and be replaced with PyGObject,[4][5] which uses GObject Introspection to generate bindings for Python and other languages on the fly. This is expected to eliminate the delay between GTK updates and corresponding language binding updates, as well as reduce maintenance burden on the developers.[6]
 The Python code below will produce a 200x200 pixel window with the words "Hello World" inside.
 PyGTK has been used in a number of notable applications, some examples:
 PyGObject provides a wrapper for use in Python programs when accessing GObject libraries. GObject is an object system used by GTK, GLib, GIO, GStreamer and other libraries.
 Like the GObject library itself, PyGObject is licensed under the GNU LGPL, so it is suitable for use in both free software and proprietary applications. It is already in use in many applications ranging from small single-purpose scripts to large full-featured applications.
 PyGObject can dynamically access any GObject libraries that use GObject Introspection. It replaces the need for separate modules such as PyGTK, GIO and python-gnome to build a full GNOME 3.0 application.  Once new functionality is added to GObject library it is instantly available as a Python API without the need for intermediate Python glue.
 PyGObject has replaced PyGTK, but it has taken a considerable amount of time for many programs to be ported. Most of the software listed here has an older version which used PyGTK.


Source: https://en.wikipedia.org/wiki/Glade_(Programmierwerkzeug)
Content: Other reasons this message may be displayed:


Source: https://en.wikipedia.org/wiki/Kivy
Content: KIVY may refer to:


Source: https://en.wikipedia.org/wiki/PyFLTK
Content: Other reasons this message may be displayed:


Source: https://en.wikipedia.org/wiki/Pip_(Python)
Content: 
 pip (also known by Python 3's alias pip3) is a package-management system written in Python and is used to install and manage software packages.[4] The Python Software Foundation recommends using pip for installing Python applications and its dependencies during deployment.[5] Pip connects to an online repository of public packages, called the Python Package Index. Pip can be configured to connect to other package repositories (local or remote), provided that they comply to Python Enhancement Proposal 503.[6][7]
 Most distributions of Python come with pip preinstalled. Python 2.7.9 and later (on the python2 series), and Python 3.4 and later include pip by default.[8]
 First introduced as pyinstall in 2008 by Ian Bicking (the creator of the virtualenv package) as an alternative to easy install,[9][10] pip was chosen as the new name from one of several suggestions that the creator received on his blog post.[11] According to Bicking himself, the name is a recursive acronym for "Pip Installs Packages".[12] In 2011, the Python Packaging Authority (PyPA) was created to take over the maintenance of pip and virtualenv from Bicking, led by Carl Meyer, Brian Rosner, and Jannis Leidel.[10]
 With the release of pip version 6.0 (2014-12-22), the version naming process was changed to have version in X.Y format and drop the preceding 1 from the version label.[13]
 Pip's command-line interface allows the install of Python software packages by issuing a command: pip install some-package-name
 Users can also remove the package by issuing a command: pip uninstall some-package-name
 pip has a feature to manage full lists of packages and corresponding version numbers, possible through a "requirements" file.[14] This permits the efficient re-creation of an entire group of packages in a separate environment (e.g. another computer) or virtual environment. This can be achieved with a properly formatted file and the following command,[15] where requirements.txt is the name of the file: pip install -r requirements.txt.
 To install some package for a specific python version, pip provides the following command, where ${version} is replaced by 2, 3, 3.4, etc.: pip${version} install some-package-name.
 Pip provides a way to install user-defined projects locally with the use of setup.py file. This method requires the python project to have the following file structure:
 Within this structure, user can add setup.py to the root of the project (i.e. example_project for above structure) with the following content:
 After this, pip can install this custom project by running the following command, from the project root directory: pip install -e.
 Besides the default PyPI repository, Pip supports custom repositories as well.[16] Such repositories can be located on an HTTP(s) URL or on a file system location.
 A custom repository can be specified using the -i or—index-url option, like so: pip install -i https://your-custom-repo/simple <package name>; or with a filesystem: pip install -i /path/to/your/custom-repo/simple <package name>.


Source: https://en.wikipedia.org/wiki/Anaconda_(Python-Distribution)
Content: Other reasons this message may be displayed:


Source: https://en.wikipedia.org/wiki/Paketverwaltung
Content: Other reasons this message may be displayed:


Source: https://en.wikipedia.org/wiki/Conda_(Paketverwaltung)
Content: Other reasons this message may be displayed:


Source: https://en.wikipedia.org/wiki/Web_Server_Gateway_Interface
Content: The Web Server Gateway Interface (WSGI, pronounced whiskey[1][2] or WIZ-ghee[3]) is a simple calling convention for web servers to forward requests to web applications or frameworks written in the Python programming language. The current version of WSGI, version 1.0.1, is specified in Python Enhancement Proposal (PEP) 3333.[4]
 WSGI was originally specified as PEP-333 in 2003.[5] PEP-3333, published in 2010, updates the specification for Python 3.
 In 2003, Python web frameworks were typically written against only CGI, FastCGI, mod_python, or some other custom API of a specific web server.[6] To quote PEP 333:
 Python currently boasts a wide variety of web application frameworks, such as Zope, Quixote, Webware, SkunkWeb, PSO, and Twisted Web -- to name just a few. This wide variety of choices can be a problem for new Python users, because generally speaking, their choice of web framework will limit their choice of usable web servers, and vice versa... By contrast, although Java has just as many web application frameworks available, Java's "servlet" API makes it possible for applications written with any Java web application framework to run in any web server that supports the servlet API. WSGI was thus created as an implementation-neutral interface between web servers and web applications or frameworks to promote common ground for portable web application development.[4]
 The WSGI has two sides:
 Between the server and the application, there may be one or more WSGI middleware components, which implement both sides of the API, typically in Python code.
 WSGI does not specify how the Python interpreter should be started, nor how the application object should be loaded or configured, and different frameworks and webservers achieve this in different ways.
 A WSGI middleware component is a Python callable that is itself a WSGI application, but may handle requests by delegating to other WSGI applications. These applications can themselves be WSGI middleware components.[7]
 A middleware component can perform such functions as:[7]
 A WSGI-compatible "Hello, World!" application written in Python:
 Where:
 A full example of a WSGI network server is outside the scope of this article. Below is a sketch of how one would call a WSGI application and retrieve its HTTP status line, response headers, and response body, as Python objects.[10] Details of how to construct the environ dict have been omitted.
 Numerous web frameworks support WSGI:
 Currently wrappers are available for FastCGI, CGI, SCGI, AJP (using flup), twisted.web, Apache (using mod_wsgi or mod_python), Nginx (using ngx_http_uwsgi_module),[26] Nginx Unit (using the Python language module),[27] and Microsoft IIS (using WFastCGI,[28] isapi-wsgi,[29] PyISAPIe,[30] or an ASP gateway).


Source: https://en.wikipedia.org/wiki/Common_Gateway_Interface
Content: In computing, Common Gateway Interface (CGI) is an interface specification that enables web servers to execute an external program to process HTTP or HTTPS user requests.[1]
 Such programs are often written in a scripting language and are commonly referred to as CGI scripts, but they may include compiled programs.[2]
 A typical use case occurs when a web user submits a web form on a web page that uses CGI. The form's data is sent to the web server within an HTTP request with a URL denoting a CGI script. The web server then launches the CGI script in a new computer process, passing the form data to it. The output of the CGI script, usually in the form of HTML, is returned by the script to the Web server, and the server relays it back to the browser as its response to the browser's request.[3]
 Developed in the early 1990s, CGI was the earliest common method available that allowed a web page to be interactive. Due to a necessity to run CGI scripts in a separate process every time the request comes in from a client, various alternatives were developed.
 In 1993, the National Center for Supercomputing Applications (NCSA) team wrote the specification for calling command line executables on the www-talk mailing list.[4][5][6] The other Web server developers adopted it, and it has been a standard for Web servers ever since. A work group chaired by Ken Coar started in November 1997 to get the NCSA definition of CGI more formally defined.[7] This work resulted in RFC 3875, which specified CGI Version 1.1. Specifically mentioned in the RFC are the following contributors:[3]
 Historically CGI programs were often written using the C programming language. RFC 3875 "The Common Gateway Interface (CGI)" partially defines CGI using C,[3] in saying that environment variables "are accessed by the C library routine getenv() or variable environ".
 The name CGI comes from the early days of the Web, where webmasters wanted to connect legacy information systems such as databases to their Web servers. The CGI program was executed by the server and provided a common "gateway" between the Web server and the legacy information system.
 Traditionally a Web server has a directory which is designated as a document collection, that is, a set of files that can be sent to Web browsers connected to the server.[8] For example, if a web server has the fully-qualified domain name www.example.com, and its document collection is stored at /usr/local/apache/htdocs/ in the local file system (its document root), then the web server will respond to a request for http://www.example.com/index.html by sending to the browser a copy of the file /usr/local/apache/htdocs/index.html (if it exists).
 For pages constructed on the fly, the server software may defer requests to separate programs and relay the results to the requesting client (usually, a Web browser that displays the page to the end user).
 Such programs usually require some additional information to be specified with the request, such as query strings or cookies. Conversely, upon returning, the script must provide all the information required by HTTP for a response to the request: the HTTP status of the request, the document content (if available), the document type (e.g. HTML, PDF, or plain text), et cetera.
 Initially, there were no standardized methods for data exchange between a browser, the HTTP server with which it was communicating and the scripts on the server that were expected to process the data and ultimately return a result to the browser. As a result, mutual incompatibilities existed between different HTTP server variants that undermined script portability.
 Recognition of this problem led to the specification of how data exchange was to be carried out, resulting in the development of CGI.  Web page-generating programs invoked by server software that adheres to the CGI specification are known as CGI scripts, even though they may actually have been written in a non-scripting language, such as C.
 The CGI specification was quickly adopted and continues to be supported by all well-known HTTP server packages, such as Apache, Microsoft IIS, and (with an extension) node.js-based servers.
 An early use of CGI scripts was to process forms. In the beginning of HTML, HTML forms typically had an "action" attribute and a button designated as the "submit" button. When the submit button is pushed the URI specified in the "action" attribute would be sent to the server with the data from the form sent as a query string. If the "action" specifies a CGI script then the CGI script would be executed, the script in turn generating an HTML page.
 A Web server that supports CGI can be configured to interpret a URL that it serves as a reference to a CGI script. A common convention is to have a cgi-bin/ directory at the base of the directory tree and treat all executable files within this directory (and no other, for security) as CGI scripts. When a Web browser requests a URL that points to a file within the CGI directory (e.g., http://example.com/cgi-bin/printenv.pl/with/additional/path?and=a&query=string), then, instead of simply sending that file (/usr/local/apache/htdocs/cgi-bin/printenv.pl) to the Web browser, the HTTP server runs the specified script and passes the output of the script to the Web browser. That is, anything that the script sends to standard output is passed to the Web client instead of being shown in the terminal window that started the web server. Another popular convention is to use filename extensions; for instance, if CGI scripts are consistently given the extension .cgi, the Web server can be configured to interpret all such files as CGI scripts. While convenient, and required by many prepackaged scripts, it opens the server to attack if a remote user can upload executable code with the proper extension.
 The CGI specification defines how additional information passed with the request is passed to the script. The Web server creates a subset of the environment variables passed to it and adds details pertinent to the HTTP environment. For instance, if a slash and additional directory name(s) are appended to the URL immediately after the name of the script (in this example, /with/additional/path), then that path is stored in the PATH_INFO environment variable before the script is called. If parameters are sent to the script via an HTTP GET request (a question mark appended to the URL, followed by param=value pairs; in the example, ?and=a&query=string), then those parameters are stored in the QUERY_STRING environment variable before the script is called. Request HTTP message body, such as form parameters sent via an HTTP POST request, are passed to the script's standard input. The script can then read these environment variables or data from standard input and adapt to the Web browser's request.[9]
 CGI is often used to process input information from the user and produce the appropriate output. An example of a CGI program is one implementing a wiki. If the user agent requests the name of an entry, the Web server executes the CGI program. The CGI program retrieves the source of that entry's page (if one exists), transforms it into HTML, and prints the result. The Web server receives the output from the CGI program and transmits it to the user agent. Then if the user agent clicks the "Edit page" button, the CGI program populates an HTML textarea or other editing control with the page's contents. Finally if the user agent clicks the "Publish page" button, the CGI program transforms the updated HTML into the source of that entry's page and saves it.
 CGI programs run, by default, in the security context of the Web server. When first introduced a number of example scripts were provided with the reference distributions of the NCSA, Apache and CERN Web servers to show how shell scripts or C programs could be coded to make use of the new CGI. One such example script was a CGI program called PHF that implemented a simple phone book.
 In common with a number of other scripts at the time, this script made use of a function: escape_shell_cmd(). The function was supposed to sanitize its argument, which came from user input and then pass the input to the Unix shell, to be run in the security context of the Web server. The script did not correctly sanitize all input and allowed new lines to be passed to the shell, which effectively allowed multiple commands to be run. The results of these commands were then displayed on the Web server. If the security context of the Web server allowed it, malicious commands could be executed by attackers.
 This was the first widespread example of a new type of Web based attack called code injection, where unsanitized data from Web users could lead to execution of code on a Web server. Because the example code was installed by default, attacks were widespread and led to a number of security advisories in early 1996.[10]
 For each incoming HTTP request, a Web server creates a new CGI process for handling it and destroys the CGI process after the HTTP request has been handled. Creating and destroying a process can be expensive: consume CPU time and memory resources than the actual work of generating the output of the process, especially when the CGI program still needs to be interpreted by a virtual machine. For a high number of HTTP requests, the resulting workload can quickly overwhelm the Web server.
 The computational overhead involved in CGI process creation and destruction can be reduced by the following techniques:
 The optimal configuration for any Web application depends on application-specific details, amount of traffic, and complexity of the transaction; these trade-offs need to be analyzed to determine the best implementation for a given task and time budget. Web frameworks offer an alternative to using CGI scripts to interact with user agents.
 


Source: https://en.wikipedia.org/wiki/Framework
Content: A framework is a generic term commonly referring to an essential supporting structure which other things are built on top of.
 Framework may refer to:


Source: https://en.wikipedia.org/wiki/Django_(Framework)
Content: Other reasons this message may be displayed:


Source: https://en.wikipedia.org/wiki/SQLAlchemy
Content: SQLAlchemy is an open-source SQL toolkit and object-relational mapper (ORM) for the Python programming language released under the MIT License.[5]
 SQLAlchemy's philosophy is that relational databases behave less like object collections as the scale gets larger and performance starts being a concern, while object collections behave less like tables and rows as more abstraction is designed into them. For this reason it has adopted the data mapper pattern (similar to Hibernate for Java) rather than the active record pattern used by a number of other object-relational mappers.[6]
 SQLAlchemy was first released in February 2006. [3] SQLAlchemy beta 2.0 was released in October 2022, and the full 2.0 release in Early 2023.[7][8] The current release, 2.0.28, was released in March, 2024.[9]
 The following example represents an n-to-1 relationship between movies and their directors. It is shown how user-defined Python classes create corresponding database tables, how instances with relationships are created from either side of the relationship, and finally how the data can be queried—illustrating automatically generated SQL queries for both lazy and eager loading.
 Creating two Python classes and corresponding database tables in the DBMS:
 One can insert a director-movie relationship via either entity:
 SQLAlchemy issues the following query to the DBMS (omitting aliases):
 The output:
 Setting lazy=True (default) instead, SQLAlchemy would first issue a query to get the list of movies and only when needed (lazy) for each director a query to get the name of the corresponding director:


Source: https://en.wikipedia.org/wiki/Web2py
Content: Web2py is an open-source web application framework written in the Python programming language. Web2py allows web developers to program dynamic web content using Python. Web2py is designed to help reduce tedious web development tasks, such as developing web forms from scratch, although a web developer may build a form from scratch if required.[2]
 Web2py was originally designed as a teaching tool with emphasis on ease of use and deployment. Therefore, it does not have any project-level configuration files. The design of web2py was inspired by the Ruby on Rails and Django frameworks. Like these frameworks, web2py focuses on rapid development, favors convention over configuration approach and follows a model–view–controller (MVC) architectural pattern.
 Web2py is a full-stack framework in that it has built-in components for all major functions, including:
 Web2py encourages sound software engineering practices such as
 Web2py uses the WSGI protocol, the Python-oriented protocol for communication between web server and web applications. It also provides handlers for CGI and the FastCGI protocols, and it includes the multi-threaded, SSL-enabled Rocket[6] wsgiserver.
 All development, debugging, testing, maintenance and remote database administration can (optionally) be performed without third-party tools, via a web interface, itself a web2py application. Internationalization (adding languages and writing translations) can also be performed from this IDE. Each application has an automatically generated database administrative interface, similar to Django. The web IDE also includes web-based testing.
 Applications can also be created from the command line or developed with other IDEs.[7] Further debugging options:[8]
 The Hello World program with web2py in its simplest form (simple web page[14] with no template) looks like:
 Web2py includes pure Python-based template language, with no indentation requirements and a server-side Document Object Model (DOM).
The template system works without web2py.[15] Joomla 1.x templates can be converted to web2py layouts.[16]
 Web2py also includes two markup libraries: the markdown2 text-to-HTML filter, which converts Markdown markup to HTML on the fly; and markmin which is inspired by markdown but supports tables, html5 video/audio and oembed protocol.
 A controller without a view automatically uses a generic view that render the variables returned by the controller, enabling the development of an application's business logic before writing HTML. The "Hello World" example using a default template:
 The dict() output of an action is automatically rendered in HTML if the page is request with a .html extension, in JSON if the page is requested with a .json extension, in XML if requested with .xml. It supports other protocols including jsonp, rss, ics, google maps, etc. and is extensible.
 Here is a more complex code example which defines a table, and exposes a grid to logged in users:
 Each web2py application comes with a ticketing system:
 Cron is a mechanism for creating and running recurrent tasks in background. It looks for an application-specific crontab file which is in standard crontab format. Three modes of operation are available:
 Since version 2.3 the use of cron is discouraged since web2py comes with a master/worker scheduler. Jobs can be defined in models and are scheduled by creating an entry in the database. Users can start work processes who pickup and execute tasks in background. The schedule is better than cron because it allows to specify more parameters (start time, stop time, number of repetitions, number of trials in case of error) and do a better job at running within constant resource utilization.
 Web2py can compile web applications for distribution in bytecode compiled form, without source code. Unlike frameworks that use specialized template languages for their views, Web2py can also compile the view code into bytecode, since it is pure Python code.
 Web2py is unique in the world of Python web frameworks because models and controllers are executed, not imported. They are not modules. They are executed in a single global environment which is initialized at each HTTP request. This design decision has pros and cons.
 The major pro is the ease of development, specifically for rapid prototyping. Another pro is that all the objects defined within this environment are cleanly reset at each HTTP request and never shared across requests. This means the developer does not need to worry about changing the state of an object (for example the readable attribute of a database field) or worry about a change leaking to other concurrent requests or other applications. A third advantage is that web2py allows the coexistence of multiple applications under the same instance without conflicts even if they use different versions of the same modules or different modules with the same name.
 The main disadvantage of the global environment is that model files and controller files are not modules and the order of execution matters (although it can be specified using conditional models). Naming conflict is more likely to occur than in normal Python modules. Some standard Python development tools may not understand objects defined in models and controllers. Moreover, developers must be aware that code in models is executed at every request and this may cause a performance penalty. Nothing in web2py prevents developers from using and importing normal Python modules (model-less approach) and for this purpose web2py provides a thread local object (current) to facilitate access to objects associated to the current request. Yet, in this case, the developer has to be aware of the same pitfalls that other frameworks incur: changing the state of an object defined in a module may affect other concurrent requests.
 Another con is that, because models and controllers are not class-based, efficient code reuse becomes more difficult, particularly as the inability to inherit from a parent controller (e.g. the ApplicationController in Ruby on Rails) means that common controller functionality must be referenced repeatedly across all controller files.
 web2py runs on Windows, Windows CE phones, Mac, Unix/Linux, Google App Engine, Amazon EC2, and almost any web hosting via Python 2.7/3.5/3.6/pypy.[2]
 The current binary version of web2py (for Windows or Mac) includes Python 2.7, but the source version can be run on 2.7 and 3.5+. Support for Python 2.6 has been dropped on 2017.
 web2py since v1.64.0 runs unmodified on Java with Jython 2.5, without any known limitation.[17]
 web2py code can run with IronPython on .NET.[18] Limitations:
 The web2py binary will[19] run from a USB drive or a portable hard drive without dependencies, like Portable Python.
 Web2py can service requests via HTTP and HTTPS with its built-in Rocket server,[20] with Apache,[21] Lighttpd,[22] Cherokee,[23] Hiawatha, Nginx and almost any other web server through CGI, FastCGI, WSGI, mod_proxy,[24][25][26] and/or mod_python.
 While a number of web2py developers use text editors such as Vim, Emacs or TextMate Web2py also has a built-in web-based IDE. Others prefer more specialized tools providing debugging, refactoring, etc.
 The database abstraction layer (DAL) of web2py dynamically and transparently generates SQL queries and runs on multiple compatible database backend without the need for database-specific SQL commands (though SQL commands can be issued explicitly).
 SQLite is included in Python and is the default web2py database. A connection string change allows connection to Firebird, IBM Db2, Informix, Ingres, Microsoft SQL Server, MySQL, Oracle, PostgreSQL, and Google App Engine (GAE) with some caveats. Specialities:
 The DAL is fast, at least comparable with SQLAlchemy and Storm.[31]
 Web2py implements a DAL, not an ORM. An ORM maps database tables into classes representing logical abstractions from the database layer (e.g., a User class or a PurchaseOrder class), and maps records into instances of those classes. The DAL instead maps database tables and records into instances of classes representing sets and records instead of higher-level abstractions. It has very similar syntax to an ORM but it is faster, and can map almost any SQL expressions into DAL expressions. The DAL can be used independently of the rest of web2py.[32]
 Here are some examples of DAL syntax:
 The latest version of the DAL has support for 2D GIS functions with Spatialite and PostGIS. The current API are experimental because of a possible move to 3D APIs.
 web2py supports database migrations—change the definition of a table and web2py ALTERs the table accordingly. Migrations are automatic, but can be disabled for any table, and migration is typically disabled when an application is ready for live distribution. Migrations and migration attempts are logged, documenting the changes.
 Limitations:
 Web2py code is released under GNU Lesser General Public License (LGPL) version 3 as of web2py version 1.91.1.[33]
 Web2py code before version 1.91.1 was released under GNU GPL v2.0 with commercial exception.
 Various third-party packages distributed with web2py have their own licenses, generally public domain, MIT or BSD-type licenses. Applications built with web2py are not covered by the LGPL license.
 Web2py is copyrighted by Massimo DiPierro. The web2py trademark is owned by Massimo DiPierro.
 In 2011 InfoWorld ranked web2py highest among the top six Python web frameworks, awarded web2py the Bossie award 2011 for best open source application development software. In 2012 web2py won the InfoWorld Technology of the Year award.[34][35]
 The base web2py documentation is The Official web2py Book, by  Massimo DiPierro. The manual is a full web2py application and it's freely available online,[36] in PDF format or printed form.
 Online documentation is linked from the web2py home page, with cookbook, videos, interactive examples, interactive API reference, epydoc s (complete library reference), FAQ, cheat sheet, online tools etc.
 The lead developer of web2py is Massimo DiPierro, an associate professor of Computer Science at DePaul University in Chicago. As of 2011, the web2py homepage lists over 70 "main contributors".[38]
 The web2py development source code is available from the main repository:
 The source code for the first public version of web2py was released under GNU GPL v2.0 on 2007-09-27 by Massimo DiPierro as the Enterprise Web Framework (EWF). The name was changed twice due to name conflicts: EWF v1.7 was followed by Gluon v1.0, and Gluon v1.15 was followed by web2py v1.16. The license was changed to LGPLv3 as of web2py version 1.91.1 on 2010-12-21.


Source: https://en.wikipedia.org/wiki/Flask
Content: Flask may refer to:


Source: https://en.wikipedia.org/wiki/Zope_(Webanwendungsserver)
Content: Other reasons this message may be displayed:


Source: https://en.wikipedia.org/wiki/Symbian-Plattform
Content: Other reasons this message may be displayed:


Source: https://en.wikipedia.org/wiki/Mobiltelefon
Content: Other reasons this message may be displayed:


Source: https://en.wikipedia.org/wiki/AmigaOS
Content: 
 AmigaOS is a family of proprietary native operating systems of the Amiga and AmigaOne personal computers. It was developed first by Commodore International and introduced with the launch of the first Amiga, the Amiga 1000, in 1985. Early versions of AmigaOS required the Motorola 68000 series of 16-bit and 32-bit microprocessors. Later versions were developed by Haage & Partner (AmigaOS 3.5 and 3.9) and then Hyperion Entertainment (AmigaOS 4.0-4.1). A PowerPC microprocessor is required for the most recent release, AmigaOS 4.
 AmigaOS is a single-user operating system based on a preemptive multitasking kernel, called Exec.[1]
 It includes an abstraction of the Amiga's hardware, a disk operating system called AmigaDOS, a windowing system API called Intuition, and a desktop environment[2] and file manager called Workbench.
 The Amiga intellectual property is fragmented between Amiga Inc., Cloanto, and Hyperion Entertainment. The copyrights for works created up to 1993 are owned by Cloanto.[3][4] In 2001, Amiga Inc. contracted AmigaOS 4 development to Hyperion Entertainment, and in 2009 they granted Hyperion an exclusive, perpetual, worldwide license to AmigaOS 3.1 in order to develop and market AmigaOS 4 and subsequent versions.[5]
 On December 29, 2015, the AmigaOS 3.1 source code leaked to the web; this was confirmed by the licensee, Hyperion Entertainment.[6][7]
 AmigaOS is a single-user operating system based on a preemptive multitasking kernel, called Exec. AmigaOS provides an abstraction of the Amiga's hardware, a disk operating system called AmigaDOS, a windowing system API called Intuition and a desktop file manager called Workbench.
 A command-line interface (CLI), called AmigaShell, is also integrated into the system, though it also is entirely window-based. The CLI and Workbench components share the same privileges. Notably, AmigaOS lacks any built-in memory protection.
 AmigaOS is formed from two parts, namely, a firmware component called Kickstart and a software portion usually referred to as Workbench. Up until AmigaOS 3.1, matching versions of Kickstart and Workbench were typically released together. However, since AmigaOS 3.5, the first release after Commodore's demise, only the software component has been updated and the role of Kickstart has been diminished somewhat. 
Firmware updates may still be applied by patching at system boot. That was until 2018 when Hyperion Entertainment (license holder to AmigaOS 3.1) released AmigaOS 3.1.4 with an updated Kickstart ROM to go with it.
 Kickstart is the bootstrap firmware, usually stored in ROM. Kickstart contains the code needed to boot standard Amiga hardware and many of the core components of AmigaOS. The function of Kickstart is comparable to the BIOS plus the main operating system kernel in IBM PC compatibles. However, Kickstart provides more functionality available at boot time than would typically be expected on PC, for example, the full windowing environment.
 Kickstart contains many core parts of the Amiga's operating system, such as Exec, Intuition, the core of AmigaDOS and functionality to initialize Autoconfig-compliant expansion hardware. Later versions of the Kickstart contained drivers for IDE and SCSI controllers, PC card ports and other built-in hardware.
 Upon start-up or reset the Kickstart performs a number of diagnostic and system checks and then initializes the Amiga chipset and some core OS components. It will then examine connected boot devices and attempt to boot from the one with the highest boot priority. If no boot device is present a screen will be displayed asking the user to insert a boot disk, typically a floppy disk.
 At start-up Kickstart attempts to boot from a bootable device (typically, a floppy disk or hard disk drive). In the case of a floppy, the system reads the first two sectors of the disk (the bootblock), and executes any boot instructions stored there. Normally this code passes control back to the OS (invoking AmigaDOS and the GUI) and using the disk as the system boot volume. Any such disk, regardless of the other contents of the disk, was referred to as a "boot disk" or "bootable disk". A bootblock could be added to a blank disk by use of the install command. Some games and demos on floppy disk used custom bootblocks, which allowed them to take over the boot sequence and manage the Amiga's hardware without AmigaOS.
 The bootblock became an obvious target for virus writers. Some games or demos that used a custom bootblock would not work if infected with a bootblock virus, as the code of the virus replaced the original. The first such virus was the SCA virus. Anti-virus attempts included custom bootblocks.
These amended bootblock advertised the presence of the virus checker while checking the system for tell-tale signs of memory-resident viruses and then passed control back to the system. Unfortunately these could not be used on disks that already relied on a custom bootblock, but did alert users to potential trouble. Several of them also replicated themselves across other disks, becoming little more than viruses in their own right.[citation needed]
 The Macintosh should have had multitasking. I can't stress enough what a big contribution it makes to the elegant design of system software. The Amiga has an excellent multitasking system, and I think it will have twice the product life of the Macintosh because of it. Exec is the multi-tasking kernel of AmigaOS. Exec provides functionality for multi-tasking, memory allocation, interrupt handling and handling of dynamic shared libraries. It acts as a scheduler for tasks running on the system, providing pre-emptive multitasking with prioritized round-robin scheduling. 
Exec also provides access to other libraries and high-level inter-process communication via message passing. Other comparable microkernels have had performance problems because of the need to copy messages between address spaces. Since the Amiga has only one address space, Exec message passing is quite efficient.[9][10]
 AmigaDOS provides the disk operating system portion of the AmigaOS. This includes file systems, file and directory manipulation, the command-line interface, file redirection, console windows, and so on. Its interfaces offer facilities such as command redirection, piping, scripting with structured programming primitives, and a system of global and local variables.
 In AmigaOS 1.x, the AmigaDOS portion was based on TRIPOS, which is written in BCPL. Interfacing with it from other languages proved a difficult and error-prone task, and the port of TRIPOS was not very efficient.
 From AmigaOS 2.x onwards, AmigaDOS was rewritten in C and Assembler, retaining 1.x BCPL program compatibility, and it incorporated parts of the third-party AmigaDOS Resource Project,[11] which had already written replacements for many of the BCPL utilities and interfaces.
 ARP also provided one of the first standardized file requesters for the Amiga, and introduced the use of more friendly UNIX-style wildcard (globbing) functions in command-line parameters. Other innovations were an improvement in the range of date formats accepted by commands and the facility to make a command resident, so that it only needs to be loaded into memory once and remains in memory to reduce the cost of loading in subsequent uses.
 In AmigaOS 4.0, the DOS abandoned the BCPL legacy completely and, starting from AmigaOS 4.1, it has been rewritten with full 64-bit support.
 File extensions are often used in AmigaOS, but they are not mandatory and they are not handled specially by the DOS, being instead just a conventional part of the file names. Executable programs are recognized using a magic number.
 The native Amiga windowing system is called Intuition, which handles input from the keyboard and mouse and rendering of screens, windows and widgets.
 Prior to AmigaOS 2.0, there was no standardized look and feel, application developers had to write their own non-standard widgets. Commodore added the GadTools library and BOOPSI in AmigaOS 2.0, both of which provided standardized widgets. Commodore also published the Amiga User Interface Style Guide, which explained how applications should be laid out for consistency. Stefan Stuntz created a popular third-party widget library, based on BOOPSI, called Magic User Interface, or MUI. MorphOS uses MUI as its official toolkit, while AROS uses a MUI clone called Zune. AmigaOS 3.5 added another widget set, ReAction, also based on BOOPSI.
 An unusual feature of AmigaOS is the use of multiple screens shown on the same display. Each screen may have a different video resolution or color depth. AmigaOS 2.0 added support for public screens, allowing applications to open windows on other applications' screens. Prior to AmigaOS 2.0, only the Workbench screen was shared.[12] A widget in the top-right corner of every screen allows screens to be cycled through. Screens can be overlaid by dragging each up or down by their title bars. AmigaOS 4 introduced screens that are draggable in any direction.
 Workbench is the native graphical file manager and desktop environment of AmigaOS. Though the term Workbench was originally used to refer to the entire operating system, with the release of AmigaOS 3.1 the operating system was renamed AmigaOS and subsequently Workbench refers to the desktop manager only. As the name suggests, the metaphor of a workbench is used, rather than that of a desktop; directories are depicted as drawers, executable files are tools, data files are projects and GUI widgets are gadgets. In many other aspects the interface resembles Mac OS, with the main desktop showing icons of inserted disks and hard drive partitions, and a single menu bar at the top of every screen. Unlike the Macintosh mouse available at the time, the standard Amiga mouse has two buttons – the right mouse button operates the pull-down menus, with a "release to select" mechanism.
 Until the release of version 3, AmigaOS only natively supported the native Amiga graphics chipset, via graphics.library, which provides an API for geometric primitives, raster graphic operations and handling of sprites. As this API could be bypassed, some developers chose to avoid OS functionality for rendering and directly program the underlying hardware for gains in efficiency.
 Third-party graphics cards were initially supported via proprietary unofficial solutions. A later solution where AmigaOS could directly support any graphics system, was termed retargetable graphics (RTG).[13] With AmigaOS 3.5, some RTG systems were bundled with the OS, allowing the use of common hardware cards other than the native Amiga chipsets. The main RTG systems are CyberGraphX, Picasso 96 and EGS. Some vector graphic libraries, like Cairo and Anti-Grain Geometry, are also available. Modern systems can use cross-platform SDL (simple DirectMedia Layer) engine for games and other multimedia programs.
 The Amiga did not have any inbuilt 3D graphics capability, and so had no standard 3D graphics API. Later, graphics card manufacturers and third-party developers provided their own standards, which included MiniGL, Warp3D, StormMesa (agl.library) and CyberGL.
 The Amiga was launched at a time when there was little support for 3D graphics libraries to enhance desktop GUIs and computer rendering capabilities. However, the Amiga became one of the first widespread 3D development platforms. VideoScape 3D was one of the earliest 3D rendering and animation systems, and Silver/TurboSilver was one of the first ray-tracing 3D programs. Then Amiga boasted many influential applications in 3D software, such as Imagine, maxon's Cinema 4D, Realsoft 3D, VistaPro, Aladdin 4D and NewTek's Lightwave (used to render movies and television shows like Babylon 5).
 Likewise, while the Amiga is well known for its ability to easily genlock with video, it has no built-in video capture interface. The Amiga supported a vast number of third-party interfaces for video capture from American and European manufacturers. There were internal and external hardware solutions, called frame-grabbers, for capturing individual or sequences of video frames, including: Newtronic Videon, Newtek DigiView,[14] Graffiti external 24-bit framebuffer, the Digilab, the Videocruncher, Firecracker 24, Vidi Amiga 12, Vidi Amiga 24-bit and 24RT (Real Time), Newtek Video Toaster, GVP Impact Vision IV24, MacroSystem VLab Motion and VLab PAR, DPS PAR (Personal Animation Recorder), VHI (Video Hardware Interface) by IOSPIRIT GmbH, DVE-10, etc. Some solutions were hardware plug-ins for Amiga graphics cards like the Merlin XCalibur module, or the DV module built for the Amiga clone Draco from the German firm Macrosystem. Modern PCI bus TV expansion cards and their capture interfaces are supported through tv.library by Elbox Computer and tvcard.library by Guido Mersmann.
 Following modern trends in evolution of graphical interfaces, AmigaOS 4.1 uses the 3D hardware-accelerated Porter-Duff image composition engine.
 Prior to version 3.5, AmigaOS only officially supported the Amiga's native sound chip, via audio.device. This facilitates playback of sound samples on four DMA-driven 8-bit PCM sound channels. The only supported hardware sample format is signed linear 8-bit two's complement.
 Support for third-party audio cards was vendor-dependent, until the creation and adoption of AHI[15] as a de facto standard. AHI offers improved functionality, such as seamless audio playback from a user-selected audio device, standardized functionality for audio recording and efficient software mixing routines for combining multiple sound channels, thus overcoming the four-channel hardware limit of the original Amiga chipset. AHI can be installed separately on AmigaOS v2.0 and later.[16]
 AmigaOS itself did not support MIDI until version 3.1, when Roger Dannenberg's camd.library was adapted as the standard MIDI API. Commodore's version of camd.library also included a built-in driver for the serial port. The later open source version of camd.library by Kjetil Matheussen did not provide a built-in driver for the serial port, but provided an external driver instead.
 AmigaOS was one of the first operating systems to feature speech synthesis with software developed by SoftVoice, Inc., which allowed text-to-speech conversion of American English.[17] This had three main components: narrator.device, which modulates the phonemes used in American English, translator.library, which translates English text to American English phonemes using a set of rules, and a high-level SPEAK: handler, which allows command-line users to redirect text output to speech. A utility called Say was included with the OS, which allowed text-to-speech synthesis with some control of voice and speech parameters. A demo was also included with AmigaBASIC programming examples. Speech synthesis was occasionally used in third-party programs, particularly educational software. For example, the word processors Prowrite and Excellence! could read out documents using the synthesizer. These speech synthesis components remained largely unchanged in later OS releases and Commodore eventually removed speech synthesis support from AmigaOS 2.1 onward because of licensing restrictions.[18]
 Despite the American English limitation of the narrator.device's phonemes, Francesco Devitt developed an unofficial version with multilingual speech synthesis. This made use of an enhanced version of the translator.library which could translate a number of languages into phonemes, given a set of rules for each language.[19]
 The AmigaOS has a dynamically sized RAM disk, which resizes itself automatically to accommodate its contents.  Starting with AmigaOS 2.x, operating system configuration files were loaded into the RAM disk on boot, greatly speeding operating system usage.  Other files could be copied to the RAM disk like any standard device for quick modification and retrieval.  Also beginning in AmigaOS 2.x, the RAM disk supported file-change notification, which was mostly used to monitor configuration files for changes.
 Starting with AmigaOS 1.3,[20] there is also a fixed-capacity recoverable RAM disk, which functions as a standard RAM disk but can maintain its contents on soft restart.  It is commonly called the RAD disk after its default device name, and it can be used as a boot disk (with boot sector).  Previously, a recoverable RAM disk, commonly called the ASDG RRD or VD0, was introduced in 1987;[21] at first, it was locked to ASDG expansion memory products.  Later, the ASDG RRD was added to the Fred Fish series of freeware, shareware, and public domain software (disks 58[22] and 241[23]).
 The AmigaOS has support for the Rexx language, called ARexx (short for "Amiga Rexx"), and is a script language which allows for full OS scripting, similar to AppleScript; intra-application scripting, similar to VBA in Microsoft Office; as well as inter-program communication. Having a single scripting language for any application on the operating system is beneficial to users, instead of having to learn a new language for each application.
 Programs can listen on an "ARexx port" for string messages. These messages can then be interpreted by the program in a similar fashion to a user pushing buttons. For example, an ARexx script run in an e-mail program could save the currently displayed email, invoke an external program which could extract and process information, and then invoke a viewer program. This allows applications to control other applications by sending data back and forth directly with memory handles, instead of saving files to disk and then reloading them.
 Since AmigaOS 4, the Python language is included with the operating system.
 John C. Dvorak stated in 1996:
 The AmigaOS "remains one of the great operating systems of the past 20 years, incorporating a small kernel and tremendous multitasking capabilities the likes of which have only recently been developed in OS/2 and Windows NT. The biggest difference is that the AmigaOS could operate fully and multitask in as little as 250 K of address space. Even today, the OS is only about 1 MB in size. And to this day, there is very little a memory-hogging CD-ROM-loading OS can do the Amiga can't. Tight code — there's nothing like it.
I've had an Amiga for maybe a decade. It's the single most reliable piece of equipment I've ever owned. It's amazing! You can easily understand why so many fanatics are out there wondering why they are alone in their love of the thing. The Amiga continues to inspire a vibrant — albeit cultlike — community, not unlike that which you have with Linux, the Unix clone."[24] AmigaOS provides a modular set of system functions through dynamically loaded shared libraries, either stored as a file on disk with a ".library" filename extension, or stored in the Kickstart firmware. All library functions are accessed via an indirect jump table, which is a negative offset to the library base pointer. That way, every library function can be patched or hooked at run-time, even if the library is stored in ROM. The core library of AmigaOS is the exec.library (Exec), which provides an interface to functions of the Amiga's microkernel.
 Device drivers are also libraries, but they implement a standardized interface. Applications do not usually call devices directly as libraries, but use the exec.library I/O functions to indirectly access them. Like libraries, devices are either files on disk (with the ".device" extension), or stored in the Kickstart ROM.
 The higher-level part of device and resource management is controlled by handlers, which are not libraries, but tasks, and communicate by passing messages. One type of handler is a filesystem handler. The AmigaOS can make use of any filesystem for which a handler has been written, a possibility that has been exploited by programs like CrossDOS and by a few "alternative" file systems to the standard OFS and FFS. These file systems allow one to add new features like journaling or file privileges, which are not found in the standard operating system. Handlers typically expose a device name to the DOS, which can be used to access the peripheral (if any) associated with the handler. As an example of these concepts is the SPEAK: handler which could have text redirected to spoken speech, through the speech synthesis system.
 Device names are case insensitive (uppercase by convention) strings followed by a colon. After the colon a specifier can be added, which gives the handler additional information about what is being accessed and how. In the case of filesystem, the specifier usually consists of a path to a file in the filesystem; for other handlers, specifiers usually set characteristics of the desired input/output channel (for the SER: serial port driver, for example, the specifier will contain bit rate, start and stop bits, etc.). Filesystems expose drive names as their device names. For example, DF0: by default refers to the first floppy drive in the system. On many systems DH0: is used to refer to the first hard drive. Filesystems also expose volume names, following the same syntax as device names: these identify the specific medium in the file system-managed drive. If DF0: contains a disk named "Workbench", then Workbench: will be a volume name that can be used to access files in DF0:. If one wanted to access a file named "Bar" located in directory "Foo" of the disk with name "Work" in drive DF0:, one could write "DF0:Foo/Bar" or "Work:Foo/Bar". However, these are not completely equivalent, since when the latter form is used, the system knows that the wanted volume is "Work" and not just any volume in DF0:. Therefore, whenever a requested file on "Work" is being accessed without volume "Work" being present in any drive, it will say something to the effect of: Please insert volume Work in any drive.
 Programs often need to access files without knowing their physical location (either the drive or the volume): they only know the "logical path" of the file, i.e. whether the file is a library, a documentation file, a translation of the program's messages, and so on. This is solved in AmigaOS by the use of assigns. An assign follows, again, the same syntax as a device name; however, it already points to a directory inside the filesystem. The place an assign points to can be changed at any time by the user (this behavior is similar to, but nevertheless distinct from, the subst command in MS-DOS, for example). Assigns were also convenient because one logical assign could point to more than one different physical location at the same time, thereby allowing an assign′s contents to expand logically, while still maintaining a separate physical organization. Standard assigns that are generally present in an AmigaOS system include:
 AmigaOS 4 introduced new system for allocating RAM and defragmenting it "on the fly" during system inactivities. It is based on slab allocation method and there is also present a memory pager that arbitrates paging memory and allows the swapping of large portions of physical RAM on mass storage devices as a sort of virtual memory.[25][26]
Co-operative paging was finally implemented in AmigaOS 4.1.
 Since the introduction of AmigaOS in 1985 there have been four major versions and several minor revisions. Up until release 3.1 of the Amiga's operating system, Commodore used Workbench to refer to the entire Amiga operating system. As a consequence Workbench was commonly used to refer to both the operating system and the file manager component. For end users Workbench was often synonymous with AmigaOS. From version 3.5 the OS was renamed "AmigaOS" and pre-3.5 versions were also retroactively referred to as "AmigaOS" (rather than Workbench). Subsequently, "Workbench" refers to the native graphical file manager only.
 From its inception, Workbench offered a highly customizable interface. The user could change the aspect of program icons replacing it with newer ones with different color combinations. Users could also take a "snapshot" of icons and windows so the icons will remain on the desktop at coordinates chosen by user and windows will open at the desired size.
 AmigaOS 1.0 was released with the first Amiga, the Amiga 1000, in 1985. The 1.x versions of AmigaOS by default used a blue and orange color scheme, designed to give high contrast on even the worst of television screens (the colors can be changed by the user). Version 1.1 consists mostly of bug fixes and, like version 1.0, was distributed for the Amiga 1000 only.
 The display was highly customizable for the era. The user was free to create and modify system and user icons, which could be of arbitrary size and design and can have two image states to produce a pseudo-animated effect when selected. Users could customize four display colors and choose from two resolutions: 640×200 or 640×400 (interlaced) on NTSC, or 640×256 or 640×512 on PAL systems. In later revisions, the TV or monitor overscan could be adjusted.
 Several features were deprecated in later versions.  For example, the gauge meter showing the free space on a file system was replaced with a percentage in AmigaOS 2.0 before being restored in 3.5.  The default "busy" pointer (a comic balloon showing "Zzz...") was replaced with a stopwatch in later versions.
 AmigaOS 2.0 was released with the launch of the Amiga 3000 in 1990. Until AmigaOS 2.0 there was no unified look and feel design standard and application developers had to write their own widgets (both buttons and menus) if they wished to enhance the already-meager selection of standard basic widgets provided by Intuition. With AmigaOS 2.0 gadtools.library was created, which provided standard widget sets. The Amiga User Interface Style Guide, was published which explained how applications should be laid out for consistency. Intuition was improved with BOOPSI (Basic Object Oriented Programming System for Intuition) which enhanced the system with an object-oriented interface to define a system of classes in which every class individuates a single widget or describes an interface event. It can be used to program object oriented interfaces into Amiga at any level.
 AmigaOS 2.0 also added support for public screens. Instead of the AmigaOS screen being the only shareable screen, applications could create their own named screens to share with other applications.
 AmigaOS 2.0 rectified the problem of applications hooking directly into the input-events stream to capture keyboard and mouse movements, sometimes locking up the whole system. AmigaOS 2.0 provided Commodities, a standard interface for modifying or scanning input events. This included a standard method for specifying global "hotkey" key-sequences, and a Commodities Exchange registry for the user to see which commodities were running.
 AmigaOS 2.1 introduced AmigaGuide, a simple text-only hypertext markup scheme and browser, for providing online help inside applications. It also introduced Installer, a standard software installation program, driven by a LISP-like scripting language.
 AmigaOS 2.1 introduced multi-lingual locale support through locale.library and for the first time AmigaOS was translated to different languages.[18]
 Version 3.0 was originally shipped with the Amiga 1200 and Amiga 4000 computers. Version 3.0 added datatypes support which allowed any application that supported datatypes to load any file format supported by datatypes.  Workbench could load any background image in any format if the required datatype was installed. A tiny application called Multiview was included that could open and display any supported file. Its capabilities were directly related to the datatypes installed in Devs:Datatypes. The established AmigaGuide hypertext system gained more usability by using document links pointing to media files, for example pictures or sounds, all recognized by the datatypes.
 Around six years after AmigaOS 3.1 was released, following Commodore's demise, Haage & Partner were granted a license to update AmigaOS, which was released in 1999 as a software-only update for existing systems, that ran at least on a 68(EC)020 processor.
 The AmigaOS look and feel, though still largely based on the earlier 3.1 release was revised somewhat, with an improved user interface based on ReAction, improved icon rendering and official support for true color backdrops. These releases included support for existing third-party GUI enhancements, such as NewIcons, by integrating these patches into the system. The 3.5 and 3.9 releases included a new set of 256 color icons and a choice of desktop wallpaper. These replaced the default all-metal gray 4/8 color scheme used on AmigaOS from release 2.0 to 3.1.
 The 3.9 release of AmigaOS was again developed by Haage&Partner and released in 2000. The main improvements were the introduction of a program start bar called AmiDock, revised user interfaces for system settings and improved utility programs.
 In September 2018, Hyperion Entertainment released AmigaOS 3.1.4; this was both a software and hardware update for all Amigas.  In 2019, AmigaOS 3.1.4.1 was released as a software only update to Amiga 3.1.4, mainly as a bug fix.[citation needed]
 It includes many fixes, modernizes several system components previously upgraded in OS 3.9, introduces support of larger hard drives (including at bootup), supports the entire line of Motorola 680x0 CPUs up to (and including) the Motorola 68060, and includes a modernized Workbench with a new, optional icon set.  Unlike AmigaOS 3.5 / 3.9, AmigaOS 3.1.4 still supports the Motorola 68000 CPU.
 In May 2021, Hyperion Entertainment released AmigaOS 3.2, which includes all features of the previous version (3.1.4.1) and adds several new improvements such as support for ReAction GUI, management of Amiga Disk File images, help system and improved datatypes.[27]
In December 2021, an update was released named AmigaOS 3.2.1, with bug fixes and other improvements. A second update, named AmigaOS 3.2.2, was released in March 2023 with more improvements and bug fixes.[28]
 This new AmigaOS, called AmigaOS 4.0 has been rewritten to become fully PowerPC compatible. It was initially developed on Cyberstorm PPC, as making it independent of the old Amiga chipsets was nontrivial.[29] Since the fourth Developer Pre-Release Update a new technique was adopted and the screens are draggable in any direction.[30] Drag and drop of Workbench icons between different screens is possible too.
 Also in AmigaOS 4.0 were a new version of Amidock, TrueType/OpenType fonts, and a movie player with DivX and MPEG-4 support.
 In AmigaOS 4.1, a new Start-up preferences feature was added which replaced the old WBStartup drawer. Additional enhancements were a new icon set to complement higher screen resolutions, new window themes including drop shadows, a new version of AmiDock with true transparency, scalable icons and AmigaOS with auto-update feature.[31]
 In October 2022, AmigaOS developer Hyperion Entertainment released an SDK for AmigaOS 4.1.[32]
 AROS Research Operating System (AROS) implements the AmigaOS API in a portable open-source operating system. Although not binary-compatible with AmigaOS (unless running on 68k), users have reported it to be highly source-code-compatible.
 MorphOS is a PowerPC native operating system which also runs on some Amiga hardware. It implements AmigaOS API and provides binary compatibility with "OS-friendly" AmigaOS applications (that is, those applications which do not access any native, legacy Amiga hardware directly just as AmigaOS 4.x unless executed on real Amiga models).
 pOS was a multiplatform closed-source operating system with source code-level compatibility with existing Amiga software.[33]
 BeOS features also a centralized datatype structure similar to MacOS Easy Open after old Amiga developers requested Be to adopt Amiga datatype service. It allows the entire OS to recognize all kinds of files (text, music, videos, documents, etc.) with standard file descriptors. The datatype system provides the entire system and any productivity tools with standard loaders and savers for these files, without the need to embed multiple file-loading capabilities into any single program.[34]
 AtheOS was inspired by AmigaOS, and originally intended to be a clone of AmigaOS.[35] Syllable is a fork of AtheOS, and includes some AmigaOS- and BeOS-like qualities.
 FriendUP is a cloud based meta operating system. It has many former Commodore and Amiga developers and employees working on the project. The operating system retains several AmigaOS-like features, including DOS Drivers, mount lists, a TRIPOS based CLI and screen dragging.[36]
 Finally, the operating system of the 3DO Interactive Multiplayer bore a very strong resemblance to AmigaOS and was developed by RJ Mical,[37] the creator of the Amiga's Intuition user interface.[38]


Source: https://en.wikipedia.org/wiki/Google_Suche
Content: Other reasons this message may be displayed:


Source: https://en.wikipedia.org/wiki/YouTube
Content: 
 YouTube is an American online video sharing and social media platform owned by Google. Accessible worldwide,[7] it was launched on February 14, 2005, by Steve Chen, Chad Hurley, and Jawed Karim, three former employees of PayPal. Headquartered in San Bruno, California, United States, it is the second most visited website in the world, after Google Search. YouTube has more than 2.5 billion monthly users,[2] who collectively watch more than one billion hours of videos every day.[8] As of May 2019[update], videos were being uploaded to the platform at a rate of more than 500 hours of content per minute,[9][10] and as of 2023, there were approximately 14 billion videos in total.[10]
 In October 2006, YouTube was bought by Google for $1.65 billion (equivalent to $2.22 billion in 2022).[11] Google's ownership of YouTube expanded the site's business model, expanding from generating revenue from advertisements alone to offering paid content such as movies and exclusive content produced by YouTube. It also offers YouTube Premium, a paid subscription option for watching content without ads. YouTube also approved creators to participate in Google's AdSense program, which seeks to generate more revenue for both parties. In 2021, YouTube's annual advertising revenue increased to $28.8 billion, an increase in revenue of $9 billion from the previous year.[1] YouTube reported revenue of $29.2 billion in 2022.[12]
 Since its purchase by Google, YouTube has expanded beyond the core website into mobile apps, network television, and the ability to link with other platforms. Video categories on YouTube include music videos, video clips, news, short films, feature films, songs, documentaries, movie trailers, teasers, live streams, vlogs, and more. Most content is generated by individuals, including collaborations between YouTubers and corporate sponsors. Established media corporations such as Disney, Paramount, NBCUniversal, and Warner Bros. Discovery have also created and expanded their corporate YouTube channels to advertise to a greater audience.
 YouTube has had unprecedented social impact, influencing popular culture, internet trends, and creating multimillionaire celebrities. Despite its growth and success, the platform is sometimes criticized for allegedly facilitating the spread of misinformation, the sharing of copyrighted content, routinely violating its users' privacy, enabling censorship, endangering child safety and wellbeing, and for its inconsistent or incorrect implementation of platform guidelines.
 YouTube was founded by Steve Chen, Chad Hurley, and Jawed Karim. The trio were early employees of PayPal, which left them enriched after the company was bought by eBay.[13] Hurley had studied design at the Indiana University of Pennsylvania, and Chen and Karim studied computer science together at the University of Illinois Urbana-Champaign.[14]
 According to a story that has often been repeated in the media, Hurley and Chen developed the idea for YouTube during the early months of 2005, after they had experienced difficulty sharing videos that had been shot at a dinner party at Chen's apartment in San Francisco. Karim did not attend the party and denied that it had occurred, but Chen remarked that the idea that YouTube was founded after a dinner party "was probably very strengthened by marketing ideas around creating a story that was very digestible".[15]
 Karim said the inspiration for YouTube came from the Super Bowl XXXVIII halftime show controversy, when Janet Jackson's breast was briefly exposed by Justin Timberlake during the halftime show. Karim could not easily find video clips of the incident and the 2004 Indian Ocean Tsunami online, which led to the idea of a video-sharing site.[16] Hurley and Chen said that the original idea for YouTube was a video version of an online dating service and had been influenced by the website Hot or Not.[15][17] They created posts on Craigslist asking attractive women to upload videos of themselves to YouTube in exchange for a $100 reward.[18] Difficulty in finding enough dating videos led to a change of plans, with the site's founders deciding to accept uploads of any video.[19]
 YouTube began as a venture capital–funded technology startup. Between November 2005 and April 2006, the company raised money from various investors, with Sequoia Capital and Artis Capital Management being the largest two.[13][20] YouTube's early headquarters were situated above a pizzeria and a Japanese restaurant in San Mateo, California.[21] In February 2005, the company activated www.youtube.com.[22] The first video was uploaded on April 23, 2005. Titled "Me at the zoo", it shows co-founder Jawed Karim at the San Diego Zoo and can still be viewed on the site.[23][24] In May, the company launched a public beta and by November, a Nike ad featuring Ronaldinho became the first video to reach one million total views.[25][26] The site launched officially on December 15, 2005, by which time the site was receiving 8 million views a day.[27][28] Clips at the time were limited to 100 megabytes, as little as 30 seconds of footage.[29]
 YouTube was not the first video-sharing site on the Internet; Vimeo was launched in November 2004, though that site remained a side project of its developers from CollegeHumor.[30] The week of YouTube's launch, NBC-Universal's Saturday Night Live ran a skit "Lazy Sunday" by The Lonely Island. Besides helping to bolster ratings and long-term viewership for Saturday Night Live, "Lazy Sunday"'s status as an early viral video helped establish YouTube as an important website.[31] Unofficial uploads of the skit to YouTube drew in more than five million collective views by February 2006 before they were removed when NBCUniversal requested it two months later based on copyright concerns.[32] Despite eventually being taken down, these duplicate uploads of the skit helped popularize YouTube's reach and led to the upload of more third-party content.[33][34] The site grew rapidly; in July 2006, the company announced that more than 65,000 new videos were being uploaded every day and that the site was receiving 100 million video views per day.[35]
 The choice of the name www.youtube.com led to problems for a similarly named website, www.utube.com. That site's owner, Universal Tube & Rollform Equipment, filed a lawsuit against YouTube in November 2006, after being regularly overloaded by people looking for YouTube. Universal Tube subsequently changed its website to www.utubeonline.com.[36][37]
 On October 9, 2006, Google announced that they had acquired YouTube for $1.65 billion in Google stock.[38][39] The deal was finalized on November 13, 2006.[40][41] Google's acquisition launched newfound interest in video-sharing sites; IAC, which now owned Vimeo, focused on supporting the content creators to distinguish itself from YouTube.[30] It is at this time YouTube issued the slogan "Broadcast Yourself".
The company experienced rapid growth. The Daily Telegraph wrote that in 2007, YouTube consumed as much bandwidth as the entire Internet in 2000.[42] By 2010, the company had reached a market share of around 43% and more than 14 billion views of videos, according to comScore.[43] That year, the company simplified its interface to increase the time users would spend on the site.[44] In 2011, more than three billion videos were being watched each day with 48 hours of new videos uploaded every minute.[45][46][47] However, most of these views came from a relatively small number of videos; according to a software engineer at that time, 30% of videos accounted for 99% of views on the site.[48] That year, the company again changed its interface and at the same time, introduced a new logo with a darker shade of red.[49][50] A subsequent interface change, designed to unify the experience across desktop, TV, and mobile, was rolled out in 2013.[51] By that point, more than 100 hours were being uploaded every minute, increasing to 300 hours by November 2014.[52][53]
 During this time, the company also went through some organizational changes. In October 2006, YouTube moved to a new office in San Bruno, California.[54] Hurley announced that he would be stepping down as chief executive officer of YouTube to take an advisory role and that Salar Kamangar would take over as head of the company in October 2010.[55]
 In December 2009, YouTube partnered with Vevo.[56] In April 2010, Lady Gaga's "Bad Romance" became the most viewed video, becoming the first video to reach 200 million views on May 9, 2010.[57]
 Susan Wojcicki was appointed CEO of YouTube in February 2014.[58] In January 2016, YouTube expanded its headquarters in San Bruno by purchasing an office park for $215 million. The complex has 51,468 square metres (554,000 square feet) of space and can house up to 2,800 employees.[59] YouTube officially launched the "polymer" redesign of its user interfaces based on Material Design language as its default, as well a redesigned logo that is built around the service's play button emblem in August 2017.[60]
 Through this period, YouTube tried several new ways to generate revenue beyond advertisements. In 2013, YouTube launched a pilot program for content providers to offer premium, subscription-based channels.[61][62] This effort was discontinued in January 2018 and relaunched in June, with US$4.99 channel subscriptions.[63][64] These channel subscriptions complemented the existing Super Chat ability, launched in 2017, which allows viewers to donate between $1 and $500 to have their comment highlighted.[65] In 2014, YouTube announced a subscription service known as "Music Key", which bundled ad-free streaming of music content on YouTube with the existing Google Play Music service.[66] The service continued to evolve in 2015 when YouTube announced YouTube Red, a new premium service that would offer ad-free access to all content on the platform (succeeding the Music Key service released the previous year), premium original series, and films produced by YouTube personalities, as well as background playback of content on mobile devices. YouTube also released YouTube Music, a third app oriented towards streaming and discovering the music content hosted on the YouTube platform.[67][68][69]
 The company also attempted to create products appealing to specific viewers. YouTube released a mobile app known as YouTube Kids in 2015, designed to provide an experience optimized for children. It features a simplified user interface, curated selections of channels featuring age-appropriate content, and parental control features.[70] Also in 2015, YouTube launched YouTube Gaming—a video gaming-oriented vertical and app for videos and live streaming, intended to compete with the Amazon.com-owned Twitch.[71]
 The company was attacked on April 3, 2018, when a shooting occurred at YouTube's headquarters in San Bruno, California, which wounded four and resulted in the death of the shooter.[72]
 By February 2017, one billion hours of YouTube videos were being watched every day, and 400 hours worth of videos were uploaded every minute.[8][73] Two years later, the uploads had risen to more than 500 hours per minute.[9] During the COVID-19 pandemic, when most of the world was under stay-at-home orders, usage of services like YouTube significantly increased. One data firm[which?] estimated that YouTube was accounting for 15% of all internet traffic, twice its pre-pandemic level.[74] In response to EU officials requesting that such services reduce bandwidth as to make sure medical entities had sufficient bandwidth to share information, YouTube and Netflix stated they would reduce streaming quality for at least thirty days as to cut bandwidth use of their services by 25% to comply with the EU's request.[75] YouTube later announced that they would continue with this move worldwide: "We continue to work closely with governments and network operators around the globe to do our part to minimize stress on the system during this unprecedented situation."[76]
 Following a 2018 complaint alleging violations of the Children's Online Privacy Protection Act (COPPA),[77] the company was fined $170 million by the FTC for collecting personal information from minors under the age of 13.[78] YouTube was also ordered to create systems to increase children's privacy.[79][80] Following criticisms of its implementation of those systems, YouTube started treating all videos designated as "made for kids" as liable under COPPA on January 6, 2020.[81][82] Joining the YouTube Kids app, the company created a supervised mode, designed more for tweens, in 2021.[83] Additionally, to compete with TikTok, YouTube released YouTube Shorts, a short-form video platform.
 During this period, YouTube entered disputes with other tech companies. For over a year, in 2018 and 2019, no YouTube app was available for Amazon Fire products.[84] In 2020, Roku removed the YouTube TV app from its streaming store after the two companies were unable to reach an agreement.[85]
 After testing earlier in 2021, YouTube removed public display of dislike counts on videos in November 2021, claiming the reason for the removal was, based on its internal research, that users often used the dislike feature as a form of cyberbullying and brigading.[86] While some users praised the move as a way to discourage trolls, others felt that hiding dislikes would make it harder for viewers to recognize clickbait or unhelpful videos and that other features already existed for creators to limit bullying. YouTube co-founder Jawed Karim referred to the update as "a stupid idea", and that the real reason behind the change was "not a good one, and not one that will be publicly disclosed." He felt that users' ability on a social platform to identify harmful content was essential, saying, "The process works, and there's a name for it: the wisdom of the crowds. The process breaks when the platform interferes with it. Then, the platform invariably declines."[87][88][89] Shortly after the announcement, software developer Dmitry Selivanov created Return YouTube Dislike, an open-source, third-party browser extension for Chrome and Firefox that allows users to see a video's number of dislikes.[90] In a letter published on January 25, 2022, by then YouTube CEO Susan Wojcicki, acknowledged that removing public dislike counts was a controversial decision, but reiterated that she stands by this decision, claiming that "it reduced dislike attacks."[91]
 In 2022, YouTube launched an experiment where the company would show users who watched longer videos on TVs a long chain of short un-skippable adverts, intending to consolidate all ads into the beginning of a video. Following public outrage over the unprecedented amount of un-skippable ads, YouTube "ended" the experiment on September 19 of that year.[92] In October, YouTube announced that they would be rolling out customizable user handles (e.g. @MrBeast6000) in addition to channel names, which would also become channel URLs.[93]
 On February 16, 2023, Wojcicki announced that she would step down as CEO, with Neal Mohan named as her successor. Wojcicki will take on an advisory role for Google and parent company Alphabet.[94]
 In late October 2023, YouTube began cracking down on the use of ad blockers on the platform. Users of ad blockers may be given a pop-up warning saying "Video player will be blocked after 3 videos". Users of ad blockers are shown a message asking them to allow ads or inviting them to subscribe to the ad-free YouTube Premium subscription plan. YouTube says that the use of ad blockers violates its terms of service.[95][96]
 YouTube has been led by a CEO since its founding in 2005, beginning with Chad Hurley, who led the company until 2010. After Google's acquisition of YouTube, the CEO role was retained. Salar Kamangar took over Hurley's position and held the job until 2014. He was replaced by Susan Wojcicki, who later resigned in 2023.[94] The current CEO is Neal Mohan, who was appointed on February 16, 2023.[94]
 YouTube primarily uses the VP9 and H.264/MPEG-4 AVC video codecs, and the Dynamic Adaptive Streaming over HTTP protocol.[97] MPEG-4 Part 2 streams contained within 3GP containers are also provided for low bandwidth connections.[98] By January 2019, YouTube had begun rolling out videos in AV1 format.[99] In 2021 it was reported that the company was considering requiring AV1 in streaming hardware in order to decrease bandwidth and increase quality.[100] Video is usually streamed alongside the Opus and AAC audio codecs.[98]
 At launch in 2005, viewing YouTube videos on a personal computer required the Adobe Flash Player plug-in to be installed in the browser.[101] In January 2010, YouTube launched an experimental version of the site that used the built-in multimedia capabilities of Web browsers supporting the HTML5 standard.[102] This allowed videos to be viewed without requiring Adobe Flash Player or any other plug-in to be installed.[103] On January 27, 2015, YouTube announced that HTML5 would be the default playback method on supported browsers.[102] HTML5 video streams use Dynamic Adaptive Streaming over HTTP (MPEG-DASH), an HTTP-based adaptive bit-rate streaming solution optimizes the bitrate and quality for the available network.[104]
 The platform can serve videos at optionally lower resolution levels starting at 144p for smoothening playback in areas and countries with limited Internet speeds, improving compatibility, as well as for the preservation of limited cellular data plans. The resolution can be adjusted automatically based on detected connection speed or set manually.[105][106]
 From 2008 to 2017, users could add "annotations" to their videos, such as pop-up text messages and hyperlinks, which allowed for interactive videos. By 2019, all annotations had been removed from videos, breaking some videos that depended on the feature. YouTube introduced standardized widgets intended to replace annotations in a cross-platform manner, including "end screens" (a customizable array of thumbnails for specified videos displayed near the end of the video).[107][108][109]
 In 2018, YouTube became an International Standard Name Identifier (ISNI) registry, and announced its intention to begin creating ISNI identifiers to uniquely identify the musicians whose videos it features.[110]
 Users can verify their account, normally through a mobile phone, to gain the ability to upload videos up to 12 hours in length, as well as produce live streams.[111][112] Users who have built sufficient channel history and have a good track record of complying with the site's Community Guidelines will also gain access to these aforementioned features as well.[113] When YouTube was launched in 2005, it was possible to upload longer videos, but a 10-minute limit was introduced in March 2006 after YouTube found that the majority of videos exceeding this length were unauthorized uploads of television shows and films.[114] The 10-minute limit was increased to 15 minutes in July 2010.[115] Videos can be at most 256 GB in size or 12 hours, whichever is less.[111] As of 2021[update], automatic closed captions using speech recognition technology when a video is uploaded are available in 13 languages, and can be machine-translated during playback.[116]
 YouTube also offers manual closed captioning as part of its creator studio.[117] YouTube formerly offered a 'Community Captions' feature, where viewers could write and submit captions for public display upon approval by the video uploader, but this was deprecated in September 2020.[118]
 YouTube accepts the most common container formats, including MP4, Matroska, FLV, AVI, WebM, 3GP, MPEG-PS, and the QuickTime File Format. Some intermediate video formats (i.e., primarily used for professional video editing, not for final delivery or storage) are also accepted, such as ProRes.[119] YouTube provides recommended encoding settings.[120]
 Each video is identified by an eleven-character case-sensitive alphanumerical Base64 string in the Uniform Resource Locator (URL) which can contain letters, digits, an underscore (_), and a dash (-).[121]
 In 2018, YouTube added a feature called Premiere which displays a notification to the user mentioning when the video will be available for the first time, like for a live stream but with a prerecorded video. When the scheduled time arrives, the video is aired as a live broadcast with a two-minute countdown. Optionally, a premiere can be initiated immediately.[122]
 YouTube originally offered videos at only one quality level, displayed at a resolution of 320×240 pixels using the Sorenson Spark codec (a variant of H.263),[123][124] with mono MP3 audio.[125] In June 2007, YouTube added an option to watch videos in 3GP format on mobile phones.[126] In March 2008, a high-quality mode was added, which increased the resolution to 480×360 pixels.[127] In December 2008, 720p HD support was added. At the time of the 720p launch, the YouTube player was changed from a 4:3 aspect ratio to a widescreen 16:9.[128] With this new feature, YouTube began a switchover to H.264/MPEG-4 AVC as its default video compression format. In November 2009, 1080p HD support was added. In July 2010, YouTube announced that it had launched a range of videos in 4K format, which allows a resolution of up to 4096×3072 pixels.[129][130] In July 2010, support for 2160p UHD was added, with the videos playing at 3840 × 2160 pixels.[131] In June 2014, YouTube began to deploy support for high frame rate videos up to 60 frames per second (as opposed to 30 before), becoming available for user uploads in October. YouTube stated that this would enhance "motion-intensive" videos, such as video game footage.[132][133][134][135] In June 2015, support for 8K resolution was added, with the videos playing at 7680×4320 pixels.[136] In November 2016, support for HDR video was added which can be encoded with hybrid log–gamma (HLG) or perceptual quantizer (PQ).[137] HDR video can be encoded with the Rec. 2020 color space.[138]
 YouTube videos are available in a range of quality levels. Viewers only indirectly influence the video quality. In the mobile apps, users choose between "Auto", which adjusts resolution based on the internet connection, "High Picture Quality" which will prioritize playing high-quality video, "Data saver" which will sacrifice video quality in favor of low data usage and "Advanced" which lets the user choose a stream resolution.[139] On desktop, users choose between "Auto" and a specific resolution.[140] It is not possible for the viewer to directly choose a higher bitrate (quality) for any selected resolution.
 Since 2009, viewers have had the ability to watch 3D videos.[141] In 2015, YouTube began natively supporting 360-degree video. Since April 2016, it allowed live streaming 360° video, and both normal and 360° video at up to 1440p, and since November 2016 both at up to 4K (2160p) resolution.[142][143][144] Citing the limited number of users who watched more than 90-degrees, it began supporting an alternative stereoscopic video format known as VR180 which it said was easier to produce,[145] which allows users to watch any video using virtual reality headsets.[146]
 In response to increased viewership during the COVID-19 pandemic, YouTube temporarily downgraded the quality of its videos.[147][148] YouTube developed its own chip, called Argos, to help with encoding higher resolution videos in 2021.[149]
 In April 2023, YouTube began offering some videos in an enhanced bitrate "1080p Premium" option for YouTube Premium subscribers on iOS.[150] In August 2023, the feature became available to subscribers on desktop platforms.[151]
 In certain cases, YouTube allows the uploader to upgrade the quality of videos uploaded a long time ago in poor quality. One such partnership with Universal Music Group included remasters of 1,000 music videos.[152]
 YouTube carried out early experiments with live streaming, including its YouTube Live event in 2008,[153] a concert by U2 in 2009, and a question-and-answer session with US President Barack Obama in February 2010.[154] These tests had relied on technology from 3rd-party partners, but in September 2010, YouTube began testing its own live streaming infrastructure.[155] In April 2011, YouTube announced the rollout of YouTube Live. The creation of live streams was initially limited to select partners.[156] It was used for real-time broadcasting of events such as the 2012 Olympics in London.[157] In October 2012, more than 8 million people watched Felix Baumgartner's jump from the edge of space as a live stream on YouTube.[158]
 In May 2013, creation of live streams was opened to verified users with at least 1,000 subscribers; in August of the same year the number was reduced to 100 subscribers,[159] and in December the limit was removed.[160] In February 2017, live streaming was introduced to the official YouTube mobile app. Live streaming via mobile was initially restricted to users with at least 10,000 subscribers,[161] but as of mid-2017 it has been reduced to 100 subscribers.[162] Live streams support HDR, can be up to 4K resolution at 60 fps, and also support 360° video.[143][163]
 Most videos enable users to leave comments, which have attracted attention for the negative aspects of their form and content.[specify] In 2006, Time praised Web 2.0 for enabling "community and collaboration on a scale never seen before", and added that YouTube "harnesses the stupidity of crowds as well as its wisdom. Some of the comments on YouTube make you weep for the future of humanity just for the spelling alone, never mind the obscenity and the naked hatred".[164] The Guardian in 2009 described users' comments on YouTube as:[165]
 Juvenile, aggressive, misspelt, sexist, homophobic, swinging from raging at the contents of a video to providing a pointlessly detailed description followed by a LOL, YouTube comments are a hotbed of infantile debate and unashamed ignorance—with the occasional burst of wit shining through. The Daily Telegraph commented in September 2008, that YouTube was "notorious" for "some of the most confrontational and ill-formed comment exchanges on the internet", and reported on YouTube Comment Snob, "a new piece of software that blocks rude and illiterate posts".[166] The Huffington Post noted in April 2012 that finding comments on YouTube that appear "offensive, stupid and crass" to the "vast majority" of the people is hardly difficult.[167]
 Google subsequently implemented a comment system oriented on Google+ on November 6, 2013, that required all YouTube users to use a Google+ account to comment on videos. The stated motivation for the change was giving creators more power to moderate and block comments, thereby addressing frequent criticisms of their quality and tone.[168] The new system restored the ability to include URLs in comments, which had previously been removed due to problems with abuse.[169][170] In response, YouTube co-founder Jawed Karim posted the question "why the fuck do I need a google+ account to comment on a video?" on his YouTube channel to express his negative opinion of the change.[171] The official YouTube announcement[172] received 20,097 "thumbs down" votes and generated more than 32,000 comments in two days.[173] Writing in the Newsday blog Silicon Island, Chase Melvin noted that "Google+ is nowhere near as popular a social media network like Facebook, but it's essentially being forced upon millions of YouTube users who don't want to lose their ability to comment on videos" and added that "Discussion forums across the Internet are already bursting with the outcry against the new comment system". In the same article Melvin goes on to say:[174]
 Perhaps user complaints are justified, but the idea of revamping the old system isn't so bad.
Think of the crude, misogynistic and racially-charged mudslinging that has transpired over the last eight years on YouTube without any discernible moderation. Isn't any attempt to curb unidentified libelers worth a shot? The system is far from perfect, but Google should be lauded for trying to alleviate some of the damage caused by irate YouTubers hiding behind animosity and anonymity. Later, on July 27, 2015, Google announced in a blog post that it would be removing the requirement to sign up to a Google+ account to post comments to YouTube.[175] On November 3, 2016, YouTube announced a trial scheme which allows the creators of videos to decide whether to approve, hide or report the comments posted on videos based on an algorithm that detects potentially offensive comments.[176] Creators may also choose to keep or delete comments with links or hashtags in order to combat spam. They can also allow other users to moderate their comments.[177]
 In December 2020, it was reported that YouTube would launch a new feature that will warn users who post a comment that "may be offensive to others."[178][179]
 On September 13, 2016, YouTube launched a public beta of Community, a social media-based feature that allows users to post text, images (including GIFs), live videos and others in a separate "Community" tab on their channel.[180] Prior to the release, several creators had been consulted to suggest tools Community could incorporate that they would find useful; these YouTubers included Vlogbrothers, AsapScience, Lilly Singh, The Game Theorists, Karmin, The Key of Awesome, The Kloons, Peter Hollens, Rosianna Halse Rojas, Sam Tsui, Threadbanger and Vsauce3.[181][non-primary source needed]
 After the feature has been officially released, the community post feature gets activated automatically for every channel that passes a specific threshold of subscriber counts or already has more subscribers. This threshold was lowered over time,[when?] from 10,000 subscribers  to 1500 subscribers, to 1000 subscribers,[182][non-primary source needed] to 500 subscribers.[183]
 Channels that the community tab becomes enabled for, get their channel discussions (previously known as channel comments) permanently erased, instead of co-existing or migrating.[184][non-primary source needed]
 Experimental features of YouTube could be accessed in an area of the site formerly named TestTube.[185][186]
For example, in October 2009, a comment search feature accessible under /comment_search was implemented as part of this program. The feature was removed later.[187] Later the same year, YouTube Feather was introduced as a "lightweight" alternative website for countries with limited internet speeds.[188] Following the transition to the Polymer layout, TestTube was disabled, and the URL redirects to video playback settings. TestTube was replaced by a new system that requires users to be premium members to enable or disable experiments.
 YouTube offers users the ability to view its videos on web pages outside their website. Each YouTube video is accompanied by a piece of HTML that can be used to embed it on any page on the Web.[189] This functionality is often used to embed YouTube videos in social networking pages and blogs. Users wishing to post a video discussing, inspired by, or related to another user's video can make a "video response". The eleven character YouTube video identifier (64 possible characters used in each position), allows for a theoretical maximum of 6411 or around 73.8 quintillion (73.8 billion billion) unique ids.
 YouTube announced that it would remove video responses for being an underused feature on August 27, 2013.[190] Embedding, rating, commenting and response posting can be disabled by the video owner.[191] YouTube does not usually offer a download link for its videos, and intends for them to be viewed through its website interface.[192] A small number of videos can be downloaded as MP4 files.[193] Numerous third-party web sites, applications and browser plug-ins allow users to download YouTube videos.[194]
 In February 2009, YouTube announced a test service, allowing some partners to offer video downloads for free or for a fee paid through Google Checkout.[195] In June 2012, Google sent cease and desist letters threatening legal action against several websites offering online download and conversion of YouTube videos.[196] In response, Zamzar removed the ability to download YouTube videos from its site.[197] Users retain copyright of their own work under the default Standard YouTube License,[198] but have the option to grant certain usage rights under any public copyright license they choose.
 Since July 2012, it has been possible to select a Creative Commons attribution license as the default, allowing other users to reuse and remix the material.[199]
 Most modern smartphones are capable of accessing YouTube videos, either within an application or through an optimized website. YouTube Mobile was launched in June 2007, using RTSP streaming for the video.[200] Not all of YouTube's videos are available on the mobile version of the site.[201]
 Since June 2007, YouTube's videos have been available for viewing on a range of Apple products. This required YouTube's content to be transcoded into Apple's preferred video standard, H.264, a process that took several months. YouTube videos can be viewed on devices including Apple TV, iPod Touch and the iPhone.[202]
 The mobile version of the site was relaunched based on HTML5 in July 2010, avoiding the need to use Adobe Flash Player and optimized for use with touch screen controls.[203] The mobile version is also available as an app for the Android platform.[204][205]
 In September 2012, YouTube launched its first app for the iPhone, following the decision to drop YouTube as one of the preloaded apps in the iPhone 5 and iOS 6 operating system.[206] According to GlobalWebIndex, YouTube was used by 35% of smartphone users between April and June 2013, making it the third-most used app.[207]
 A TiVo service update in July 2008 allowed the system to search and play YouTube videos.[208]
 In January 2009, YouTube launched "YouTube for TV", a version of the website tailored for set-top boxes and other TV-based media devices with web browsers, initially allowing its videos to be viewed on the PlayStation 3 and Wii video game consoles.[209][210]
 During the month of June that same year, YouTube XL was introduced, which has a simplified interface designed for viewing on a standard television screen.[211] YouTube is also available as an app on Xbox Live.[212]
 On November 15, 2012, Google launched an official app for the Wii, allowing users to watch YouTube videos from the Wii channel.[213] An app was available for Wii U and Nintendo 3DS, but was discontinued in August 2019.[214] Videos can also be viewed on the Wii U Internet Browser using HTML5.[215][non-primary source needed] Google made YouTube available on the Roku player on December 17, 2013,[216] and, in October 2014, the Sony PlayStation 4.[217]
 YouTube launched as a downloadable app for the Nintendo Switch in November 2018.[218]
 In early years of operation, Google faced some criticism for 'encouraging the dominance of US values', by prioritising English over other languages. On June 19, 2007, at a conference in Paris, Google CEO Eric Schmidt launched YouTube localization, with stated aims including customizing the YouTube experience by country, including country-specific comments, metrics, and video rankings. From 2007, YouTube's localization was rolled out.[219][220]
 A 2015 report on YouTube's localization showed it to be continuing, and expanding.[221] In February 2023, YouTube made it possible to upload a single video in multiple languages. Prior to 2023, the only option for YouTubers to broaden their content's reach to audiences speaking different languages was to launch an entirely separate secondary channel for each language and upload dubbed versions of their videos across all those channels. MrBeast called multi-language dub tracks a “giant win” for creators. With the introduction of the dubbing localization option, many creators switched from uploading to separate channels to uploading to their main channel with dubbed versions.[222]
 As of 2024, the interface of YouTube is available with localized versions in 104 countries, one territory (Hong Kong) and a worldwide version[223] and continues to extend the availability of its localized version to additional countries and regions.[224]
 If YouTube is unable to identify a specific country or region according to the IP address, the default location is the United States. However, YouTube offers language and content preferences for all accessible countries, regions, and languages.[225]
 The YouTube interface suggests which local version should be chosen based on the IP address of the user. In some cases, the message "This video is not available in your country" may appear because of copyright restrictions or inappropriate content.[274] The interface of the YouTube website is available in 76 language versions, including Amharic, Albanian, Armenian, Burmese, Haitian Creole, Kyrgyz, Malagasy, Mongolian, Persian, Samoan, Somali and Uzbek, which do not have local channel versions. Access to YouTube was blocked in Turkey between 2008 and 2010, following controversy over the posting of videos deemed insulting to Mustafa Kemal Atatürk and some material offensive to Muslims.[275][276] In October 2012, a local version of YouTube was launched in Turkey, with the domain youtube.com.tr. The local version is subject to the content regulations found in Turkish law.[277] In March 2009, a dispute between YouTube and the British royalty collection agency PRS for Music led to premium music videos being blocked for YouTube users in the United Kingdom. The removal of videos posted by the major record companies occurred after failure to reach an agreement on a licensing deal. The dispute was resolved in September 2009.[278] In April 2009, a similar dispute led to the removal of premium music videos for users in Germany.[279]
 In January 2012, it was estimated that visitors to YouTube spent an average of 15 minutes a day on the site, in contrast to the four or five hours a day spent by a typical US citizen watching television.[280] In 2017, viewers on average watched YouTube on mobile devices for more than an hour every day.[281]
 In December 2012, two billion views were removed from the view counts of Universal and Sony music videos on YouTube, prompting a claim by The Daily Dot that the views had been deleted due to a violation of the site's terms of service, which ban the use of automated processes to inflate view counts. This was disputed by Billboard, which said that the two billion views had been moved to Vevo, since the videos were no longer active on YouTube.[282][283] On August 5, 2015, YouTube patched the formerly notorious behavior which caused a video's view count to freeze at "301" (later "301+") until the actual count was verified to prevent view count fraud.[284] YouTube view counts once again updated in real time.[285]
 Since September 2019, subscriber counts are abbreviated. Only three leading digits of channels' subscriber counts are indicated publicly, compromising the function of third-party real-time indicators such as that of Social Blade. Exact counts remain available to channel operators inside YouTube Studio.[286]
 On November 11, 2021, after testing out this change in March of the same year, YouTube announced it would start hiding dislike counts on videos, making them invisible to viewers. The company stated the decision was in response to experiments which confirmed that smaller YouTube creators were more likely to be targeted in dislike brigading and harassment. Creators will still be able to see the number of likes and dislikes in the YouTube Studio dashboard tool, according to YouTube.[287][288][289]
 YouTube has faced numerous challenges and criticisms in its attempts to deal with copyright, including the site's first viral video, Lazy Sunday, which had to be taken down, due to copyright concerns.[31] At the time of uploading a video, YouTube users are shown a message asking them not to violate copyright laws.[290] Despite this advice, many unauthorized clips of copyrighted material remain on YouTube. YouTube does not view videos before they are posted online, and it is left to copyright holders to issue a DMCA takedown notice pursuant to the terms of the Online Copyright Infringement Liability Limitation Act. Any successful complaint about copyright infringement results in a YouTube copyright strike. Three successful complaints for copyright infringement against a user account will result in the account and all of its uploaded videos being deleted.[291][292] From 2007 to 2009 organizations including Viacom, Mediaset, and the English Premier League have filed lawsuits against YouTube, claiming that it has done too little to prevent the uploading of copyrighted material.[293][294][295]
 In August 2008, a US court ruled in Lenz v. Universal Music Corp. that copyright holders cannot order the removal of an online file without first determining whether the posting reflected fair use of the material.[296] YouTube's owner Google announced in November 2015 that they would help cover the legal cost in select cases where they believe fair use defenses apply.[297]
 In the 2011 case of Smith v. Summit Entertainment LLC, professional singer Matt Smith sued Summit Entertainment for the wrongful use of copyright takedown notices on YouTube.[298] He asserted seven causes of action, and four were ruled in Smith's favor.[299] In April 2012, a court in Hamburg ruled that YouTube could be held responsible for copyrighted material posted by its users.[300] On November 1, 2016, the dispute with GEMA was resolved, with Google content ID being used to allow advertisements to be added to videos with content protected by GEMA.[301]
 In April 2013, it was reported that Universal Music Group and YouTube have a contractual agreement that prevents content blocked on YouTube by a request from UMG from being restored, even if the uploader of the video files a DMCA counter-notice.[302][303] As part of YouTube Music, Universal and YouTube signed an agreement in 2017, which was followed by separate agreements other major labels, which gave the company the right to advertising revenue when its music was played on YouTube.[304] By 2019, creators were having videos taken down or demonetized when Content ID identified even short segments of copyrighted music within a much longer video, with different levels of enforcement depending on the record label.[305] Experts noted that some of these clips said qualified for fair use.[305]
 In June 2007, YouTube began trials of a system for automatic detection of uploaded videos that infringe copyright. Google CEO Eric Schmidt regarded this system as necessary for resolving lawsuits such as the one from Viacom, which alleged that YouTube profited from content that it did not have the right to distribute.[306] The system, which was initially called "Video Identification"[307][308] and later became known as Content ID,[309] creates an ID File for copyrighted audio and video material, and stores it in a database. When a video is uploaded, it is checked against the database, and flags the video as a copyright violation if a match is found.[310] When this occurs, the content owner has the choice of blocking the video to make it unviewable, tracking the viewing statistics of the video, or adding advertisements to the video.
 An independent test in 2009 uploaded multiple versions of the same song to YouTube and concluded that while the system was "surprisingly resilient" in finding copyright violations in the audio tracks of videos, it was not infallible.[311] The use of Content ID to remove material automatically has led to controversy in some cases, as the videos have not been checked by a human for fair use.[312] If a YouTube user disagrees with a decision by Content ID, it is possible to fill in a form disputing the decision.[313]
 Before 2016, videos were not monetized until the dispute was resolved. Since April 2016, videos continue to be monetized while the dispute is in progress, and the money goes to whoever won the dispute.[314] Should the uploader want to monetize the video again, they may remove the disputed audio in the "Video Manager".[315] YouTube has cited the effectiveness of Content ID as one of the reasons why the site's rules were modified in December 2010 to allow some users to upload videos of unlimited length.[316]
 YouTube has a set of community guidelines aimed to reduce abuse of the site's features. The uploading of videos containing defamation, pornography, and material encouraging criminal conduct is forbidden by YouTube's "Community Guidelines".[317][better source needed] Generally prohibited material includes sexually explicit content, videos of animal abuse, shock videos, content uploaded without the copyright holder's consent, hate speech, spam, and predatory behavior.[317] YouTube relies on its users to flag the content of videos as inappropriate, and a YouTube employee will view a flagged video to determine whether it violates the site's guidelines.[317] Despite the guidelines, YouTube has faced criticism over aspects of its operations,[318] its recommendation algorithms perpetuating videos that promote conspiracy theories and falsehoods,[319] hosting videos ostensibly targeting children but containing violent or sexually suggestive content involving popular characters,[320] videos of minors attracting pedophilic activities in their comment sections,[321] and fluctuating policies on the types of content that is eligible to be monetized with advertising.[318]
 YouTube contracts companies to hire content moderators, who view content flagged as potentially violating YouTube's content policies and determines if they should be removed. In September 2020, a class-action suit was filed by a former content moderator who reported developing post-traumatic stress disorder (PTSD) after an 18-month period on the job. The former content moderator said that she was regularly made to exceed YouTube's stated limit of four hours per day of viewing graphic content. The lawsuit alleges that YouTube's contractors gave little to no training or support for its moderators' mental health, made prospective employees sign NDAs before showing them any examples of content they would see while reviewing, and censored all mention of trauma from its internal forums. It also purports that requests for extremely graphic content to be blurred, reduced in size or made monochrome, per recommendations from the National Center for Missing and Exploited Children, were rejected by YouTube as not a high priority for the company.[322][323][324]
 To limit the spread of misinformation and fake news via YouTube, it has rolled out a comprehensive policy regarding how it plans to deal with technically manipulated videos.[325]
 Controversial content has included material relating to Holocaust denial and the Hillsborough disaster, in which 96 football fans from Liverpool were crushed to death in 1989.[326][327] In July 2008, the Culture and Media Committee of the House of Commons of the United Kingdom stated that it was "unimpressed" with YouTube's system for policing its videos, and argued that "proactive review of content should be standard practice for sites hosting user-generated content". YouTube responded by stating:
 We have strict rules on what's allowed, and a system that enables anyone who sees inappropriate content to report it to our 24/7 review team and have it dealt with promptly. We educate our community on the rules and include a direct link from every YouTube page to make this process as easy as possible for our users. Given the volume of content uploaded on our site, we think this is by far the most effective way to make sure that the tiny minority of videos that break the rules come down quickly.[328] (July 2008) In October 2010, U.S. Congressman Anthony Weiner urged YouTube to remove from its website videos of imam Anwar al-Awlaki.[329] YouTube pulled some of the videos in November 2010, stating they violated the site's guidelines.[330] In December 2010, YouTube added the ability to flag videos for containing terrorism content.[331]
 In 2018, YouTube introduced a system that would automatically add information boxes to videos that its algorithms determined may present conspiracy theories and other fake news, filling the infobox with content from Encyclopædia Britannica and Wikipedia as a means to inform users to minimize misinformation propagation without impacting freedom of speech.[332][333] In the wake of the Notre-Dame fire on April 15, 2019, several user-uploaded videos of the landmark fire were flagged by YouTube' system automatically with an Encyclopædia Britannica article on the false conspiracy theories around the September 11 attacks. Several users complained to YouTube about this inappropriate connection. YouTube officials apologized for this, stating that their algorithms had misidentified the fire videos and added the information block automatically, and were taking steps to remedy this.[334]
 On April 18, 2023, YouTube revealed its changes in handling content associated with eating disorders. This social media platform's Community Guidelines now prohibit content that could encourage emulation from at-risk users. This content includes behavior that shows severe calorie tracking and purging after eating. However, videos featuring positive behavior such as in the context of recovery will be permitted on the platform under two conditions—the user must have a registered (logged-in) account and must be older than 18.
This policy was created in collaboration with nonprofit organizations as well as the National Eating Disorder Association. Garth Graham, YouTube's Global Head of Healthcare revealed in an interview with CNN that this policy change was geared at ensuring that this video-sharing platform provides an avenue for "community recovery and resources" while ensuring continued viewer protection.[335]
 Five leading content creators whose channels were based on LGBTQ+ materials filed a federal lawsuit against YouTube in August 2019, alleging that YouTube's algorithms divert discovery away from their channels, impacting their revenue. The plaintiffs claimed that the algorithms discourage content with words like "lesbian" or "gay", which would be predominant in their channels' content, and because of YouTube's near-monopolization of online video services, they are abusing that position.[336]
 In June 2022, Media Matters, a media watchdog group, reported that homophobic and transphobic content calling LGBT people "predators" and "groomers" was becoming more common on YouTube.[337] The report also referred to common accusations in YouTube videos that LGBT people are mentally ill.[337] The report stated the content appeared to be in violation of YouTube's hate speech policy.[337]
 From 2020 on, the issue of videos featuring animal cruelty on YouTube has started to attract increasing attention in the media. In late 2020, animal welfare charity Lady Freethinker identified 2,053 videos on YouTube in which they stated animals were "deliberately harmed for entertainment or were shown to be under severe psychological distress, physical pain or dead."[338]
 In 2021, Lady Freethinker filed a lawsuit accusing YouTube of a breach of contract in allowing a large number of videos on its site showing animal abuse and failing to remove them when notified. YouTube responded by stating that they had "expanded its policy on animal abuse videos" in 2021, and since the introduction of the new policy "removed hundreds of thousands of videos and terminated thousands of channels for violations."[339]
 In 2022, Google defeated the Lady Freethinker lawsuit, with a judge ruling that YouTube was protected by Section 230 of the Communications Decency Act, that shields internet platforms from lawsuits based on content posted by their users.[340]
 In 2023, YouTube stated that animal abuse "has no place on their platforms, and they are working to remove content (of that nature)".[341][342][343][344][345][346]
 YouTube has been criticized for using an algorithm that gives great prominence to videos that promote conspiracy theories, falsehoods and incendiary fringe discourse.[347][348][349] According to an investigation by The Wall Street Journal, "YouTube's recommendations often lead users to channels that feature conspiracy theories, partisan viewpoints and misleading videos, even when those users haven't shown interest in such content. When users show a political bias in what they choose to view, YouTube typically recommends videos that echo those biases, often with more-extreme viewpoints."[347][350] When users search for political or scientific terms, YouTube's search algorithms often give prominence to hoaxes and conspiracy theories.[349][351] After YouTube drew controversy for giving top billing to videos promoting falsehoods and conspiracy when people made breaking-news queries during the 2017 Las Vegas shooting, YouTube changed its algorithm to give greater prominence to mainstream media sources.[347][352][353][354] In 2018, it was reported that YouTube was again promoting fringe content about breaking news, giving great prominence to conspiracy videos about Anthony Bourdain's death.[355]
 In 2017, it was revealed that advertisements were being placed on extremist videos, including videos by rape apologists, anti-Semites, and hate preachers who received ad payouts.[356] After firms started to stop advertising on YouTube in the wake of this reporting, YouTube apologized and said that it would give firms greater control over where ads got placed.[356]
 Alex Jones, known for far-right conspiracy theories, had built a massive audience on YouTube.[357] YouTube drew criticism in 2018 when it removed a video from Media Matters compiling offensive statements made by Jones, stating that it violated its policies on "harassment and bullying".[358] On August 6, 2018, however, YouTube removed Alex Jones' YouTube page following a content violation.[359]
 University of North Carolina professor Zeynep Tufekci has referred to YouTube as "The Great Radicalizer", saying "YouTube may be one of the most powerful radicalizing instruments of the 21st century."[360] Jonathan Albright of the Tow Center for Digital Journalism at Columbia University described YouTube as a "conspiracy ecosystem".[349][361]
 In January 2019, YouTube said that it had introduced a new policy starting in the United States intended to stop recommending videos containing "content that could misinform users in harmful ways." YouTube gave flat earth theories, miracle cures, and 9/11 truther-isms as examples.[362] Efforts within YouTube engineering to stop recommending borderline extremist videos falling just short of forbidden hate speech, and track their popularity were originally rejected because they could interfere with viewer engagement.[363]
 In January 2019, the site announced it would be implementing measures directed towards "raising authoritative content and reducing borderline content and harmful misinformation."[364] That June, YouTube announced it would be banning Holocaust denial and neo-Nazi content.[364] YouTube has blocked the neo-Nazi propaganda film Europa: The Last Battle from being uploaded.[365]
 Multiple research studies have investigated cases of misinformation in YouTube. In a July 2019 study based on ten YouTube searches using the Tor Browser related to climate and climate change, the majority of videos were videos that communicated views contrary to the scientific consensus on climate change.[366] A May 2023 study found that YouTube was monetizing and profiting from videos that included misinformation about climate change.[367] A 2019 BBC investigation of YouTube searches in ten different languages found that YouTube's algorithm promoted health misinformation, including fake cancer cures.[368] In Brazil, YouTube has been linked to pushing pseudoscientific misinformation on health matters, as well as elevated far-right fringe discourse and conspiracy theories.[369] In the Philippines, numerous channels disseminated misinformation related to the 2022 Philippine elections.[370] Additionally, research on the dissemination of Flat Earth beliefs in social media, has shown that networks of YouTube channels form an echo chamber that polarizes audiences by appearing to confirm preexisting beliefs.[371]
 Before 2019, YouTube took steps to remove specific videos or channels related to supremacist content that had violated its acceptable use policies but otherwise did not have site-wide policies against hate speech.[372]
 In the wake of the March 2019 Christchurch mosque attacks, YouTube and other sites like Facebook and Twitter that allowed user-submitted content drew criticism for doing little to moderate and control the spread of hate speech, which was considered to be a factor in the rationale for the attacks.[373][374] These platforms were pressured to remove such content, but in an interview with The New York Times, YouTube's chief product officer Neal Mohan said that unlike content such as ISIS videos which take a particular format and thus easy to detect through computer-aided algorithms, general hate speech was more difficult to recognize and handle, and thus could not readily take action to remove without human interaction.[375]
 YouTube joined an initiative led by France and New Zealand with other countries and tech companies in May 2019 to develop tools to be used to block online hate speech and to develop regulations, to be implemented at the national level, to be levied against technology firms that failed to take steps to remove such speech, though the United States declined to participate.[376][377] Subsequently, on June 5, 2019, YouTube announced a major change to its terms of service, "specifically prohibiting videos alleging that a group is superior in order to justify discrimination, segregation or exclusion based on qualities like age, gender, race, caste, religion, sexual orientation or veteran status." YouTube identified specific examples of such videos as those that "promote or glorify Nazi ideology, which is inherently discriminatory". YouTube further stated it would "remove content denying that well-documented violent events, like the Holocaust or the shooting at Sandy Hook Elementary, took place."[372][378]
 In October 2019, YouTube banned the main channel of Red Ice, a white supremacist multimedia company, for hate speech violations. The channel had about 330,000 subscribers. Lana Lokteff and Red Ice promoted a backup channel in an attempt to circumvent the ban.[379][380] A week later, the backup channel was also removed by YouTube.[381][382]
 In June 2020, YouTube banned several channels associated with white supremacy, including those of Stefan Molyneux, David Duke, and Richard B. Spencer, asserting these channels violated their policies on hate speech. The ban occurred the same day that Reddit announced the ban on several hate speech sub-forums including r/The Donald.[383]
 Following the dissemination via YouTube of misinformation related to the COVID-19 pandemic that 5G communications technology was responsible for the spread of coronavirus disease 2019 which led to multiple 5G towers in the United Kingdom being attacked by arsonists, YouTube removed all such videos linking 5G and the coronavirus in this manner.[384]
 In September 2021, YouTube extended this policy to cover videos disseminating misinformation related to any vaccine, including those long approved against measles or Hepatitis B, that had received approval from local health authorities or the World Health Organization.[385][386] The platform proceeded to remove the accounts of anti-vaccine campaigners such as Robert F. Kennedy Jr. and Joseph Mercola.[386]
 YouTube had extended this moderation to non-medical areas. In the weeks following the 2020 United States presidential election, the site added policies to remove or label videos promoting election fraud claims;[387][388] however, it reversed this policy in June 2023, citing that the removal was necessary to "openly debate political ideas, even those that are controversial or based on disproven assumptions".[389][390]
 Google and YouTube implemented policies in October 2021 to deny monetization or revenue to advertisers or content creators that promoted climate change denial, which "includes content referring to climate change as a hoax or a scam, claims denying that long-term trends show the global climate is warming, and claims denying that greenhouse gas emissions or human activity contribute to climate change."[391] In January 2024, the Center for Countering Digital Hate reported that climate change deniers were instead pushing other forms of climate change denial that have not yet been banned by YouTube, including false claims that global warming is "beneficial or harmless", and which undermined climate solutions and climate science.[392][393]
 In July 2022, YouTube announced policies to combat misinformation surrounding abortion, such as videos with instructions to perform abortion methods that are considered unsafe and videos that contain misinformation about the safety of abortion.[394]
 Leading into 2017, there was a significant increase in the number of videos related to children, coupled between the popularity of parents vlogging their family's activities, and previous content creators moving away from content that often was criticized or demonetized into family-friendly material. In 2017, YouTube reported that time watching family vloggers had increased by 90%.[395][396] However, with the increase in videos featuring children, the site began to face several controversies related to child safety. During Q2 2017, the owners of popular channel FamilyOFive, which featured themselves playing "pranks" on their children, were accused of child abuse. Their videos were eventually deleted, and two of their children were removed from their custody.[397][398][399][400] A similar case happened in 2019 when the owner of the channel Fantastic Adventures was accused of abusing her adopted children. Her videos would later be deleted.[401]
 Later that year, YouTube came under criticism for showing inappropriate videos targeted at children and often featuring popular characters in violent, sexual or otherwise disturbing situations, many of which appeared on YouTube Kids and attracted millions of views. The term "Elsagate" was coined on the Internet and then used by various news outlets to refer to this controversy.[402][403][404][405] On November 11, 2017, YouTube announced it was strengthening site security to protect children from unsuitable content. Later that month, the company started to mass delete videos and channels that made improper use of family-friendly characters. As part of a broader concern regarding child safety on YouTube, the wave of deletions also targeted channels that showed children taking part in inappropriate or dangerous activities under the guidance of adults. Most notably, the company removed Toy Freaks, a channel with over 8.5 million subscribers, that featured a father and his two daughters in odd and upsetting situations.[406][407][408][409][410] According to analytics specialist SocialBlade, it earned up to $11.2 million annually prior to its deletion in November 2017.[411]
 Even for content that appears to be aimed at children and appears to contain only child-friendly content, YouTube's system allows for anonymity of who uploads these videos. These questions have been raised in the past, as YouTube has had to remove channels with children's content which, after becoming popular, then suddenly include inappropriate content masked as children's content.[412] Alternative, some of the most-watched children's programming on YouTube comes from channels that have no identifiable owners, raising concerns of intent and purpose. One channel that had been of concern was "Cocomelon" which provided numerous mass-produced animated videos aimed at children. Up through 2019, it had drawn up to US$10 million a month in ad revenue and was one of the largest kid-friendly channels on YouTube before 2020. Ownership of Cocomelon was unclear outside of its ties to "Treasure Studio", itself an unknown entity, raising questions as to the channel's purpose,[412][413][414] but Bloomberg News had been able to confirm and interview the small team of American owners in February 2020 regarding "Cocomelon", who stated their goal for the channel was to simply entertain children, wanting to keep to themselves to avoid attention from outside investors.[415] The anonymity of such channel raise concerns because of the lack of knowledge of what purpose they are trying to serve.[416] The difficulty to identify who operates these channels "adds to the lack of accountability", according to Josh Golin of the Campaign for a Commercial-Free Childhood, and educational consultant Renée Chernow-O'Leary found the videos were designed to entertain with no intent to educate, all leading to critics and parents to be concerned for their children becoming too enraptured by the content from these channels.[412] Content creators that earnestly make child-friendly videos have found it difficult to compete with larger channels, unable to produce content at the same rate as them, and lacking the same means of being promoted through YouTube's recommendation algorithms that the larger animated channel networks have shared.[416]
 In January 2019, YouTube officially banned videos containing "challenges that encourage acts that have an inherent risk of severe physical harm" (such as the Tide Pod Challenge) and videos featuring pranks that "make victims believe they're in physical danger" or cause emotional distress in children.[417]
 Also in November 2017, it was revealed in the media that many videos featuring children—often uploaded by the minors themselves, and showing innocent content such as the children playing with toys or performing gymnastics—were attracting comments from pedophiles[418][419] with predators finding the videos through private YouTube playlists or typing in certain keywords in Russian.[419] Other child-centric videos originally uploaded to YouTube began propagating on the dark web, and uploaded or embedded onto forums known to be used by pedophiles.[420]
 As a result of the controversy, which added to the concern about "Elsagate", several major advertisers whose ads had been running against such videos froze spending on YouTube.[405][421] In December 2018, The Times found more than 100 grooming cases in which children were manipulated into sexually implicit behavior (such as taking off clothes, adopting overtly sexual poses and touching other children inappropriately) by strangers.[422] After a reporter flagged the videos in question, half of them were removed, and the rest were removed after The Times contacted YouTube's PR department.[422]
 In February 2019, YouTube vlogger Matt Watson identified a "wormhole" that would cause the YouTube recommendation algorithm to draw users into this type of video content, and make all of that user's recommended content feature only these types of videos.[423] Most of these videos had comments from sexual predators commenting with timestamps of when the children were shown in compromising positions or otherwise making indecent remarks. In some cases, other users had re-uploaded the video in unlisted form but with incoming links from other videos, and then monetized these, propagating this network.[424] In the wake of the controversy, the service reported that they had deleted over 400 channels and tens of millions of comments, and reported the offending users to law enforcement and the National Center for Missing and Exploited Children. A spokesperson explained that "any content—including comments—that endangers minors is abhorrent and we have clear policies prohibiting this on YouTube. There's more to be done, and we continue to work to improve and catch abuse more quickly."[425][426] Despite these measures, AT&T, Disney, Dr. Oetker, Epic Games, and Nestlé all pulled their advertising from YouTube.[424][427]
 Subsequently, YouTube began to demonetize and block advertising on the types of videos that have drawn these predatory comments. The service explained that this was a temporary measure while they explore other methods to eliminate the problem.[428] YouTube also began to flag channels that predominantly feature children, and preemptively disable their comments sections. "Trusted partners" can request that comments be re-enabled, but the channel will then become responsible for moderating comments. These actions mainly target videos of toddlers, but videos of older children and teenagers may be protected as well if they contain actions that can be interpreted as sexual, such as gymnastics. YouTube stated it was also working on a better system to remove comments on other channels that matched the style of child predators.[429][430]
 A related attempt to algorithmically flag videos containing references to the string "CP" (an abbreviation of child pornography) resulted in some prominent false positives involving unrelated topics using the same abbreviation, including videos related to the mobile video game Pokémon Go (which uses "CP" as an abbreviation of the statistic "Combat Power"), and Club Penguin. YouTube apologized for the errors and reinstated the affected videos.[431] Separately, online trolls have attempted to have videos flagged for takedown or removal by commenting with statements similar to what the child predators had said; this activity became an issue during the PewDiePie vs T-Series rivalry in early 2019. YouTube stated they do not take action on any video with these comments but those that they have flagged that are likely to draw child predator activity.[432]
 In June 2019, The New York Times cited researchers who found that users who watched erotic videos could be recommended seemingly innocuous videos of children.[433] As a result, Senator Josh Hawley stated plans to introduce federal legislation that would ban YouTube and other video sharing sites from including videos that predominantly feature minors as "recommended" videos, excluding those that were "professionally produced", such as videos of televised talent shows.[434] YouTube has suggested potential plans to remove all videos featuring children from the main YouTube site and transferring them to the YouTube Kids site where they would have stronger controls over the recommendation system, as well as other major changes on the main YouTube site to the recommended feature and auto-play system.[435]
 An August 2022 report by the Center for Countering Digital Hate, a British think tank, found that harassment against women was flourishing on YouTube. It noted that channels espousing a similar ideology to that of men's rights influencer Andrew Tate were using YouTube to grow their audience, despite Tate being banned from the platform.[436] In his 2022 book Like, Comment, Subscribe: Inside YouTube's Chaotic Rise to World Domination, Bloomberg reporter Mark Bergen said that many female content creators were dealing with harassment, bullying, and stalking.[436]
 In 2021, two accounts linked to RT Deutsch, the German channel of the Russian RT network were removed as well for breaching YouTube's policies relating to COVID-19.[385] Russia threatened to ban YouTube after the platform deleted two German RT channels in September 2021.[437]
 Shortly after the Russian invasion of Ukraine in 2022, YouTube removed all channels funded by the Russian state.[438] YouTube expanded the removal of Russian content from its site to include channels described as 'pro-Russian'. In June 2022, the War Gonzo channel run by Russian military blogger and journalist Semyon Pegov was deleted.[439] In July 2023, YouTube removed the channel of British journalist Graham Phillips, active in covering the War in Donbas from 2014.[440]
 In August 2023, a Moscow court fined Google 3 million rubles, around $35,000, for not deleting what it said was "fake news about the war in Ukraine".[441]
 YouTube featured an April Fools prank on the site on April 1 of every year from 2008 to 2016. In 2008, all links to videos on the main page were redirected to Rick Astley's music video "Never Gonna Give You Up", a prank known as "rickrolling".[442][443] The next year, when clicking on a video on the main page, the whole page turned upside down, which YouTube claimed was a "new layout".[444] In 2010, YouTube temporarily released a "TEXTp" mode which rendered video imagery into ASCII art letters "in order to reduce bandwidth costs by $1 per second."[445]
 The next year, the site celebrated its "100th anniversary" with a range of sepia-toned silent, early 1900s-style films, including a parody of Keyboard Cat.[446] In 2012, clicking on the image of a DVD next to the site logo led to a video about a purported option to order every YouTube video for home delivery on DVD.[447]
 In 2013, YouTube teamed up with satirical newspaper company The Onion to claim in an uploaded video that the video-sharing website was launched as a contest which had finally come to an end, and would shut down for ten years before being re-launched in 2023, featuring only the winning video. The video starred several YouTube celebrities, including Antoine Dodson. A video of two presenters announcing the nominated videos streamed live for 12 hours.[448][449]
 In 2014, YouTube announced that it was responsible for the creation of all viral video trends, and revealed previews of upcoming trends, such as "Clocking", "Kissing Dad", and "Glub Glub Water Dance".[450] The next year, YouTube added a music button to the video bar that played samples from "Sandstorm" by Darude.[451] In 2016, YouTube introduced an option to watch every video on the platform in 360-degree mode with Snoop Dogg.[452]
 YouTube Premium (formerly YouTube Red) is YouTube's premium subscription service. It offers advertising-free streaming, access to original programming, and background and offline video playback on mobile devices.[453] YouTube Premium was originally announced on November 12, 2014, as "Music Key", a subscription music streaming service, and was intended to integrate with and replace the existing Google Play Music "All Access" service.[454][455][456] On October 28, 2015, the service was relaunched as YouTube Red, offering ad-free streaming of all videos and access to exclusive original content.[457][458][459] As of November 2016[update], the service has 1.5 million subscribers, with a further million on a free-trial basis.[460] As of June 2017[update], the first season of YouTube Originals had received 250 million views in total.[461]
 YouTube Kids is an American children's video app developed by YouTube, a subsidiary of Google. The app was developed in response to parental and government scrutiny on the content available to children. The app provides a version of the service-oriented towards children, with curated selections of content, parental control features, and filtering of videos deemed inappropriate viewing for children aged under 13, 8 or 5 depending on the age grouping chosen. First released on February 15, 2015, as an Android and iOS mobile app, the app has since been released for LG, Samsung, and Sony smart TVs, as well as for Android TV. On May 27, 2020, it became available on Apple TV. As of September 2019, the app is available in 69 countries, including Hong Kong and Macau, and one province. YouTube launched a web-based version of YouTube Kids on August 30, 2019.
 On September 28, 2016, YouTube named Lyor Cohen, the co-founder of 300 Entertainment and former Warner Music Group executive, the Global Head of Music.[462]
 In early 2018, Cohen began hinting at the possible launch of YouTube's new subscription music streaming service, a platform that would compete with other services such as Spotify and Apple Music.[463] On May 22, 2018, the music streaming platform named "YouTube Music" was launched.[464][465]
 YouTube Movies & TV is a video on demand service that offers movies and television shows for purchase or rental, depending on availability, along with a selection of movies (encompassing between 100 and 500 titles overall) that are free to stream, with interspersed ad breaks. YouTube began offering free-to-view movie titles to its users in November 2018; selections of new movies are added and others removed, unannounced each month.[466]
 In March 2021, Google announced plans to gradually deprecate the Google Play Movies & TV app, and eventually migrate all users to the YouTube app's Movies & TV store to view, rent and purchase movies and TV shows (first affecting Roku, Samsung, LG, and Vizio smart TV users on July 15).[467][468] Google Play Movies & TV formally shut down on January 17, 2024, with the web version of that platform migrated to YouTube as an expansion of the Movies & TV store to desktop users. (Other functions of Google Play Movies & TV were integrated into the Google TV service.)[469]
 On November 1, 2022, YouTube launched Primetime Channels, a channel store platform offering third-party subscription streaming add-ons sold a la carte through the YouTube website and app, competing with similar subscription add-on stores operated by Apple, Prime Video and Roku. The add-ons can be purchased through the YouTube Movies & TV hub or through the official YouTube channels of the available services; subscribers of YouTube TV add-ons that are sold through Primetime Channels can also access their content via the YouTube app and website. A total of 34 streaming services (including Paramount+, Showtime, Starz, Epix, AMC+ and ViX+) were initially available for purchase.[470][471]
 NFL Sunday Ticket, as part of a broader residential distribution deal with Google signed in December 2022 that also made it available to YouTube TV subscribers, was added to Primetime Channels as a standalone add-on on August 16, 2023.[472][473] The ad-free tier of Max was added to Primetime Channels on December 12, 2023, coinciding with YouTube TV converting its separate HBO (for base plan subscribers) and HBO Max (for all subscribers) linear/VOD add-ons into a single combined Max offering.[474][475][note 1]
 On February 28, 2017, in a press announcement held at YouTube Space Los Angeles, YouTube announced YouTube TV, an over-the-top MVPD-style subscription service that would be available for United States customers at a price of US$65 per month. Initially launching in five major markets (New York City, Los Angeles, Chicago, Philadelphia and San Francisco) on April 5, 2017,[476][477] the service offers live streams of programming from the five major broadcast networks (ABC, CBS, The CW, Fox and NBC, along with selected MyNetworkTV affiliates and independent stations in certain markets), as well as approximately 60 cable channels owned by companies such as The Walt Disney Company, Paramount Global, Fox Corporation, NBCUniversal, Allen Media Group and Warner Bros. Discovery (including among others Bravo, USA Network, Syfy, Disney Channel, CNN, Cartoon Network, E!, Fox Sports 1, Freeform, FX and ESPN).[478][479]
 Subscribers can also receive premium cable channels (including HBO (via a combined Max add-on that includes in-app and log-in access to the service), Cinemax, Showtime, Starz and MGM+) and other subscription services (such as NFL Sunday Ticket, MLB.tv, NBA League Pass, Curiosity Stream and Fox Nation) as optional add-ons for an extra fee, and can access YouTube Premium original content.[478][479] In September  2022, YouTube TV began allowing customers to purchase most of its premium add-ons (excluding certain services such as NBA League Pass and AMC+) without an existing subscription to its base package.[480]
 In September 2016, YouTube Go was announced,[481] as an Android app created for making YouTube easier to access on mobile devices in emerging markets. It was distinct from the company's main Android app and allowed videos to be downloaded and shared with other users. It also allowed users to preview videos, share downloaded videos through Bluetooth, and offered more options for mobile data control and video resolution.[482]
 In February 2017, YouTube Go was launched in India, and expanded in November 2017 to 14 other countries, including Nigeria, Indonesia, Thailand, Malaysia, Vietnam, the Philippines, Kenya, and South Africa.[483][484] On February 1, 2018, it was rolled out in 130 countries worldwide, including Brazil, Mexico, Turkey, and Iraq. Before it shut down, the app was available to around 60% of the world's population.[485][486] In May 2022, Google announced that they would be shutting down YouTube Go in August 2022.[487]
 In September 2020, YouTube announced that it would be launching a beta version of a new platform of 15-second videos, similar to TikTok, called YouTube Shorts.[488][489] The platform was first tested in India but as of March 2021 has expanded to other countries including the United States with videos now able to be up to 1 minute long.[490] The platform is not a standalone app, but is integrated into the main YouTube app. Like TikTok, it gives users access to built-in creative tools, including the possibility of adding licensed music to their videos.[491] The platform had its global beta launch in July 2021.[492]
 In 2018, YouTube started testing a new feature initially called "YouTube Reels".[493] The feature is nearly identical to Instagram Stories and Snapchat Stories. YouTube later renamed the feature "YouTube Stories". It is only available to creators who have more than 10,000 subscribers and can only be posted/seen in the YouTube mobile app.[494] On May 25, 2023, YouTube announced that it would be shutting down this feature on June 26, 2023.[495][496]
 Private individuals[497] and large production corporations[498] have used YouTube to grow their audiences. Indie creators have built grassroots followings numbering in the thousands at very little cost or effort, while mass retail and radio promotion proved problematic.[497] Concurrently, old media celebrities moved into the website at the invitation of a YouTube management that witnessed early content creators accruing substantial followings and perceived audience sizes potentially larger than that attainable by television.[498] While YouTube's revenue-sharing "Partner Program" made it possible to earn a substantial living as a video producer—its top five hundred partners each earning more than $100,000 annually[499] and its ten highest-earning channels grossing from $2.5 million to $12 million[500]—in 2012 CMU business editor characterized YouTube as "a free-to-use ... promotional platform for the music labels."[501] In 2013 Forbes' Katheryn Thayer asserted that digital-era artists' work must not only be of high quality, but must elicit reactions on the YouTube platform and social media.[502] Videos of the 2.5% of artists categorized as "mega", "mainstream" and "mid-sized" received 90.3% of the relevant views on YouTube and Vevo in that year.[503] By early 2013, Billboard had announced that it was factoring YouTube streaming data into calculation of the Billboard Hot 100 and related genre charts.[504]
 Observing that face-to-face communication of the type that online videos convey has been "fine-tuned by millions of years of evolution", TED curator Chris Anderson referred to several YouTube contributors and asserted that "what Gutenberg did for writing, online video can now do for face-to-face communication."[505] Anderson asserted that it is not far-fetched to say that online video will dramatically accelerate scientific advance, and that video contributors may be about to launch "the biggest learning cycle in human history."[505] In education, for example, the Khan Academy grew from YouTube video tutoring sessions for founder Salman Khan's cousin into what Forbes' Michael Noer called "the largest school in the world," with technology poised to disrupt how people learn.[506] YouTube was awarded a 2008 George Foster Peabody Award,[507] the website being described as a Speakers' Corner that "both embodies and promotes democracy."[508] The Washington Post reported that a disproportionate share of YouTube's most subscribed channels feature minorities, contrasting with mainstream television in which the stars are largely white.[509] A Pew Research Center study reported the development of "visual journalism", in which citizen eyewitnesses and established news organizations share in content creation.[510] The study also concluded that YouTube was becoming an important platform by which people acquire news.[511]
 YouTube has enabled people to more directly engage with government, such as in the CNN/YouTube presidential debates (2007) in which ordinary people submitted questions to U.S. presidential candidates via YouTube video, with a techPresident co-founder saying that Internet video was changing the political landscape.[512] Describing the Arab Spring (2010–2012), sociologist Philip N. Howard quoted an activist's succinct description that organizing the political unrest involved using "Facebook to schedule the protests, Twitter to coordinate, and YouTube to tell the world."[513] In 2012, more than a third of the U.S. Senate introduced a resolution condemning Joseph Kony 16 days after the "Kony 2012" video was posted to YouTube, with resolution co-sponsor Senator Lindsey Graham remarking that the video "will do more to lead to (Kony's) demise than all other action combined."[514]
 Conversely, YouTube has also allowed government to more easily engage with citizens, the White House's official YouTube channel being the seventh top news organization producer on YouTube in 2012[517] and in 2013 a healthcare exchange commissioned Obama impersonator Iman Crosson's YouTube music video spoof to encourage young Americans to enroll in the Affordable Care Act (Obamacare)-compliant health insurance.[518] In February 2014, U.S. President Obama held a meeting at the White House with leading YouTube content creators not only to promote awareness of Obamacare[519] but more generally to develop ways for government to better connect with the "YouTube Generation."[515] Whereas YouTube's inherent ability to allow presidents to directly connect with average citizens was noted, the YouTube content creators' new media savvy was perceived necessary to better cope with the website's distracting content and fickle audience.[515]
 Some YouTube videos have themselves had a direct effect on world events, such as Innocence of Muslims (2012) which spurred protests and related anti-American violence internationally.[520] TED curator Chris Anderson described a phenomenon by which geographically distributed individuals in a certain field share their independently developed skills in YouTube videos, thus challenging others to improve their own skills, and spurring invention and evolution in that field.[505] Journalist Virginia Heffernan stated in The New York Times that such videos have "surprising implications" for the dissemination of culture and even the future of classical music.[521]
 A 2017 article in The New York Times Magazine posited that YouTube had become "the new talk radio" for the far right.[522] Almost a year before YouTube's January 2019 announcement that it would begin a "gradual change" of "reducing recommendations of borderline content and content that could misinform users in harmful ways",[523] Zeynep Tufekci had written in The New York Times that, "(g)iven its billion or so users, YouTube may be one of the most powerful radicalizing instruments of the 21st century".[524] Under YouTube's changes to its recommendation engine, the most recommended channel evolved from conspiracy theorist Alex Jones (2016) to Fox News (2019).[525] According to a 2020 study, "An emerging journalistic consensus theorizes the central role played by the video 'recommendation engine,' but we believe that this is premature. Instead, we propose the 'Supply and Demand' framework for analyzing politics on YouTube."[526] A 2022 study found that "despite widespread concerns that YouTube's algorithms send people down 'rabbit holes' with recommendations to extremist videos, little systematic evidence exists to support this conjecture", "exposure to alternative and extremist channel videos on YouTube is heavily concentrated among a small group of people with high prior levels of gender and racial resentment.", and "contrary to the 'rabbit holes' narrative, non-subscribers are rarely recommended videos from alternative and extremist channels and seldom follow such recommendations when offered."[527]
 The Legion of Extraordinary Dancers[528] and the YouTube Symphony Orchestra[529] selected their membership based on individual video performances.[505][529] Further, the cyber-collaboration charity video "We Are the World 25 for Haiti (YouTube edition)" was formed by mixing performances of 57 globally distributed singers into a single musical work,[530] with The Tokyo Times noting the "We Pray for You" YouTube cyber-collaboration video as an example of a trend to use crowdsourcing for charitable purposes.[531]
The anti-bullying It Gets Better Project expanded from a single YouTube video directed to discouraged or suicidal LGBT teens,[532] that within two months drew video responses from hundreds including U.S. President Barack Obama, Vice President Biden, White House staff, and several cabinet secretaries.[533] Similarly, in response to fifteen-year-old Amanda Todd's video "My story: Struggling, bullying, suicide, self-harm", legislative action was undertaken almost immediately after her suicide to study the prevalence of bullying and form a national anti-bullying strategy.[534] In May 2018, after London Metropolitan Police claimed that drill music videos glamorizing violence gave rise to gang violence, YouTube deleted 30 videos.[535]
 Prior to 2020, Google did not provide detailed figures for YouTube's running costs, and YouTube's revenues in 2007 were noted as "not material" in a regulatory filing.[536] In June 2008, a Forbes magazine article projected the 2008 revenue at $200 million, noting progress in advertising sales.[537] In 2012, YouTube's revenue from its ads program was estimated at $3.7 billion.[538] In 2013, it nearly doubled and estimated to hit $5.6 billion according to e-Marketer,[538][539] while others estimated $4.7 billion.[538] The vast majority of videos on YouTube are free to view and supported by advertising.[61] In May 2013, YouTube introduced a trial scheme of 53 subscription channels with prices ranging from $0.99 to $6.99 a month.[540] The move was seen as an attempt to compete with other providers of online subscription services such as Netflix, Amazon Prime, and Hulu.[61]
 Google first published exact revenue numbers for YouTube in February 2020 as part of Alphabet's 2019 financial report. According to Google, YouTube had made US$15.1 billion in ad revenue in 2019, in contrast to US$8.1 billion in 2017 and US$11.1 billion in 2018. YouTube's revenues made up nearly 10% of the total Alphabet revenue in 2019.[541][542] These revenues accounted for approximately 20 million subscribers combined between YouTube Premium and YouTube Music subscriptions, and 2 million subscribers to YouTube TV.[543]
 YouTube had $29.2 billion ads revenue in 2022, up by $398 million from the prior year.[544]
 YouTube entered into a marketing and advertising partnership with NBC in June 2006.[545] In March 2007, it struck a deal with BBC for three channels with BBC content, one for news and two for entertainment.[546] In November 2008, YouTube reached an agreement with MGM, Lions Gate Entertainment, and CBS, allowing the companies to post full-length films and television episodes on the site, accompanied by advertisements in a section for U.S. viewers called "Shows". The move was intended to create competition with websites such as Hulu, which features material from NBC, Fox, and Disney.[547][548] In November 2009, YouTube launched a version of "Shows" available to UK viewers, offering around 4,000 full-length shows from more than 60 partners.[549] In January 2010, YouTube introduced an online film rentals service,[550] which is only available to users in the United States, Canada, and the UK as of 2010.[551][552][needs update] The service offers over 6,000 films.[553]
 In March 2017, the government of the United Kingdom pulled its advertising campaigns from YouTube, after reports that its ads had appeared on videos containing extremist content. The government demanded assurances that its advertising would "be delivered safely and appropriately". The Guardian newspaper, as well as other major British and U.S. brands, similarly suspended their advertising on YouTube in response to their advertising appearing near offensive content. Google stated that it had "begun an extensive review of our advertising policies and have made a public commitment to put in place changes that give brands more control over where their ads appear".[554][555] In early April 2017, the YouTube channel h3h3Productions presented evidence claiming that a Wall Street Journal article had fabricated screenshots showing major brand advertising on an offensive video containing Johnny Rebel music overlaid on a Chief Keef music video, citing that the video itself had not earned any ad revenue for the uploader. The video was retracted after it was found that the ads had been triggered by the use of copyrighted content in the video.[556][557]
 On April 6, 2017, YouTube announced that to "ensure revenue only flows to creators who are playing by the rules", it would change its practices to require that a channel undergo a policy compliance review, and have at least 10,000-lifetime views, before they may join the Partner Program.[558]
 In May 2007, YouTube launched its Partner Program (YPP), a system based on AdSense which allows the uploader of the video to share the revenue produced by advertising on the site.[559] YouTube typically takes 45 percent of the advertising revenue from videos in the Partner Program, with 55 percent going to the uploader.[560][561]
 There are over two million members of the YouTube Partner Program.[562] According to TubeMogul, in 2013 a pre-roll advertisement on YouTube (one that is shown before the video starts) cost advertisers on average $7.60 per 1000 views. Usually, no more than half of the eligible videos have a pre-roll advertisement, due to a lack of interested advertisers.[563]
 YouTube's policies restrict certain forms of content from being included in videos being monetized with advertising, including videos containing violence, strong language, sexual content, "controversial or sensitive subjects and events, including subjects related to war, political conflicts, natural disasters and tragedies, even if graphic imagery is not shown" (unless the content is "usually newsworthy or comedic and the creator's intent is to inform or entertain"),[564] and videos whose user comments contain "inappropriate" content.[565]
 In 2013, YouTube introduced an option for channels with at least a thousand subscribers to require a paid subscription in order for viewers to watch videos.[566][567] In April 2017, YouTube set an eligibility requirement of 10,000 lifetime views for a paid subscription.[568] On January 16, 2018, the eligibility requirement for monetization was changed to 4,000 hours of watch-time within the past 12 months and 1,000 subscribers.[568] The move was seen as an attempt to ensure that videos being monetized did not lead to controversy, but was criticized for penalizing smaller YouTube channels.[569]
 YouTube Play Buttons, a part of the YouTube Creator Rewards, are a recognition by YouTube of its most popular channels.[570] The trophies made of nickel plated copper-nickel alloy, golden plated brass, silver plated metal, ruby, and red tinted crystal glass are given to channels with at least one hundred thousand, a million, ten million, fifty million subscribers, and one hundred million subscribers, respectively.[571][572]
 YouTube's policies on "advertiser-friendly content" restrict what may be incorporated into videos being monetized; this includes strong violence, language,[573] sexual content, and "controversial or sensitive subjects and events, including subjects related to war, political conflicts, natural disasters and tragedies, even if graphic imagery is not shown", unless the content is "usually newsworthy or comedic and the creator's intent is to inform or entertain".[574] In September 2016, after introducing an enhanced notification system to inform users of these violations, YouTube's policies were criticized by prominent users, including Phillip DeFranco and Vlogbrothers. DeFranco argued that not being able to earn advertising revenue on such videos was "censorship by a different name". A YouTube spokesperson stated that while the policy itself was not new, the service had "improved the notification and appeal process to ensure better communication to our creators".[575][576][577] Boing Boing reported in 2019 that LGBT keywords resulted in demonetization.[578]
 As of November 2020 in the United States, and June 2021 worldwide,[579] YouTube reserves the right to monetize any video on the platform, even if their uploader is not a member of the YouTube Partner Program. This will occur on channels whose content is deemed "advertiser-friendly", and all revenue will go directly to Google without any share given to the uploader.[580]
 The majority of YouTube's advertising revenue goes to the publishers and video producers who hold the rights to their videos; the company retains 45% of the ad revenue.[581] In 2010, it was reported that nearly a third of the videos with advertisements were uploaded without permission of the copyright holders. YouTube gives an option for copyright holders to locate and remove their videos or to have them continue running for revenue.[582] In May 2013, Nintendo began enforcing its copyright ownership and claiming the advertising revenue from video creators who posted screenshots of its games.[583] In February 2015, Nintendo agreed to share the revenue with the video creators through the Nintendo Creators Program.[584][585][586] On March 20, 2019, Nintendo announced on Twitter that the company will end the Creators program. Operations for the program ceased on March 20, 2019.[587][588]
 YouTube has been censored, filtered, or banned for a variety of reasons, including:[589]
 Access to specific videos is sometimes prevented due to copyright and intellectual property protection laws (e.g. in Germany), violations of hate speech, and preventing access to videos judged inappropriate for youth,[590] which is also done by YouTube with the YouTube Kids app and with "restricted mode".[591] Businesses, schools, government agencies, and other private institutions often block social media sites, including YouTube, due to its bandwidth limitations[592][593] and the site's potential for distraction.[589][594]
 As of 2018[update], public access to YouTube is blocked in many countries, including China, North Korea, Iran, Turkmenistan,[595] Uzbekistan,[596][597] Tajikistan, Eritrea, Sudan and South Sudan. In some countries, YouTube is blocked for more limited periods of time such as during periods of unrest, the run-up to an election, or in response to upcoming political anniversaries. In cases where the entire site is banned due to one particular video, YouTube will often agree to remove or limit access to that video in order to restore service.[589]
 Reports emerged that since October 2019, comments posted with Chinese characters insulting the Chinese Communist Party (共匪 "communist bandit" or 五毛 "50 Cent Party", referring to state-sponsored commentators) were being automatically deleted within 15 seconds.[598]
 Specific incidents where YouTube has been blocked include:


Source: https://en.wikipedia.org/wiki/EVE_Online
Content: 
 
 Eve Online (stylised EVE Online) is a space-based, persistent world massively multiplayer online role-playing game (MMORPG) developed and published by CCP Games. Players of Eve Online can participate in a number of in-game professions and activities, including mining, piracy, manufacturing, trading, exploration, and combat (both player versus environment (PVE) and player versus player (PVP)). The game contains a total of 7,800 star systems that can be visited by players.[2][3]
 The game is renowned for its scale and complexity in regard to player interactions. In its single, shared game world, players engage in unscripted economic competition, warfare, and political schemes with other players.[4] The Bloodbath of B-R5RB, a battle involving thousands of players in a single star system, took 21 hours and was recognized as one of the largest and most expensive battles in gaming history.[5] Eve Online was exhibited at the Museum of Modern Art with a video including the historical events and accomplishments of the playerbase.[6]
 Eve Online was released in North America and Europe in May 2003. It was published from May to December 2003 by Simon & Schuster Interactive in North America and by Crucial Entertainment in the United Kingdom,[7][8] after which CCP purchased the rights and began to self-publish via a digital distribution scheme.[9] On January 22, 2008, it was announced that Eve Online would be distributed via Steam.[10] On March 10, 2009, the game was again made available in boxed form in stores, released by Atari.[1] In February 2013, Eve Online reached over 500,000 subscribers.[11] On November 11, 2016, Eve Online added a limited free-to-play version.[12]
 Set more than 21,000 years in the future, the background story of Eve Online explains that humanity, having used up most of Earth's resources through centuries of explosive population growth, began colonizing the rest of the Milky Way.[13][14] As on Earth, this expansion also led to competition and fighting over available resources, but everything changed with the discovery of a natural wormhole leading to an unexplored galaxy subsequently dubbed "New Eden". Dozens of colonies were founded, and a structure, a gate of sorts (which bears the inscription "EVE" on the New Eden side), was built to stabilize the wormhole that linked the colonies of New Eden with the rest of human civilization. However, when the wormhole unexpectedly collapsed, it destroyed the gate as well as the connection between the colonies of New Eden and the Milky Way. Cut off from the rest of humanity and supplies from Earth, the colonies of New Eden were left starving and disconnected from one another; many died out entirely. Over the millennia the descendants of the surviving colonists managed to rebuild their own societies, but by this time the memories and knowledge of humanity's origins, of Earth and the Milky Way galaxy, as well as the history of the settling of New Eden, were lost; what little information that survived transmission over the generations was misunderstood, lost in translation, or consigned to mythology. Five major distinct societies rose to prominence from the surviving colonies, all growing into interstellar spaceflight-capable civilizations. The states based around these societies  make up the five major empires in Eve Online: the Amarr Empire, the Caldari State, the Gallente Federation, the Minmatar Republic, and the Jove Empire.[15][16]
 The Amarr, a militantly theocratic empire, was the first of the playable races to rediscover faster-than-light travel.[14][17] In terms of physical proximity, the space occupied by this society is physically nearest to the demolished EVE gate. Armed with this new technology and the strength of their faith in their god, the Amarr expanded their empire by conquering and enslaving several races, including the Minmatar race, who had only just begun colonizing other planets.[18][19] Generations later, after the intense culture shock of encountering the Gallente Federation, and in the wake of a disastrous attempted invasion of Jovian space, many Minmatar took the opportunity to rebel and successfully overthrew their enslavers, forming their own government. However, much of their population remain enslaved by the Amarr, and some, having adopted the Amarrian religion and sided with their masters during the revolution, were released from bondage and incorporated into the Empire as commoners in the Ammatar Mandate. The free Minmatar Republic, taking as inspiration the ideals and practices of the Gallente Federation, is presently a strong military and economic power actively seeking the emancipation of their brethren and all other slaves.[citation needed]
 The Gallente and the Caldari homeworlds are situated in the same star system.[20][21] The Gallente homeworld was originally settled by descendants of the French colonists of Tau Ceti; Caldari Prime on the other hand was purchased by a multinational megacorporation that began to terraform it.[22][23] The terraforming of Caldari Prime was incomplete at the time of the EVE wormhole's collapse, hence the planet remained environmentally inhospitable for millennia. The Gallente restored themselves to a high-functioning technological society some hundred years before the Caldari, building the first lastingly democratic republic of New Eden in the form of the Gallente Federation. Originally the Caldari composed a member race within the Federation, but cultural animosity between the two peoples spiralled into a war during which the Caldari seceded from the Federation to found their own Caldari State. The war lasted 93 years, with neither nation able to overwhelm the other.[22][23] The planet Caldari Prime was initially retained by the Gallente Federation during the war, and did not become part of the new Caldari State. Much more recently, however, a Caldari offensive managed to recapture their lost homeworld, a fact which is viewed with abhorrence by the Gallente, who see the presence of a significant Caldari fleet about the planet as a mass hostage taking.[citation needed]
 Both the Gallente Federation and Caldari State are economy- and trade-oriented nations. However, the Gallente favour liberal economic policies, encourage individual entrepreneurship and social democracy, and maintain a progressive approach to social welfare, whereas the Caldari State is organised as a form of statist corporatocracy; the Caldari State itself is owned by and operated on behalf of a few trust-like megaconglomerates. The Gallente Federation's official policies regarding multiculturalism and encouragement of diversity attract many immigrants to Gallente space; a third of all ethnic Minmatars reside as citizens there. As the Caldari did not share this enthusiasm for diversity with the Gallente, the Caldari State at the time of its formation found itself at a relative population deficit compared to its Gallente adversary; rather than encourage massive immigration to and diversity within the State, this population shortage was rectified by a Statewide programme of artificial reproduction, producing a generation of so-called 'Tube Children' raised by the Caldari State apparatus to enlarge the labour pools available to the megacorporations that ruled the State.[citation needed]
 The Jovians (a non-playable race) were also descended from colonists. Unlike the other races of Eve Online, they maintained a relatively high-functioning technological society after the collapse of the EVE wormhole and did not need to spend millennia recapitulating ancient societal developments as the others did, and while the other four major races were still grounded, Jovian history saw two major periods of spacefaring imperialism.[24] They expanded outward and eventually turned to genetic engineering in order to mold themselves into a species more suited for deep-space life and long-range interstellar exploration. Genetic experimentation and manipulation were not without their drawbacks, however: by the time period in which players enter the Eve Online universe, millennia of human genetic manipulation have rendered the Jovians barely recognizable as human; more critically, the Jovian manipulation of their genome has resulted in the eventually fatal "Jovian Disease", an inherited psychological disorder which, despite the best efforts of the Jovians to reverse it, has affected every individual of the Jovian race and thus crippled their civilization. Having experienced a catastrophic population decline (the Jovian societal structure is believed to be barely maintained by their immensely-advanced technological systems), the Jovians have effectively retreated to inhabit a region of space inaccessible to outsiders.[25]
 In addition to different backgrounds and histories, the races have characteristic philosophies of starship design. Minmatar ships tend to be quite fast but fragile, and rely on their high speed and maneuverability to evade the tracking systems of heavier weapons, while using projectile weapons such as artillery or autocannons, more sophisticated kin to today's munitions technology. Amarr ships are ponderous, densely armored, and pack batteries of powerful laser-based energy weaponry. Gallente ships are sleek and designed to favor armor plating; they specialize in deploying fleets of robotic drones while mounting hybrid weapons that operate using superconducting magnets to accelerate mass toward targets at great speed (see Railgun). Finally, Caldari ships are typically poorly armored and not particularly fast but utilize powerful energy shields, and make extensive use of torpedo/missile launchers and hybrid guns, favoring engagement at extreme ranges.[26] However, there are exceptions to these general rules in each race.[citation needed]
 Players start the game by either selecting a previously created character or by creating a new one. Each Eve Online account allows for up to three characters.[27] When a player creates a new character, they start by choosing one of the four playable races – Amarr, Gallente, Minmatar, or Caldari. Each race is further divided into three bloodlines that give characters different pre-defined appearances, which can be finely tuned by the player.
 Unlike many other MMOs, where there are numerous copies of the game universe intended to run at once (i.e., servers), Eve Online is functionally a single-universe game. There are at least four copies of the universe running: the main server "Tranquility", the Chinese-based "Serenity", the test server "Singularity" (also "Sisi") which is a general, public test server, and an event server, "Thunderdome", which is used for tournaments.[28][29] A new test server was announced called "Buckingham" to replace "Singularity" as the main EVE Online test server while "Singularity" was used for Dust 514/EVE Online joint testing. As Dust 514 is no longer active, "Singularity" is now the main test server again and "Buckingham" is a closed test server for the CCP developers.[30]
 The playing environment in Eve Online consists of more than 5000 star systems,[31] as well as 2500 randomly accessible wormhole systems, taking place in 23341 C.E.[32] Systems are classified by their Security Status, on a decimal scale from −1.0 to 1.0.[33][34][35] These systems are categorized into three groups, each determining the response from CONCORD (Consolidated Co-operation and Relations Command) NPC law enforcement units.[36] Star systems classed as 0.5–1.0 security are considered "high security" and any unauthorized/unprovoked attack by a character on another character anywhere in the system will result in the appearance of law enforcement. These units will attack and destroy the aggressor, and are designed to reinforce in such strength that they will always win an engagement. However, CONCORD is not preventive, but punitive, meaning there is a short window between beginning an attack and getting destroyed where a player (or group of) can destroy another player's ship. Systems classified as 0.1–0.4 are considered "low security", where CONCORD law enforcement units will not destroy aggressors, but do monitor unprovoked acts of aggression and have automated sentry guns in some locations. Unprovoked attacks will flag the aggressor as a free target for other players, and attacks within sight of sentry guns will cause them to fire on the aggressor. Systems classified 0.0 to −1.0 are called "zero space" or "null sec", and feature no law enforcement; individual systems, or groups of systems, may be controlled by player alliances, up to the creation of player-owned empires reaching across entire "regions" (an aggregate group of star systems). Wormhole systems are accessible only by wormholes that randomly appear and disappear, and are also lawless space, showing as −1.0. However, player-run corporations cannot claim sovereignty in wormhole systems. Star systems contain different types of celestial objects, making them more or less suitable for different kinds of operations. Typically, players find asteroid fields, planets, stations, stargates, and moons in a system. Many of the game's most profitable income sources are found in dangerous null or low security systems, giving players incentive to engage in high-risk, high-reward activities in which they must survive the possible harassment of other players who may also enter the system.[citation needed]
 The game's primary mode of play is flying space ships. Players can dock at stations, where they are safe and can use station services such as repairing, refitting, and the regional market. All space combat takes place in real time at sub-light speeds from around 100 m/s to in excess of 8000 m/s, depending on ship size and setup. While players can manually control their ships as in space combat simulators such as Wing Commander or X-Wing following the release of the Rhea expansion on December 9, 2014, most opt instead to give commands such as Orbit, Approach or Align to their flight computer, which does its best to comply. Weapon aiming, however, cannot be done manually; instead, the player locks on to an opponent and orders their weapons to fire, and the result is determined through calculations based on factors such as range, velocity, weapon tracking, and a degree of randomness.[citation needed]
 Travel across distances longer than hundreds of kilometers is mostly done with the ship's Warp Drive, which every ship and escape pod has. Alternatively a player may also choose to "slow boat" across these kind of distances, that is, traveling without warp drive. To warp, the player issues a command to warp to an object greater than 150 km away and in the same star system; after an alignment maneuver, their ship will enter warp. Warp speeds are measured in astronomical units per second and vary by ship class and fittings. A ship's warp drive can be temporarily disabled by warp disruption weapons, which is an essential part of combat to prevent a target from escaping.[citation needed]
 For most ships, travel between star systems is only possible by using structures called "Stargates". Each stargate is linked to a partner stargate in another system; most star systems have more than two stargates, forming a network through which players travel. While players can travel to any number of destinations in individual systems, the need to use stargates to travel between systems makes them focal points for combat.[citation needed]
 Besides using Stargates, capital ships can also utilize jump drives, which require another ship to create a "Cynosural Field" which the capital ship can then jump to. While this allows the capital ship to travel instantaneously, it requires a trusted second party (or an alternate account) to create the beacon. Jump drives also consume fuel (in contrast to stargates, which require nothing), drain the ship of its capacitor, leaving it nearly defenseless until it is recharged, and incur "jump fatigue", which prevents the pilot from jumping for progressively longer periods of time after each consecutive jump. Titans are also capable of allowing other ships to instantaneously travel by creating temporary bridges to cynosural fields. Black Ops battleships can create similar, but undetectable, bridges capable of transporting only specific types of stealth ships such as Stealth Bombers.[citation needed]
 Player-run corporations that claim sovereignty over two null sec systems within range of each other can also set up a jump bridge at a player owned starbase (POS) that is in orbit of a moon. Jump bridges allow instantaneous travel to the other system's jump bridge, at the cost of using fuel (requiring supply by the owning corporation) scaled to the mass of ships that use them. This also leads to the accumulation of jump fatigue. As the aging POS systems & code have been phased out of the game, a deployable structure has effectively replaced the old jump bridge. However, unlike the old POS jump bridges, it allows players to customize who may use the gate based on settings such as standings or corporation / alliance affiliation. It also does not need to be deployed in a POS, and as such is often deployed near player owned Citadel structures.[citation needed]
 Unlike other massively multiplayer online games, player characters in Eve Online advance continuously over time by training skills, a passive process that occurs in real world time so that the learning process continues even if the player is not logged in.[37] The skill training queue allows up to 50 skills to be scheduled, with up to a 10-year total training schedule. Before the November 4, 2014 "Phoebe" release, the skill training queue allowed skills to be scheduled to start training only up to 24 hours in the future.[38] Some skills require other prerequisite skills to be trained to a certain level to be trained, and some skills require more time to train than others; for example, the skill to fly a Titan-class spaceship takes 8 times as long to train as the skill to fly a frigate ship, with a significant number of prerequisite skills.[citation needed]
 Until the Odyssey expansion, it was not possible to train more than one character per account at the same time. Odyssey introduced "Dual Character Training", which allows players to expend PLEX (see accounts and subscriptions) in order to allow that account to train a second character for 30 days, equivalent to paying for a 30-day subscription on another account to train a single character.[39] Odyssey 1.2 introduced the more generalized "Multiple Character Training" which allows players to expend more PLEX to activate this feature for a third character on the account.[40]
 The in-game economy in Eve Online is an open economy that is largely player-driven. Non-player character (NPC) merchants sell skill books used by players to learn new skills and blueprints to manufacture ships and modules. NPC merchants also buy and sell Trade Goods. Some Trade Goods are only for trade between NPC merchants while others have a functional use and are not good for trade with NPCs. The characters themselves gather the necessary raw materials to manufacture almost all of the ships and ship modules in the game. NPC ships can be looted and salvaged for items and materials. Non-player created ships and equipment may be purchased from various NPC factions as a character gains status with them, and can be resold in the in-game economy. The in-game currency is ISK (Interstellar Kredits), which is also the currency code of the Icelandic króna, the real-world currency of Iceland, where the Eve Online development studio is located.[citation needed]
 The amount of money or materials in the universe is not fixed and, as such, the economy operates under supply and demand. Market manipulation is possible on a large scale, particular examples being ramping and bear raids. CCP does not issue refunds on in-game purchases. Hence, there is always the risk of certain types of confidence tricks or other scams.[41] The economy is balanced by the automatic introduction of extra materials in underpopulated areas. This encourages a more even spread of players.[42]
 The game provides support for the trading of in-game resources, including graphs of item price history, with Donchian Channel and daily average price. Some player characters operate primarily as traders, generating profits through buying, selling, and transporting goods. Others operate primarily as producers, obtaining components or raw materials and transforming them, sometimes on massive scales, into useful items such as weapons, ships, ammunition, items, or various technologies in demand by other players. Some less combat-oriented players operate as miners or salvagers, collecting and processing ores used in manufacturing or collecting salvage materials to make into items, respectively. Finally, some characters operate as mercenaries or pirates, being paid primarily to be battle-ready and either to attack or defend other profitable enterprises.[citation needed]
 Unlike some games such as Second Life, in-game currency is not freely convertible with real world currency. Players may only buy specific in-game items (such as the Pilot License Extension (PLEX), a token of which 500 can be redeemed for 30 days of Omega time) from CCP with real-world currency. The player can then sell the items on the in-game market for ISK (game currency). The reverse process, selling in-game currency or items for real-world money, is prohibited.[43] The developers' reasoning for this policy, as related by a CCP representative at Fanfest 2010, is that free interchange of currency causes in-game banking to fall under the same regulatory domain as real-world banking. CCP would rather not place this restriction on in-game behavior, due both to the difficulty of regulatory enforcement and the desire to allow players to create illegitimate in-game banks or Ponzi schemes if they wish to do so.[citation needed]
 Commentators have attempted to estimate the value of Eve Online entities in real-world currency based on the current value of PLEX. One such conversion valued a fleet-ready titan (the most powerful ship in the game) at US$7,600,[44] though estimates vary. Generally, no player expends such amounts of real-world currency to acquire such sums of in-game wealth, opting instead to do activities in-game that net high amounts of profit.[citation needed]
 In 2007, CCP was the first game developer to hire an economist, Eyjólfur Guðmundsson, to oversee Eve's in-game economy. Guðmundsson was previously dean of the faculty of business and science at the University of Akureyri.[45] Eyjólfur Guðmundsson would leave CCP in 2014 to the position of Rector at the University of Akureyri in July 2014.[46]
 Owing to the game's focus on freedom, consequence, and autonomy, many behaviours that are considered griefing in most MMOs are allowed in Eve. This includes stealing from other players, extortion, and causing other players to be killed by large groups of NPCs.[47]
 Only malicious, prolonged and concentrated harassment where no material gain is involved and a few other actions are considered to be illicit griefing by the game's developers.[48] Escaping retribution by CONCORD, the NPC space police force that punishes criminal activity in higher security solar systems,[49] for criminal actions is also forbidden, as CONCORD is intentionally designed by game mechanics to be unstoppable.[50]
 Ships in Eve Online are organized into classes, from tiny frigates only a few dozen meters in length to gigantic capital ships up to 17 kilometers long. Ships fill different roles and vary in size, speed, hull strength, and firepower; smaller ships are generally faster and capable of disabling their targets, but lack the damage output necessary to destroy larger ships, while capital ships do very high amounts of damage but have difficulty striking smaller, mobile targets. Each of the four races has its own unique ship design preferences and strengths and weaknesses, although all races have ships that are meant for the same basic roles and are balanced for play against each other. This means that there is no "best ship" in Eve Online. According to the player's preferred style of play, the player might want their characters to fly a ship with a huge cargo hold, one that is suited for mining, one that has a powerful array of weapons, or a ship that moves quickly through space; but the fluid, ever-changing nature of Eve Online means that no ship will be perfect at all of these tasks, nor is there any guarantee that the "best ship for the job" today will continue to be the best ship tomorrow.[citation needed]
 Furthermore, unlike many online games, Eve Online does not feature racial bonuses; that is, characters of different races do not gain intrinsic advantages for flying ships designed by their own races. While a character will begin with more advanced skills in their own race's ships, a character of another race can reach the same proficiency through training. Thus, players are encouraged to use starships that meet their preferred style of play, and the game does not provide incentives for playing as one race rather than another. However, the ships of different races receive unique bonuses to certain things.[citation needed]
 Ships in Eve Online come in four size classes: 
 Each spaceship within the Eve Online universe has a different set of characteristics, and can be fitted with different combinations of modules, subject to their fitting requirements. Ships have a wide variety of characteristics, including power grid, CPU, capacitor size and recharge rate, energy shields, armor, maximum velocity, agility, locking range, and maximum number of lockable targets. A ship's systems also receive bonuses depending on the ship's pilot's levels of various skills. These bonuses usually correspond to the role the ship has been designed for, and thus vary widely. For instance, the Caldari "Caracal" cruiser has a bonus to the rate of fire of certain missile launcher types, while the Gallente "Vexor" cruiser has a bonus to the damage and hitpoints of certain types of combat drones.[citation needed]
 One important characteristic of a ship is the slots it has available for modules. Slots and modules come in three variants—high-, mid-, and low-power. 
 Ships of different sizes have different numbers and ratios of module slots.[citation needed]
 A ship may also have two to three slots for rigs, which are modules that require no power grid or CPU, but instead require a ship resource called calibration. Installing a rig is a semi-permanent action, as a rig cannot be removed from the ship without being destroyed. Rigs come in four sizes, small, medium, large, and capital, which roughly correspond to the size of the ship, and are used to affect other aspects of the ship such as maximum speed or cargo capacity, or to augment the capabilities of other modules installed in the ship. Most rigs also incur a penalty to a certain aspect of the ship; for example, armor rigs reduce the maximum velocity of the ship.[citation needed]
 All ships in the game are also classed according to Tech level, from Tech I to Tech III. 
 Players have several interaction options when playing Eve Online. Every activity is possible for solo players but larger and more complicated tasks become more feasible for groups, such as pirate clans or corporations.
 Players can organize themselves into corporations (similar to guilds or clans in other MMOs). Corporations are run by one chief executive officer (CEO) who controls the corporation's assets. The CEO assigns roles to corporation members such as director, accountant and personnel manager. Corporations may also band together to form alliances. Corporations and alliances come in different shapes and sizes. Some player groups write press releases about new business openings and send out IPO information to potential in-game venture capital investors. Alliances can control enough star systems that their territory can be plotted on the Eve Online game map.[52] Alliances based in lawless space often form unofficial political power blocs with other alliances. These power blocs are typically referred to as "coalitions". Unlike formally established alliances, coalitions are similar in nature to Japanese keiretsu –  an informal 'business arrangement' in which several corporations band together to provide mutual financial, military and/or industrial support.[citation needed]
 Corporations take up numerous business models such as mining, manufacturing or "ratting" (hunting NPC pirates for their bounties and loot). Corporations can levy income taxes on their members, which skim off a percentage of every member's earnings. Many corporations offer a variety of benefits to their members, such as free or discounted ships, equipment, formal training, and organized corporate group operations.[citation needed]
 Among the many activities that corporations can organize is piracy. Actions considered piracy generally involve breaking the in-game law, and can come in a variety of forms. Pirates may camp stargates waiting for other players to arrive, attack players operating in asteroid belts or hunt for players carrying out an NPC agent-assigned mission. Because these activities are considered to be "illegal" within the game mechanics, pirate characters often will have low security status and may even be branded as outlaws by CONCORD. Likewise, victims of overt piracy may retaliate without intervention from CONCORD, often via an expressed right to destroy the pirate ship (i.e., "kill right"). Although piracy activities are "illegal" within the game universe, they are not against the rules of the game, i.e., there will only be in-game retaliation and punishment for them.[citation needed]
 Illegally attacking another player in secure space will result in a loss of security standing; CONCORD, the interstellar NPC police, will arrive shortly to destroy the aggressor's ship. There are, however, legal ways to attack other players in high-security space.[citation needed]
 Whole corporations and whole alliances can officially declare war on (or "war-dec") other corporations or alliances for a weekly fee, permitting all members of the involved corporations or alliances to attack each other without loss of security status or the intervention of CONCORD.[53] The weekly fee can be eliminated if the war declaration is reciprocated. War declarations will clearly flag a player's enemies, so the player can determine who can legally attack and be attacked.[citation needed]
 In March 2006, CCP made a deal with Optic Communications to start working on bringing Eve Online to the Chinese gaming audience. Closed alpha testing was held on a small cluster for some time, with about 3,000 players chosen from an initial pool of 50,000.[54] The Chinese open beta test began on June 13, 2006, and proved to be very popular, gaining numbers comparable to Eve Online's main server cluster.[55] In order to avoid the shock of quickly adding thousands of new players to the current server (Tranquility), CCP Games decided to launch Eve in China on its own server (Serenity).[54] In 2011, CCP allowed its licensing agreement with CDC Games, which had acquired Optic in July 2007,[56] to expire. CCP created a new partnership with TianCity to relaunch Serenity on December 11, 2012.[57]
 The code base between Serenity (serving China) and Tranquility (serving the rest of the world) is synchronised, so that feature development is distributed to both server clusters, although the game worlds are not connected. Eve Online fully supports Unicode and has a back-end system to enable localization of each and every aspect of the game's content and UI.[58]
 In October 2006, the average age of an Eve Online player was 27, and 95% of players were male. The average weekly playtime was 17 hours, or just under 2.5 hours per day.[42] By May 6, 2009, Eve Online claimed to have more than 300,000 active subscriptions and 45,000 active trial accounts.[59][60][61][62][63]  The total active subscription count at end of 2010 was 357,000 accounts.[64]
 On July 7, 2011, CCP announced that it planned to partner with Nexon Co. Ltd. to bring a "fully localized game client and product services for CCP's award winning... EVE Online" to Japan in the fall. Localized services for Japanese players would enable them to access the game in their native language through the Tranquility server, which currently hosts over 350,000 subscribers from around the world in three languages: English, German and Russian.[65]
 On May 5, 2013, Eve Online claimed a new record for the maximum number of simultaneous pilots online with 65,303 concurrent accounts logged on to the same server at the same time. This record was set on the eve of Eve Online's 10 year anniversary, and topped the previous record of 63,170 set January 23, 2011. Eve Online typically experiences the highest number of users on Sundays and the peak player records have almost exclusively been broken on Sundays.[66]
 During two weekends in July 2006, a live streaming video production called Eve TV[67][68] covered the events of the 2nd Caldari Alliance Tournament. The tournament pitted three-man teams from the top alliances against each other. Eve TV provided live in-game footage of the battles along with expert commentary. Analysis of the teams and strategies, interviews with CCP staff and behind-the-scenes specials were also aired between battles. Eve TV was produced and hosted primarily by DJs[67] from Eve-Radio (a player-run streaming radio station) with resources provided by CCP. A total of 95 matches were scheduled, with the Band of Brothers[69] alliance emerging the winner.[70]
 The first two weekends in December 2006 saw the 3rd Alliance tournament. This was once again broadcast via live streaming video by Eve TV[68] The tournament saw 40 Alliances[71] pitting five-man teams against each other. Once again, the Band of Brothers[69] alliance emerged as the winner. Of particular note in this tournament was the fielding of an Imperial Apocalypse by the Interstellar Alcohol Conglomerate. The ship was destroyed in the semi-finals of the tournament by the COW (Cult of War) team. A last-minute attempt to arrange an 8 billion ISK ransom for the ship fell through.[citation needed]
 The fourth Alliance tournament in September 2007 brought several upsets, with Star Fraction defeating Band of Brothers in the second round, using only tech 1 cruisers, and Hun Reloaded sweeping both the semifinals and finals to win.[72]
 The two weekends starting February 29, 2008, and March 7, 2008, saw the fifth Alliance Tournament.[73] Eve TV provided coverage via live streaming video.[74] During the six days a total of 40 teams competed in 95 matches. The last tournament's winner, HUN Reloaded, made its way into the quarter-finals where it lost to Ev0ke alliance, who later became tournament champion after having won all eight of its matches.[73]
 The sixth Alliance Tournament was held during three consecutive weekends starting January 24, 2009, and ending on February 8, 2009. A total of 64 teams took part in the qualifying rounds on opening weekend. While the final weekend was broadcast live via Eve TV, the qualifying rounds were broadcast through various Eve Online radio channels. A number of changes were made to the tournament rules.[75] This was also the first tournament Factional Militias were able to take part alongside traditional alliance teams.[76] In the final match, R.U.R. went up against Pandemic Legion with Pandemic Legion emerging as the tournament winner.[77]
 Alliance Tournament 7 took place in September 2009, with Pandemic Legion winning a second successive tournament, beating Circle of Two in the final.[need quotation to verify] Alliance Tournament 8 took place in June 2010, with Pandemic Legion winning for the third time, beating Hydra Reloaded,[78] while Alliance Tournament 9 took place in June 2011, with Hydra Reloaded as the winner in the uncontested final match against Outbreak.[79]
 Alliance Tournament 10 took place over four consecutive weekends in July 2012. 64 Teams took part in the Tournament, with all matches being broadcast live on EVE TV. A number of changes were made to the format of matches, which included increasing the maximum number of pilots from 10 to 12.[80] Verge of Collapse were eventually crowned Champions, defeating Alliance Tournament 4 winners HUN Reloaded in the final. The Alliance stunned everyone beating top teams to claim the title of Champions.[81]
 Alliance Tournament 11 took place over the course of three weekends in July and August 2013, with Pandemic Legion beating Exodus. in the loser's bracket, then coming back from a 2–0 score in a best of five match against Hydra Reloaded to win 3 matches in a row and win their fourth Alliance Tournament, and their first victory since Alliance Tournament 8.[82]
 The Interstellar Services Department (ISD) is a group of volunteers, made up of subscribed players, who assist in a variety of tasks like answering questions from players, bug hunting and QA testing, covering player-driven news,[83] and writing game fiction. It includes ECAID (Equipment Certification and Anomaly Investigations Division), STAR (Support, Training and Resources), IC (Interstellar Correspondents), M (Mercury), CCL (Community Communication Liaisons), and YARR (Yulai Archives & Records Repository Team).[84]
 According to the developers, Eve Online evolved from the classic computer game Elite, which itself was based on concepts from the science-fiction role-playing game Traveller. Eve combined concepts from Elite with the multi player chat and player-versus-player aspects of Ultima Online.[85] Elite had four single-player aspects: missions,[86] mining, trade routes and combat with random hostile NPCs,[87] all of which are aspects of the first incarnations of Eve Online.[88]
 One of the original developers of Elite, David Braben, believes Eve Online is a reimplementation of the 1980s game, not its true successor.[89] Some of the developers (John Cameron, James Cassidy, Joe Chaney) also believe that this game creates a world where players can become someone else only possible in their imaginations.[citation needed]
 Both the server and the client software for Eve Online are developed in Stackless Python, a variant of the Python programming language. Stackless Python allows a relatively large number of players to perform tasks without the overhead of using the call stack used in the standard Python distribution. This frees the game developers from performing some routine work and allows them to apply changes to the game universe without resetting the server.[90]  However, the Eve cluster is taken offline daily for database and server maintenance.[91]
 On March 14, 2006, the Eve Online development team announced that they would be upgrading the graphics engine of Eve Online to a DirectX 10 / Windows Vista graphics platform.[92] Revelations patch 1.4 had patch notes quoted as saying that the current Eve Online client should work in Vista "as well as it does in XP".[93]
 On September 10, 2007, CCP Games announced that the new Trinity 2 graphics engine will be using DirectX 9.0.[94] This was released on December 5, 2007.[95]
 Official support for Linux and Mac platforms, using Transgaming Technologies Cedega and Cider (forks of Wine) for Linux and Mac compatibility, was introduced with the Revelations 2.3 patch released on November 6, 2007.[96][97] At Fanfest 2008 Transgaming announced and demonstrated that the Premium graphics client is now running on Mac OS X 10.5 Leopard. In February 2009 CCP announced that they planned to discontinue the official Linux client with the next major patch,[98] and advised on using third-party programs to run the Windows version of the client under Linux (namely Wine).[99][100] The discontinuation of the official Linux client was primarily because the game ran better using the Windows client via Wine than it did with the official one, as a result many Linux users were already using Wine instead.[citation needed]
 On December 4, 2020, CCP Games announced the development of a native Mac client, running on macOS Big Sur and using the Metal graphics framework; the initial release supporting Intel-based Macs was targeted for the first half of 2021, with a subsequent release to support M1-based Macs.[101]
 Third-party applications supplement players' Eve Online experience. Some of these, such as automated applications designed to claim publicly available contracts accidentally put up without an associated cost, can result in a ban if discovered,[citation needed] while others are endorsed, tacitly or explicitly, by CCP. EVEMon, a .NET application that monitors and forecasts skill training times, is one example of an explicitly authorized external application.[102][103] Another such application, Eve Fitting Tool or EFT, allows players to try different ship setups and see how certain skills and modules will affect that ship.[104]
 Third-party applications to monitor character information and skill training exist for both the Android and iPhone mobile platforms.[105][106]
 In May 2005, CCP announced the Eve API Project; third-party utilities such as EVEMon now interface with character data, market, and other data through an API.[107]
 Since the initial release of Eve Online, CCP has added twenty-one expansions to the game, free of additional charge to its subscribers.[108] The twentieth expansion "Rubicon", was released on November 19, 2013, and focused on new faction ships, introduction of mobile structures, and the first steps towards "The Future of EVE" outlined by CCP Seagull.[109] The nineteenth expansion, "Odyssey", was released on June 4, 2013, and focused on exploration and rebalancing battleships.[110] The eighteenth expansion, "Retribution", focused on a newly re-worked Crimewatch system. It also introduced the newly rebalanced frigates, destroyers, cruisers, and battlecruisers.[111] The seventeenth expansion, "Inferno", added enhanced graphics for missile systems, a host of new ship modules, and a controversial new "Unified Inventory" UI.[112] The sixteenth expansion, "Crucible", was released in November 2011 and shifted the focus from cosmetic changes to game mechanics.[113] The fifteenth expansion, "Incarna", was released in the first stage of CCP's controversial Ambulation project, also known as the "Walking in Stations" project. "Incarna" added "Captain's Quarters" to stations, the first phase of allowing players to explore stations as human avatars, as well as an update to ship turret models.[114] The fourteenth expansion, "Incursion", was released in stages, the second of which introduced the Sansha Incursions, in which Sansha's Nation invaded constellations, disrupting all forms of activity in the area, but provided large rewards for fighting back the incursions, and an overhaul of the character creation tool, paving the way for the Incarna expansion.[115]
 The eleventh expansion of Eve Online, "Apocrypha", was released on March 10, 2009, and introduced features such as further graphics updates as started in the Trinity expansion; the ability for players to group their vessels' weapons for easier interaction;[116] changes to autopilot routes and avoidance of player-defined star systems.[117] The twelfth expansion, "Dominion", was released on December 1, 2009, and overhauled the sovereignty system,[118] while the thirteenth expansion, "Tyrannis", released on May 26, 2010, added planetary interaction as well as the online platform "EVE Gate".[119]
 Over time, expansions have added features such as conquerable and constructible space stations, massive capital ships, advanced versions of existing ships, or Epic Arcs for players to master. Apocrypha included an overhauled probing system, wormholes, a smarter and tougher AI and customizable Tech 3 ships as its major features.[120] Dominion, which became available for download on December 1, 2009, included an overhaul of the sovereignty system, more realistic astronomy, the Titan Doomsday weapon functionality being changed and the in-game web browser with Chromium's WebKit. It also included a redesign of the UI and in-game mailing system. Tyrannis added new features, such as the ability to exploit planetary resources, a social networking program called EVE Gate, new technology for ships, and graphical updates.[citation needed]
 On May 6, 2014, at their yearly Fanfest convention, CCP announced the move from the current development cycle of two expansions per year, to ten feature-releases per year on a rapid release cycle. Senior Producer of EVE Online Andie Nordgren (CCP Seagull) stated that the move was necessary for future developments to have a more flexible release cycle, rather than the deadline imposed on the previous system, allowing smaller patches and changes to be deployed more rapidly and large projects not having to be rushed due to the expansion deadline.[121][122]
 However, in September 2015, Nordgren announced that CCP, while continuing with a five-week release cycle, would return to intermittently releasing large scale expansions for EVE.  She described the new release cycle as a "hybrid form", where expansions would be "a set of big, connected features" with no fixed number per year, while the feature-releases would continue to bring "quality of life changes, ship balance changes, visual upgrades" and other smaller additions.[123] The first new expansion, which was announced at EVE Vegas 2015, introduced a new modular starbase called a Citadel, and an overhaul of capital-class ships.[124]
 The developers have been working on a game feature to allow players to exit pods and interact with other player avatars in the communal setting of a station interior.[125]  The first iteration, called Captain's Quarters, was released on June 21, 2011.[126] The second iteration, with stylised quarters for each race, was released with the Crucible expansion.[127]  Player interaction in station environments has not yet been announced and only preliminary details have emerged.[citation needed]
 The ability to enter a planet's atmosphere and to interact with its surface is mentioned as one of the future development plans. The "Future Vision" trailer portrays Dreadnaught-class ships performing planetary orbital bombardments on Dust battlefields. The first glimpse trailer of Dust 514 ended with a ship exploding in the atmosphere.  However, the interaction between Dust and Eve has not been fully defined and no official confirmation has yet been provided.  At Eve Fanfest 2005, a working prototype was demonstrated in which a Caldari Crow interceptor could be seen navigating a makeshift landscape superimposed on a nebula.  However, this effort was later abandoned.  CCP has stated that full-scale integration of such features requires an enormous effort and is only planned for post-Revelations (expansion) production phases, 2006.[128]  Subsequently, it was stated that, until a proven in-game reason is found for planetary access, further work on this feature will not have a high priority.[citation needed]
 During the 2009 Games Convention in Cologne, Germany, CCP unveiled Dust 514, a console-based MMOFPS title meant to integrate with the Eve Online universe. According to developers, players hired by Eve Online alliances would fight over planets.[129]
 According to Eve Online Creative Director Torfi Frans Ólafsson, at the 2009 Eve Online Fanfest, the Dominion release is planned to involve sovereignty, the ownership of districts on planets that are capable of creating industry.[130] This player ownership system will be dynamic as it will be subject to PvP diplomacy as well as subjugation. The latter allows for linking with Dust 514, whereby players within Eve Online will be able to contract, in-game, Dust 514 players to take control of planetary regions within the Eve Online universe and these 'mercenaries' will then vie with other Dust 514 players hired by the opposing faction. The integration between the console MMOFPS game and the Eve Online MMORPG is both through community interaction and through the changeable battlefields based on the planetary architecture of a common universe –  the outcome of these battles in Dust 514 will affect the status and ownership of the corresponding planets in Eve Online as well. At E3 2011, it was announced that Dust 514 is a PlayStation 3 exclusive with a PS Vita tie-in, both to be released in early 2012.[citation needed]
 On May 20, 2011, decompiled EVE Online source code was published by an unknown person on a GitHub repository.[131] After being online for four days, CCP issued a DMCA take-down request which was granted by GitHub.[132]
 The Eve Online soundtrack was composed by Jón Hallur Haraldsson, also known as Real-X. Icelandic rap-rock group Quarashi also composed several tracks for the game. A digital soundtrack titled EVE Online: Original Soundtrack, Vol. 1 was released on iTunes on August 12, 2009. The soundtrack comes with an audio book track EVE Chronicle – Taught Thoughts. The soundtrack has since been removed from iTunes.[citation needed]
 The game itself contains an extensive in-game soundtrack. On December 4, 2012, the "Retribution" expansion of Eve Online was released. Among its features was the removal of the Jukebox, which enabled players to select their favorite songs to play.[133] In tandem with this, CCP Games announced that the entire game soundtrack (consisting of music in the game at the time) would be available to download for free from SoundCloud.[134] The soundtrack consists of 74 songs, with a running time of nearly seven hours.[citation needed]
 Piracy (in the ship-to-ship sense) is part of the game, as are protection racketeering, theft, and ransom.[135] Eve Online periodically has arisen for discussion within the wider gaming community as players of the game find various ingenious methods of scamming, deceiving, or attacking each other. One infamous example was a corporate infiltration and heist where one corporation infiltrated a target corporation over the course of nearly a year. They then performed a virtual assassination on the target's CEO and proceeded to steal corporate property to which they had gained access.  The target corporation lost billions of ISK worth of property (amounting to about US$16,500) and a great deal of prestige; the CEO's expensive ship and cybernetic implants were destroyed in the attack.[136] Events of this nature are debated both inside the game world and in the media.[137]
 In 2009, a player alliance known as Goonswarm was contacted by a disgruntled director of rival alliance Band of Brothers, one of the largest alliances in the game at that time. The defecting director then stripped Band of Brothers of a large quantity of assets including ships, money and territory, and disbanded the alliance.[138]
 A player-run bank known as 'EBank' was also involved in controversy in 2009 when Ricdic, the CEO of the bank, withdrew 200 billion ISK and converted it into real world currency worth about A$6,100 (US$4,586.47) to make a down payment on a home and pay for medical expenses.[139][140]
 Such dangers are an inherent part of Eve Online's virtual economy and thus are purposely not dealt with by the developers.[141] Players are expected to make financial decisions based (among other factors) on the possibility of other players' fiduciary malfeasance, much as in real-life economics.
 The most common acts of piracy in Eve are ganking, gate camping, and small gang warfare. Every pirate corporation in Eve Online has its own preferred activities, strategies, and tactics. Some utilize cheap but high damage ships to "suicide gank" and kill players in high-security space (where they should theoretically be safe) quickly before CONCORD law enforcement units arrive to destroy them (thus "suiciding"), in the knowledge that certain ships they destroy will be carrying valuable commodities or expensive gear capable of recovering the cost of the pirate vessels lost in the gank. Others choose to set up gate camping fleets consisting of varied ship types and roles capable of rapidly disabling and destroying any unwitting passersby, thereby locking down star systems and killing or robbing whoever tries to pass through. Other pirates choose to roam in very fast and versatile skirmish ships, such as interceptors, recons, or heavy assault ships, killing anyone they encounter in lightning attacks. On gaining the upper hand in a fight many pirates will ask the victim for an ISK ransom, in exchange for sparing their ship or escape pod.[citation needed]
 Suicide ganking has declined in overall popularity since the release of the Crucible expansion; while players may opt to insure their ships against loss using in-game currency, pilots will no longer be reimbursed if their ship is destroyed by CONCORD.  Such changes have been the subject of intense debate on the game's official forums, with opinions divided on whether or not players should be truly 'safe' while flying.[citation needed]
 Instances of developer misconduct in Eve Online have been substantiated, leading to debates and controversy. On February 9, 2007, a player known as Kugutsumen[142] hacked an enemy corporation's private forum to find out and reveal that Eve Online developer t20 had provided his corporation, Reikoku, with six valuable blueprints, giving them an advantage over competing corporations.[143] Some within the Eve Online community asked for t20's dismissal. While an apology letter was left for the community in the form of a dev blog, he remained an Eve Online developer until late 2008. Kugutsumen was permanently banned from the Eve Online universe for violating the game's terms of service and end-user license agreement by revealing t20's real name.[142]
 In response to public concerns, CCP decided to set up an internal affairs division whose responsibility is to monitor the activities of both privileged and player accounts operated by CCP staff in-game.[144]
 In part due to the matters above, CCP invited users to stand for the first Council of Stellar Management (CSM) in March 2008, resulting in 66 candidates seeking election to nine positions.[145][146][147][148] It was a requirement that candidates release their full real names in addition to stating their in-game details.[149] In May, after a two-week voting period, the first Council was elected, comprising seven men and two women; three each from the Netherlands and the United Kingdom, two from the US and one from Denmark, their ages ranging from 17 to 52.[148]
 The remit of the council has been changed since it was first proposed and is now seen by CCP primarily as a route for players to make requests for changes and improvements to the game mechanics, presentation, and game content of Eve Online. The first four Councils served for six months, after which new ones were to be elected.[147] Each individual was only permitted to serve twice. Each CSM gets the authority to put requests to CCP three times during their term of office which CCP have stated must be answered; once in person in Iceland and twice by e-mail, with most of the costs of their visit to Iceland being borne by CCP.[147] The rules were changed for the fifth CSM to feature one-year terms with two Iceland trips and four email requests, as well as the abolition of the two-term limit.[citation needed]
 The first meeting of the CSM with CCP took place in Reykjavik between June 19 and 23, 2008, and included not only the nine CSM members but a number of developers, designers, game masters, and producers from CCP and members of print and video media.[150] Matters discussed by players on the Eve Online forums were reviewed in detail and whilst some were rejected for technical reasons, many were accepted by CCP as useful improvements to the game which would be introduced either in an early so-called point release or added to the development plans for a future major update.[citation needed]
 Nominations for the second CSM opened on September 26, 2008, with voting commencing on November 9. The following third Council of Stellar Management included a modified age restriction: candidates under the age of 21 are then no longer eligible as CSM members.[151]
 Beginning with the eighth CSM, a single transferable vote system was introduced, allowing voters to select up to 14 candidates.[152]
 Users start playing Eve Online by creating a free account (known as an Alpha account), being invited to the game as via the game's Recruit-A-Friend program, or purchasing the Eve Online Special Edition retail box.[153] Alpha accounts are freely available through both the Eve Online website and the Steam content delivery system. The accounts through the Recruit-A-Friend program function identically to normal Alpha accounts, but start with some additional unallocated skill points (worth approximately 5–6 days worth of training time) on their first character. In addition, if the recruited account is subsequently converted to a subscription account (known as an Omega account), the referrer is rewarded with 30 free days of subscription time or the equivalent amount of PLEX.[citation needed]
 Alpha accounts are free and allow players to access most of the Eve Online game, with exceptions. Alpha players cannot train skills for some advanced ship types or modules, including all Tech 2 ships and most Tech 2 modules.  Alpha accounts can also only passive train up to 5 million skill points using the normal skill training system.  After that limit, the only way they can acquire more skill points is by converting to Omega, using skill point injectors (available on the in-game market), or skill point rewards, e.g. by the AIR Career Program or the Daily Skilling Spree.  In December, 2017, an additional item called a Daily Alpha Injector was added to the game, available for purchase via the official game store for PLEX.[154]  This item is only usable by Alpha accounts, is only usable once per day, awards a number of skill points roughly equal to a full 24 hours of training time, and costs slightly more than 1/30th as much as a 30-day subscription.  This allows Alpha accounts to progress at a similar rate and cost to an Omega account even beyond the 5 million skill point limit, but to purchase that progression in smaller increments than a full 30-day subscription.  Alpha accounts are limited to a specific list of skills and levels in those skills, and are unable to inject skill points into disallowed skills or beyond the maximum level in allowed skills.  Alpha accounts have approximately 20.5 million skillpoints worth of skills available to them.[citation needed]
 Omega accounts that lapse on their subscription are downgraded to Alpha accounts.  These accounts don't lose any skill points or skills, but any skills or skill levels beyond those allowed for an Alpha account are inactive and cannot be used to fulfill the prerequisites of modules or ships, nor do the passive effects of those skills take effect.  Functionally, Omega accounts that lapse into Alpha accounts can only access and use any of those skills that are on the Alpha skill list (and thus may not be able to fly ships that they could as an Omega pilot), but if they later upgrade back to Omega, they regain access to their full skill list.[citation needed]
 As of June 2008, Eve Time Codes (or ETCs) are available exclusively in 60-day increments. Before then, they were also offered in 30-, 50-, 90-, 100- and 120-day increments. Discontinued cards remain valid. Players using ETCs are treated like normal subscribers in every way. Eve Time Codes are available through CCP's online store as well as via online resellers.[155] Cards purchased through resellers are usually delivered through email for immediate use while codes issued through the Eve Online store are issued via postal mail or in-game item, and as such ETCs do not violate the EULA and can be bought and sold within the game.[156] There are no distinguishing differences in functionality between digital and hard-copy codes. Both provide the exact amount of specified game time, are entered into the same account section and can be exchanged between players for ISK using a secure exchange system facilitated by a "Timecode Bazaar" forum.[citation needed]
 In November 2008, CCP introduced PLEX, the CONCORD Pilots License EXtension, which is an in-game item that can be used to extend a subscription for 30 days.  PLEX can be purchased on the Eve Online website for real money, or inside the game for ISK.  60-day ETCs can be converted to two 30-day PLEX within the Eve Online client, which can then be sold via the in-game market. PLEX is the reason that the cost of in-game assets can be translated into real-life money. As the price of a PLEX is around €15, it can be calculated how much a certain ship or implant is theoretically worth in a real currency.[citation needed]
 In May 2017, CCP converted PLEX into smaller increments.  All existing PLEX were converted to 500 of the new PLEX (which are exclusively referred to as PLEX in-game, rather than Pilot License EXtension, though the acronym meaning remains unchanged).  30 days subscriptions now cost 500 PLEX, effectively the same cost as previously.  The previous premium currency, Aurum, was retired and converted to PLEX at the same exchange rate as PLEX could be exchanged for Aurum previously (1 old PLEX per 3500 Aurum, so 1 new PLEX per 7 Aurum), provided the account had at least 1000 Aurum to convert.  All items in the premium store that used to cost Aurum now cost an equivalent number of PLEX.  This allowed CCP to market more granular deals on PLEX, as well as price certain services for PLEX with more granularity.  For example, Multiple Character Training used to cost 1 PLEX, the same prices as 30 days of subscription time, but now costs 450 PLEX, 90% of the cost of 30 days of subscription time.  In the same patch, CCP also introduced the PLEX Vault, a safe and secure way of moving PLEX around in-game without risking the item being lost if one's ship were destroyed.[157]
 As of March 10, 2009, a boxed edition is available in shops. The distribution is being managed by Atari.[1] The boxed edition includes a 60-day ETC, instant standings update to facilitate quicker entry into factional warfare, exclusive online new player guide, and an exclusive expanded cargo hold shuttle. Although marketed as included in the retail box, the bonus items are only available to new accounts created with the 60-day ETC.[158]
 Eve Online received mixed reviews at the launch of the game in 2003 but more favorable reviews later in its lifecycle. The original version received 75% on GameRankings and 69/100 on Metacritic.[166][168] The Special Edition released in 2009 has an aggregate score of 88/100 on Metacritic.[169]
 In 2004, the Academy of Interactive Arts & Sciences nominated the expansion Eve Online: Second Genesis for "Massively Multiplayer/Persistent World Game of the Year" at the 7th Annual Interactive Achievement Awards.[178] Then for three consecutive years, the Academy also nominated the following expansions for "Massively Multiplayer Game of the Year": Revelations in 2007,[179] Trinity in 2008,[180] and Quantum Rise in 2009.[181]
 In 2013, PC Gamer placed Eve Online at #12 on their list of 100 Greatest Games of All Time.[182]
 In June 2013, the Museum of Modern Art added Eve Online to its permanent collection of video games. The game is showcased as a "day in the universe" video. To create this, CCP Games called upon Eve Online's large player-base to provide gameplay footage of the game. Furthermore, CCP Games contributed a large amount of data from its servers, which were compiled to produce a "stunning view" of the accomplishments of player collaborations.[183]
 In 2013, CCP said that they had made deals to create both a comic book and a television series based on Eve Online. The comic, titled Eve: True Stories, was released by Dark Horse Comics and made available for free online in 2014.[193] Following this, a physical version of the graphic novel containing bonus material was made available that same year. To make the television series, CCP signed a deal with Icelandic director Baltasar Kormákur. As of May 2013, no information has been given about the title or the premiere date of the television series. The storylines from both the graphic novel and the television series will be based on actual player-driven events that happened in the game.[194][195][196]


Source: https://en.wikipedia.org/wiki/World_in_Conflict
Content: World in Conflict is a 2007 real-time tactics video game developed by the Swedish video game company Massive Entertainment and published by Vivendi Games for Microsoft Windows. The game was released in September 2007, receiving generally favorable reviews and several awards.[3][4][5][6][7]  The game is considered by some to be the spiritual successor of Ground Control, another game by Massive Entertainment,[8] and is generally conceived by its designers to be a real-time tactics game, despite being marketed as a RTS game.[9]
 The game's setting and story takes place in an alternate 1989, in which an impending economic collapse and the failure to achieve aid diplomatically from the West, leads the Soviet Union to invade Western Europe, triggering World War III. The single-player story sees players assume the role of a United States Army officer who takes command of battalions of US and NATO forces; the main bulk of their operations focus on combating a surprise invasion of the United States from Seattle, Washington, as well as operations in Southern France, Russia, and New York.[10]
 A March 2009 expansion pack, World in Conflict: Soviet Assault, added additional content, including additional campaign missions in which players assume the role of a Soviet military officer who commands Soviet forces in Europe, Russia and the US state of Washington.[9][11][12]
 The game offers multiplayer functionality, supporting up to 16 players online or over LAN. In December 2015, Ubisoft shut down the official Massgate servers that supported multiplayer functions,[13] though the player community restored these functions in 2016, through an unaffiliated version of Massgate.[14] Ubisoft revived multiplayer through published open-source Massgate in 2017.[15][16][17][18]
 World in Conflict focuses on real-time tactics (RTT) gameplay, in a similar manner to Ground Control, a game also developed by Massive Entertainment,[8] in which players deploy units onto a battlefield and must carefully make use of them to achieve victory, making use of support assets to further assist them. World in Conflict contains three factions: the United States, Soviet Union, and NATO. While players may only play as US and NATO forces during the single-player campaign, all three factions can be used in multiplayer games.
 During a game, players are given a pre-determined number of reinforcement points, with which to purchase units with varying costs. Once the player deploys the units they purchase, they must wait 20 seconds for them to be airdropped to the field. If a unit is destroyed, the points are refunded to the player in order to allow them to bring in more units. During the single-player campaign, most missions vary what units the player can recruit, while some missions will offer the opportunity to recruit free units, though these cannot be replaced if destroyed. Each unit has strengths and weaknesses, such as mobile anti-air guns being most effective against enemy helicopters, and repair tanks being most effective at keeping vehicles and armor repaired. Each unit possesses a defensive ability, such as deploying smokescreens, while some units possess an offensive ability, such as marking targets for bombardment or using grenade launchers on enemy infantry. Once a unit's special ability(ies) has been spent, players must wait for them to recharge before they can be used again.
 In addition to controlling units, players may also call in tactical aid by spending tactical aid points. Points are primarily earned from destroying enemy units in battle. Tactical Aids allow the player to call in anything, from airstrikes on enemy positions, the deployment of paratroopers, to launching carpet bombing raids and tactical nuclear strikes. Tactical aids can allow up to three deployments, after which the player must wait until the support has recharged. In the single-player campaign, players are restricted by what tactical aid they can use, which can change during a mission.
 The game interface for World in Conflict has no framing in the game. A list of units occupies the bottom center, whereas the top right-hand corner contains the expandable reinforcement procurement list. The mini-map is in the bottom left-hand corner, while the bottom right-hand corner contains the special abilities buttons (including unit formation). Players can also use a messaging system that is designed to allow conversation between individuals regardless of whether they are on the same server or playing the same game. World in Conflict features a fully rotational 360-degree camera.
 The single-player campaign places players in the role of Lieutenant Parker (voiced by Alec Baldwin), a United States Army officer, who takes command of a company of troops from both the US and NATO and who narrates the events of the game's campaign prior to each mission; he neither speaks during missions and cutscenes nor is his face shown. During missions, players take on enemies scripted for them to deal with while the AI handles the remainder of action on the battlefield, though a large portion of the action is still focused on the player, which is in contrast to the approach used in RTS titles, in which players are in charge of whole armies and thus responsible for most of the action on the battlefield. Unlike other game modes, players are restricted in missions by what units they can deploy and what tactical aid they can call in, sometimes having to rely on the units they begin with and acquire during a mission.
 The narrative of the single-player story owes much of its inspiration from both the Call of Duty and Medal of Honor series (see the 'Influences' section below)
 Multiplayer games support up to sixteen players and can be played on a LAN or over the Internet. Three types of maps are featured: domination maps, where players must control command points to win the game, assault maps, where one team defends a series of command points which the other teams' assaults, and tug of war maps, where teams must fight to capture a series of command points on the front line, whereupon the line shifts towards a new set of points closer to the losing team. One side plays as either the United States or NATO, while the other is the Soviet Union.
 In multiplayer gameplay the player may choose one of four roles in battle: infantry, air, support, or armor. The infantry role gives access to various infantry squads such as anti-tank teams, snipers, and light transport vehicles whereas armor allows players to use various classes of tanks, the dominant direct fire land combat unit of the game. Players choosing the air role have access to attack, scout, and transport helicopters. Finally, the support role contains anti-air, artillery, and repair units. Each role's basic units can be purchased by everyone but are more expensive for players with a different role. In addition, each role has its own exclusive units that aren't available for purchase by other roles.
 The game ends when one side is completely dominant over the other, or when 20 minutes are up, in which case, whichever side is winning at the time is declared the winner. A bar is displayed at the top of the screen showing the status of both armies. After the game is over, the score sheet will be displayed, and the players' rank updated.
 The online component of the game uses the in-game massgate system, which is derived from Ground Control[citation needed]. The system helps players keep track of friends, allowing them to see whether they are online or playing a game. Clans can be created and kept track of in-game, with features such as ranks and clan matches. Massgate includes leaderboards and a ranking system based on US Army military ranks. Players can increase their rank and leaderboard position in a way similar to Battlefield 2, by accumulating earnings and scoring points, medals, and badges. Achieving higher ranks becomes progressively more difficult. The leaderboard also keeps track of clan rankings.
 In 1988, the economically crippled Soviet Union pursues military action against NATO. The United States deploys the bulk of its troops to reinforce Europe, but in doing so, is caught off guard when the Soviet Union invades the Northwestern United States, starting with Seattle in November 1989. Lieutenant Parker, the player's character, joins Captains Mark Bannon and James Webb in the retreat from the city under Colonel Jeremiah Sawyer, who held previous commands over all three. Under Sawyer's command, the Soviet advance is temporarily stalled, and the U.S. manages to win a tactical victory in the town of Pine Valley.
 By Christmas, Soviet troops launch a new offensive to capture Fort Teller, the headquarters for the Strategic Defense Initiative. Knowing that should the Soviets realize that the project is a ruse (thereby exposing America to a nuclear strike), Sawyer and his command are ordered to intercept, delay and ultimately halt the Soviet advance on the facility. At the garrison town of Cascade Falls, Sawyer leads an initially successful defense against the Soviets. However, he is soon informed the Soviets have deployed fresh reinforcements to the town, which are too many for his already under-strength forces to hold against. Sawyer is granted clearance to detonate a tactical nuclear missile above the Soviet formations, retreating just before impact. Bannon, however, volunteers to stay behind, knowing the Soviets will become suspicious. The nuclear blast annihilates the assaulting Soviet forces, as well as Bannon and his company.
 In the days following the outbreak of the war in Europe, Sawyer, as well as Bannon and Parker, are deployed to France to assist French forces in a counterattack against Soviet troops near Marseille. Bannon's arrogant lust for glory aggravates Sawyer, which inadvertently causes the death of Commandant Sabatier, Sawyer's French liaison, during a major operation. Sawyer is then deployed alongside Norwegian special forces on a raiding operation inside the Soviet Union. Bannon, however, sullies the mission by accidentally killing surrendering civil-defense volunteers and is unable to prevent the escape of a Soviet Typhoon-class submarine at berth near Murmansk. Fed up with Bannon's incompetence, Sawyer arranges for his transfer out of his battalion and replaces him with Webb, who has been serving as an instructor due to a sustained combat injury. Prior to rotating back home, Sawyer and his battalion are rerouted to New York City to assist U.S. Army Rangers in recapturing Ellis, Governors and Liberty Islands from Spetsnaz commandos.
 Returning to the immediate aftermath of the nuclear detonation, Parker contacts Webb and several friendly and hostile stragglers before reuniting with Sawyer. They also learn that China has entered the war, siding with the Soviets, and has sent a fleet to reinforce the Soviet beachhead in Seattle. The U.S. President orders Sawyer's forces to retake Seattle before the Chinese can land, but also plans to destroy the Chinese fleet and the city of Seattle with a nuclear missile should Sawyer fail. After successfully penetrating through the Soviet perimeter, U.S. forces successfully re-capture the city. The Chinese, lacking the amphibious means necessary for a landing, turn back.
 In December 2015, the official Massgate servers were shut down by Massive Entertainment[13] for several reasons, despite a community outcry.[19]
 In early 2016, online multiplayer functionality and a new community-run Massgate were restored by a group of players unaffiliated with Ubisoft or Massive Entertainment.[14]
 The game's designers have cited the 1984 film Red Dawn as one of their key influences.[20] The film's main premise is the invasion of America by Soviet and Central American troops. Echoes of the film can be seen in the initial paratroop landings (though in the film they happen in Colorado) and in the use of civilian transports to disguise a Soviet invasion force; again, this differs slightly from the film. Also, in the Soviet Assault expansion, the name of the Soviet invasion of Germany (and presumably the United States) is referred to as Operation Red Dawn.
 Another influence for the game, according to issue 7 of the WiC Journal, are the first-person shooter game series Call of Duty and Medal of Honor, and how the games give the player a relatively small role in a big conflict and will command small numbers of units at a time rather than whole hordes. The developers, still according to the journal, have also looked to the games Battlefield 2 and Counter-Strike: Source for inspiration.
 The collector's edition of World in Conflict comes in a limited edition collector's box art cloth packaging (with a Soviet flag on one side and Russian wording of "World in Conflict", and the US flag on the other with English "World in Conflict") and includes an authentic piece of the Berlin Wall,[21] Modern Marvels: The Berlin Wall DVD by the History Channel, Behind the Scenes DVD and World in Conflict exclusive Creative HS-390 headset (Europe Only).[22] Those who had preordered the game were given access to the Beta, the ability to preserve their username and clans, and either received the Modern Marvels: Strategic Air Command or the Declassified: The Rise and Fall of The Wall DVD by the History Channel depending upon which area of the world one was situated in.[23]
 The collector's edition in Poland is different compared to collector's editions in other countries. It includes an exclusive World in Conflict wooden container, limited edition collector's box art packaging (Soviet or US flag), a full-sized flag of the US or Soviet Union, an exclusive World in Conflict poster, a T-shirt and cap with the World in Conflict logo and decorations, and a World in Conflict exclusive Trust Hs-2200 headset.[24]
 The collector's edition available in Taiwan is also different, as there was no preorder scheme put into place there. It includes an exclusive flag of the Soviet Union, a Modern Marvels: Strategic Air Command DVD by the History Channel, Special translated behind the scenes DVD, Metallic packaging featuring the Soviet flag on the front, and the US flag on the back.[25][26]
 The game was re-released under World in Conflict Complete Edition including the new expansion Soviet Assault all in one game.[27]
 World in Conflict received "generally favorable reviews" from game critics according to the review aggregator Metacritic.[28] GameSpot called the game "the studio's masterwork", giving it 9.5 out of 10.[5] In PC Gamer (UK), Kieron Gillen singled out World in Conflict's multiplayer, praising the cooperative mechanics and the in-game communication system; while calling the single-player campaign "fun[...] but not exactly deep."[31] Dan Whitehead of Eurogamer called the game "absorbing", highlighting the game's focus on tactical objectives instead of resource management, saying "it plays like a strategy game, but feels like an action game".[29]
 Prior to its initial release in September 2007, World in Conflict received several awards from its E3 presentation in 2007.
 After release, the game earned editor's choice awards from GameSpot, IGN[6] and the Australian gaming magazine PC PowerPlay, as well as PC Zone's classic award.[7] PC Gamer US also awarded the game its editor's choice award, as well as naming it the 2007 RTS game of the year. During the 11th Annual Interactive Achievement Awards, World in Conflict received nominations for "Strategy/Simulation Game of the Year" and "Outstanding Achievement in Online Gameplay" by the Academy of Interactive Arts & Sciences.[38] The game was included in the book 1001 Video Games You Must Play Before You Die.
 It topped weekly sales charts in North America, Germany, and Australia in the week it was released.[3]
 A new expansion of the game, World in Conflict: Soviet Assault,[39] was released for Windows in March 2009.[40] Plans to release the game under the same name for home consoles, the Xbox 360 and the PlayStation 3, were dropped.[41] The new edition included a brand new campaign from the Soviet perspective. New maps were included as well as new movies and cut scenes, however there were no new units included.[42]
 On July 29, 2008, Activision dropped World in Conflict: Soviet Assault from production along with a number of other games putting the future of the game in question.[43] On August 6, 2008, Activision Blizzard put  Massive Entertainment up for sale.[44] Massive Entertainment has since been acquired by Ubisoft. The game was released on March 13, 2009, in several formats. It was packaged under World in Conflict: Complete Edition which is the new retail collection, containing both World in Conflict and the expansion, Soviet Assault. The complete pack was available through retail stores, Steam download and Direct2Drive download. Soviet Assault was also released separately as a download for owners of the original World in Conflict, through Steam and D2D and also in a retail version.


Source: https://en.wikipedia.org/wiki/Civilization_IV
Content: Civilization IV (also known as Sid Meier's Civilization IV) is a 4X turn-based strategy computer game and the fourth installment of the Civilization series, and designed by Soren Johnson under the direction of Sid Meier and his video game development studio Firaxis Games. It was released in North America, Europe, and Australia, between October 25 and November 4, 2005, and followed by Civilization V.
 Civilization IV uses the 4X empire-building model for turn-based strategy gameplay, in which the player's main objective is to construct a civilization from limited initial resources. Most standard full-length games start the player with a settler unit and/or a city unit in the year 4000 BC. As with other games in the series, there are by default five objectives the player can pursue in order to finish the game: conquering all other civilizations, controlling a supermajority of the game world's land and population, building and sending the first sleeper ship to the Alpha Centauri star system, increasing the "Culture ratings" of at least three different cities to "legendary" levels, or winning a "World Leader" popularity contest by the United Nations. If the time limit for the game is reached and none of the previous goals has been fulfilled by any players including game AI players, the civilization with the highest total game score is declared winner. A large departure from earlier Civilization games is a new graphics engine created from scratch, based on the Gamebryo engine by Numerical Design Limited (NDL).
 The game has received critical acclaim and was hailed as an exemplary product of one of the leading video game producers in the turn-based strategy genre, and has been listed as one of the best video games of all time. Civilization IV sold over 3 million copies by 2008 and won multiple awards, including several Game of the Year awards. Its title song, "Baba Yetu", was the first piece of video game music to win a Grammy Award. Two major expansions were released, Civilization IV: Warlords and Civilization IV: Beyond the Sword, as well as the stand-alone expansion pack Civilization IV: Colonization, which were all combined in 2009 into one release edition titled Sid Meier's Civilization IV: The Complete Edition.
 Civilization IV follows some of the 4X model of turn-based strategy games, a genre in which players control an empire and "explore, expand, exploit, and exterminate", by having the player attempt to lead a modest group of peoples from a base with initially scarce resources into a successful empire or civilization.[3][4] The condition for winning the game is accomplished through one of the five ways: militarily defeating all other civilizations in the game world, controlling over two-thirds of the game world's land and population, building the first spaceship in the Space Age and sending it to Alpha Centauri, having the most dominant Culture ratings over other civilizations, or becoming "World Leader" through the United Nations votes.[5] Additionally, there are multiple game scores for each civilization throughout the game based on the actions of each civilization and a number of different factors, allowing for a win condition based on the total of these points if the game timer runs out. The game can be played in multiple modes: as a single player facing against one or more computer-controlled opponents, in hot seat mode, or through online multiplayer games.[6]
 As with other turn-based strategy video games, the player can customize the look and feel of their game world as well as the difficulty of any game AI players before the game starts. Each map space has a terrain type, such as plains, tundra, or desert, that affects the available resources players can extract from their environments and the movements of certain units through that terrain. The player is then given a total of 18 different civilizations to choose from, each with their own pros and cons, plus a leader avatar, an initial set of civilization technology, and any units unique to that civilization. When the game starts, however, it chooses random locations to place across a predefined square grid map. Like other strategy games, Civilization IV has a fog of war feature, in which unexplored territory remains darkened and territories without any units stationed on its designated square is shaded with darker colors.[7]
 Most units that the player can generate and use are military units, with certain attributes such as combat strength and movement rate particular to each military type. Each unit can gain experience through combat, which later translates into promotions that the player can use to assign military units new bonuses.[5] Initially, most combat takes place on land, but further advancements in the game's technology tree can allow the player to build ships and planes with which to fight battles on sea and in the air. Any number of units can be stacked onto a single space and move as a group if so assigned, but the overall combat phase is resolved by one-on-one unit battles.
 Combat is initiated when moving military units are moved onto the square occupied by an opposing force's military units and cities, and combat is then resolved with calculating statistics of each unit combined with some random chance.  Defeated units are removed from the game (apart from workers and settlers, which are captured by the attacking force), and any attacking units that are able to defeat the last defending military unit on a space will move to occupy that space.  If the space is occupied by a city the player may choose to occupy and capture the city as their own or raze it. Other than combat, military units can also be assigned to fortify a specific space, perform sentry duties, destroy enemy city improvements, or explore the game world.
 Non-military units include settlers who are used to found cities, workers who are used to improve space and gather resources, spies who can perform counterintelligence and espionage, and religious missionaries who can be sent to convert different civilizations and cities. Also, with the Beyond the Sword expansion, new units are added such as executives, who can spread corporations to new cities. Throughout the game, players may also generate a special unit called a "Great Person". These are named after historical figures and can be used for one-time advantage boosts in various ways; examples include Great Engineers who increase overall production levels and Great Scientists who improve technology. Assigning inhabitants of cities to work as 'specialists' (scientists, engineers, artists, spies, merchants or priests), building certain wonders or discovering certain technologies can improve the rate at which Great People are generated.  Great Generals are generated when a player's forces achieve a certain number of victories, and can be used either to give a small amount of extra experience to all units trained in a city, or a very high level of experience to one unit.  However, like other units, Great People of all kinds can be attacked and killed before the players can use them.[6]
 Once a city is founded, it will automatically begin extracting resources from surrounding spaces; the amount of spaces it can extract from is determined by the city's population size. The game automatically allocates the spaces a city uses and how its resources are maintained as the city grows, but the player is free to manage the city directly. This feature can be utilized to turn a part of the population into one of several specialized occupations; at the cost of having one less space from the city, these specialists increase gathering and production of the resources of their targeted specializations.
 Each city can only produce one military unit or one building at a time; any additional units or buildings are placed into a queue. The rate of construction is determined by the amount of material collected from the surrounding spaces; players can also choose to speed production by sacrificing gold or population if they adopt the required governmental policy; called civics. The player can instead specialize the city towards gathering a particular resource instead of constructing additional units or buildings. Also, in order to produce some units or buildings, certain resources must be collected within the empire and connected to the empires trade network by roads or harbours (for example, horses are needed for mounted units, and iron for swords, while stone or marble increase production of certain Wonders.).
 Buildings perform any number of different functions depending on building type; for example, early buildings such as granaries improve food storage to boost the city's growth and barracks produce better military units, while later buildings such as factories increase general production levels. There are also a number of unique buildings throughout the game.  Most notable are World Wonders, which can be accessed through research nodes in the technology tree and construction through the worker unit. World Wonders provide advantages that are unique to each civilization, as they are limited to only one or two players.
 Through buildings and specialists, each city also generates the "Culture" resource that contributes towards both the area upon which the city can influence for extracting resources and the overall civilization's cultural value. When two cities of different civilizations are adjacent to each other, the culture values of each city influences the space they can control; it is even possible that a city close to another civilization's city will join that civilization if their culture is strong enough. The high levels of culture gathering and attainment are also one of the default conditions that can be used to win the game.[5]
 Once the first city has been founded, the player can select their first technology node from the game's technology tree to research, once the required number of research points have been accumulated, the technology is obtained. As with other types of technology trees, more advanced technology nodes require the research of other previous technology nodes.  The player can also select a future technology and immediately place into the game queue any technology nodes between the current technology level and the specified technology node. Technology can also determine another win condition; several endgame technology nodes are required to develop a colony ship to reach Alpha Centauri.[5]
 Within the technology tree are technology nodes relating to government civics and state religions, each with their own pros and cons. The player has the option of selecting which set of government civics or state religions that the player wants their civilization to follow, but not all civics and religions can be encompassed.
 Once the player has formally met another civilization, they can perform diplomacy at any time. For example, if the two civilizations are on friendly terms, the player can ask to trade units and/or technology for gold and vice versa, or request opening of national borders in order to freely explore in the other civilization's territories. The player can also use the diplomacy menu to request help in a war against a third civilization, or formally declare war on any civilization in which they engage diplomacy.
 Religion plays a much more important role in Civilization IV than in the previous installments of the franchise. Impacting many of the game's key mechanics like government civics and diplomatic relations, the game's new religious system, according to Firaxis Games producer Barry Caudill, was added to increase gameplay depth over the entire game.[8] The game features seven religions (Buddhism, Christianity, Confucianism, Hinduism, Islam, Judaism, and Taoism) that are founded by the first civilization to research a certain technology which varies per religion. Religions can then be spread actively through the production of missionary units or passively through means such as trade routes. Religions may be spread to domestic and foreign cities and there is no limit to the number of religions that can be present in any single city. The player may only choose one state religion at a time and all seven offer identical advantages (e.g. cities with the state religion receive bonuses in happiness, and potentially in production, science, gold and even military unit experience points).
 Civilization IV is much more open to modification than its predecessors were. Game data and rules are stored in XML files, and a Software Development Kit was released in April 2006 to allow customization of game AI. The map editor supports Python.[6][9]
 The World Builder allows a player to either design a map from scratch or edit a preloaded template map as a starting point for a new game. The player can modify the map by placing and modifying any number of rivers, landmasses, mountains, resources, units, and cities, as well as their attributes such as Culture generation. The World Builder for Civilization IV is in-game, in contrast to previous Civilization games where the Map Editor was an external application.[10]
 More game attributes are stored in XML files, which can be edited with an external text editor or application. On September 20, 2005, Firaxis Games senior producer Barry Caudill stated that "[e]diting these files will allow players to tweak simple game rules and change or add content. For instance, they can add new unit or building types, change the cost of wonders, or add new civilizations. Players can also change the sounds played at certain times or edit the play list for your soundtrack."[10]
 The Civilization IV software development kit was released on April 13, 2006, to coincide with the release of the v1.61 patch. The kit allows players to view, modify, or even completely re-write the game's DLL source code, enabling the modification of the game's AI and other integral parts of the game that were previously not accessible.[11]
 The game engine for Civilization IV was built entirely from scratch,[12] with some help from NDL's Gamebryo engine.[2] This decision resulted in a full 3D immersion of the game, which was the first in the series,[13] and which allowed easier readability[5] and smoother, more in-depth zooming capabilities.[4] Some of the quotes relating to the technology tree and narration for the game were provided by Leonard Nimoy.[5][7] The soundtrack for the game features compositions of Medieval, Renaissance, and Baroque origin,[1] and design for the title music was influenced by The Lion King.[1][7] Composer Christopher Tin wrote the opening theme song  "Baba Yetu" (Swahili "Our Father"), a rendition of the Lord's Prayer, which was performed by Stanford University's Talisman A Cappella. The song, when rereleased, became the first piece of video game music to be nominated for and to win a Grammy Award.[14]
 Sid Meier's Civilization IV also released some bonus content, mainly to show modding capabilities:
 The game had a viral marketing campaign, revolving around a fictitious self-help organization known as Civilization Anonymous (shortened to CivAnon), the intention being to satirise how addictive the game was. With the slogan "No More Turns", the premise was the following: "Rumors have begun to circulate that the newest edition of the "One More Turn" franchise is on its way. STAY AWAY from this game at all costs. You will likely be powerless to its extreme addictive properties once exposed".[21]  Various characters were created, and their scenarios were included in various trailers showing the "inside [of] a Civanon meeting for [Civilization] addicts,"[22] the first of which being played during E3 2005 once an hour at the 2K Games booth.[23] These "video testimonials of supposedly recovering Civilization addicts"[24] also featured cameos by Sid Meier. In addition to this, an "official" website was created by 2K Games with extra content. The Civilization Anonymous campaign was brought back for the following game Civilization V. Break described the campaign as "hilarious",[22] while Kotaku described it as a "great promotional campaign" that "comes across as terrifyingly realistic".[25] Destructoid shared this view, saying the support group campaign is "a clever marketing tool", but wishing it existed as "we all know there really are people who suffer from one-more-turn-itis".[26] VantureBeat said the campaign was "incredibly clever and funny", adding "what made it so powerful was not the near-flawless execution and fine detail; it was the fact that it could have been real". 'Ctrl-Alt-Play: Essays on Control in Video Gaming' noted the spoof highlighted the series' "hyper-addictive turn-based gameplay".[24] As part of the Civilization V version of the campaign, 2K Games asked Civilization addicts to submit stories via video in order to win $2,500 and a "framed, limited edition Civilization Anonymous poster signed by Firaxis Games".[26] The 2010 version was "updated for modern concerns, like the franchise's broader audience and iPhone editions of the game acting as a gateway drug."[27]
 Civilization IV was published for the Windows and Mac OS X platforms. Aspyr released the Mac OS X version on June 26, 2006,[28] and a Mac digital version was released January 2010 on gameagent.com.[29] Users who quickly purchased the game after initial release reported having problems playing the game on particular sets of video drivers, which were later resolved by a Firaxis patch, according to GameSpy.[9] By the end of March 2008, the game sold over three million units.[30] Firaxis Games has also published two expansion packs for Civilization IV, entitled Civilization IV: Warlords and Civilization IV: Beyond the Sword. A remake of the original 1994 Sid Meier's Colonization, rebuilt with Civilization IV's game engine and titled Civilization IV: Colonization, was released for Mac and PC as a standalone game and later bundled with the two expansions as Civilization IV: The Complete Edition.[31]
 Civilization IV was a commercial success, and sold more than 1 million units by mid-March 2006. By that time, it had held a top-10 position on every weekly computer game sales chart released by The NPD Group since the game's launch.[32] NPD declared Civilization IV the 11th-best-selling computer game of 2005,[33] and it rose to ninth place on the firm's annual computer game sales chart for the following year.[34] It returned to NPD's year-end top 20 in 2008 with a 13th-place finish.[35] The game also received a "Silver" sales award from the Entertainment and Leisure Software Publishers Association (ELSPA),[36] indicating sales of at least 100,000 copies in the United Kingdom.[37] According to Take-Two Interactive, sales of Civilization IV surpassed 3 million units by March 2008.[38]
 Civilization IV received universal critical acclaim with an aggregate score of 94 on Metacritic[39] and an aggregate score of 93.36% on GameRankings. Critics such as GameSpot's Andrew Park praised not only the improvements in the turn-based strategy genre that carried over from other installments in the series, but also noted the game's more advanced 3D computer graphics, difficult AI opponents, and multiplayer feature.[7] The Times praised the improvements of the 3D engine used to generally build the game,[44] and GameGuru appreciated its ability to zoom onto individual squares.[4] IGN commented that the game AI was much more intelligent than it was in Civilization III,[5] which designer Soren Johnson stated was thanks to their "secret pre-beta group" testing the game before release.[1] Game Revolution remarked that "the multiplayer... actually works this time"[42] and GameSpot added that both "single-player and multiplayer options have been improved in Civ IV."[7]
 Though it had received mostly positive feedback, video game critics pointed out functions in the game which could have been improved to enhance their enjoyment of the game. Steve Butts from IGN suggested that in-game announcements about enemy achievements should be displayed more prominently, and that the multiplayer feature could have included "an option to save my username and password."[5] Likewise, GameSpy's Dave Kosak pointed out other flaws of the multiplayer interface, stating that it "is a little rough in places. The built-in server browser, for instance, will keep jumping around the list as you try to find a game."[9] GameSpot also noted the lack of a "religion-based victory condition", which downplayed the influence that the revamped religion system was supposed to have on the game.[7]
 In August 2016, Civilization IV placed 11th on Time's The 50 Best Video Games of All Time list.[45]
 Civilization IV won multiple awards at various events and gaming websites.[46]
 The editors of Computer Games Magazine named Civilization IV the best computer game of 2005, and presented it with their awards for "Best Strategy Game", "Best AI" and "Best Interface". They wrote, "It's a towering achievement, even for a series as revered as this one. Even for long-time fans of the series, playing Civilization IV is like discovering it for the very first time."[61] Civilization IV also won PC Gamer US's "Best Turn-Based Strategy Game 2005" award. The magazine's Dan Stapleton called it "a huge facelift to a winning formula." It was nominated as PC Gamer US's "Best Multiplayer Game 2005" and overall game of the year, but lost in these categories to Battlefield 2.[62]


Source: https://en.wikipedia.org/wiki/100-Dollar-Laptop
Content: Other reasons this message may be displayed:


Source: https://en.wikipedia.org/wiki/Grafische_Benutzeroberfl%C3%A4che
Content: Other reasons this message may be displayed:


Source: https://en.wikipedia.org/wiki/Quellcode
Content: Other reasons this message may be displayed:


Source: https://en.wikipedia.org/wiki/Raspberry_Pi
Content: 
 Raspberry Pi (/paɪ/) is a series of small single-board computers (SBCs) developed in the United Kingdom by the Raspberry Pi Foundation in association with Broadcom. Since 2013, Raspberry Pi devices have been developed and supported by a subsidiary of the Raspberry Pi Foundation, now named Raspberry Pi Ltd.[3] The Raspberry Pi project originally leaned toward the promotion of teaching basic computer science in schools.[4][5][6] The original model became more popular than anticipated,[7] selling outside its target market for diverse uses such as robotics, home and industrial automation, and by computer and electronic hobbyists, because of its low cost, modularity, open design, and its adoption of the HDMI and USB standards.
 After the release of the second board type, the Raspberry Pi Foundation set up a new entity, named Raspberry Pi (Trading) Ltd, and installed Eben Upton as CEO, with the responsibility for developing their computers.[8] The Foundation was rededicated as an educational charity for promoting the teaching of basic computer science in schools and developing countries. Most Raspberry Pis are made in a Sony factory in Pencoed, Wales,[9] while others are made in China and Japan.[10][11]
 In 2015, the Raspberry Pi surpassed the ZX Spectrum in unit sales, becoming the best-selling British computer.[12]
 In 2021, Raspberry Pi (Trading) Ltd changed its name to Raspberry Pi Ltd.[13]
 There are three series of Raspberry Pi, and several generations of each have been released. Raspberry Pi SBCs feature a Broadcom system on a chip (SoC) with an integrated ARM-compatible central processing unit (CPU) and on-chip graphics processing unit (GPU), while Raspberry Pi Pico has a RP2040 system on chip with an integrated ARM-compatible central processing unit (CPU).
 
 As of 4 May 2021, Raspberry Pi is committed to manufacture most Pi models until at least January 2026. Even the 1 GB Pi 4B can still be specially-ordered.[44]
 The Raspberry Pi hardware has evolved through several versions that feature variations in the type of the central processing unit, amount of memory capacity, networking support, and peripheral-device support.
 This block diagram describes models B, B+, A and A+. The Pi Zero models are similar, but lack the Ethernet and USB hub components. The Ethernet adapter is internally connected to an additional USB port. In Model A, A+, and the Pi Zero, the USB port is connected directly to the system on a chip (SoC). On the Pi 1 Model B+ and later models the USB/Ethernet chip contains a five-port USB hub, of which four ports are available, while the Pi 1 Model B only provides two. On the Pi Zero, the USB port is also connected directly to the SoC, but it uses a micro USB (OTG) port. Unlike all other Pi models, the 40 pin GPIO connector is omitted on the Pi Zero, with solderable through-holes only in the pin locations. The Pi Zero WH remedies this.
 Processor speed ranges from 700 MHz to 1.4 GHz for the Pi 3 Model B+ or 1.5 GHz for the Pi 4; on-board memory ranges from 256 MB to 8 GB random-access memory (RAM), with only the Raspberry Pi 4 and the Raspberry Pi 5 having more than 1 GB. Secure Digital (SD) cards in MicroSDHC form factor (SDHC on early models) are used to store the operating system and program memory, however some models also come with onboard eMMC storage[45] and the Raspberry Pi 4 can also make use of USB-attached SSD storage for its operating system.[46] The boards have one to five USB ports. For video output, HDMI and composite video are supported, with a standard 3.5 mm tip-ring-sleeve jack carrying mono audio together with composite video. Lower-level output is provided by a number of GPIO pins, which support common protocols like I²C. The B-models have an 8P8C Ethernet port and the Pi 3, Pi 4 and Pi Zero W have on-board Wi-Fi 802.11n and Bluetooth.[47]
 The Broadcom BCM2835 SoC used in the first generation Raspberry Pi[48] includes a RISC-based 700 MHz 32-bit ARM1176JZF-S processor, VideoCore IV graphics processing unit (GPU),[49] and RAM. It has a level 1 (L1) cache of 16 KB and a level 2 (L2) cache of 128 KB. The level 2 cache is used primarily by the GPU. The SoC is stacked underneath the RAM chip, so only its edge is visible. The ARM1176JZ(F)-S is the same CPU used in the original iPhone,[50] although at a higher clock rate, and mated with a much faster GPU.
 The earlier V1.1 model of the Raspberry Pi 2 used a Broadcom BCM2836 SoC with a 900 MHz 32-bit, quad-core ARM Cortex-A7 processor, with 256 KB shared L2 cache.[51] The Raspberry Pi 2 V1.2 was upgraded to a Broadcom BCM2837 SoC with a 1.2 GHz 64-bit quad-core ARM Cortex-A53 processor,[14] the same one which is used on the Raspberry Pi 3, but underclocked (by default) to the same 900 MHz CPU clock speed as the V1.1. The BCM2836 SoC is no longer in production as of late 2016.
 The Raspberry Pi 3 Model B uses a Broadcom BCM2837 SoC with a 1.2 GHz 64-bit quad-core ARM Cortex-A53 processor, with 512 KB shared L2 cache. The Model A+ and B+ are 1.4 GHz[52][53][54]
 The Raspberry Pi 4 uses a Broadcom BCM2711 SoC with a 1.5 GHz (later models: 1.8 GHz) 64-bit quad-core ARM Cortex-A72 processor, with 1 MB shared L2 cache.[55][56] Unlike previous models, which all used a custom interrupt controller poorly suited for virtualisation, the interrupt controller on this SoC is compatible with the ARM Generic Interrupt Controller (GIC) architecture 2.0, providing hardware support for interrupt distribution when using ARM virtualisation capabilities.[57][58] The VideoCore IV of the previous models has also been replaced with a VideoCore VI running at 500 MHz.
 The Raspberry Pi Zero and Zero W use the same Broadcom BCM2835 SoC as the first generation Raspberry Pi, although now running at 1 GHz CPU clock speed.[59]
 The Raspberry Pi Zero 2 W uses the RP3A0-AU, which is a System-in-Package (SiP) design. The package contains a Broadcom BCM2710A1 processor, which is a 64-bit quad-core ARM Cortex-A53 clocked at 1 GHz, along with 512 MB of LPDDR2 SDRAM layered above.[60][61] The Raspberry Pi 3 also uses the BCM2710A1 in its Broadcom BCM2837 SoC, but clocked at a higher 1.2 GHz.
 The Raspberry Pi Pico uses the RP2040,[62] a microcontroller containing dual ARM Cortex-M0+ cores running at 133 MHz, 6 banks of SRAM totaling 264 KB, and programmable IO for peripherals.[63]
 The Raspberry Pi 5 uses the Broadcom BCM2712 SoC, which is a chip designed in collaboration with Raspberry Pi. The SoC features a quad-core ARM Cortex-A76 processor clocked at 2.4 GHz, alongside a VideoCore VII GPU clocked at 800 MHz. The BCM2712 SoC also features support for cryptographic extensions for the first time on a Raspberry Pi model. Alongside the new processor and graphics unit, the monolithic design of the earlier BCM2711 has been replaced with a CPU and chipset (southbridge) architecture, as the IO functionality has been moved to the Raspberry Pi 5's custom RP1 chip.[64]
 While operating at 700 MHz by default, the first generation Raspberry Pi provided a real-world performance roughly equivalent to 0.041 GFLOPS.[65][66] On the CPU level the performance is similar to a 300 MHz Pentium II of 1997–99. The GPU provides 1 Gpixel/s or 1.5 Gtexel/s of graphics processing or 24 GFLOPS of general purpose computing performance. The graphical capabilities of the Raspberry Pi are roughly equivalent to the performance of the Xbox of 2001.
 Raspberry Pi 2 V1.1 included a quad-core Cortex-A7 CPU running at 900 MHz and 1 GB RAM. It was described as 4–6 times more powerful than its predecessor. The GPU was identical to the original.[51] In parallelised benchmarks, the Raspberry Pi 2 V1.1 could be up to 14 times faster than a Raspberry Pi 1 Model B+.[67]
 The Raspberry Pi 3, with a quad-core Cortex-A53 processor, is described as having ten times the performance of a Raspberry Pi 1.[68] Benchmarks showed the Raspberry Pi 3 to be approximately 80% faster than the Raspberry Pi 2 in parallelised tasks.[69]
 The Raspberry Pi 4, with a quad-core Cortex-A72 processor, is described as having three times the performance of a Raspberry Pi 3.[17]
 Most Raspberry Pi systems-on-chip can be overclocked to various degrees utilising the built in config.txt file in the boot sector of the Raspberry Pi OS. Overclocking is generally safe and does not automatically void the warranty of the Raspberry Pi; however, setting the "force_turbo" option to 1 bypasses voltage and temperature limits and voids the users warranty.[70] In Raspberry Pi OS the overclocking options on boot can also be made by a software command running "sudo raspi-config" on Raspberry Pi 1, 2, and original 3B without voiding the warranty.[71] In those cases the Pi automatically shuts the overclocking down if the chip temperature reaches 85 °C (185 °F); an appropriately sized heat sink is needed to protect the chip from thermal throttling.
 Newer versions of the firmware contain the option to choose between five overclock ("turbo") presets that, when used, attempt to maximise the performance of the SoC without impairing the lifetime of the board. This is done by monitoring the core temperature of the chip and the CPU load, and dynamically adjusting clock speeds and the core voltage. When the demand is low on the CPU or it is running too hot, the performance is throttled, but if the CPU has much to do and the chip's temperature is acceptable, performance is temporarily increased with CPU clock speeds of up to 1.1 GHz, depending on the board version and on which of the turbo settings is used.
 The overclocking modes are:
 In the highest (turbo) mode the SDRAM clock speed was originally 500 MHz, but this was later changed to 600 MHz because of occasional SD card corruption. Simultaneously, in high mode the core clock speed was lowered from 450 to 250 MHz, and in medium mode from 333 to 250 MHz.
 The CPU of the first and second generation Raspberry Pi board did not require cooling with a heat sink or fan, even when overclocked, but the Raspberry Pi 3 may generate more heat when overclocked.[73]
 The early designs of the Raspberry Pi Model A and B boards included only 256 MB of random access memory (RAM). Of this, the early beta Model B boards allocated 128 MB to the GPU by default, leaving only 128 MB for the CPU.[74] On the early 256 MB releases of models A and B, three different splits were possible. The default split was 192 MB for the CPU, which should be sufficient for standalone 1080p video decoding, or for simple 3D processing. 224 MB was for Linux processing only, with only a 1080p framebuffer, and was likely to fail for any video or 3D. 128 MB was for heavy 3D processing, possibly also with video decoding.[75] In comparison, the Nokia 701 uses 128 MB for the Broadcom VideoCore IV.[76]
 The later Model B with 512 MB RAM, was released on 15 October 2012 and was initially released with new standard memory split files (arm256_start.elf, arm384_start.elf, arm496_start.elf) with 256 MB, 384 MB, and 496 MB CPU RAM, and with 256 MB, 128 MB, and 16 MB video RAM, respectively. But about one week later, the foundation released a new version of start.elf that could read a new entry in config.txt (gpu_mem=xx) and could dynamically assign an amount of RAM (from 16 to 256 MB in 8 MB steps) to the GPU, obsoleting the older method of splitting memory, and a single start.elf worked the same for 256 MB and 512 MB Raspberry Pis.[77]
 The Raspberry Pi 2 has 1 GB of RAM.
 The Raspberry Pi 3 has 1 GB of RAM in the B and B+ models, and 512 MB of RAM in the A+ model.[78][79][80] The Raspberry Pi Zero and Zero W have 512 MB of RAM.
 The Raspberry Pi 4 is available with 1, 2, 4 or 8 GB of RAM.[81] A 1 GB model was originally available at launch in June 2019 but was discontinued in March 2020,[41] and the 8 GB model was introduced in May 2020.[82] The 1 GB model returned in October 2021.[83]
 The Model A, A+ and Pi Zero have no Ethernet circuitry and are commonly connected to a network using an external user-supplied USB Ethernet or Wi-Fi adapter. On the Model B and B+ the Ethernet port is provided by a built-in USB Ethernet adapter using the SMSC LAN9514 chip.[84] The Raspberry Pi 3 and Pi Zero W (wireless) are equipped with 2.4 GHz WiFi 802.11n (150 Mbit/s) and Bluetooth 4.1 (24 Mbit/s) based on the Broadcom BCM43438 FullMAC chip with no official support for monitor mode (though it was implemented through unofficial firmware patching[85]) and the Pi 3 also has a 10/100 Mbit/s Ethernet port. The Raspberry Pi 3B+ features dual-band IEEE 802.11b/g/n/ac WiFi, Bluetooth 4.2, and Gigabit Ethernet (limited to approximately 300 Mbit/s by the USB 2.0 bus between it and the SoC). The Raspberry Pi 4 has full gigabit Ethernet (throughput is not limited as it is not funnelled via the USB chip.)
 The RPi Zero, RPi1A, RPi3A+[86] and RPi4 can be used as a USB device or "USB gadget", plugged into another computer via a USB port on another machine. It can be configured in multiple ways, such as functioning as a serial or Ethernet device.[87] Although originally requiring software patches, this was added into the mainline Raspbian distribution in May 2016.[87]
 Raspberry Pi models with a newer chipset can boot from USB mass storage, such as from a flash drive. Booting from USB mass storage is not available in the original Raspberry Pi models, the Raspberry Pi Zero, the Raspberry Pi Pico, the Raspberry Pi 2 A models, and the Raspberry Pi 2 B models with versions lower than 1.2.[88]
 Although often pre-configured to operate as a headless computer, the Raspberry Pi may also optionally be operated with any generic USB computer keyboard and mouse.[89] It may also be used with USB storage, USB to MIDI converters, and virtually any other device/component with USB capabilities, depending on the installed device drivers in the underlying operating system (many of which are included by default).
 Other peripherals can be attached through the various pins and connectors on the surface of the Raspberry Pi.[90]
 The video controller can generate standard modern TV resolutions, such as HD and Full HD, and higher or lower monitor resolutions as well as older NTSC or PAL standard CRT TV resolutions. As shipped (i.e., without custom overclocking) it can support the following resolutions: 640×350 EGA; 640×480 VGA; 800×600 SVGA; 1024×768 XGA; 1280×720 720p HDTV; 1280×768 WXGA variant; 1280×800 WXGA variant; 1280×1024 SXGA; 1366×768 WXGA variant; 1400×1050 SXGA+; 1600×1200 UXGA; 1680×1050 WXGA+; 1920×1080 1080p HDTV; 1920×1200 WUXGA.[91]
 Higher resolutions, up to 2048×1152, may work[92][93] or even 3840×2160 at 15 Hz (too low a frame rate for convincing video).[94] Allowing the highest resolutions does not imply that the GPU can decode video formats at these resolutions; in fact, the Raspberry Pis are known to not work reliably for H.265 (at those high resolutions),[95] commonly used for very high resolutions (however, most common formats up to Full HD do work).
 Although the Raspberry Pi 3 does not have H.265 decoding hardware, the CPU is more powerful than its predecessors, potentially fast enough to allow the decoding of H.265-encoded videos in software.[96] The GPU in the Raspberry Pi 3 runs at higher clock frequencies of 300 MHz or 400 MHz, compared to previous versions which ran at 250 MHz.[97]
 The Raspberry Pis can also generate 576i and 480i composite video signals, as used on old-style (CRT) TV screens and less-expensive monitors through standard connectors – either RCA or 3.5 mm phono connector depending on model. The television signal standards supported are PAL-B/G/H/I/D, PAL-M, PAL-N, NTSC and NTSC-J.[98]
 When booting, the time defaults to being set over the network using the Network Time Protocol (NTP). The source of time information can be another computer on the local network that does have a real-time clock, or to a NTP server on the internet. If no network connection is available, the time may be set manually or configured to assume that no time passed during the shutdown. In the latter case, the time is monotonic (files saved later in time always have later timestamps) but may be considerably earlier than the actual time. For systems that require a built-in real-time clock, a number of small, low-cost add-on boards with real-time clocks are available.[99][100] The Raspberry Pi 5 is the first to include a real-time clock.[101] If an external battery is not plugged in, the Raspberry Pi 5 will use the Network Time Protocol, or will need to be set manually, as was the case in previous models.
 The RP2040 microcontroller has a built-in real-time clock, but it can not be set without some form of user entry or network facility being added.
 
 
 
 Raspberry Pi 1 Models A+ and B+, Pi 2 Model B, Pi 3 Models A+, B and B+, Pi 4, and Pi Zero, Zero W, Zero WH and Zero W 2 have the same 40-pin pinout (designated J8 across all models).[102] Raspberry Pi 1 Models A and B have only the first 26 pins.[103][104][105] The J8 header is commonly referred to as the GPIO connector as a whole, even though only a subset of the pins are GPIO pins. In the Pi Zero and Zero W, the 40 GPIO pins are unpopulated, having the through-holes exposed for soldering instead. The Zero WH (Wireless + Header) has the header pins preinstalled.
 Model B rev. 2 also has a pad (called P5 on the board and P6 on the schematics) of 8 pins offering access to an additional 4 GPIO connections.[106] These GPIO pins were freed when the four board version identification links present in revision 1.0 were removed.[107]
 Models A and B provide GPIO access to the ACT status LED using GPIO 16. Models A+ and B+ provide GPIO access to the ACT status LED using GPIO 47, and the power status LED using GPIO 35.
 $80 (8GB)[120][101]
 1.6 A (8 W) for "power virus" workloads[126]​[154] 3 A (15 W) power supply recommended.​[17]
 Raspberry Pi provides Raspberry Pi OS (formerly called Raspbian), a Debian-based Linux distribution for download, as well as third-party Ubuntu, Windows 10 IoT Core, RISC OS, LibreELEC (specialised media centre distribution)[167] and specialised distributions for the Kodi media centre and classroom management.[168] It promotes Python and Scratch as the main programming languages, with support for many other languages.[169] The default firmware is closed source, while unofficial open source firmware is available.[170][171][172] Many other operating systems can also run on the Raspberry Pi. The formally verified microkernel seL4 is also supported.[173] There are several ways of installing multiple operating systems on one SD card.[174]
 Raspberry Pi can use a VideoCore IV GPU via a binary blob, which is loaded into the GPU at boot time from the SD-card, and additional software, that initially was closed source.[207] This part of the driver code was later released.[208] However, much of the actual driver work is done using the closed source GPU code. Application software makes calls to closed source run-time libraries (OpenMAX IL, OpenGL ES or OpenVG), which in turn call an open source driver inside the Linux kernel, which then calls the closed source VideoCore IV GPU driver code. The API of the kernel driver is specific for these closed libraries. Video applications use OpenMAX IL, 3D applications use OpenGL ES and 2D applications use OpenVG, which both in turn use EGL. OpenMAX IL and EGL use the open source kernel driver in turn.[209]
 Raspberry Pi first announced it was working on a Vulkan driver in February 2020.[210] A working Vulkan driver running Quake 3 at 100 frames per second on a 3B+ was revealed by a graphics engineer who had been working on it as a hobby project on 20 June.[211] On 24 November 2020 Raspberry Pi announced that their driver for the Raspberry Pi 4 is Vulkan 1.0 conformant.[212] Raspberry Pi Trading announced further driver conformance for Vulkan 1.1 and 1.2 on 26 October 2021[213] and 1 August 2022.[214]
 The official firmware is a freely redistributable[215] binary blob, that is proprietary software.[176] A minimal proof-of-concept open source firmware is also available, mainly aimed at initialising and starting the ARM cores as well as performing minimal startup that is required on the ARM side. It is also capable of booting a very minimal Linux kernel, with patches to remove the dependency on the mailbox interface being responsive. It is known to work on Raspberry Pi 1, 2 and 3, as well as some variants of Raspberry Pi Zero.[216]
 In February 2015, a switched-mode power supply chip, designated U16, of the Raspberry Pi 2 Model B version 1.1 (the initially released version) was found to be vulnerable to flashes of light,[246] particularly the light from xenon camera flashes and green[247] and red laser pointers. The U16 chip has WL-CSP packaging, which exposes the bare silicon die. The Raspberry Pi Foundation blog recommended covering U16 with opaque material (such as Sugru or Blu-Tak) or putting the Raspberry Pi 2 in a case.[248][247] This issue was not discovered before the release of the Raspberry Pi 2 because it is not standard or common practice to test susceptibility to optical interference,[246] while commercial electronic devices are routinely subjected to tests of susceptibility to radio interference.
 Technology writer Glyn Moody described the project in May 2011 as a "potential BBC Micro 2.0", not by replacing PC compatible machines but by supplementing them.[249] In March 2012 Stephen Pritchard echoed the BBC Micro successor sentiment in ITPRO.[250] Alex Hope, co-author of the Next Gen report, is hopeful that the computer will engage children with the excitement of programming.[251] Co-author Ian Livingstone suggested that the BBC could be involved in building support for the device, possibly branding it as the BBC Nano.[252] The Centre for Computing History strongly supports the Raspberry Pi project, feeling that it could "usher in a new era".[253] Before release, the board was showcased by ARM's CEO Warren East at an event in Cambridge outlining Google's ideas to improve UK science and technology education.[254]
 Harry Fairhead, however, suggests that more emphasis should be put on improving the educational software available on existing hardware, using tools such as Google App Inventor to return programming to schools, rather than adding new hardware choices.[255] Simon Rockman, writing in a ZDNet blog, was of the opinion that teens will have "better things to do", despite what happened in the 1980s.[256]
 In October 2012, the Raspberry Pi won T3's Innovation of the Year award,[257] and futurist Mark Pesce cited a (borrowed) Raspberry Pi as the inspiration for his ambient device project MooresCloud.[258] In October 2012, the British Computer Society reacted to the announcement of enhanced specifications by stating, "it's definitely something we'll want to sink our teeth into."[259]
 In June 2017, Raspberry Pi won the Royal Academy of Engineering MacRobert Award.[260] The citation for the award to the Raspberry Pi said it was "for its inexpensive credit card-sized microcomputers, which are redefining how people engage with computing, inspiring students to learn coding and computer science and providing innovative control solutions for industry."[261]
 Clusters of hundreds of Raspberry Pis have been used for testing programs destined for supercomputers.[262]
 The Raspberry Pi community was described by Jamie Ayre of FOSS software company AdaCore as one of the most exciting parts of the project.[263] Community blogger Russell Davis said that the community strength allows the Foundation to concentrate on documentation and teaching.[263] The community developed a fanzine around the platform called The MagPi[264] which in 2015, was handed over to Raspberry Pi (Trading) Ltd by its volunteers to be continued in-house.[265] A series of community Raspberry Jam events have been held across the UK and around the world.[266]
 As of January 2012[update], enquiries about the board in the United Kingdom have been received from schools in both the state and private sectors, with around five times as much interest from the latter. It is hoped that businesses will sponsor purchases for less advantaged schools.[267] The CEO of Premier Farnell said that the government of a country in the Middle East has expressed interest in providing a board to every schoolgirl, to enhance her employment prospects.[268][269]
 In 2014, the Raspberry Pi Foundation hired a number of its community members including ex-teachers and software developers to launch a set of free learning resources for its website.[270] The Foundation also started a teacher training course called Picademy with the aim of helping teachers prepare for teaching the new computing curriculum using the Raspberry Pi in the classroom.[271]
 In 2018, NASA launched the JPL Open Source Rover Project, which is a scaled down version of Curiosity rover and uses a Raspberry Pi as the control module, to encourage students and hobbyists to get involved in mechanical, software, electronics, and robotics engineering.[272]
 There are a number of developers and applications that are using the Raspberry Pi for home automation. These programmers are making an effort to modify the Raspberry Pi into a cost-affordable solution in energy monitoring and power consumption. Because of the relatively low cost of the Raspberry Pi, this has become a popular and economical alternative to the more expensive commercial solutions.[citation needed]
 In June 2014, Polish industrial automation manufacturer TECHBASE released ModBerry, an industrial computer based on the Raspberry Pi Compute Module. The device has a number of interfaces, most notably RS-485/232 serial ports, digital and analogue inputs/outputs, CAN and economical 1-Wire buses, all of which are widely used in the automation industry. The design allows the use of the Compute Module in harsh industrial environments, leading to the conclusion that the Raspberry Pi is no longer limited to home and science projects, but can be widely used as an Industrial IoT solution and achieve goals of Industry 4.0.[273]
 In March 2018, SUSE announced commercial support for SUSE Linux Enterprise on the Raspberry Pi 3 Model B to support a number of undisclosed customers implementing industrial monitoring with the Raspberry Pi.[274]
 In January 2021, TECHBASE announced a Raspberry Pi Compute Module 4 cluster for AI accelerator, routing and file server use. The device contains one or more standard Raspberry Pi Compute Module 4s in an industrial DIN rail housing, with some versions containing one or more Coral Edge tensor processing units.[275]
 The Organelle is a portable synthesizer, a sampler, a sequencer, and an effects processor designed and assembled by Critter & Guitari. It incorporates a Raspberry Pi computer module running Linux.[276]
 OTTO is a digital camera created by Next Thing Co. It incorporates a Raspberry Pi Compute Module. It was successfully crowd-funded in a May 2014 Kickstarter campaign.[277]
 Slice is a digital media player which also uses a Compute Module as its heart. It was crowd-funded in an August 2014 Kickstarter campaign. The software running on Slice is based on Kodi.[278]
 Numerous commercial thin client computer terminals use the Raspberry Pi.[279]
 AutoPi TMU device is a telematics unit which is built on top of a Raspberry Pi Compute Module 4 and incorporates the philosophy of which Raspberry Pi was built upon.[280]
 During the COVID-19 pandemic, demand increased primarily due to the increase in remote work, but also because of the use of many Raspberry Pi Zeros in ventilators for COVID-19 patients in countries such as Colombia,[281] which were used to combat strain on the healthcare system. In March 2020, Raspberry Pi sales reached 640,000 units, the second largest month of sales in the company's history.[282]
 A project was launched in December 2014 at an event held by the UK Space Agency. The Astro Pi was an augmented Raspberry Pi that included a sensor hat with a visible light or infrared camera. The Astro Pi competition, called Principia, was officially opened in January and was opened to all primary and secondary school aged children who were residents of the United Kingdom. During his mission, British ESA astronaut Tim Peake deployed the computers on board the International Space Station.[283] He loaded the winning code while in orbit, collected the data generated and then sent this to Earth where it was distributed to the winning teams. Covered themes during the competition included spacecraft sensors, satellite imaging, space measurements, data fusion and space radiation.
 The organisations involved in the Astro Pi competition include the UK Space Agency, UKspace, Raspberry Pi, ESERO-UK and ESA.
 In 2017, the European Space Agency ran another competition open to all students in the European Union called Proxima. The winning programs were run on the ISS by Thomas Pesquet, a French astronaut.[284] In December 2021, the Dragon 2 spacecraft launched by NASA had a pair of Astro Pi in it.[285]
 The computer is inspired by Acorn's BBC Micro of 1981.[286][287] The Model A, Model B and Model B+ names are references to the original models of the British educational BBC Micro computer, developed by Acorn Computers.[288]
 According to Upton, the name "Raspberry Pi" was chosen with "Raspberry" as an ode to a tradition of naming early computer companies after fruit, and "Pi" as a reference to the Python programming language.[289]
 In 2006, early concepts of the Raspberry Pi were based on the Atmel ATmega644 microcontroller. Its schematics and PCB layout are publicly available.[290] Foundation trustee Eben Upton assembled a group of teachers, academics and computer enthusiasts to devise a computer to inspire children.[267]
 The first ARM prototype version of the computer was mounted in a package the same size as a USB memory stick.[291] It had a USB port on one end and an HDMI port on the other.
 The Foundation's goal was to offer two versions, priced at US$25 and $35. They started accepting orders for the higher priced Model B on 29 February 2012,[292] the lower cost Model A on 4 February 2013.[293] and the even lower cost (US$20) A+ on 10 November 2014.[109] On 26 November 2015, the cheapest Raspberry Pi yet, the Raspberry Pi Zero, was launched at US$5 or £4.[294]
 
 According to Raspberry Pi, more than 5 million Raspberry Pis were sold by February 2015, making it the best-selling British computer.[12] By November 2016 they had sold 11 million units,[367][386] and 12.5 million by March 2017, making it the third best-selling "general purpose computer".[387] In July 2017, sales reached nearly 15 million,[388] climbing to 19 million in March 2018.[16] By December 2019, a total of 30 million devices had been sold.[389][390]
 The global chip shortage starting in 2020, as well as an uptake in demand starting in early 2021, notably affected the Raspberry Pi, causing significant availability issues from that time onward.[391]  The company explained its approach to the shortages in 2021,[392] and April 2022,[393] explaining that it was prioritising business and industrial customers.
 The situation is sufficiently long term that at least one automated stock checker is online.[394]


Source: https://en.wikipedia.org/wiki/ROM
Content: Read-only memory (ROM) is a type of non-volatile memory used in computers and other electronic devices. Data stored in ROM cannot be electronically modified after the manufacture of the memory device. Read-only memory is useful for storing software that is rarely changed during the life of the system, also known as firmware. Software applications (like video games) for programmable devices can be distributed as plug-in cartridges containing ROM.
 Strictly speaking, read-only memory refers to memory that is hard-wired, such as diode matrix or a mask ROM integrated circuit (IC), which cannot be electronically[a] changed after manufacture. Although discrete circuits can be altered in principle, through the addition of bodge wires and/or the removal or replacement of components, ICs cannot. Correction of errors, or updates to the software, require new devices to be manufactured and to replace the installed device.
 Floating-gate ROM semiconductor memory in the form of erasable programmable read-only memory (EPROM), electrically erasable programmable read-only memory (EEPROM) and flash memory can be erased and re-programmed. But usually, this can only be done at relatively slow speeds, may require special equipment to achieve, and is typically only possible a certain number of times.[1]
 The term "ROM" is sometimes used to refer to a ROM device containing specific software or a file with software to be stored in a writable ROM device. For example, users modifying or replacing the Android operating system describe files containing a modified or replacement operating system as "custom ROMs" after the type of storage the file used to be written to, and they may distinguish between ROM (where software and data is stored, usually Flash memory) and RAM.
 IBM used capacitor read-only storage (CROS) and transformer read-only storage (TROS) to store microcode for the smaller System/360 models, the 360/85, and the initial two System/370 models (370/155 and 370/165). On some models there was also a writeable control store (WCS) for additional diagnostics and emulation support. The Apollo Guidance Computer used core rope memory, programmed by threading wires through magnetic cores.
 The simplest type of solid-state ROM is as old as the semiconductor technology itself. Combinational logic gates can be joined manually to map n-bit address input onto arbitrary values of m-bit data output (a look-up table). With the invention of the integrated circuit came mask ROM. Mask ROM consists of a grid of word lines (the address input) and bit lines (the data output), selectively joined with transistor switches, and can represent an arbitrary look-up table with a regular physical layout and predictable propagation delay. Mask ROM is programmed with photomasks in photolithography during semiconductor manufacturing. The mask defines physical features or structures that will be removed, or added in the ROM chips, and the presence or absence of these features will represent either a 1 or a 0 bit, depending on the ROM design.[2] Thus by design, any attempts to electronically change the data will fail, since the data is defined by the presence or absence of physical features or structures that cannot be electronically changed. For every software program, even for revisions of the same program, the entire mask must be changed, which can be costly.
 In mask ROM, the data is physically encoded in the circuit, so it can only be programmed during fabrication. This leads to a number of serious disadvantages:
 Subsequent developments have addressed these shortcomings. Programmable read-only memory (PROM), invented by Wen Tsing Chow in 1956,[3][4] allowed users to program its contents exactly once by physically altering its structure with the application of high-voltage pulses. This addressed problems 1 and 2 above, since a company can simply order a large batch of fresh PROM chips and program them with the desired contents at its designers' convenience.
 The advent of the metal–oxide–semiconductor field-effect transistor (MOSFET), invented at Bell Labs in 1959,[5] enabled the practical use of metal–oxide–semiconductor (MOS) transistors as memory cell storage elements in semiconductor memory, a function previously served by magnetic cores in computer memory.[6] In 1967, Dawon Kahng and Simon Sze of Bell Labs proposed that the floating gate of a MOS semiconductor device could be used for the cell of a reprogrammable ROM, which led to Dov Frohman of Intel inventing erasable programmable read-only memory (EPROM) in 1971.[7] The 1971 invention of EPROM essentially solved problem 3, since EPROM (unlike PROM) can be repeatedly reset to its unprogrammed state by exposure to strong ultraviolet light.
 Electrically erasable programmable read-only memory (EEPROM), developed by Yasuo Tarui, Yutaka Hayashi and Kiyoko Naga at the Electrotechnical Laboratory in 1972,[8] went a long way to solving problem 4, since an EEPROM can be programmed in-place if the containing device provides a means to receive the program contents from an external source (for example, a personal computer via a serial cable). Flash memory, invented by Fujio Masuoka at Toshiba in the early 1980s and commercialized in the late 1980s, is a form of EEPROM that makes very efficient use of chip area and can be erased and reprogrammed thousands of times without damage. It permits erasure and programming of only a specific part of the device, instead of the entire device. This can be done at high speed, hence the name "flash".[9][10]
 All of these technologies improved the flexibility of ROM, but at a significant cost-per-chip, so that in large quantities mask ROM would remain an economical choice for many years. (Decreasing cost of reprogrammable devices had almost eliminated the market for mask ROM by the year 2000.) Rewriteable technologies were envisioned as replacements for mask ROM.
 The most recent development is NAND flash, also invented at Toshiba. Its designers explicitly broke from past practice, stating plainly that "the aim of NAND flash is to replace hard disks,"[11] rather than the traditional use of ROM as a form of non-volatile primary storage. As of 2021[update], NAND has nearly completely achieved this goal by offering throughput higher than hard disks, lower latency, higher tolerance of physical shock, extreme miniaturization (in the form of USB flash drives and tiny microSD memory cards, for example), and much lower power consumption.
 Many stored-program computers use a form of non-volatile storage (that is, storage that retains its data when power is removed) to store the initial program that runs when the computer is powered on or otherwise begins execution (a process known[b] as bootstrapping, often abbreviated to "booting" or "booting up"). Likewise, every non-trivial computer needs some form of mutable memory to record changes in its state as it executes.
 Forms of read-only memory were employed as non-volatile storage for programs in most early stored-program computers, such as ENIAC after 1948. (Until then it was not a stored-program computer as every program had to be manually wired into the machine, which could take days to weeks.) Read-only memory was simpler to implement since it needed only a mechanism to read stored values, and not to change them in-place, and thus could be implemented with very crude electromechanical devices (see historical examples below). With the advent of integrated circuits in the 1960s, both ROM and its mutable counterpart static RAM were implemented as arrays of transistors in silicon chips; however, a ROM memory cell could be implemented using fewer transistors than an SRAM memory cell, since the latter needs a latch (comprising 5-20 transistors) to retain its contents, while a ROM cell might consist of the absence (logical 0) or presence (logical 1) of one transistor connecting a bit line to a word line.[12] Consequently, ROM could be implemented at a lower cost-per-bit than RAM for many years.
 Most home computers of the 1980s stored a BASIC interpreter or operating system in ROM as other forms of non-volatile storage such as magnetic disk drives were too costly. For example, the Commodore 64 included 64 KB of RAM and 20 KB of ROM containing a BASIC interpreter and the KERNAL operating system. Later home or office computers such as the IBM PC XT often included magnetic disk drives, and larger amounts of RAM, allowing them to load their operating systems from disk into RAM, with only a minimal hardware initialization core and bootloader remaining in ROM (known as the BIOS in IBM-compatible computers). This arrangement allowed for a more complex and easily upgradeable operating system.
 In modern PCs, "ROM" is used to store the basic bootstrapping firmware for the processor, as well as the various firmware needed to internally control self-contained devices such as graphic cards, hard disk drives, solid state drives, optical disc drives, TFT screens, etc., in the system. Today, many of these "read-only" memories – especially the BIOS/UEFI – are often replaced with EEPROM or Flash memory (see below), to permit in-place reprogramming should the need for a firmware upgrade arise. However, simple and mature sub-systems (such as the keyboard or some communication controllers in the integrated circuits on the main board, for example) may employ mask ROM or OTP (one-time programmable).
 ROM and successor technologies such as flash are prevalent in embedded systems. These are in everything from industrial robots to home appliances and consumer electronics (MP3 players, set-top boxes, etc.) all of which are designed for specific functions, but are based on general-purpose microprocessors. With software usually tightly coupled to hardware, program changes are rarely needed in such devices (which typically lack hard disks for reasons of cost, size, or power consumption). As of 2008, most products use Flash rather than mask ROM, and many provide some means for connecting to a PC for firmware updates; for example, a digital audio player might be updated to support a new file format. Some hobbyists have taken advantage of this flexibility to reprogram consumer products for new purposes; for example, the iPodLinux and OpenWrt projects have enabled users to run full-featured Linux distributions on their MP3 players and wireless routers, respectively.
 ROM is also useful for binary storage of cryptographic data, as it makes them difficult to replace, which may be desirable in order to enhance information security.
 Since ROM (at least in hard-wired mask form) cannot be modified, it is only suitable for storing data which is not expected to need modification for the life of the device. To that end, ROM has been used in many computers to store look-up tables for the evaluation of mathematical and logical functions (for example, a floating-point unit might tabulate the sine function in order to facilitate faster computation). This was especially effective when CPUs were slow and ROM was cheap compared to RAM.
 Notably, the display adapters of early personal computers stored tables of bitmapped font characters in ROM. This usually meant that the text display font could not be changed interactively. This was the case for both the CGA and MDA adapters available with the IBM PC XT.
 The use of ROM to store such small amounts of data has disappeared almost completely in modern general-purpose computers. However, NAND Flash has taken over a new role as a medium for mass storage or secondary storage of files.
 Mask ROM is a read-only memory whose contents are programmed by the integrated circuit manufacturer (rather than by the user). The desired memory contents are furnished by the customer to the device manufacturer. The desired data is converted into a custom photomask/mask layer for the final metallization of interconnections on the memory chip (hence the name).
 Mask ROM can be made in several ways, all of which aim to change the electrical response of a transistor when it is addressed on a grid, such as:
 Mask ROM transistors can be arranged in either NOR or NAND configurations and can achieve one of the smallest cell sizes possible as each bit is represented by only one transistor. NAND offers higher storage density than NOR. OR configurations are also possible, but compared to NOR it only connects transistors to Vcc instead of Vss.[13] Mask ROMs used to be the most inexpensive, and are the simplest semiconductor memory devices, with only one metal layer and one polysilicon layer, making it the type of semiconductor memory with the highest manufacturing yield[2] (the highest number of working devices per manufacturing run). ROM can be made using one of several semiconductor device fabrication technologies such as CMOS, nMOS, pMOS, and bipolar transistors.[14]
 It is common practice to use rewritable non-volatile memory – such as UV-EPROM or EEPROM – for the development phase of a project, and to switch to mask ROM when the code has been finalized. For example, Atmel microcontrollers come in both EEPROM and mask ROM formats.
 The main advantage of mask ROM is its cost. Per bit, mask ROM was more compact than any other kind of semiconductor memory. Since the cost of an integrated circuit strongly depends on its size, mask ROM is significantly cheaper than any other kind of semiconductor memory.
 However, the one-time masking cost is high and there is a long turn-around time from design to product phase. Design errors are costly: if an error in the data or code is found, the mask ROM is useless and must be replaced in order to change the code or data.[15]
 As of 2003[update], four companies produce most such mask ROM chips: Samsung Electronics, NEC Corporation, Oki Electric Industry, and Macronix.[16][needs update]
 Some integrated circuits contain only mask ROM. Other integrated circuits contain mask ROM as well as a variety of other devices. In particular, many microprocessors have mask ROM to store their microcode. Some microcontrollers have mask ROM to store the bootloader or all of their firmware.
 Classic mask-programmed ROM chips are integrated circuits that physically encode the data to be stored, and thus it is impossible to change their contents after fabrication.
 It is also possible to write the contents of a Laser ROM by using a laser to alter the electrical properties of only some diodes on the ROM, or by using a laser to cut only some polysilicon links, instead of using a mask.[17][18][13]
 By applying write protection, some types of reprogrammable ROMs may temporarily become read-only memory.
 There are other types of non-volatile memory which are not based on solid-state IC technology, including:
 Although the relative speed of RAM vs. ROM has varied over time, as of 2007[update] large RAM chips can be read faster than most ROMs. For this reason (and to allow uniform access), ROM content is sometimes copied to RAM or shadowed before its first use, and subsequently read from RAM.
 For those types of ROM that can be electrically modified, writing speed has traditionally been much slower than reading speed, and it may need unusually high voltage, the movement of jumper plugs to apply write-enable signals, and special lock/unlock command codes. Modern NAND Flash can be used to achieve the highest write speeds of any rewritable ROM technology, with speeds as high as 10 GB/s in an SSD. This has been enabled by the increased investment in both consumer and enterprise solid state drives and flash memory products for higher end mobile devices. On a technical level the gains have been achieved by increasing parallelism both in controller design and of storage, the use of large DRAM read/write caches and the implementation of memory cells which can store more than one bit (DLC, TLC and MLC). The latter approach is more failure prone but this has been largely mitigated by overprovisioning (the inclusion of spare capacity in a product which is visible only to the drive controller) and by increasingly sophisticated read/write algorithms in drive firmware.
 Because they are written by forcing electrons through a layer of electrical insulation onto a floating transistor gate, rewriteable ROMs can withstand only a limited number of write and erase cycles before the insulation is permanently damaged. In the earliest EPROMs, this might occur after as few as 1,000 write cycles, while in modern Flash EEPROM the endurance may exceed 1,000,000. The limited endurance, as well as the higher cost per bit, means that Flash-based storage is unlikely to completely supplant magnetic disk drives in the near future.[citation needed]
 The timespan over which a ROM remains accurately readable is not limited by write cycling. The data retention of EPROM, EAROM, EEPROM, and Flash may be time-limited by charge leaking from the floating gates of the memory cell transistors. Early generation EEPROM's, in the mid-1980s generally cited 5 or 6 year data retention. A review of EEPROM's offered in the year 2020 shows manufacturers citing 100 year data retention. Adverse environments will reduce the retention time (leakage is accelerated by high temperatures or radiation). Masked ROM and fuse/antifuse PROM do not suffer from this effect, as their data retention depends on physical rather than electrical permanence of the integrated circuit, although fuse re-growth was once a problem in some systems.[21]
 The contents of ROM chips can be extracted with special hardware devices and relevant controlling software. This practice is common for, as a main example, reading the contents of older video game console cartridges. Another example is making backups of firmware/OS ROMs from older computers or other devices - for archival purposes, as in many cases, the original chips are PROMs and thus at risk of exceeding their usable data lifetime.
 The resultant memory dump files are known as ROM images or abbreviated ROMs, and can be used to produce duplicate ROMs - for example to produce new cartridges or as digital files for playing in console emulators. The term ROM image originated when most console games were distributed on cartridges containing ROM chips, but achieved such widespread usage that it is still applied to images of newer games distributed on CD-ROMs or other optical media.
 ROM images of commercial games, firmware, etc. usually contain copyrighted software. The unauthorized copying and distribution of copyrighted software is a violation of copyright laws in many jurisdictions, although duplication for backup purposes may be considered fair use depending on location. In any case, there is a thriving community engaged in the distribution and trading of such software for preservation/sharing purposes.


Source: https://en.wikipedia.org/wiki/Raspberry_Pi#Raspberry_Pi_OS
Content: 
 Raspberry Pi (/paɪ/) is a series of small single-board computers (SBCs) developed in the United Kingdom by the Raspberry Pi Foundation in association with Broadcom. Since 2013, Raspberry Pi devices have been developed and supported by a subsidiary of the Raspberry Pi Foundation, now named Raspberry Pi Ltd.[3] The Raspberry Pi project originally leaned toward the promotion of teaching basic computer science in schools.[4][5][6] The original model became more popular than anticipated,[7] selling outside its target market for diverse uses such as robotics, home and industrial automation, and by computer and electronic hobbyists, because of its low cost, modularity, open design, and its adoption of the HDMI and USB standards.
 After the release of the second board type, the Raspberry Pi Foundation set up a new entity, named Raspberry Pi (Trading) Ltd, and installed Eben Upton as CEO, with the responsibility for developing their computers.[8] The Foundation was rededicated as an educational charity for promoting the teaching of basic computer science in schools and developing countries. Most Raspberry Pis are made in a Sony factory in Pencoed, Wales,[9] while others are made in China and Japan.[10][11]
 In 2015, the Raspberry Pi surpassed the ZX Spectrum in unit sales, becoming the best-selling British computer.[12]
 In 2021, Raspberry Pi (Trading) Ltd changed its name to Raspberry Pi Ltd.[13]
 There are three series of Raspberry Pi, and several generations of each have been released. Raspberry Pi SBCs feature a Broadcom system on a chip (SoC) with an integrated ARM-compatible central processing unit (CPU) and on-chip graphics processing unit (GPU), while Raspberry Pi Pico has a RP2040 system on chip with an integrated ARM-compatible central processing unit (CPU).
 
 As of 4 May 2021, Raspberry Pi is committed to manufacture most Pi models until at least January 2026. Even the 1 GB Pi 4B can still be specially-ordered.[44]
 The Raspberry Pi hardware has evolved through several versions that feature variations in the type of the central processing unit, amount of memory capacity, networking support, and peripheral-device support.
 This block diagram describes models B, B+, A and A+. The Pi Zero models are similar, but lack the Ethernet and USB hub components. The Ethernet adapter is internally connected to an additional USB port. In Model A, A+, and the Pi Zero, the USB port is connected directly to the system on a chip (SoC). On the Pi 1 Model B+ and later models the USB/Ethernet chip contains a five-port USB hub, of which four ports are available, while the Pi 1 Model B only provides two. On the Pi Zero, the USB port is also connected directly to the SoC, but it uses a micro USB (OTG) port. Unlike all other Pi models, the 40 pin GPIO connector is omitted on the Pi Zero, with solderable through-holes only in the pin locations. The Pi Zero WH remedies this.
 Processor speed ranges from 700 MHz to 1.4 GHz for the Pi 3 Model B+ or 1.5 GHz for the Pi 4; on-board memory ranges from 256 MB to 8 GB random-access memory (RAM), with only the Raspberry Pi 4 and the Raspberry Pi 5 having more than 1 GB. Secure Digital (SD) cards in MicroSDHC form factor (SDHC on early models) are used to store the operating system and program memory, however some models also come with onboard eMMC storage[45] and the Raspberry Pi 4 can also make use of USB-attached SSD storage for its operating system.[46] The boards have one to five USB ports. For video output, HDMI and composite video are supported, with a standard 3.5 mm tip-ring-sleeve jack carrying mono audio together with composite video. Lower-level output is provided by a number of GPIO pins, which support common protocols like I²C. The B-models have an 8P8C Ethernet port and the Pi 3, Pi 4 and Pi Zero W have on-board Wi-Fi 802.11n and Bluetooth.[47]
 The Broadcom BCM2835 SoC used in the first generation Raspberry Pi[48] includes a RISC-based 700 MHz 32-bit ARM1176JZF-S processor, VideoCore IV graphics processing unit (GPU),[49] and RAM. It has a level 1 (L1) cache of 16 KB and a level 2 (L2) cache of 128 KB. The level 2 cache is used primarily by the GPU. The SoC is stacked underneath the RAM chip, so only its edge is visible. The ARM1176JZ(F)-S is the same CPU used in the original iPhone,[50] although at a higher clock rate, and mated with a much faster GPU.
 The earlier V1.1 model of the Raspberry Pi 2 used a Broadcom BCM2836 SoC with a 900 MHz 32-bit, quad-core ARM Cortex-A7 processor, with 256 KB shared L2 cache.[51] The Raspberry Pi 2 V1.2 was upgraded to a Broadcom BCM2837 SoC with a 1.2 GHz 64-bit quad-core ARM Cortex-A53 processor,[14] the same one which is used on the Raspberry Pi 3, but underclocked (by default) to the same 900 MHz CPU clock speed as the V1.1. The BCM2836 SoC is no longer in production as of late 2016.
 The Raspberry Pi 3 Model B uses a Broadcom BCM2837 SoC with a 1.2 GHz 64-bit quad-core ARM Cortex-A53 processor, with 512 KB shared L2 cache. The Model A+ and B+ are 1.4 GHz[52][53][54]
 The Raspberry Pi 4 uses a Broadcom BCM2711 SoC with a 1.5 GHz (later models: 1.8 GHz) 64-bit quad-core ARM Cortex-A72 processor, with 1 MB shared L2 cache.[55][56] Unlike previous models, which all used a custom interrupt controller poorly suited for virtualisation, the interrupt controller on this SoC is compatible with the ARM Generic Interrupt Controller (GIC) architecture 2.0, providing hardware support for interrupt distribution when using ARM virtualisation capabilities.[57][58] The VideoCore IV of the previous models has also been replaced with a VideoCore VI running at 500 MHz.
 The Raspberry Pi Zero and Zero W use the same Broadcom BCM2835 SoC as the first generation Raspberry Pi, although now running at 1 GHz CPU clock speed.[59]
 The Raspberry Pi Zero 2 W uses the RP3A0-AU, which is a System-in-Package (SiP) design. The package contains a Broadcom BCM2710A1 processor, which is a 64-bit quad-core ARM Cortex-A53 clocked at 1 GHz, along with 512 MB of LPDDR2 SDRAM layered above.[60][61] The Raspberry Pi 3 also uses the BCM2710A1 in its Broadcom BCM2837 SoC, but clocked at a higher 1.2 GHz.
 The Raspberry Pi Pico uses the RP2040,[62] a microcontroller containing dual ARM Cortex-M0+ cores running at 133 MHz, 6 banks of SRAM totaling 264 KB, and programmable IO for peripherals.[63]
 The Raspberry Pi 5 uses the Broadcom BCM2712 SoC, which is a chip designed in collaboration with Raspberry Pi. The SoC features a quad-core ARM Cortex-A76 processor clocked at 2.4 GHz, alongside a VideoCore VII GPU clocked at 800 MHz. The BCM2712 SoC also features support for cryptographic extensions for the first time on a Raspberry Pi model. Alongside the new processor and graphics unit, the monolithic design of the earlier BCM2711 has been replaced with a CPU and chipset (southbridge) architecture, as the IO functionality has been moved to the Raspberry Pi 5's custom RP1 chip.[64]
 While operating at 700 MHz by default, the first generation Raspberry Pi provided a real-world performance roughly equivalent to 0.041 GFLOPS.[65][66] On the CPU level the performance is similar to a 300 MHz Pentium II of 1997–99. The GPU provides 1 Gpixel/s or 1.5 Gtexel/s of graphics processing or 24 GFLOPS of general purpose computing performance. The graphical capabilities of the Raspberry Pi are roughly equivalent to the performance of the Xbox of 2001.
 Raspberry Pi 2 V1.1 included a quad-core Cortex-A7 CPU running at 900 MHz and 1 GB RAM. It was described as 4–6 times more powerful than its predecessor. The GPU was identical to the original.[51] In parallelised benchmarks, the Raspberry Pi 2 V1.1 could be up to 14 times faster than a Raspberry Pi 1 Model B+.[67]
 The Raspberry Pi 3, with a quad-core Cortex-A53 processor, is described as having ten times the performance of a Raspberry Pi 1.[68] Benchmarks showed the Raspberry Pi 3 to be approximately 80% faster than the Raspberry Pi 2 in parallelised tasks.[69]
 The Raspberry Pi 4, with a quad-core Cortex-A72 processor, is described as having three times the performance of a Raspberry Pi 3.[17]
 Most Raspberry Pi systems-on-chip can be overclocked to various degrees utilising the built in config.txt file in the boot sector of the Raspberry Pi OS. Overclocking is generally safe and does not automatically void the warranty of the Raspberry Pi; however, setting the "force_turbo" option to 1 bypasses voltage and temperature limits and voids the users warranty.[70] In Raspberry Pi OS the overclocking options on boot can also be made by a software command running "sudo raspi-config" on Raspberry Pi 1, 2, and original 3B without voiding the warranty.[71] In those cases the Pi automatically shuts the overclocking down if the chip temperature reaches 85 °C (185 °F); an appropriately sized heat sink is needed to protect the chip from thermal throttling.
 Newer versions of the firmware contain the option to choose between five overclock ("turbo") presets that, when used, attempt to maximise the performance of the SoC without impairing the lifetime of the board. This is done by monitoring the core temperature of the chip and the CPU load, and dynamically adjusting clock speeds and the core voltage. When the demand is low on the CPU or it is running too hot, the performance is throttled, but if the CPU has much to do and the chip's temperature is acceptable, performance is temporarily increased with CPU clock speeds of up to 1.1 GHz, depending on the board version and on which of the turbo settings is used.
 The overclocking modes are:
 In the highest (turbo) mode the SDRAM clock speed was originally 500 MHz, but this was later changed to 600 MHz because of occasional SD card corruption. Simultaneously, in high mode the core clock speed was lowered from 450 to 250 MHz, and in medium mode from 333 to 250 MHz.
 The CPU of the first and second generation Raspberry Pi board did not require cooling with a heat sink or fan, even when overclocked, but the Raspberry Pi 3 may generate more heat when overclocked.[73]
 The early designs of the Raspberry Pi Model A and B boards included only 256 MB of random access memory (RAM). Of this, the early beta Model B boards allocated 128 MB to the GPU by default, leaving only 128 MB for the CPU.[74] On the early 256 MB releases of models A and B, three different splits were possible. The default split was 192 MB for the CPU, which should be sufficient for standalone 1080p video decoding, or for simple 3D processing. 224 MB was for Linux processing only, with only a 1080p framebuffer, and was likely to fail for any video or 3D. 128 MB was for heavy 3D processing, possibly also with video decoding.[75] In comparison, the Nokia 701 uses 128 MB for the Broadcom VideoCore IV.[76]
 The later Model B with 512 MB RAM, was released on 15 October 2012 and was initially released with new standard memory split files (arm256_start.elf, arm384_start.elf, arm496_start.elf) with 256 MB, 384 MB, and 496 MB CPU RAM, and with 256 MB, 128 MB, and 16 MB video RAM, respectively. But about one week later, the foundation released a new version of start.elf that could read a new entry in config.txt (gpu_mem=xx) and could dynamically assign an amount of RAM (from 16 to 256 MB in 8 MB steps) to the GPU, obsoleting the older method of splitting memory, and a single start.elf worked the same for 256 MB and 512 MB Raspberry Pis.[77]
 The Raspberry Pi 2 has 1 GB of RAM.
 The Raspberry Pi 3 has 1 GB of RAM in the B and B+ models, and 512 MB of RAM in the A+ model.[78][79][80] The Raspberry Pi Zero and Zero W have 512 MB of RAM.
 The Raspberry Pi 4 is available with 1, 2, 4 or 8 GB of RAM.[81] A 1 GB model was originally available at launch in June 2019 but was discontinued in March 2020,[41] and the 8 GB model was introduced in May 2020.[82] The 1 GB model returned in October 2021.[83]
 The Model A, A+ and Pi Zero have no Ethernet circuitry and are commonly connected to a network using an external user-supplied USB Ethernet or Wi-Fi adapter. On the Model B and B+ the Ethernet port is provided by a built-in USB Ethernet adapter using the SMSC LAN9514 chip.[84] The Raspberry Pi 3 and Pi Zero W (wireless) are equipped with 2.4 GHz WiFi 802.11n (150 Mbit/s) and Bluetooth 4.1 (24 Mbit/s) based on the Broadcom BCM43438 FullMAC chip with no official support for monitor mode (though it was implemented through unofficial firmware patching[85]) and the Pi 3 also has a 10/100 Mbit/s Ethernet port. The Raspberry Pi 3B+ features dual-band IEEE 802.11b/g/n/ac WiFi, Bluetooth 4.2, and Gigabit Ethernet (limited to approximately 300 Mbit/s by the USB 2.0 bus between it and the SoC). The Raspberry Pi 4 has full gigabit Ethernet (throughput is not limited as it is not funnelled via the USB chip.)
 The RPi Zero, RPi1A, RPi3A+[86] and RPi4 can be used as a USB device or "USB gadget", plugged into another computer via a USB port on another machine. It can be configured in multiple ways, such as functioning as a serial or Ethernet device.[87] Although originally requiring software patches, this was added into the mainline Raspbian distribution in May 2016.[87]
 Raspberry Pi models with a newer chipset can boot from USB mass storage, such as from a flash drive. Booting from USB mass storage is not available in the original Raspberry Pi models, the Raspberry Pi Zero, the Raspberry Pi Pico, the Raspberry Pi 2 A models, and the Raspberry Pi 2 B models with versions lower than 1.2.[88]
 Although often pre-configured to operate as a headless computer, the Raspberry Pi may also optionally be operated with any generic USB computer keyboard and mouse.[89] It may also be used with USB storage, USB to MIDI converters, and virtually any other device/component with USB capabilities, depending on the installed device drivers in the underlying operating system (many of which are included by default).
 Other peripherals can be attached through the various pins and connectors on the surface of the Raspberry Pi.[90]
 The video controller can generate standard modern TV resolutions, such as HD and Full HD, and higher or lower monitor resolutions as well as older NTSC or PAL standard CRT TV resolutions. As shipped (i.e., without custom overclocking) it can support the following resolutions: 640×350 EGA; 640×480 VGA; 800×600 SVGA; 1024×768 XGA; 1280×720 720p HDTV; 1280×768 WXGA variant; 1280×800 WXGA variant; 1280×1024 SXGA; 1366×768 WXGA variant; 1400×1050 SXGA+; 1600×1200 UXGA; 1680×1050 WXGA+; 1920×1080 1080p HDTV; 1920×1200 WUXGA.[91]
 Higher resolutions, up to 2048×1152, may work[92][93] or even 3840×2160 at 15 Hz (too low a frame rate for convincing video).[94] Allowing the highest resolutions does not imply that the GPU can decode video formats at these resolutions; in fact, the Raspberry Pis are known to not work reliably for H.265 (at those high resolutions),[95] commonly used for very high resolutions (however, most common formats up to Full HD do work).
 Although the Raspberry Pi 3 does not have H.265 decoding hardware, the CPU is more powerful than its predecessors, potentially fast enough to allow the decoding of H.265-encoded videos in software.[96] The GPU in the Raspberry Pi 3 runs at higher clock frequencies of 300 MHz or 400 MHz, compared to previous versions which ran at 250 MHz.[97]
 The Raspberry Pis can also generate 576i and 480i composite video signals, as used on old-style (CRT) TV screens and less-expensive monitors through standard connectors – either RCA or 3.5 mm phono connector depending on model. The television signal standards supported are PAL-B/G/H/I/D, PAL-M, PAL-N, NTSC and NTSC-J.[98]
 When booting, the time defaults to being set over the network using the Network Time Protocol (NTP). The source of time information can be another computer on the local network that does have a real-time clock, or to a NTP server on the internet. If no network connection is available, the time may be set manually or configured to assume that no time passed during the shutdown. In the latter case, the time is monotonic (files saved later in time always have later timestamps) but may be considerably earlier than the actual time. For systems that require a built-in real-time clock, a number of small, low-cost add-on boards with real-time clocks are available.[99][100] The Raspberry Pi 5 is the first to include a real-time clock.[101] If an external battery is not plugged in, the Raspberry Pi 5 will use the Network Time Protocol, or will need to be set manually, as was the case in previous models.
 The RP2040 microcontroller has a built-in real-time clock, but it can not be set without some form of user entry or network facility being added.
 
 
 
 Raspberry Pi 1 Models A+ and B+, Pi 2 Model B, Pi 3 Models A+, B and B+, Pi 4, and Pi Zero, Zero W, Zero WH and Zero W 2 have the same 40-pin pinout (designated J8 across all models).[102] Raspberry Pi 1 Models A and B have only the first 26 pins.[103][104][105] The J8 header is commonly referred to as the GPIO connector as a whole, even though only a subset of the pins are GPIO pins. In the Pi Zero and Zero W, the 40 GPIO pins are unpopulated, having the through-holes exposed for soldering instead. The Zero WH (Wireless + Header) has the header pins preinstalled.
 Model B rev. 2 also has a pad (called P5 on the board and P6 on the schematics) of 8 pins offering access to an additional 4 GPIO connections.[106] These GPIO pins were freed when the four board version identification links present in revision 1.0 were removed.[107]
 Models A and B provide GPIO access to the ACT status LED using GPIO 16. Models A+ and B+ provide GPIO access to the ACT status LED using GPIO 47, and the power status LED using GPIO 35.
 $80 (8GB)[120][101]
 1.6 A (8 W) for "power virus" workloads[126]​[154] 3 A (15 W) power supply recommended.​[17]
 Raspberry Pi provides Raspberry Pi OS (formerly called Raspbian), a Debian-based Linux distribution for download, as well as third-party Ubuntu, Windows 10 IoT Core, RISC OS, LibreELEC (specialised media centre distribution)[167] and specialised distributions for the Kodi media centre and classroom management.[168] It promotes Python and Scratch as the main programming languages, with support for many other languages.[169] The default firmware is closed source, while unofficial open source firmware is available.[170][171][172] Many other operating systems can also run on the Raspberry Pi. The formally verified microkernel seL4 is also supported.[173] There are several ways of installing multiple operating systems on one SD card.[174]
 Raspberry Pi can use a VideoCore IV GPU via a binary blob, which is loaded into the GPU at boot time from the SD-card, and additional software, that initially was closed source.[207] This part of the driver code was later released.[208] However, much of the actual driver work is done using the closed source GPU code. Application software makes calls to closed source run-time libraries (OpenMAX IL, OpenGL ES or OpenVG), which in turn call an open source driver inside the Linux kernel, which then calls the closed source VideoCore IV GPU driver code. The API of the kernel driver is specific for these closed libraries. Video applications use OpenMAX IL, 3D applications use OpenGL ES and 2D applications use OpenVG, which both in turn use EGL. OpenMAX IL and EGL use the open source kernel driver in turn.[209]
 Raspberry Pi first announced it was working on a Vulkan driver in February 2020.[210] A working Vulkan driver running Quake 3 at 100 frames per second on a 3B+ was revealed by a graphics engineer who had been working on it as a hobby project on 20 June.[211] On 24 November 2020 Raspberry Pi announced that their driver for the Raspberry Pi 4 is Vulkan 1.0 conformant.[212] Raspberry Pi Trading announced further driver conformance for Vulkan 1.1 and 1.2 on 26 October 2021[213] and 1 August 2022.[214]
 The official firmware is a freely redistributable[215] binary blob, that is proprietary software.[176] A minimal proof-of-concept open source firmware is also available, mainly aimed at initialising and starting the ARM cores as well as performing minimal startup that is required on the ARM side. It is also capable of booting a very minimal Linux kernel, with patches to remove the dependency on the mailbox interface being responsive. It is known to work on Raspberry Pi 1, 2 and 3, as well as some variants of Raspberry Pi Zero.[216]
 In February 2015, a switched-mode power supply chip, designated U16, of the Raspberry Pi 2 Model B version 1.1 (the initially released version) was found to be vulnerable to flashes of light,[246] particularly the light from xenon camera flashes and green[247] and red laser pointers. The U16 chip has WL-CSP packaging, which exposes the bare silicon die. The Raspberry Pi Foundation blog recommended covering U16 with opaque material (such as Sugru or Blu-Tak) or putting the Raspberry Pi 2 in a case.[248][247] This issue was not discovered before the release of the Raspberry Pi 2 because it is not standard or common practice to test susceptibility to optical interference,[246] while commercial electronic devices are routinely subjected to tests of susceptibility to radio interference.
 Technology writer Glyn Moody described the project in May 2011 as a "potential BBC Micro 2.0", not by replacing PC compatible machines but by supplementing them.[249] In March 2012 Stephen Pritchard echoed the BBC Micro successor sentiment in ITPRO.[250] Alex Hope, co-author of the Next Gen report, is hopeful that the computer will engage children with the excitement of programming.[251] Co-author Ian Livingstone suggested that the BBC could be involved in building support for the device, possibly branding it as the BBC Nano.[252] The Centre for Computing History strongly supports the Raspberry Pi project, feeling that it could "usher in a new era".[253] Before release, the board was showcased by ARM's CEO Warren East at an event in Cambridge outlining Google's ideas to improve UK science and technology education.[254]
 Harry Fairhead, however, suggests that more emphasis should be put on improving the educational software available on existing hardware, using tools such as Google App Inventor to return programming to schools, rather than adding new hardware choices.[255] Simon Rockman, writing in a ZDNet blog, was of the opinion that teens will have "better things to do", despite what happened in the 1980s.[256]
 In October 2012, the Raspberry Pi won T3's Innovation of the Year award,[257] and futurist Mark Pesce cited a (borrowed) Raspberry Pi as the inspiration for his ambient device project MooresCloud.[258] In October 2012, the British Computer Society reacted to the announcement of enhanced specifications by stating, "it's definitely something we'll want to sink our teeth into."[259]
 In June 2017, Raspberry Pi won the Royal Academy of Engineering MacRobert Award.[260] The citation for the award to the Raspberry Pi said it was "for its inexpensive credit card-sized microcomputers, which are redefining how people engage with computing, inspiring students to learn coding and computer science and providing innovative control solutions for industry."[261]
 Clusters of hundreds of Raspberry Pis have been used for testing programs destined for supercomputers.[262]
 The Raspberry Pi community was described by Jamie Ayre of FOSS software company AdaCore as one of the most exciting parts of the project.[263] Community blogger Russell Davis said that the community strength allows the Foundation to concentrate on documentation and teaching.[263] The community developed a fanzine around the platform called The MagPi[264] which in 2015, was handed over to Raspberry Pi (Trading) Ltd by its volunteers to be continued in-house.[265] A series of community Raspberry Jam events have been held across the UK and around the world.[266]
 As of January 2012[update], enquiries about the board in the United Kingdom have been received from schools in both the state and private sectors, with around five times as much interest from the latter. It is hoped that businesses will sponsor purchases for less advantaged schools.[267] The CEO of Premier Farnell said that the government of a country in the Middle East has expressed interest in providing a board to every schoolgirl, to enhance her employment prospects.[268][269]
 In 2014, the Raspberry Pi Foundation hired a number of its community members including ex-teachers and software developers to launch a set of free learning resources for its website.[270] The Foundation also started a teacher training course called Picademy with the aim of helping teachers prepare for teaching the new computing curriculum using the Raspberry Pi in the classroom.[271]
 In 2018, NASA launched the JPL Open Source Rover Project, which is a scaled down version of Curiosity rover and uses a Raspberry Pi as the control module, to encourage students and hobbyists to get involved in mechanical, software, electronics, and robotics engineering.[272]
 There are a number of developers and applications that are using the Raspberry Pi for home automation. These programmers are making an effort to modify the Raspberry Pi into a cost-affordable solution in energy monitoring and power consumption. Because of the relatively low cost of the Raspberry Pi, this has become a popular and economical alternative to the more expensive commercial solutions.[citation needed]
 In June 2014, Polish industrial automation manufacturer TECHBASE released ModBerry, an industrial computer based on the Raspberry Pi Compute Module. The device has a number of interfaces, most notably RS-485/232 serial ports, digital and analogue inputs/outputs, CAN and economical 1-Wire buses, all of which are widely used in the automation industry. The design allows the use of the Compute Module in harsh industrial environments, leading to the conclusion that the Raspberry Pi is no longer limited to home and science projects, but can be widely used as an Industrial IoT solution and achieve goals of Industry 4.0.[273]
 In March 2018, SUSE announced commercial support for SUSE Linux Enterprise on the Raspberry Pi 3 Model B to support a number of undisclosed customers implementing industrial monitoring with the Raspberry Pi.[274]
 In January 2021, TECHBASE announced a Raspberry Pi Compute Module 4 cluster for AI accelerator, routing and file server use. The device contains one or more standard Raspberry Pi Compute Module 4s in an industrial DIN rail housing, with some versions containing one or more Coral Edge tensor processing units.[275]
 The Organelle is a portable synthesizer, a sampler, a sequencer, and an effects processor designed and assembled by Critter & Guitari. It incorporates a Raspberry Pi computer module running Linux.[276]
 OTTO is a digital camera created by Next Thing Co. It incorporates a Raspberry Pi Compute Module. It was successfully crowd-funded in a May 2014 Kickstarter campaign.[277]
 Slice is a digital media player which also uses a Compute Module as its heart. It was crowd-funded in an August 2014 Kickstarter campaign. The software running on Slice is based on Kodi.[278]
 Numerous commercial thin client computer terminals use the Raspberry Pi.[279]
 AutoPi TMU device is a telematics unit which is built on top of a Raspberry Pi Compute Module 4 and incorporates the philosophy of which Raspberry Pi was built upon.[280]
 During the COVID-19 pandemic, demand increased primarily due to the increase in remote work, but also because of the use of many Raspberry Pi Zeros in ventilators for COVID-19 patients in countries such as Colombia,[281] which were used to combat strain on the healthcare system. In March 2020, Raspberry Pi sales reached 640,000 units, the second largest month of sales in the company's history.[282]
 A project was launched in December 2014 at an event held by the UK Space Agency. The Astro Pi was an augmented Raspberry Pi that included a sensor hat with a visible light or infrared camera. The Astro Pi competition, called Principia, was officially opened in January and was opened to all primary and secondary school aged children who were residents of the United Kingdom. During his mission, British ESA astronaut Tim Peake deployed the computers on board the International Space Station.[283] He loaded the winning code while in orbit, collected the data generated and then sent this to Earth where it was distributed to the winning teams. Covered themes during the competition included spacecraft sensors, satellite imaging, space measurements, data fusion and space radiation.
 The organisations involved in the Astro Pi competition include the UK Space Agency, UKspace, Raspberry Pi, ESERO-UK and ESA.
 In 2017, the European Space Agency ran another competition open to all students in the European Union called Proxima. The winning programs were run on the ISS by Thomas Pesquet, a French astronaut.[284] In December 2021, the Dragon 2 spacecraft launched by NASA had a pair of Astro Pi in it.[285]
 The computer is inspired by Acorn's BBC Micro of 1981.[286][287] The Model A, Model B and Model B+ names are references to the original models of the British educational BBC Micro computer, developed by Acorn Computers.[288]
 According to Upton, the name "Raspberry Pi" was chosen with "Raspberry" as an ode to a tradition of naming early computer companies after fruit, and "Pi" as a reference to the Python programming language.[289]
 In 2006, early concepts of the Raspberry Pi were based on the Atmel ATmega644 microcontroller. Its schematics and PCB layout are publicly available.[290] Foundation trustee Eben Upton assembled a group of teachers, academics and computer enthusiasts to devise a computer to inspire children.[267]
 The first ARM prototype version of the computer was mounted in a package the same size as a USB memory stick.[291] It had a USB port on one end and an HDMI port on the other.
 The Foundation's goal was to offer two versions, priced at US$25 and $35. They started accepting orders for the higher priced Model B on 29 February 2012,[292] the lower cost Model A on 4 February 2013.[293] and the even lower cost (US$20) A+ on 10 November 2014.[109] On 26 November 2015, the cheapest Raspberry Pi yet, the Raspberry Pi Zero, was launched at US$5 or £4.[294]
 
 According to Raspberry Pi, more than 5 million Raspberry Pis were sold by February 2015, making it the best-selling British computer.[12] By November 2016 they had sold 11 million units,[367][386] and 12.5 million by March 2017, making it the third best-selling "general purpose computer".[387] In July 2017, sales reached nearly 15 million,[388] climbing to 19 million in March 2018.[16] By December 2019, a total of 30 million devices had been sold.[389][390]
 The global chip shortage starting in 2020, as well as an uptake in demand starting in early 2021, notably affected the Raspberry Pi, causing significant availability issues from that time onward.[391]  The company explained its approach to the shortages in 2021,[392] and April 2022,[393] explaining that it was prioritising business and industrial customers.
 The situation is sufficiently long term that at least one automated stock checker is online.[394]


Source: https://en.wikipedia.org/wiki/Wissenschaftsgemeinde
Content: Other reasons this message may be displayed:


Source: https://en.wikipedia.org/wiki/Project_Jupyter#Jupyter_Notebook
Content: Project Jupyter (/ˈdʒuːpɪtər/ ⓘ) is a project to develop open-source software, open standards, and services for interactive computing across multiple programming languages.
 It was spun off from IPython in 2014 by Fernando Pérez and Brian Granger. Project Jupyter's name is a reference to the three core programming languages supported by Jupyter, which are Julia, Python and R. Its name and logo are an homage to Galileo's discovery of the moons of Jupiter, as documented in notebooks attributed to Galileo.
 Jupyter is financially sponsored by NumFOCUS.[1]
 The first version of Notebooks for IPython was released in 2011 by a team including Fernando Pérez, Brian Granger, and Min Ragan-Kelley.[2] In 2014, Pérez announced a spin-off project from IPython called Project Jupyter.[3] IPython continues to exist as a Python shell and a kernel for Jupyter, while the notebook and other language-agnostic parts of IPython moved under the Jupyter name.[4][5] Jupyter supports execution environments (called "kernels") in several dozen languages, including Julia, R, Haskell, Ruby, and Python (via the IPython kernel).
 In 2015, about 200,000 Jupyter notebooks were available on GitHub. By 2018, about 2.5 million were available.[6] In January 2021, nearly 10 million were available, including notebooks about the first observation of gravitational waves[7] and about the 2019 discovery of a supermassive black hole.[8]
 Major cloud computing providers have adopted the Jupyter Notebook or derivative tools as a frontend interface for cloud users. Examples include Amazon SageMaker Notebooks,[9] Google's Colaboratory,[10][11] and Microsoft's Azure Notebook.[12]
 Visual Studio Code supports local development of Jupyter notebooks. As of July 2022, the Jupyter extension for VS Code has been downloaded over 40 million times, making it the second-most popular extension in the VS Code Marketplace.[13]
 The steering committee of Project Jupyter received the 2017 ACM Software System Award, an annual award that honors people or an organization "for developing a software system that has had a lasting influence, reflected in contributions to concepts, in commercial acceptance, or both".[14]
 The Atlantic published an article entitled "The Scientific Paper Is Obsolete" in 2018, discussing the role of Jupyter Notebook and the Mathematica notebook in the future of scientific publishing.[15] Economist Paul Romer, in response, published a blog post in which he reflected on his experiences using Mathematica and Jupyter for research, concluding in part that Jupyter "does a better job of delivering what Theodore Gray had in mind when he designed the Mathematica notebook."[16]
 In 2021, Nature named Jupyter as one of ten computing projects that transformed science.[8]
 Jupyter Notebook can colloquially refer to two different concepts, either the user-facing application to edit code and text, or the underlying file format which is interoperable across many implementations.
 Jupyter Notebook (formerly IPython Notebook) is a web-based interactive computational environment for creating notebook documents. Jupyter Notebook is built using several open-source libraries, including IPython, ZeroMQ, Tornado, jQuery, Bootstrap, and MathJax. A Jupyter Notebook application is a browser-based REPL containing an ordered list of input/output cells which can contain code, text (using Github Flavored Markdown), mathematics, plots and rich media.
 Jupyter Notebook is similar to the notebook interface of other programs such as Maple, Mathematica, and SageMath, a computational interface style that originated with Mathematica in the 1980s. Jupyter interest overtook the popularity of the Mathematica notebook interface in early 2018.[15]
 JupyterLab is a newer user interface for Project Jupyter, offering a flexible user interface and more features than the classic notebook UI. The first stable release was announced on February 20, 2018.[17][18] In 2015, a joint $6 million grant from The Leona M. and Harry B. Helmsley Charitable Trust, The Gordon and Betty Moore Foundation, and The Alfred P. Sloan Foundation funded work that led to expanded capabilities of the core Jupyter tools, as well as to the creation of JupyterLab.[19]
 GitHub announced in November 2022 that JupyterLab would be available in its online Coding platform called Codespace.[20]
 In August 2023, Jupyter AI, a Jupyter extension, was released. This extension incorporates generative artificial intelligence into Jupyter notebooks, enabling users to explain and generate code, rectify errors, summarize content, inquire about their local files, and generate complete notebooks based on natural language prompts. [21]
 JupyterHub is a multi-user server for Jupyter Notebooks.  It is designed to support many users by spawning, managing, and proxying many singular Jupyter Notebook servers.[22]
 A Jupyter Notebook document is a JSON file, following a versioned schema, usually ending with the ".ipynb" extension.
The main parts of the Jupyter Notebooks are: Metadata, Notebook format and list of cells. Metadata is a data Dictionary of definitions to set up and display the notebook. Notebook Format is a version number of the software. List of cells are different types of Cells for Markdown (display), Code (to execute), and output of the code type cells.[23]
 While JSON is the most common format, it is possible to forgo some features (like storing images and metadata), and save notebooks as markdown documents using extensions like JupyText.[24] Jupytext is often used in conjunction with version control to make diffing and merging of notebook simpler.


Source: https://en.wikipedia.org/wiki/NumPy
Content: NumPy (pronounced /ˈnʌmpaɪ/ NUM-py) is a library for the Python programming language, adding support for large, multi-dimensional arrays and matrices, along with a large collection of high-level mathematical functions to operate on these arrays.[3] The predecessor of NumPy, Numeric, was originally created by Jim Hugunin with contributions from several other developers. In 2005, Travis Oliphant created NumPy by incorporating features of the competing Numarray into Numeric, with extensive modifications. NumPy is open-source software and has many contributors. NumPy is a NumFOCUS fiscally sponsored project.[4]
 The Python programming language was not originally designed for numerical computing, but attracted the attention of the scientific and engineering community early on. In 1995 the special interest group (SIG) matrix-sig was founded with the aim of defining an array computing package; among its members was Python designer and maintainer Guido van Rossum, who extended Python's syntax (in particular the indexing syntax[5]) to make array computing easier.[6]
 An implementation of a matrix package was completed by Jim Fulton, then generalized[further explanation needed] by Jim Hugunin and called Numeric[6] (also variously known as the "Numerical Python extensions" or "NumPy"), with influences from the APL family of languages, Basis, MATLAB, FORTRAN, S and S+, and others.[7][8]
Hugunin, a graduate student at the Massachusetts Institute of Technology (MIT),[8]: 10  joined the Corporation for National Research Initiatives (CNRI) in 1997 to work on JPython,[6] leaving Paul Dubois of Lawrence Livermore National Laboratory (LLNL) to take over as maintainer.[8]: 10  Other early contributors include David Ascher, Konrad Hinsen and Travis Oliphant.[8]: 10 
 A new package called Numarray was written as a more flexible replacement for Numeric.[9] Like Numeric, it too is now deprecated.[10][11] Numarray had faster operations for large arrays, but was slower than Numeric on small ones,[12] so for a time both packages were used in parallel for different use cases. The last version of Numeric (v24.2) was released on 11 November 2005, while the last version of numarray (v1.5.2) was released on 24 August 2006.[13]
 There was a desire to get Numeric into the Python standard library, but Guido van Rossum decided that the code was not maintainable in its state then.[when?][14]
 In early 2005, NumPy developer Travis Oliphant wanted to unify the community around a single array package and ported Numarray's features to Numeric, releasing the result as NumPy 1.0 in 2006.[9] This new project was part of SciPy. To avoid installing the large SciPy package just to get an array object, this new package was separated and called NumPy. Support for Python 3 was added in 2011 with NumPy version 1.5.0.[15]
 In 2011, PyPy started development on an implementation of the NumPy API for PyPy.[16] As of 2023, it is not yet fully compatible with NumPy.[17]
 NumPy targets the CPython reference implementation of Python, which is a non-optimizing bytecode interpreter. Mathematical algorithms written for this version of Python often run much slower than compiled equivalents due to the absence of compiler optimization. NumPy addresses the slowness problem partly by providing multidimensional arrays and functions and operators that operate efficiently on arrays; using these requires rewriting some code, mostly inner loops, using NumPy.
 Using NumPy in Python gives functionality comparable to MATLAB since they are both interpreted,[18] and they both allow the user to write fast programs as long as most operations work on arrays or matrices instead of scalars. In comparison, MATLAB boasts a large number of additional toolboxes, notably Simulink, whereas NumPy is intrinsically integrated with Python, a more modern and complete programming language. Moreover, complementary Python packages are available; SciPy is a library that adds more MATLAB-like functionality and Matplotlib is a plotting package that provides MATLAB-like plotting functionality. Although matlab can perform sparse matrix operations, numpy alone cannot perform such operations and requires the use of the scipy.sparse library. Internally, both MATLAB and NumPy rely on BLAS and LAPACK for efficient linear algebra computations.
 Python bindings of the widely used computer vision library OpenCV utilize NumPy arrays to store and operate on data.
Since images with multiple channels are simply represented as three-dimensional arrays, indexing, slicing or masking with other arrays are very efficient ways to access specific pixels of an image.
The NumPy array as universal data structure in OpenCV for images, extracted feature points, filter kernels and many more vastly simplifies the programming workflow and debugging.[citation needed]
 Importantly, many NumPy operations release the global interpreter lock, which allows for multithreaded processing.[19]
 NumPy also provides a C API, which allows Python code to interoperate with external libraries written in low-level languages.[20]
 The core functionality of NumPy is its "ndarray", for n-dimensional array, data structure. These arrays are strided views on memory.[9] In contrast to Python's built-in list data structure, these arrays are homogeneously typed: all elements of a single array must be of the same type.
 Such arrays can also be views into memory buffers allocated by C/C++, Python, and Fortran extensions to the CPython interpreter without the need to copy data around, giving a degree of compatibility with existing numerical libraries. This functionality is exploited by the SciPy package, which wraps a number of such libraries (notably BLAS and LAPACK). NumPy has built-in support for memory-mapped ndarrays.[9]
 Inserting or appending entries to an array is not as trivially possible as it is with Python's lists.
The np.pad(...) routine to extend arrays actually creates new arrays of the desired shape and padding values, copies the given array into the new one and returns it.
NumPy's np.concatenate([a1,a2]) operation does not actually link the two arrays but returns a new one, filled with the entries from both given arrays in sequence.
Reshaping the dimensionality of an array with np.reshape(...) is only possible as long as the number of elements in the array does not change.
These circumstances originate from the fact that NumPy's arrays must be views on contiguous memory buffers. A replacement package called Blaze attempts to overcome this limitation.[21]
 Algorithms that are not expressible as a vectorized operation will typically run slowly because they must be implemented in "pure Python", while vectorization may increase memory complexity of some operations from constant to linear, because temporary arrays must be created that are as large as the inputs. Runtime compilation of numerical code has been implemented by several groups to avoid these problems; open source solutions that interoperate with NumPy include numexpr[22] and Numba.[23] Cython and Pythran are static-compiling alternatives to these.
 Many modern large-scale scientific computing applications have requirements that exceed the capabilities of the NumPy arrays.
For example, NumPy arrays are usually loaded into a computer's memory, which might have insufficient capacity for the analysis of large datasets.
Further, NumPy operations are executed on a single CPU.
However, many linear algebra operations can be accelerated by executing them on clusters of CPUs or of specialized hardware, such as GPUs and TPUs, which many deep learning applications rely on.
As a result, several alternative array implementations have arisen in the scientific python ecosystem over the recent years, such as Dask for distributed arrays and TensorFlow or JAX for computations on GPUs.
Because of its popularity, these often implement a subset of NumPy's API or mimic it, so that users can change their array implementation with minimal changes to their code required.[3] A library named CuPy,[24] accelerated by Nvidia's CUDA framework, has also shown potential for faster computing, being a 'drop-in replacement' of NumPy.[25]
 Iterative Python algorithm and vectorized NumPy version.
 Quickly wrap native code for faster scripts.[26][27][28]


Source: https://en.wikipedia.org/wiki/Matplotlib
Content: Matplotlib is a plotting library for the Python programming language and its numerical mathematics extension NumPy. It provides an object-oriented API for embedding plots into applications using general-purpose GUI toolkits like Tkinter, wxPython, Qt, or GTK. There is also a procedural "pylab" interface based on a state machine (like OpenGL), designed to closely resemble that of MATLAB, though its use is discouraged.[3] SciPy makes use of Matplotlib.
 Matplotlib was originally written by John D. Hunter. Since then it has had an active development community[4] and is distributed under a BSD-style license. Michael Droettboom was nominated as matplotlib's lead developer shortly before John Hunter's death in August 2012[5] and was further joined by Thomas Caswell.[6][7] Matplotlib is a NumFOCUS fiscally sponsored project.[8]
 Pyplot is a Matplotlib module that provides a MATLAB-like interface.[9] Matplotlib is designed to be as usable as MATLAB, with the ability to use Python, and the advantage of being free and open-source.[citation needed]
 Several toolkits are available which extend Matplotlib functionality. Some are separate downloads, others ship with the Matplotlib source code but have external dependencies.[10]


Source: https://en.wikipedia.org/wiki/Anaconda_(Python-Distribution)
Content: Other reasons this message may be displayed:


Source: https://en.wikipedia.org/wiki/SciPy
Content: SciPy (pronounced /ˈsaɪpaɪ/ "sigh pie"[2]) is a free and open-source Python library used for scientific computing and technical computing.[3]
 SciPy contains modules for optimization, linear algebra, integration, interpolation, special functions, FFT, signal and image processing, ODE solvers and other tasks common in science and engineering.
 SciPy is also a family of conferences for users and developers of these tools: SciPy (in the United States), EuroSciPy (in Europe) and SciPy.in (in India).[4] Enthought originated the SciPy conference in the United States and continues to sponsor many of the international conferences as well as host the SciPy website.
 The SciPy library is currently distributed under the BSD license, and its development is sponsored and supported by an open community of developers. It is also supported by NumFOCUS, a community foundation for supporting reproducible and accessible science.
 The SciPy package is at the core of Python's scientific computing capabilities. Available sub-packages include:
 The basic data structure used by SciPy is a multidimensional array provided by the NumPy module. NumPy provides some functions for linear algebra, Fourier transforms, and random number generation, but not with the generality of the equivalent functions in SciPy. NumPy can also be used as an efficient multidimensional container of data with arbitrary datatypes. This allows NumPy to seamlessly and speedily integrate with a wide variety of databases. Older versions of SciPy used Numeric as an array type, which is now deprecated in favor of the newer NumPy array code.[6]
 In the 1990s, Python was extended to include an array type for numerical computing called Numeric. (This package was eventually replaced by NumPy, which was written by Travis Oliphant in 2006 as a blending of Numeric and Numarray, with Numarray itself being started in 2001.) As of 2000, there was a growing number of extension modules and increasing interest in creating a complete environment for scientific and technical computing. In 2001, Travis Oliphant, Eric Jones, and Pearu Peterson merged code they had written and called the resulting package SciPy. The newly created package provided a standard collection of common numerical operations on top of the Numeric array data structure. Shortly thereafter, Fernando Pérez released IPython, an enhanced interactive shell widely used in the technical computing community, and John Hunter released the first version of Matplotlib, the 2D plotting library for technical computing. Since then the SciPy environment has continued to grow with more packages and tools for technical computing.[7][8][9]


Source: https://en.wikipedia.org/wiki/TensorFlow
Content: 
 TensorFlow is a free and open-source software library for machine learning and artificial intelligence. It can be used across a range of tasks but has a particular focus on training and inference of deep neural networks.[3][4]
 It was developed by the Google Brain team for Google's internal use in research and production.[5][6][7] The initial version was released under the Apache License 2.0 in 2015.[1][8] Google released an updated version, TensorFlow 2.0, in September 2019.[9]
 TensorFlow can be used in a wide variety of programming languages, including Python, JavaScript, C++, and Java,[10] facilitating its use in a range of applications in many sectors.
 Starting in 2011, Google Brain built DistBelief as a proprietary machine learning system based on deep learning neural networks. Its use grew rapidly across diverse Alphabet companies in both research and commercial applications.[11][12] Google assigned multiple computer scientists, including Jeff Dean, to simplify and refactor the codebase of DistBelief into a faster, more robust application-grade library, which became TensorFlow.[13] In 2009, the team, led by Geoffrey Hinton, had implemented generalized backpropagation and other improvements, which allowed generation of neural networks with substantially higher accuracy, for instance a 25% reduction in errors in speech recognition.[14]
 TensorFlow is Google Brain's second-generation system. Version 1.0.0 was released on February 11, 2017.[15]  While the reference implementation runs on single devices, TensorFlow can run on multiple CPUs and GPUs (with optional CUDA and SYCL extensions for general-purpose computing on graphics processing units).[16] TensorFlow is available on 64-bit Linux, macOS, Windows, and mobile computing platforms including Android and iOS.
 Its flexible architecture allows for the easy deployment of computation across a variety of platforms (CPUs, GPUs, TPUs), and from desktops to clusters of servers to mobile and edge devices.
 TensorFlow computations are expressed as stateful dataflow graphs. The name TensorFlow derives from the operations that such neural networks perform on multidimensional data arrays, which are referred to as tensors.[17] During the Google I/O Conference in June 2016, Jeff Dean stated that 1,500 repositories on GitHub mentioned TensorFlow, of which only 5 were from Google.[18]
 In December 2017, developers from Google, Cisco, RedHat, CoreOS, and CaiCloud introduced Kubeflow at a conference. Kubeflow allows operation and deployment of TensorFlow on Kubernetes.
 In March 2018, Google announced TensorFlow.js version 1.0 for machine learning in JavaScript.[19]
 In Jan 2019, Google announced TensorFlow 2.0.[20] It became officially available in Sep 2019.[9]
 In May 2019, Google announced TensorFlow Graphics for deep learning in computer graphics.[21]
 In May 2016, Google announced its Tensor processing unit (TPU), an application-specific integrated circuit (ASIC, a hardware chip) built specifically for machine learning and tailored for TensorFlow. A TPU is a programmable AI accelerator designed to provide high throughput of low-precision arithmetic (e.g., 8-bit), and oriented toward using or running models rather than training them. Google announced they had been running TPUs inside their data centers for more than a year, and had found them to deliver an order of magnitude better-optimized performance per watt for machine learning.[22]
 In May 2017, Google announced the second-generation, as well as the availability of the TPUs in Google Compute Engine.[23] The second-generation TPUs deliver up to 180 teraflops of performance, and when organized into clusters of 64 TPUs, provide up to 11.5 petaflops.
 In May 2018, Google announced the third-generation TPUs delivering up to 420 teraflops of performance and 128 GB high bandwidth memory (HBM). Cloud TPU v3 Pods offer 100+ petaflops of performance and 32 TB HBM.[24]
 In February 2018, Google announced that they were making TPUs available in beta on the Google Cloud Platform.[25]
 In July 2018, the Edge TPU was announced. Edge TPU is Google's purpose-built ASIC chip designed to run TensorFlow Lite machine learning (ML) models on small client computing devices such as smartphones[26] known as edge computing.
 In May 2017, Google announced a software stack specifically for mobile development, TensorFlow Lite.[27] In January 2019, the TensorFlow team released a developer preview of the mobile GPU inference engine with OpenGL ES 3.1 Compute Shaders on Android devices and Metal Compute Shaders on iOS devices.[28] In May 2019, Google announced that their TensorFlow Lite Micro (also known as TensorFlow Lite for Microcontrollers) and ARM's uTensor would be merging.[29]
 In October 2017, Google released the Google Pixel 2 which featured their Pixel Visual Core (PVC), a fully programmable image, vision and AI processor for mobile devices. The PVC supports TensorFlow for machine learning (and Halide for image processing).
 As TensorFlow's market share among research papers was declining to the advantage of PyTorch,[30] the TensorFlow Team announced a release of a new major version of the library in September 2019. TensorFlow 2.0 introduced many changes, the most significant being TensorFlow eager, which changed the automatic differentiation scheme from the static computational graph to the "Define-by-Run" scheme originally made popular by Chainer and later PyTorch.[30] Other major changes included removal of old libraries, cross-compatibility between trained models on different versions of TensorFlow, and significant improvements to the performance on GPU.[31][non-primary source needed]
 AutoDifferentiation is the process of automatically calculating the gradient vector of a model with respect to each of its parameters. With this feature, TensorFlow can automatically compute the gradients for the parameters in a model, which is useful to algorithms such as backpropagation which require gradients to optimize performance.[32] To do so, the framework must keep track of the order of operations done to the input Tensors in a model, and then compute the gradients with respect to the appropriate parameters.[32]
 TensorFlow includes an “eager execution” mode, which means that operations are evaluated immediately as opposed to being added to a computational graph which is executed later.[33] Code executed eagerly can be examined step-by step-through a debugger, since data is augmented at each line of code rather than later in a computational graph.[33] This execution paradigm is considered to be easier to debug because of its step by step transparency.[33]
 In both eager and graph executions, TensorFlow provides an API for distributing computation across multiple devices with various distribution strategies.[34] This distributed computing can often speed up the execution of training and evaluating of TensorFlow models and is a common practice in the field of AI.[34][35]
 To train and assess models, TensorFlow provides a set of loss functions (also known as cost functions).[36] Some popular examples include mean squared error (MSE) and binary cross entropy (BCE).[36] These loss functions compute the “error” or “difference” between a model's output and the expected output (more broadly, the difference between two tensors). For different datasets and models, different losses are used to prioritize certain aspects of performance.
 In order to assess the performance of machine learning models, TensorFlow gives API access to commonly used metrics. Examples include various accuracy metrics (binary, categorical, sparse categorical) along with other metrics such as Precision, Recall, and Intersection-over-Union (IoU).[37]
 TensorFlow.nn is a module for executing primitive neural network operations on models.[38] Some of these operations include variations of convolutions (1/2/3D, Atrous, depthwise), activation functions (Softmax, RELU, GELU, Sigmoid, etc.) and their variations, and other operations (max-pooling, bias-add, etc.).[38]
 TensorFlow offers a set of optimizers for training neural networks, including ADAM, ADAGRAD, and Stochastic Gradient Descent (SGD).[39] When training a model, different optimizers offer different modes of parameter tuning, often affecting a model's convergence and performance.[40]
 TensorFlow serves as a core platform and library for machine learning. TensorFlow's APIs use Keras to allow users to make their own machine-learning models.[41] In addition to building and training their model, TensorFlow can also help load the data to train the model, and deploy it using TensorFlow Serving.[42]
 TensorFlow provides a stable Python Application Program Interface (API),[43] as well as APIs without backwards compatibility guarantee for Javascript,[44] C++,[45] and Java.[46][10] Third-party language binding packages are also available for C#,[47][48] Haskell,[49] Julia,[50] MATLAB,[51] Object Pascal,[52] R,[53] Scala,[54] Rust,[55] OCaml,[56] and Crystal.[57] Bindings that are now archived and unsupported include Go[58] and Swift.[59]
 TensorFlow also has a library for machine learning in JavaScript. Using the provided JavaScript APIs, TensorFlow.js allows users to use either Tensorflow.js models or converted models from TensorFlow or TFLite, retrain the given models, and run on the web.[42][60]
 TensorFlow Lite has APIs for mobile apps or embedded devices to generate and deploy TensorFlow models.[61] These models are compressed and optimized in order to be more efficient and have a higher performance on smaller capacity devices.[62]
 TensorFlow Lite uses FlatBuffers as the data serialization format for network models, eschewing the Protocol Buffers format used by standard TensorFlow models.[62]
 TensorFlow Extended (abbrev. TFX) provides numerous components to perform all the operations needed for end-to-end production.[63] Components include loading, validating, and transforming data, tuning, training, and evaluating the machine learning model, and pushing the model itself into production.[42][63]
 Numpy is one of the most popular Python data libraries, and TensorFlow offers integration and compatibility with its data structures.[64] Numpy NDarrays, the library's native datatype, are automatically converted to TensorFlow Tensors in TF operations; the same is also true vice versa.[64] This allows for the two libraries to work in unison without requiring the user to write explicit data conversions. Moreover, the integration extends to memory optimization by having TF Tensors share the underlying memory representations of Numpy NDarrays whenever possible.[64]
 TensorFlow also offers a variety of libraries and extensions to advance and extend the models and methods used.[65] For example, TensorFlow Recommenders and TensorFlow Graphics are libraries for their respective functionalities in recommendation systems and graphics, TensorFlow Federated provides a framework for decentralized data, and TensorFlow Cloud allows users to directly interact with Google Cloud to integrate their local code to Google Cloud.[66] Other add-ons, libraries, and frameworks include TensorFlow Model Optimization, TensorFlow Probability, TensorFlow Quantum, and TensorFlow Decision Forests.[65][66]
 Google also released Colaboratory, a TensorFlow Jupyter notebook environment that does not require any setup.[67] It runs on Google Cloud and allows users free access to GPUs and the ability to store and share notebooks on Google Drive.[68]
 Google JAX is a machine learning framework for transforming numerical functions.[69][70][71] It is described as bringing together a modified version of autograd (automatic obtaining of the gradient function through differentiation of a function) and TensorFlow's XLA (Accelerated Linear Algebra). It is designed to follow the structure and workflow of NumPy as closely as possible and works with TensorFlow as well as other frameworks such as PyTorch. The primary functions of JAX are:[69]
 GE Healthcare used TensorFlow to increase the speed and accuracy of MRIs in identifying specific body parts.[72] Google used TensorFlow to create DermAssist, a free mobile application that allows users to take pictures of their skin and identify potential health complications.[73] Sinovation Ventures used TensorFlow to identify and classify eye diseases from optical coherence tomography (OCT) scans.[73]
 Twitter implemented TensorFlow to rank tweets by importance for a given user, and changed their platform to show tweets in order of this ranking.[74] Previously, tweets were simply shown in reverse chronological order.[74] The photo sharing app VSCO used TensorFlow to help suggest custom filters for photos.[73]
 Google officially released RankBrain on October 26, 2015, backed by TensorFlow.[75]
 InSpace, a virtual learning platform, used TensorFlow to filter out toxic chat messages in classrooms.[76] Liulishuo, an online English learning platform, utilized TensorFlow to create an adaptive curriculum for each student.[77] TensorFlow was used to accurately assess a student's current abilities, and also helped decide the best future content to show based on those capabilities.[77]
 The e-commerce platform Carousell used TensorFlow to provide personalized recommendations for customers.[73] The cosmetics company ModiFace used TensorFlow to create an augmented reality experience for customers to test various shades of make-up on their face.[78]
 TensorFlow is the foundation for the automated image-captioning software DeepDream.[79]


Source: https://en.wikipedia.org/wiki/Keras
Content: Keras is an open-source library that provides a Python interface for artificial neural networks. Keras acts as an interface for the TensorFlow library.[citation needed]
 The name 'Keras' derives from the Ancient Greek word κέρας (Keras) meaning 'horn'.[2]
 Designed to enable fast experimentation with deep neural networks, Keras focuses on being user-friendly, modular, and extensible. It was developed as part of the research effort of project ONEIROS (Open-ended Neuro-Electronic Intelligent Robot Operating System),[3] and its primary author and maintainer is François Chollet, a Google engineer. Chollet is also the author of the Xception deep neural network model.[4]
 Up until version 2.3, Keras supported multiple backends, including TensorFlow, Microsoft Cognitive Toolkit, Theano, and PlaidML.[5][6][7]
 As of version 2.4, only TensorFlow is supported. Starting with version 3.0 (as well as its preview version, Keras Core), however, Keras is to become multi-backend again, supporting TensorFlow, JAX, and PyTorch.[8]
 Keras contains numerous implementations of commonly used neural-network building blocks such as layers, objectives, activation functions, optimizers, and a host of tools for working with image and text data to simplify programming in deep neural network area. The code is hosted on GitHub, and community support forums include the GitHub issues page, and a Slack channel.[citation needed]
 In addition to standard neural networks, Keras has support for convolutional and recurrent neural networks. It supports other common utility layers like dropout, batch normalization, and pooling.[9]
 Keras allows users to produce deep models on smartphones (iOS and Android), on the web, or on the Java Virtual Machine.[6] It also allows use of distributed training of deep-learning models on clusters of graphics processing units (GPU) and tensor processing units (TPU).[10]


Source: https://en.wikipedia.org/wiki/Scikit-learn
Content: scikit-learn (formerly scikits.learn and also known as sklearn) is a free software machine learning library for the Python programming language.[3]
It features various classification, regression and clustering algorithms including support-vector machines, random forests, gradient boosting, k-means and DBSCAN, and is designed to interoperate with the Python numerical and scientific libraries NumPy and SciPy. Scikit-learn is a NumFOCUS fiscally sponsored project.[4]
 The scikit-learn project started as scikits.learn, a Google Summer of Code project by French data scientist David Cournapeau. The name of the project stems from the notion that it is a "SciKit" (SciPy Toolkit), a separately developed and distributed third-party extension to SciPy.[5]
The original codebase was later rewritten by other developers. In 2010, contributors Fabian Pedregosa, Gaël Varoquaux, Alexandre Gramfort and Vincent Michel, from the French Institute for Research in Computer Science and Automation in Saclay, France, took leadership of the project and released the first public version of the library on February 1, 2010.[6] In November 2012, scikit-learn as well as scikit-image were described as two of the "well-maintained and popular"  scikits libraries[update].[7] In 2019, it was noted that scikit-learn is one of the most popular machine learning libraries on GitHub.[8]
 scikit-learn is largely written in Python, and uses NumPy extensively for high-performance linear algebra and array operations. Furthermore, some core algorithms are written in Cython to improve performance. Support vector machines are implemented by a Cython wrapper around LIBSVM; logistic regression and linear support vector machines by a similar wrapper around LIBLINEAR. In such cases, extending these methods with Python may not be possible.
 scikit-learn integrates well with many other Python libraries, such as Matplotlib and plotly for plotting, NumPy for array vectorization, Pandas dataframes, SciPy, and many more.
 scikit-learn was initially developed by David Cournapeau as a Google Summer of Code project in 2007. Later that year, Matthieu Brucher joined the project and started to use it as a part of his thesis work. In 2010, INRIA, the French Institute for Research in Computer Science and Automation, got involved and the first public release (v0.1 beta) was published in late January 2010.


Source: https://en.wikipedia.org/wiki/PyTorch
Content: PyTorch is a machine learning library based on the Torch library,[4][5][6] used for applications such as computer vision and natural language processing,[7] originally developed by Meta AI and now part of the Linux Foundation umbrella.[8][9][10][11] It is recognized as one of the two most popular machine learning libraries alongside TensorFlow, offering free and open-source software released under the modified BSD license. Although the Python interface is more polished and the primary focus of development, PyTorch also has a C++ interface.[12]
 A number of pieces of deep learning software are built on top of PyTorch, including Tesla Autopilot,[13] Uber's Pyro,[14] Hugging Face's Transformers,[15] PyTorch Lightning,[16][17] and Catalyst.[18][19]
 PyTorch provides two high-level features:[20]
 Meta (formerly known as Facebook) operates both PyTorch and Convolutional Architecture for Fast Feature Embedding (Caffe2), but models defined by the two frameworks were mutually incompatible. The Open Neural Network Exchange (ONNX) project was created by Meta and Microsoft in September 2017 for converting models between frameworks. Caffe2 was merged into PyTorch at the end of March 2018.[21] In September 2022, Meta announced that PyTorch would be governed by PyTorch Foundation, a newly created independent organization – a subsidiary of Linux Foundation.[22]
 PyTorch 2.0 was released on 15 March 2023.[23]
 PyTorch defines a class called Tensor (torch.Tensor) to store and operate on homogeneous multidimensional rectangular arrays of numbers. PyTorch Tensors are similar to NumPy Arrays, but can also be operated on a CUDA-capable NVIDIA GPU. PyTorch has also been developing support for other GPU platforms, for example, AMD's ROCm and Apple's Metal Framework.[24]
 PyTorch supports various sub-types of Tensors.[25]
 Note that the term "tensor" here does not carry the same meaning as tensor in mathematics or physics. The meaning of the word in machine learning is only superficially related to its original meaning as a certain kind of object in linear algebra. Tensors in PyTorch are simply multi-dimensional arrays.
 PyTorch defines a class called nn (torch.nn) to describe neural networks and to support training. This module offers a comprehensive collection of building blocks for neural networks, including various layers and activation functions, enabling the construction of complex models.
 The following program shows the low-level functionality of the library with a simple example
 The following code-block shows an example of the higher level functionality provided nn module. A neural network with linear layers is defined in the example.

Source: https://en.wikipedia.org/wiki/Maschinelles_Lernen
Content: Other reasons this message may be displayed:


Source: https://en.wikipedia.org/wiki/Deep_Learning
Content: Deep learning is the subset of machine learning methods based on artificial neural networks (ANNs) with representation learning. The adjective "deep" refers to the use of multiple layers in the network. Methods used can be either supervised, semi-supervised or unsupervised.[2]
 Deep-learning architectures such as deep neural networks, deep belief networks, recurrent neural networks, convolutional neural networks and transformers have been applied to fields including computer vision, speech recognition, natural language processing, machine translation, bioinformatics, drug design, medical image analysis, climate science, material inspection and board game programs, where they have produced results comparable to and in some cases surpassing human expert performance.[3][4][5]
 Artificial neural networks were inspired by information processing and distributed communication nodes in biological systems. ANNs have various differences from biological brains. Specifically, artificial neural networks tend to be static and symbolic, while the biological brain of most living organisms is dynamic (plastic) and analog.[6][7] ANNs are generally seen as low quality models for brain function.[8]
 Deep learning is a class of machine learning algorithms that[9]: 199–200  uses multiple layers to progressively extract higher-level features from the raw input. For example, in image processing, lower layers may identify edges, while higher layers may identify the concepts relevant to a human such as digits or letters or faces.
 From another angle to view deep learning, deep learning refers to "computer-simulate" or "automate" human learning processes from a source (e.g., an image of dogs) to a learned object (dogs). Therefore, a notion coined as "deeper" learning or "deepest" learning[10] makes sense. The deepest learning refers to the fully automatic learning from a source to a final learned object. A deeper learning thus refers to a mixed learning process: a human learning process from a source to a learned semi-object, followed by a computer learning process from the human learned semi-object to a final learned object.
 Most modern deep learning models are based on multi-layered artificial neural networks such as convolutional neural networks and transformers, although they can also include propositional formulas or latent variables organized layer-wise in deep generative models such as the nodes in deep belief networks and deep Boltzmann machines.[11]
 In deep learning, each level learns to transform its input data into a slightly more abstract and composite representation. In an image recognition application, the raw input may be a matrix of pixels; the first representational layer may abstract the pixels and encode edges; the second layer may compose and encode arrangements of edges; the third layer may encode a nose and eyes; and the fourth layer may recognize that the image contains a face. Importantly, a deep learning process can learn which features to optimally place in which level on its own. This does not eliminate the need for hand-tuning; for example, varying numbers of layers and layer sizes can provide different degrees of abstraction.[12][13]
 The word "deep" in "deep learning" refers to the number of layers through which the data is transformed. More precisely, deep learning systems have a substantial credit assignment path (CAP) depth. The CAP is the chain of transformations from input to output. CAPs describe potentially causal connections between input and output. For a feedforward neural network, the depth of the CAPs is that of the network and is the number of hidden layers plus one (as the output layer is also parameterized). For recurrent neural networks, in which a signal may propagate through a layer more than once, the CAP depth is potentially unlimited.[14] No universally agreed-upon threshold of depth divides shallow learning from deep learning, but most researchers agree that deep learning involves CAP depth higher than 2. CAP of depth 2 has been shown to be a universal approximator in the sense that it can emulate any function.[15] Beyond that, more layers do not add to the function approximator ability of the network. Deep models (CAP > 2) are able to extract better features than shallow models and hence, extra layers help in learning the features effectively.
 Deep learning architectures can be constructed with a greedy layer-by-layer method.[16] Deep learning helps to disentangle these abstractions and pick out which features improve performance.[12]
 For supervised learning tasks, deep learning methods enable elimination of feature engineering, by translating the data into compact intermediate representations akin to principal components, and derive layered structures that remove redundancy in representation.
 Deep learning algorithms can be applied to unsupervised learning tasks. This is an important benefit because unlabeled data are more abundant than the labeled data. Examples of deep structures that can be trained in an unsupervised manner are deep belief networks.[12][17]
 Machine learning models are now adept at identifying complex patterns in financial market data. Due to the benefits of artificial intelligence, investors are increasingly utilizing deep learning techniques to forecast and analyze trends in stock and foreign exchange markets.[18]
 Deep neural networks are generally interpreted in terms of the universal approximation theorem[19][20][21][22][23] or probabilistic inference.[24][9][12][14][25]
 The classic universal approximation theorem concerns the capacity of feedforward neural networks with a single hidden layer of finite size to approximate continuous functions.[19][20][21][22] In 1989, the first proof was published by George Cybenko for sigmoid activation functions[19] and was generalised to feed-forward multi-layer architectures in 1991 by Kurt Hornik.[20] Recent work also showed that universal approximation also holds for non-bounded activation functions such as Kunihiko Fukushima's rectified linear unit.[26][27]
 The universal approximation theorem for deep neural networks concerns the capacity of networks with bounded width but the depth is allowed to grow. Lu et al.[23] proved that if the width of a deep neural network with ReLU activation is strictly larger than the input dimension, then the network can approximate any Lebesgue integrable function; if the width is smaller or equal to the input dimension, then a deep neural network is not a universal approximator.
 The probabilistic interpretation[25] derives from the field of machine learning. It features inference,[9][11][12][14][17][25] as well as the optimization concepts of training and testing, related to fitting and generalization, respectively. More specifically, the probabilistic interpretation considers the activation nonlinearity as a cumulative distribution function.[25] The probabilistic interpretation led to the introduction of dropout as regularizer in neural networks. The probabilistic interpretation was introduced by researchers including Hopfield, Widrow and Narendra and popularized in surveys such as the one by Bishop.[28]
 There are two types of artificial neural network (ANN): feedforward neural networks (FNNs) and recurrent neural networks (RNNs). RNNs have cycles in their connectivity structure, FNNs don't. In the 1920s, Wilhelm Lenz and Ernst Ising created and analyzed the Ising model[29] which is essentially a non-learning RNN architecture consisting of neuron-like threshold elements. In 1972, Shun'ichi Amari made this architecture adaptive.[30][31] His learning RNN was popularised by John Hopfield in 1982.[32] RNNs have become central for speech recognition and language processing.
 Charles Tappert writes that Frank Rosenblatt developed and explored all of the basic ingredients of the deep learning systems of today,[33] referring to Rosenblatt's 1962 book[34] which introduced multilayer perceptron (MLP) with 3 layers: an input layer, a hidden layer with randomized weights that did not learn, and an output layer. It also introduced variants, including a version with four-layer perceptrons where the last two layers have learned weights (and thus a proper multilayer perceptron).[34]: section 16  In addition, term deep learning was proposed in 1986 by Rina Dechter[35] although the history of its appearance is apparently more complicated.[36]
 The first general, working learning algorithm for supervised, deep, feedforward, multilayer perceptrons was published by Alexey Ivakhnenko and Lapa in 1967.[37] A 1971 paper described a deep network with eight layers trained by the group method of data handling.[38]
 The first deep learning multilayer perceptron trained by stochastic gradient descent[39] was published in 1967 by Shun'ichi Amari.[40][31] In computer experiments conducted by Amari's student Saito, a five layer MLP with two modifiable layers learned  internal representations to classify non-linearily separable pattern classes.[31] In 1987 Matthew Brand reported that wide 12-layer nonlinear perceptrons could be fully end-to-end trained to reproduce logic functions of nontrivial circuit depth via gradient descent on small batches of random input/output samples, but concluded that training time on contemporary hardware (sub-megaflop computers) made the technique impractical, and proposed using fixed random early layers as an input hash for a single modifiable layer.[41]  Instead, subsequent developments in hardware and hyperparameter tunings have made end-to-end stochastic gradient descent the currently dominant training technique.
 In 1970, Seppo Linnainmaa published the reverse mode of automatic differentiation of discrete connected networks of nested differentiable functions.[42][43][44] This became known as backpropagation.[14] It is an efficient application of the chain rule derived by Gottfried Wilhelm Leibniz in 1673[45] to networks of differentiable nodes.[31] 
The terminology "back-propagating errors" was actually introduced in 1962 by Rosenblatt,[34][31] but he did not know how to implement this, although Henry J. Kelley had a continuous precursor of backpropagation[46] already in 1960 in the context of control theory.[31] In 1982, Paul Werbos applied backpropagation to MLPs in the way that has become standard.[47][48][31] In 1985, David E. Rumelhart et al. published an experimental analysis of the technique.[49]
 Deep learning architectures for convolutional neural networks (CNNs) with convolutional layers and downsampling layers began with the Neocognitron introduced by Kunihiko Fukushima in 1980.[50] In 1969, he also introduced the ReLU (rectified linear unit) activation function.[26][31] The rectifier has become the most popular activation function for CNNs and deep learning in general.[51] CNNs have become an essential tool for computer vision.
 The term Deep Learning was introduced to the machine learning community by Rina Dechter in 1986,[35] and to artificial neural networks by Igor Aizenberg and colleagues in 2000, in the context of Boolean threshold neurons.[52][53]
 In 1988, Wei Zhang et al. applied the backpropagation algorithm 
to a convolutional neural network (a simplified Neocognitron with convolutional interconnections between the image feature layers and the last fully connected layer) for alphabet recognition. They also proposed an implementation of the CNN with an optical computing system.[54][55] 
In 1989, Yann LeCun et al. applied backpropagation to a CNN with the purpose of recognizing handwritten ZIP codes on mail. While the algorithm worked, training required 3 days.[56] Subsequently, Wei Zhang, et al. modified their model by removing the last fully connected layer and applied it for medical image object segmentation in 1991[57] and breast cancer detection in mammograms in 1994.[58] LeNet-5 (1998), a 7-level CNN by Yann LeCun et al.,[59] that classifies digits, was applied by several banks to recognize hand-written numbers on checks  digitized in 32x32 pixel images.
 In the 1980s, backpropagation did not work well for deep learning with long credit assignment paths. To overcome this problem, Jürgen Schmidhuber (1992) proposed a hierarchy of RNNs pre-trained one level at a time by self-supervised learning.[60] It uses predictive coding  to learn internal representations at multiple self-organizing time scales. This can substantially facilitate downstream deep learning. The RNN hierarchy can be collapsed into a single RNN, by distilling a higher level chunker network into a lower level automatizer network.[60][31] In 1993, a chunker solved a deep learning task whose depth exceeded 1000.[61]
 In 1992, Jürgen Schmidhuber also published an alternative to RNNs[62] which is now called a linear Transformer or a  Transformer with linearized self-attention[63][64][31] (save for a normalization operator). It learns internal spotlights of attention:[65] a slow feedforward neural network learns by gradient descent to control the fast weights of another neural network through outer products of self-generated activation patterns FROM and TO (which are now called key and value for self-attention).[63] This fast weight attention mapping is applied to a query pattern.
 The modern Transformer was introduced by Ashish Vaswani et al. in their 2017 paper "Attention Is All You Need".[66] 
It combines this with a softmax operator and a projection matrix.[31]
Transformers have increasingly become the model of choice for natural language processing.[67] Many modern large language models such as ChatGPT, GPT-4, and BERT use it. Transformers are also increasingly being used in computer vision.[68]
 In 1991, Jürgen Schmidhuber also published adversarial neural networks that contest with each other in the form of a zero-sum game, where one network's gain is the other network's loss.[69][70][71] The first network is a generative model that models a probability distribution over output patterns. The second network learns by gradient descent to predict the reactions of the environment to these patterns. This was called "artificial curiosity". In 2014, this principle was used in a generative adversarial network (GAN) by Ian Goodfellow et al.[72] Here the environmental reaction is 1 or 0 depending on whether the first network's output is in a given set. This can be used to create realistic deepfakes.[73] Excellent image quality is achieved by Nvidia's StyleGAN (2018)[74] based on the Progressive GAN by Tero Karras et al.[75] Here the GAN generator is grown from small to large scale in a pyramidal fashion.
 Sepp Hochreiter's diploma thesis (1991)[76] was called "one of the most important documents in the history of machine learning" by his supervisor Schmidhuber.[31] It not only tested the neural history compressor,[60] but also identified and analyzed the vanishing gradient problem.[76][77] Hochreiter proposed recurrent residual connections to solve this problem. This led to the deep learning method called long short-term memory (LSTM), published in 1997.[78] LSTM recurrent neural networks can learn "very deep learning" tasks[14] with long credit assignment paths that require memories of events that happened thousands of discrete time steps before. The "vanilla LSTM" with forget gate was introduced in 1999 by Felix Gers, Schmidhuber and Fred Cummins.[79] LSTM has become the  most cited neural network of the 20th century.[31]
In 2015, Rupesh Kumar Srivastava, Klaus Greff, and Schmidhuber used LSTM principles to create the Highway network, a feedforward neural network with hundreds of layers, much deeper than previous networks.[80][81] 7 months later, Kaiming He, Xiangyu Zhang;  Shaoqing Ren, and Jian Sun won the ImageNet 2015 competition with an open-gated or gateless Highway network variant called Residual neural network.[82] This has become the most cited neural network of the 21st century.[31]
 In 1994, André de Carvalho, together with Mike Fairhurst and David Bisset, published experimental results of a multi-layer boolean neural network, also known as a weightless neural network, composed of a 3-layers self-organising feature extraction neural network module (SOFT) followed by a multi-layer classification neural network module (GSN), which were independently trained. Each layer in the feature extraction module extracted features with growing complexity regarding the previous layer.[83]
 In 1995, Brendan Frey demonstrated that it was possible to train (over two days) a network containing six fully connected layers and several hundred hidden units using the wake-sleep algorithm, co-developed with Peter Dayan and Hinton.[84]
 Since 1997, Sven Behnke extended the feed-forward hierarchical convolutional approach in the Neural Abstraction Pyramid[85] by lateral and backward connections in order to flexibly incorporate context into decisions and iteratively resolve local ambiguities.
 Simpler models that use task-specific handcrafted features such as Gabor filters and support vector machines (SVMs) were a popular choice in the 1990s and 2000s, because of artificial neural networks' computational cost and a lack of understanding of how the brain wires its biological networks.
 Both shallow and deep learning (e.g., recurrent nets) of ANNs for speech recognition have been explored for many years.[86][87][88] These methods never outperformed non-uniform internal-handcrafting Gaussian mixture model/Hidden Markov model (GMM-HMM) technology based on generative models of speech trained discriminatively.[89] Key difficulties have been analyzed, including gradient diminishing[76] and weak temporal correlation structure in neural predictive models.[90][91] Additional difficulties were the lack of training data and limited computing power. Most speech recognition researchers moved away from neural nets to pursue generative modeling. An exception was at SRI International in the late 1990s. Funded by the US government's NSA and DARPA, SRI studied deep neural networks (DNNs) in speech and speaker recognition. The speaker recognition team led by Larry Heck reported significant success with deep neural networks in speech processing in the 1998 National Institute of Standards and Technology Speaker Recognition evaluation.[92] The SRI deep neural network was then deployed in the Nuance Verifier, representing the first major industrial application of deep learning.[93] The principle of elevating "raw" features over hand-crafted optimization was first explored successfully in the architecture of deep autoencoder on the "raw" spectrogram or linear filter-bank features in the late 1990s,[93] showing its superiority over the Mel-Cepstral features that contain stages of fixed transformation from spectrograms. The raw features of speech, waveforms, later produced excellent larger-scale results.[94]
 Speech recognition was taken over by LSTM. In 2003, LSTM started to become competitive with traditional speech recognizers on certain tasks.[95] In 2006, Alex Graves, Santiago Fernández, Faustino Gomez, and Schmidhuber combined it with connectionist temporal classification (CTC)[96] in stacks of LSTM RNNs.[97] In 2015, Google's speech recognition reportedly experienced a dramatic performance jump of 49% through CTC-trained LSTM, which they made available through Google Voice Search.[98]
 The impact of deep learning in industry began in the early 2000s, when CNNs already processed an estimated 10% to 20% of all the checks written in the US, according to Yann LeCun.[99] Industrial applications of deep learning to large-scale speech recognition started around 2010.
 In 2006, publications by Geoff Hinton, Ruslan Salakhutdinov, Osindero and Teh[100][101][102] showed how a many-layered feedforward neural network could be effectively pre-trained one layer at a time, treating each layer in turn as an unsupervised restricted Boltzmann machine, then fine-tuning it using supervised backpropagation.[103] The papers referred to learning for deep belief nets.
 The 2009 NIPS Workshop on Deep Learning for Speech Recognition was motivated by the limitations of deep generative models of speech, and the possibility that given more capable hardware and large-scale data sets that deep neural nets might become practical. It was believed that pre-training DNNs using generative models of deep belief nets (DBN) would overcome the main difficulties of neural nets. However, it was discovered that replacing pre-training with large amounts of training data for straightforward backpropagation when using DNNs with large, context-dependent output layers produced error rates dramatically lower than then-state-of-the-art Gaussian mixture model (GMM)/Hidden Markov Model (HMM) and also than more-advanced generative model-based systems.[104] The nature of the recognition errors produced by the two types of systems was characteristically different,[105] offering technical insights into how to integrate deep learning into the existing highly efficient, run-time speech decoding system deployed by all major speech recognition systems.[9][106][107] Analysis around 2009–2010, contrasting the GMM (and other generative speech models) vs. DNN models, stimulated early industrial investment in deep learning for speech recognition.[105]  That analysis was done with comparable performance (less than 1.5% in error rate) between discriminative DNNs and generative models.[104][105][108]
In 2010, researchers extended deep learning from TIMIT to large vocabulary speech recognition, by adopting large output layers of the DNN based on context-dependent HMM states constructed by decision trees.[109][110][111][106]
 Deep learning is part of state-of-the-art systems in various disciplines, particularly computer vision and automatic speech recognition (ASR). Results on commonly used evaluation sets such as TIMIT (ASR) and MNIST (image classification), as well as a range of large-vocabulary speech recognition tasks have steadily improved.[104][112] Convolutional neural networks were superseded for ASR by CTC[96] for LSTM.[78][98][113][114][115] but are more successful in computer vision.
 Advances in hardware have driven renewed interest in deep learning. In 2009, Nvidia was involved in what was called the "big bang" of deep learning, "as deep-learning neural networks were trained with Nvidia graphics processing units (GPUs)".[116] That year, Andrew Ng determined that GPUs could increase the speed of deep-learning systems by about 100 times.[117] In particular, GPUs are well-suited for the matrix/vector computations involved in machine learning.[118][119][120] GPUs speed up training algorithms by orders of magnitude, reducing running times from weeks to days.[121][122] Further, specialized hardware and algorithm optimizations can be used for efficient processing of deep learning models.[123]
 In the late 2000s, deep learning started to outperform other methods in machine learning competitions.
In 2009, a long short-term memory trained by connectionist temporal classification (Alex Graves, Santiago Fernández, Faustino Gomez, and Jürgen Schmidhuber, 2006)[96] was the first RNN to win pattern recognition contests, winning three competitions in connected handwriting recognition.[124][14] Google later used CTC-trained LSTM for speech recognition on the smartphone.[125][98]
 Significant impacts in image or object recognition were felt from 2011 to 2012. Although CNNs trained by backpropagation had been around for decades,[54][56] and GPU implementations of NNs for years,[118] including CNNs,[120][14] faster implementations of CNNs on GPUs were needed to progress on computer vision. In 2011, the DanNet[126][3] by Dan Ciresan, Ueli Meier, Jonathan Masci, Luca Maria Gambardella, and Jürgen Schmidhuber achieved for the first time superhuman performance in a visual pattern recognition contest, outperforming traditional methods by a factor of 3.[14] Also in 2011, DanNet won the ICDAR Chinese handwriting contest, and in May 2012, it won the ISBI image segmentation contest.[127] Until 2011, CNNs did not play a major role at computer vision conferences, but in June 2012, a paper by Ciresan et al. at the leading conference CVPR[3] showed how max-pooling CNNs on GPU can dramatically improve many vision benchmark records.  In September 2012, DanNet also won the ICPR contest on analysis of large medical images for cancer detection, and in the following year also the MICCAI Grand Challenge on the same topic.[128] In October 2012, the similar AlexNet by Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton[4] won the large-scale ImageNet competition by a significant margin over shallow machine learning methods. 
The VGG-16 network by Karen Simonyan and Andrew Zisserman[129] further reduced the error rate and
won the ImageNet 2014 competition, following a similar trend in large-scale speech recognition.
 Image classification was then extended to the more challenging task of generating descriptions (captions) for images, often as a combination of CNNs and LSTMs.[130][131][132]
 In 2012, a team led by George E. Dahl won the "Merck Molecular Activity Challenge" using multi-task deep neural networks to predict the biomolecular target of one drug.[133][134] In 2014, Sepp Hochreiter's group used deep learning to detect off-target and toxic effects of environmental chemicals in nutrients, household products and drugs and won the "Tox21 Data Challenge" of NIH, FDA and NCATS.[135][136][137]
 In 2016, Roger Parloff mentioned a "deep learning revolution" that has transformed the AI industry.[138]
 In March 2019, Yoshua Bengio, Geoffrey Hinton and Yann LeCun were awarded the Turing Award for conceptual and engineering breakthroughs that have made deep neural networks a critical component of computing.
 Artificial neural networks (ANNs) or connectionist systems are computing systems inspired by the biological neural networks that constitute animal brains. Such systems learn (progressively improve their ability) to do tasks by considering examples, generally without task-specific programming. For example, in image recognition, they might learn to identify images that contain cats by analyzing example images that have been manually labeled as "cat" or "no cat" and using the analytic results to identify cats in other images. They have found most use in applications difficult to express with a traditional computer algorithm using rule-based programming.
 An ANN is based on a collection of connected units called artificial neurons, (analogous to biological neurons in a biological brain). Each connection (synapse) between neurons can transmit a signal to another neuron. The receiving (postsynaptic) neuron can process the signal(s) and then signal downstream neurons connected to it. Neurons may have state, generally represented by real numbers, typically between 0 and 1. Neurons and synapses may also have a weight that varies as learning proceeds, which can increase or decrease the strength of the signal that it sends downstream.
 Typically, neurons are organized in layers. Different layers may perform different kinds of transformations on their inputs. Signals travel from the first (input), to the last (output) layer, possibly after traversing the layers multiple times.
 The original goal of the neural network approach was to solve problems in the same way that a human brain would. Over time, attention focused on matching specific mental abilities, leading to deviations from biology such as backpropagation, or passing information in the reverse direction and adjusting the network to reflect that information.
 Neural networks have been used on a variety of tasks, including computer vision, speech recognition, machine translation, social network filtering, playing board and video games and medical diagnosis.
 As of 2017, neural networks typically have a few thousand to a few million units and millions of connections. Despite this number being several order of magnitude less than the number of neurons on a human brain, these networks can perform many tasks at a level beyond that of humans (e.g., recognizing faces, or playing "Go"[140]).
 A deep neural network (DNN) is an artificial neural network with multiple layers between the input and output layers.[11][14] There are different types of neural networks but they always consist of the same components: neurons, synapses, weights, biases, and functions.[141] These components as a whole function in a way that mimics functions of the human brain, and can be trained like any other ML algorithm.[citation needed]
 For example, a DNN that is trained to recognize dog breeds will go over the given image and calculate the probability that the dog in the image is a certain breed. The user can review the results and select which probabilities the network should display (above a certain threshold, etc.) and return the proposed label. Each mathematical manipulation as such is considered a layer,[citation needed] and complex DNN have many layers, hence the name "deep" networks.
 DNNs can model complex non-linear relationships. DNN architectures generate compositional models where the object is expressed as a layered composition of primitives.[142] The extra layers enable composition of features from lower layers, potentially modeling complex data with fewer units than a similarly performing shallow network.[11] For instance, it was proved that sparse multivariate polynomials are exponentially easier to approximate with DNNs than with shallow networks.[143]
 Deep architectures include many variants of a few basic approaches. Each architecture has found success in specific domains. It is not always possible to compare the performance of multiple architectures, unless they have been evaluated on the same data sets.
 DNNs are typically feedforward networks in which data flows from the input layer to the output layer without looping back. At first, the DNN creates a map of virtual neurons and assigns random numerical values, or "weights", to connections between them. The weights and inputs are multiplied and return an output between 0 and 1. If the network did not accurately recognize a particular pattern, an algorithm would adjust the weights.[144] That way the algorithm can make certain parameters more influential, until it determines the correct mathematical manipulation to fully process the data.
 Recurrent neural networks, in which data can flow in any direction, are used for applications such as language modeling.[145][146][147][148][149] Long short-term memory is particularly effective for this use.[78][150]
 Convolutional neural networks (CNNs) are used in computer vision.[151] CNNs also have been applied to acoustic modeling for automatic speech recognition (ASR).[152]
 As with ANNs, many issues can arise with naively trained DNNs. Two common issues are overfitting and computation time.
 DNNs are prone to overfitting because of the added layers of abstraction, which allow them to model rare dependencies in the training data. Regularization methods such as Ivakhnenko's unit pruning[38] or weight decay (




ℓ

2




{\displaystyle \ell _{2}}

-regularization) or sparsity (




ℓ

1




{\displaystyle \ell _{1}}

-regularization) can be applied during training to combat overfitting.[153] Alternatively dropout regularization randomly omits units from the hidden layers during training. This helps to exclude rare dependencies.[154] Finally, data can be augmented via methods such as cropping and rotating such that smaller training sets can be increased in size to reduce the chances of overfitting.[155]
 DNNs must consider many training parameters, such as the size (number of layers and number of units per layer), the learning rate, and initial weights. Sweeping through the parameter space for optimal parameters may not be feasible due to the cost in time and computational resources. Various tricks, such as batching (computing the gradient on several training examples at once rather than individual examples)[156] speed up computation. Large processing capabilities of many-core architectures (such as GPUs or the Intel Xeon Phi) have produced significant speedups in training, because of the suitability of such processing architectures for the matrix and vector computations.[157][158]
 Alternatively, engineers may look for other types of neural networks with more straightforward and convergent training algorithms. CMAC (cerebellar model articulation controller) is one such kind of neural network. It doesn't require learning rates or randomized initial weights. The training process can be guaranteed to converge in one step with a new batch of data, and the computational complexity of the training algorithm is linear with respect to the number of neurons involved.[159][160]
 Since the 2010s, advances in both machine learning algorithms and computer hardware have led to more efficient methods for training deep neural networks that contain many layers of non-linear hidden units and a very large output layer.[161] By 2019, graphic processing units (GPUs), often with AI-specific enhancements, had displaced CPUs as the dominant method of training large-scale commercial cloud AI.[162] OpenAI estimated the hardware computation used in the largest deep learning projects from AlexNet (2012) to AlphaZero (2017), and found a 300,000-fold increase in the amount of computation required, with a doubling-time trendline of 3.4 months.[163][164]
 Special electronic circuits called deep learning processors were designed to speed up deep learning algorithms. Deep learning processors include neural processing units (NPUs) in Huawei cellphones[165] and cloud computing servers such as tensor processing units (TPU) in the Google Cloud Platform.[166] Cerebras Systems has also built a dedicated system to handle large deep learning models, the CS-2, based on the largest processor in the industry, the second-generation Wafer Scale Engine (WSE-2).[167][168]
 Atomically thin semiconductors are considered promising for energy-efficient deep learning hardware where the same basic device structure is used for both logic operations and data storage.
In 2020, Marega et al. published experiments with a large-area active channel material for developing logic-in-memory devices and circuits based on floating-gate field-effect transistors (FGFETs).[169]
 In 2021, J. Feldmann et al. proposed an integrated photonic hardware accelerator for parallel convolutional processing.[170] The authors identify two key advantages of integrated photonics over its electronic counterparts: (1) massively parallel data transfer through wavelength division multiplexing in conjunction with frequency combs, and (2) extremely high data modulation speeds.[170] Their system can execute trillions of multiply-accumulate operations per second, indicating the potential of integrated photonics in data-heavy AI applications.[170]
 Large-scale automatic speech recognition is the first and most convincing successful case of deep learning. LSTM RNNs can learn "Very Deep Learning" tasks[14] that involve multi-second intervals containing speech events separated by thousands of discrete time steps, where one time step corresponds to about 10 ms. LSTM with forget gates[150] is competitive with traditional speech recognizers on certain tasks.[95]
 The initial success in speech recognition was based on small-scale recognition tasks based on TIMIT. The data set contains 630 speakers from eight major dialects of American English, where each speaker reads 10 sentences.[171] Its small size lets many configurations be tried. More importantly, the TIMIT task concerns phone-sequence recognition, which, unlike word-sequence recognition, allows weak phone bigram language models. This lets the strength of the acoustic modeling aspects of speech recognition be more easily analyzed. The error rates listed below, including these early results and measured as percent phone error rates (PER), have been summarized since 1991.
 The debut of DNNs for speaker recognition in the late 1990s and speech recognition around 2009-2011 and of LSTM around 2003–2007, accelerated progress in eight major areas:[9][108][106]
 All major commercial speech recognition systems (e.g., Microsoft Cortana, Xbox, Skype Translator, Amazon Alexa, Google Now, Apple Siri, Baidu and iFlyTek voice search, and a range of Nuance speech products, etc.) are based on deep learning.[9][176][177]
 A common evaluation set for image classification is the MNIST database data set. MNIST is composed of handwritten digits and includes 60,000 training examples and 10,000 test examples. As with TIMIT, its small size lets users test multiple configurations. A comprehensive list of results on this set is available.[178]
 Deep learning-based image recognition has become "superhuman", producing more accurate results than human contestants. This first occurred in 2011 in recognition of traffic signs, and in 2014, with recognition of human faces.[179][180]
 Deep learning-trained vehicles now interpret 360° camera views.[181] Another example is Facial Dysmorphology Novel Analysis (FDNA) used to analyze cases of human malformation connected to a large database of genetic syndromes.
 Closely related to the progress that has been made in image recognition is the increasing application of deep learning techniques to various visual art tasks. DNNs have proven themselves capable, for example, of
 Neural networks have been used for implementing language models since the early 2000s.[145] LSTM helped to improve machine translation and language modeling.[146][147][148]
 Other key techniques in this field are negative sampling[184] and word embedding. Word embedding, such as word2vec, can be thought of as a representational layer in a deep learning architecture that transforms an atomic word into a positional representation of the word relative to other words in the dataset; the position is represented as a point in a vector space. Using word embedding as an RNN input layer allows the network to parse sentences and phrases using an effective compositional vector grammar. A compositional vector grammar can be thought of as probabilistic context free grammar (PCFG) implemented by an RNN.[185] Recursive auto-encoders built atop word embeddings can assess sentence similarity and detect paraphrasing.[185] Deep neural architectures provide the best results for constituency parsing,[186] sentiment analysis,[187] information retrieval,[188][189] spoken language understanding,[190] machine translation,[146][191] contextual entity linking,[191] writing style recognition,[192] named-entity recognition (token classification),[193] text classification, and others.[194]
 Recent developments generalize word embedding to sentence embedding.
 Google Translate (GT) uses a large end-to-end long short-term memory (LSTM) network.[195][196][197][198] Google Neural Machine Translation (GNMT) uses an example-based machine translation method in which the system "learns from millions of examples".[196] It translates "whole sentences at a time, rather than pieces". Google Translate supports over one hundred languages.[196] The network encodes the "semantics of the sentence rather than simply memorizing phrase-to-phrase translations".[196][199] GT uses English as an intermediate between most language pairs.[199]
 A large percentage of candidate drugs fail to win regulatory approval. These failures are caused by insufficient efficacy (on-target effect), undesired interactions (off-target effects), or unanticipated toxic effects.[200][201] Research has explored use of deep learning to predict the biomolecular targets,[133][134] off-targets, and toxic effects of environmental chemicals in nutrients, household products and drugs.[135][136][137]
 AtomNet is a deep learning system for structure-based rational drug design.[202] AtomNet was used to predict novel candidate biomolecules for disease targets such as the Ebola virus[203] and multiple sclerosis.[204][203]
 In 2017 graph neural networks were used for the first time to predict various properties of molecules in a large toxicology data set.[205] In 2019, generative neural networks were used to produce molecules that were validated experimentally all the way into mice.[206][207]
 Deep reinforcement learning has been used to approximate the value of possible direct marketing actions, defined in terms of RFM variables. The estimated value function was shown to have a natural interpretation as customer lifetime value.[208]
 Recommendation systems have used deep learning to extract meaningful features for a latent factor model for content-based music and journal recommendations.[209][210] Multi-view deep learning has been applied for learning user preferences from multiple domains.[211] The model uses a hybrid collaborative and content-based approach and enhances recommendations in multiple tasks.
 An autoencoder ANN was used in bioinformatics, to predict gene ontology annotations and gene-function relationships.[212]
 In medical informatics, deep learning was used to predict sleep quality based on data from wearables[213] and predictions of health complications from electronic health record data.[214]
 Deep neural networks can be used to estimate the entropy of a stochastic process and called Neural Joint Entropy Estimator (NJEE).[215] Such an estimation provides insights on the effects of input random variables on an independent random variable. Practically, the DNN is trained as a classifier that maps an input vector or matrix X to an output probability distribution over the possible classes of random variable Y, given input X. For example, in image classification tasks, the NJEE maps a vector of pixels' color values to probabilities over possible image classes. In practice, the probability distribution of Y is obtained by a Softmax layer with number of nodes that is equal to the alphabet size of Y. NJEE uses continuously differentiable activation functions, such that the conditions for the universal approximation theorem holds. It is shown that this method provides a strongly consistent estimator and outperforms other methods in case of large alphabet sizes.[215]
 Deep learning has been shown to produce competitive results in medical application such as cancer cell classification, lesion detection, organ segmentation and image enhancement.[216][217] Modern deep learning tools demonstrate the high accuracy of detecting various diseases and the helpfulness of their use by specialists to improve the diagnosis efficiency.[218][219]
 Finding the appropriate mobile audience for mobile advertising is always challenging, since many data points must be considered and analyzed before a target segment can be created and used in ad serving by any ad server.[220] Deep learning has been used to interpret large, many-dimensioned advertising datasets. Many data points are collected during the request/serve/click internet advertising cycle. This information can form the basis of machine learning to improve ad selection.
 Deep learning has been successfully applied to inverse problems such as denoising, super-resolution, inpainting, and film colorization.[221] These applications include learning methods such as "Shrinkage Fields for Effective Image Restoration"[222] which trains on an image dataset, and Deep Image Prior, which trains on the image that needs restoration.
 Deep learning is being successfully applied to financial fraud detection, tax evasion detection,[223] and anti-money laundering.[224]
 In November 2023, researchers at Google DeepMind and Lawrence Berkeley National Laboratory announced that they had developed an AI system known as GNoME. This system has contributed to materials science by discovering over 2 million new materials within a relatively short timeframe. GNoME employs deep learning techniques to efficiently explore potential material structures, achieving a significant increase in the identification of stable inorganic crystal structures. The system's predictions were validated through autonomous robotic experiments, demonstrating a noteworthy success rate of 71%. The data of newly discovered materials is publicly available through the Materials Project database, offering researchers the opportunity to identify materials with desired properties for various applications. This development has implications for the future of scientific discovery and the integration of AI in material science research, potentially expediting material innovation and reducing costs in product development. The use of AI and deep learning suggests the possibility of minimizing or eliminating manual lab experiments and allowing scientists to focus more on the design and analysis of unique compounds.[225][226][227]
 The United States Department of Defense applied deep learning to train robots in new tasks through observation.[228]
 Physics informed neural networks have been used to solve partial differential equations in both forward and inverse problems in a data driven manner.[229] One example is the reconstructing fluid flow governed by the Navier-Stokes equations. Using physics informed neural networks does not require the often expensive mesh generation that conventional CFD methods relies on.[230][231]
 Image reconstruction is the reconstruction of the underlying images from the image-related measurements. Several works showed the better and superior performance of the deep learning methods compared to analytical methods for various applications, e.g., spectral imaging [232] and ultrasound imaging.[233]
 An epigenetic clock is a biochemical test that can be used to measure age. Galkin et al. used deep neural networks to train an epigenetic aging clock of unprecedented accuracy using >6,000 blood samples.[234] The clock uses information from 1000 CpG sites and predicts people with certain conditions older than healthy controls: IBD, frontotemporal dementia, ovarian cancer, obesity. The aging clock was planned to be released for public use in 2021 by an Insilico Medicine spinoff company Deep Longevity.
 Deep learning is closely related to a class of theories of brain development (specifically, neocortical development) proposed by cognitive neuroscientists in the early 1990s.[235][236][237][238] These developmental theories were instantiated in computational models, making them predecessors of deep learning systems. These developmental models share the property that various proposed learning dynamics in the brain (e.g., a wave of nerve growth factor) support the self-organization somewhat analogous to the neural networks utilized in deep learning models. Like the neocortex, neural networks employ a hierarchy of layered filters in which each layer considers information from a prior layer (or the operating environment), and then passes its output (and possibly the original input), to other layers. This process yields a self-organizing stack of transducers, well-tuned to their operating environment. A 1995 description stated, "...the infant's brain seems to organize itself under the influence of waves of so-called trophic-factors ... different regions of the brain become connected sequentially, with one layer of tissue maturing before another and so on until the whole brain is mature".[239]
 A variety of approaches have been used to investigate the plausibility of deep learning models from a neurobiological perspective. On the one hand, several variants of the backpropagation algorithm have been proposed in order to increase its processing realism.[240][241] Other researchers have argued that unsupervised forms of deep learning, such as those based on hierarchical generative models and deep belief networks, may be closer to biological reality.[242][243] In this respect, generative neural network models have been related to neurobiological evidence about sampling-based processing in the cerebral cortex.[244]
 Although a systematic comparison between the human brain organization and the neuronal encoding in deep networks has not yet been established, several analogies have been reported. For example, the computations performed by deep learning units could be similar to those of actual neurons[245] and neural populations.[246] Similarly, the representations developed by deep learning models are similar to those measured in the primate visual system[247] both at the single-unit[248] and at the population[249] levels.
 Facebook's AI lab performs tasks such as automatically tagging uploaded pictures with the names of the people in them.[250]
 Google's DeepMind Technologies developed a system capable of learning how to play Atari video games using only pixels as data input. In 2015 they demonstrated their AlphaGo system, which learned the game of Go well enough to beat a professional Go player.[251][252][253] Google Translate uses a neural network to translate between more than 100 languages.
 In 2017, Covariant.ai was launched, which focuses on integrating deep learning into factories.[254]
 As of 2008,[255] researchers at The University of Texas at Austin (UT) developed a machine learning framework called Training an Agent Manually via Evaluative Reinforcement, or TAMER, which proposed new methods for robots or computer programs to learn how to perform tasks by interacting with a human instructor.[228] First developed as TAMER, a new algorithm called Deep TAMER was later introduced in 2018 during a collaboration between U.S. Army Research Laboratory (ARL) and UT researchers. Deep TAMER used deep learning to provide a robot with the ability to learn new tasks through observation.[228] Using Deep TAMER, a robot learned a task with a human trainer, watching video streams or observing a human perform a task in-person. The robot later practiced the task with the help of some coaching from the trainer, who provided feedback such as "good job" and "bad job".[256]
 Deep learning has attracted both criticism and comment, in some cases from outside the field of computer science.
 A main criticism concerns the lack of theory surrounding some methods.[257] Learning in the most common deep architectures is implemented using well-understood gradient descent. However, the theory surrounding other algorithms, such as contrastive divergence is less clear.[citation needed] (e.g., Does it converge? If so, how fast? What is it approximating?) Deep learning methods are often looked at as a black box, with most confirmations done empirically, rather than theoretically.[258]
 Others point out that deep learning should be looked at as a step towards realizing strong AI, not as an all-encompassing solution. Despite the power of deep learning methods, they still lack much of the functionality needed to realize this goal entirely. Research psychologist Gary Marcus noted:
 Realistically, deep learning is only part of the larger challenge of building intelligent machines. Such techniques lack ways of representing causal relationships (...) have no obvious ways of performing logical inferences, and they are also still a long way from integrating abstract knowledge, such as information about what objects are, what they are for, and how they are typically used. The most powerful A.I. systems, like Watson (...) use techniques like deep learning as just one element in a very complicated ensemble of techniques, ranging from the statistical technique of Bayesian inference to deductive reasoning.[259]
 In further reference to the idea that artistic sensitivity might be inherent in relatively low levels of the cognitive hierarchy, a published series of graphic representations of the internal states of deep (20-30 layers) neural networks attempting to discern within essentially random data the images on which they were trained[260] demonstrate a visual appeal: the original research notice received well over 1,000 comments, and was the subject of what was for a time the most frequently accessed article on The Guardian's[261] website.
 Some deep learning architectures display problematic behaviors,[262] such as confidently classifying unrecognizable images as belonging to a familiar category of ordinary images (2014)[263] and misclassifying minuscule perturbations of correctly classified images (2013).[264] Goertzel hypothesized that these behaviors are due to limitations in their internal representations and that these limitations would inhibit integration into heterogeneous multi-component artificial general intelligence (AGI) architectures.[262] These issues may possibly be addressed by deep learning architectures that internally form states homologous to image-grammar[265] decompositions of observed entities and events.[262] Learning a grammar (visual or linguistic) from training data would be equivalent to restricting the system to commonsense reasoning that operates on concepts in terms of grammatical production rules and is a basic goal of both human language acquisition[266] and artificial intelligence (AI).[267]
 As deep learning moves from the lab into the world, research and experience show that artificial neural networks are vulnerable to hacks and deception.[268] By identifying patterns that these systems use to function, attackers can modify inputs to ANNs in such a way that the ANN finds a match that human observers would not recognize. For example, an attacker can make subtle changes to an image such that the ANN finds a match even though the image looks to a human nothing like the search target. Such manipulation is termed an "adversarial attack".[269]
 In 2016 researchers used one ANN to doctor images in trial and error fashion, identify another's focal points, and thereby generate images that deceived it. The modified images looked no different to human eyes. Another group showed that printouts of doctored images then photographed successfully tricked an image classification system.[270] One defense is reverse image search, in which a possible fake image is submitted to a site such as TinEye that can then find other instances of it. A refinement is to search using only parts of the image, to identify images from which that piece may have been taken.[271]
 Another group showed that certain psychedelic spectacles could fool a facial recognition system into thinking ordinary people were celebrities, potentially allowing one person to impersonate another. In 2017 researchers added stickers to stop signs and caused an ANN to misclassify them.[270]
 ANNs can however be further trained to detect attempts at deception, potentially leading attackers and defenders into an arms race similar to the kind that already defines the malware defense industry. ANNs have been trained to defeat ANN-based anti-malware software by repeatedly attacking a defense with malware that was continually altered by a genetic algorithm until it tricked the anti-malware while retaining its ability to damage the target.[270]
 In 2016, another group demonstrated that certain sounds could make the Google Now voice command system open a particular web address, and hypothesized that this could "serve as a stepping stone for further attacks (e.g., opening a web page hosting drive-by malware)".[270]
 In "data poisoning", false data is continually smuggled into a machine learning system's training set to prevent it from achieving mastery.[270]
 Most Deep Learning systems rely on training and verification data that is generated and/or annotated by humans.[272] It has been argued in media philosophy that not only low-paid clickwork (e.g. on Amazon Mechanical Turk) is regularly deployed for this purpose, but also implicit forms of human microwork that are often not recognized as such.[273] The philosopher Rainer Mühlhoff distinguishes five types of "machinic capture" of human microwork to generate training data: (1) gamification (the embedding of annotation or computation tasks in the flow of a game), (2) "trapping and tracking" (e.g. CAPTCHAs for image recognition or click-tracking on Google search results pages), (3) exploitation of social motivations (e.g. tagging faces on Facebook to obtain labeled facial images), (4) information mining (e.g. by leveraging quantified-self devices such as activity trackers) and (5) clickwork.[273]
 Mühlhoff argues that in most commercial end-user applications of Deep Learning such as Facebook's face recognition system, the need for training data does not stop once an ANN is trained. Rather, there is a continued demand for human-generated verification data to constantly calibrate and update the ANN. For this purpose, Facebook introduced the feature that once a user is automatically recognized in an image, they receive a notification. They can choose whether or not they like to be publicly labeled on the image, or tell Facebook that it is not them in the picture.[274] This user interface is a mechanism to generate "a constant stream of verification data"[273] to further train the network in real-time. As Mühlhoff argues, the involvement of human users to generate training and verification data is so typical for most commercial end-user applications of Deep Learning that such systems may be referred to as "human-aided artificial intelligence".[273]


Source: https://en.wikipedia.org/wiki/K%C3%BCnstliche_Intelligenz
Content: Other reasons this message may be displayed:


Source: https://en.wikipedia.org/wiki/Don%E2%80%99t_repeat_yourself
Content: Other reasons this message may be displayed:


Source: https://en.wikipedia.org/wiki/Multiprozessor
Content: Other reasons this message may be displayed:


Source: https://en.wikipedia.org/wiki/Softwareseitiges_Multithreading
Content: Other reasons this message may be displayed:


Source: https://en.wikipedia.org/wiki/Jython
Content: 
 Jython is an implementation of the Python programming language designed to run on the Java platform. It was known as JPython until 1999.[3]
 Jython programs can import and use any Java class. Except for some standard modules, Jython programs use Java classes instead of Python modules. Jython includes almost all of the modules in the standard Python programming language distribution, lacking only some of the modules implemented originally in C. For example, a user interface in Jython could be written with Swing, AWT or SWT. Jython compiles Python source code to Java bytecode (an intermediate language) either on demand or statically.
 Jython was initially created in late 1997 to replace C with Java for performance-intensive code accessed by Python programs, moving to SourceForge in October 2000. The Python Software Foundation awarded a grant in January 2005.  Jython 2.5 was released in June 2009.[4]
 The most recent release is Jython 2.7.3. It was released on September 10, 2022 and is compatible with Python 2.7.[5]
 Python 3 compatible changes are planned in Jython 3 Roadmap.[6]
 Although Jython implements the Python language specification, it has some differences and incompatibilities with CPython, which is the reference implementation of Python.[7][8]
 From version 2.2 on, Jython (including the standard library) is released under the Python Software Foundation License (v2). Older versions are covered by the Jython 2.0, 2.1 license and the JPython 1.1.x Software License.[9]
 The command-line interpreter is available under the Apache Software License.


Source: https://en.wikipedia.org/wiki/IronPython
Content: IronPython is an implementation of the Python programming language targeting the .NET and Mono frameworks. The project is currently maintained by a group of volunteers at GitHub. It is free and open-source software, and can be implemented with Python Tools for Visual Studio, which is a free and open-source extension for Microsoft's Visual Studio IDE.[2][3]
 IronPython is written entirely in C#, although some of its code is automatically generated by a code generator written in Python.
 IronPython is implemented on top of the Dynamic Language Runtime (DLR), a library running on top of the Common Language Infrastructure that provides dynamic typing and dynamic method dispatch, among other things, for dynamic languages.[4] The DLR is part of the .NET Framework 4.0 and is also a part of Mono since version 2.4 from 2009.[5] The DLR can also be used as a library on older CLI implementations.
 Jim Hugunin created the project and actively contributed to it up until Version 1.0 which was released on September 5, 2006.[6] IronPython 2.0 was released on December 10, 2008.[7] After version 1.0 it was maintained by a small team at Microsoft until the 2.7 Beta 1 release. Microsoft abandoned IronPython (and its sister project IronRuby) in late 2010, after which Hugunin left to work at Google.[8] The project is currently maintained by a group of volunteers at GitHub.
 There are some differences between the Python reference implementation CPython and IronPython.[23] Some projects built on top of IronPython are known not to work under CPython.[24] Conversely, CPython applications that depend on extensions to the language that are implemented in C are not compatible with IronPython
,[25] unless they are implemented in a .NET interop. For example, NumPy was wrapped by Microsoft in 2011, allowing code and libraries dependent on it to be run directly from .NET Framework.[26]
 IronPython is supported on Silverlight (which is deprecated by Microsoft and already has lost support in most web browsers[27]). It can be used as a scripting engine in the browser just like the JavaScript engine.[28] IronPython scripts are passed like simple client-side JavaScript scripts in <script>-tags. It is then also possible to modify embedded XAML markup.
 The technology behind this is called Gestalt.[citation needed]
 The same works for IronRuby.
 Until version 0.6, IronPython was released under the terms of Common Public License.[29] Following recruitment of the project lead in August 2004, IronPython was made available as part of Microsoft's Shared Source initiative. This license is not OSI-approved but the authors claim it meets the open-source definition.[30] With the 2.0 alpha release, the license was changed to the Microsoft Public License,[31] which the OSI has approved. The latest versions are released under the terms of the Apache License 2.0.
 One of IronPython's key advantages is in its function as an extensibility layer to application frameworks written in a .NET language. It is relatively simple to integrate an IronPython interpreter into an existing .NET application framework. Once in place, downstream developers can use scripts written in IronPython that interact with .NET objects in the framework, thereby extending the functionality in the framework's interface, without having to change any of the framework's code base.[32]
 IronPython makes extensive use of reflection. When passed in a reference to a .NET object, it will automatically import the types and methods available to that object. This results in a highly intuitive experience when working with .NET objects from within an IronPython script.
 The following IronPython script manipulates .NET Framework objects. This script can be supplied by a third-party client-side application developer and passed into the server-side framework through an interface. Note that neither the interface, nor the server-side code is modified to support the analytics required by the client application.
 In this case, assume that the .NET Framework implements a class, BookDictionary, in a module called BookService, and publishes an interface into which IronPython scripts can be sent and executed.
 This script, when sent to that interface, will iterate over the entire list of books maintained by the framework, and pick out those written by Booker Prize-winning authors.
 What's interesting is that the responsibility for writing the actual analytics reside with the client-side developer. The demands on the server-side developer are minimal, essentially just providing access to the data maintained by the server. This design pattern greatly simplifies the deployment and maintenance of complex application frameworks.
 The following script uses the .NET Framework to create a simple Hello World message.
 The performance characteristics of IronPython compared to CPython, the reference implementation of Python, depends on the exact benchmark used. IronPython performs worse than CPython on most benchmarks taken with the PyStone script but better on other benchmarks.[33]
IronPython may perform better in Python programs that use threads or multiple cores, as it has a JIT compiler, and also because it doesn't have the Global Interpreter Lock.[34][35]


Source: https://en.wikipedia.org/wiki/Perl_(Programmiersprache)
Content: Other reasons this message may be displayed:


Source: https://en.wikipedia.org/wiki/PHP
Content: PHP is a general-purpose scripting language geared towards web development.[8] It was originally created by Danish-Canadian programmer Rasmus Lerdorf in 1993 and released in 1995.[9][10] The PHP reference implementation is now produced by the PHP Group.[11] PHP was originally an abbreviation of Personal Home Page,[12][13] but it now stands for the recursive initialism PHP: Hypertext Preprocessor.[14]
 PHP code is usually processed on a web server by a PHP interpreter implemented as a module, a daemon or a Common Gateway Interface (CGI) executable. On a web server, the result of the interpreted and executed PHP code—which may be any type of data, such as generated HTML or binary image data—would form the whole or part of an HTTP response. Various web template systems, web content management systems, and web frameworks exist that can be employed to orchestrate or facilitate the generation of that response. Additionally, PHP can be used for many programming tasks outside the web context, such as standalone graphical applications[15] and drone control.[16] PHP code can also be directly executed from the command line.
 The standard PHP interpreter, powered by the Zend Engine, is free software released under the PHP License. PHP has been widely ported and can be deployed on most web servers on a variety of operating systems and platforms.[17]
 The PHP language has evolved without a written formal specification or standard, with the original implementation acting as the de facto standard that other implementations aimed to follow. 
 W3Techs reports that as of 2024[update], PHP is used by 76.6% of all websites whose programming language could be determined, and 58.8% thereof use PHP 7 which is outdated and known to be insecure.[18] Additionally, the most used version of 8.x, 8.0, is no longer supported,[19] meaning the vast majority of PHP users utilize outdated versions, and well over half of all websites, PHP-using or not (whose programming language could be determined) are insecure.
 PHP development began in 1993[9] when Rasmus Lerdorf wrote several Common Gateway Interface (CGI) programs in C,[20][21] which he used to maintain his personal homepage. He extended them to work with web forms and to communicate with databases, and called this implementation "Personal Home Page/Forms Interpreter" or PHP/FI.
 An example of the early PHP syntax:[22]
 PHP/FI could be used to build simple, dynamic web applications. To accelerate bug reporting and improve the code, Lerdorf initially announced the release of PHP/FI as "Personal Home Page Tools (PHP Tools) version 1.0" on the Usenet discussion group comp.infosystems.www.authoring.cgi on 8 June 1995.[1][23] This release included basic functionality such as Perl-like variables, form handling, and the ability to embed HTML. By this point, the syntax had changed to resemble that of Perl, but was simpler, more limited, and less consistent.[12][11]
 Early PHP was never intended to be a new programming language; rather, it grew organically, with Lerdorf noting in retrospect: "I don't know how to stop it [...] there was never any intent to write a programming language [...] I have absolutely no idea how to write a programming language [...] I just kept adding the next logical step on the way."[24] A development team began to form and, after months of work and beta testing, officially released PHP/FI 2 in November 1997[25].[citation needed]
 The fact that PHP was not originally designed, but instead was developed organically has led to inconsistent naming of functions and inconsistent ordering of their parameters.[26] In some cases, the function names were chosen to match the lower-level libraries which PHP was "wrapping",[27] while in some very early versions of PHP the length of the function names was used internally as a hash function, so names were chosen to improve the distribution of hash values.[28]
 Zeev Suraski and Andi Gutmans rewrote the parser in 1997 and formed the base of PHP 3, changing the language's name to the recursive acronym PHP: Hypertext Preprocessor.[11][29] Afterwards, public testing of PHP 3 began, and the official launch came in June 1998. Suraski and Gutmans then started a new rewrite of PHP's core, producing the Zend Engine in 1999.[30] They also founded Zend Technologies in Ramat Gan, Israel.[11]
 On 22 May 2000, PHP 4, powered by the Zend Engine 1.0, was released.[11] By August 2008, this branch had reached version 4.4.9. PHP 4 is now no longer under development and nor are any security updates planned to be released.[31][32]
 On 1 July 2004, PHP 5 was released, powered by the new Zend Engine II.[11] PHP 5 included new features such as improved support for object-oriented programming, the PHP Data Objects (PDO) extension (which defines a lightweight and consistent interface for accessing databases), and numerous performance enhancements.[33] In 2008, PHP 5 became the only stable version under development. Late static binding had been missing from previous versions of PHP, and was added in version 5.3.[34][35]
 Many high-profile open-source projects ceased to support PHP 4 in new code from February 5, 2008, because of the GoPHP5 initiative,[36] provided by a consortium of PHP developers promoting the transition from PHP 4 to PHP 5.[37][38]
 Over time, PHP interpreters became available on most existing 32-bit and 64-bit operating systems, either by building them from the PHP source code or by using pre-built binaries.[39] For PHP versions 5.3 and 5.4, the only available Microsoft Windows binary distributions were 32-bit IA-32 builds,[40][41] requiring Windows 32-bit compatibility mode while using Internet Information Services (IIS) on a 64-bit Windows platform. PHP version 5.5 made the 64-bit x86-64 builds available for Microsoft Windows.[42]
 Official security support for PHP 5.6 ended on 31 December 2018.[43]
 PHP received mixed reviews due to lacking native Unicode support at the core language level.[44][45] In 2005, a project headed by Andrei Zmievski was initiated to bring native Unicode support throughout PHP, by embedding the International Components for Unicode (ICU) library, and representing text strings as UTF-16 internally.[46] Since this would cause major changes both to the internals of the language and to user code, it was planned to release this as version 6.0 of the language, along with other major features then in development.[47]
 However, a shortage of developers who understood the necessary changes, and performance problems arising from conversion to and from UTF-16, which is rarely used in a web context, led to delays in the project.[48] As a result, a PHP 5.3 release was created in 2009, with many non-Unicode features back-ported from PHP 6, notably namespaces. In March 2010, the project in its current form was officially abandoned, and a PHP 5.4 release was prepared to contain most remaining non-Unicode features from PHP 6, such as traits and closure re-binding.[49] Initial hopes were that a new plan would be formed for Unicode integration, but by 2014 none had been adopted.[citation needed]
 During 2014 and 2015, a new major PHP version was developed, PHP 7. The numbering of this version involved some debate among internal developers.[50] While the PHP 6 Unicode experiments had never been released, several articles and book titles referenced the PHP 6 names, which might have caused confusion if a new release were to reuse the name.[51] After a vote, the name PHP 7 was chosen.[52]
 The foundation of PHP 7 is a PHP branch that was originally dubbed PHP next generation (phpng). It was authored by Dmitry Stogov, Xinchen Hui and Nikita Popov,[53] and aimed to optimize PHP performance by refactoring the Zend Engine while retaining near-complete language compatibility.[54] By 14 July 2014, WordPress-based benchmarks, which served as the main benchmark suite for the phpng project, showed an almost 100% increase in performance. Changes from phpng make it easier to improve performance in future versions, as more compact data structures and other changes are seen as better suited for a successful migration to a just-in-time (JIT) compiler.[55] Because of the significant changes, the reworked Zend Engine was called Zend Engine 3, succeeding Zend Engine 2 used in PHP 5.[56]
 Because of the major internal changes in phpng, it must receive a new major version number of PHP, rather than a minor PHP 5 release, according to PHP's release process.[57] Major versions of PHP are allowed to break backward-compatibility of code and therefore PHP 7 presented an opportunity for other improvements beyond phpng that require backward-compatibility breaks.[citation needed] In particular, it involved the following changes:
 PHP 7 also included new language features. Most notably, it introduced return type declarations for functions[70] which complement the existing parameter type declarations, and support for the scalar types (integer, float, string, and boolean) in parameter and return type declarations.[71]
 PHP 8 was released on 26 November 2020, and is currently the second-most used PHP major version. PHP 8 is a major version and has breaking changes from previous versions.[72][73] New features and notable changes include:
 Just-in-time compilation is supported in PHP 8.[74]
 PHP 8's JIT compiler can provide substantial performance improvements for some use cases,[75][76] while PHP developer Nikita Popov stated that the performance improvements for most websites will be less substantial than the upgrade from PHP 5 to PHP 7.[77] Substantial improvements are expected more for mathematical-type operations than for common web-development use cases.[77] Additionally, the JIT compiler provides the future potential to move some code from C to PHP, due to the performance improvements for some use cases.[78]
 PHP 8 introduced the match expression.[79] The match expression is conceptually similar to a switch statement and is more compact for some use cases.[80] Because match is an expression, its result can be assigned to a variable or returned from a function.[81]
 PHP 8 introduced union types, a new static return type, and a new mixed type.[72]
 "Attributes", often referred to as "annotations" in other programming languages, were added in PHP 8, which allow metadata to be added to classes.[72]
 throw was changed from being a statement to being an expression.[82] This allows exceptions to be thrown in places that were not previously possible.[72]
 PHP 8 includes changes to allow alternate, more concise, or more consistent syntaxes in a number of scenarios. For example, the nullsafe operator is similar to the null coalescing operator ??, but used when calling methods.[83] The following code snippet will not throw an error if getBirthday() returns null:
 Constructor property promotion has been added as "syntactic sugar," allowing class properties to be set automatically when parameters are passed into a class constructor.[72] This reduces the amount of boilerplate code that must be written.[citation needed]
 Other minor changes include support for use of ::class on objects, which serves as an alternative for the use of get_class();[72] non-capturing catches in try-catch blocks; variable syntax tweaks to resolve inconsistencies; support for named arguments; and support for trailing commas in parameter lists, which adds consistency with support for trailing commas in other contexts, such as in arrays.[citation needed]
 PHP 8.1 was released on November 25, 2021.[87] It added support for enumerations (also called "enums"), declaring properties as readonly (which prevents modification of the property after initialization), and array unpacking with string keys. The new never type can be used to indicate that a function does not return.[88]
 PHP 8.2 was released on December 8, 2022.[89] New in this release are readonly classes (whose instance properties are implicitly readonly), disjunctive normal form (DNF) types, and the random extension, which provides a pseudorandom number generator with an object-oriented API.[90]
 Beginning on 28 June 2011, the PHP Development Team implemented a timeline for the release of new versions of PHP.[57] Under this system, at least one release should occur every month. Once per year, a minor release should occur which may include new features. Every minor release should at least be supported for two years with security and bug fixes, followed by at least one year of only security fixes, for a total of a three-year release process for every minor release. No new features, unless small and self-contained, are to be introduced into a minor release during the three-year release process.
 The mascot of the PHP project is the elePHPant, a blue elephant with the PHP logo on its side, designed by Vincent Pontier[201] in 1998.[202] "The (PHP) letters were forming the shape of an elephant if viewed in a sideways angle."[203] The elePHPant is sometimes differently coloured when in plush toy form.[204]
 Many variations of this mascot have been made over the years. Only the elePHPants based on the original design by Vincent Pontier are considered official by the community.[205] These are collectable and some of them are extremely rare.[206]
 The following "Hello, World!" program is written in PHP code embedded in an HTML document:
 However, as no requirement exists for PHP code to be embedded in HTML, the simplest version of Hello, World! may be written like this, with the closing tag ?> omitted as preferred in files containing pure PHP code.[207]
 The PHP interpreter only executes PHP code within its delimiters. Anything outside of its delimiters is not processed by PHP, although the non-PHP text is still subject to control structures described in PHP code. The most common delimiters are <?php to open and ?> to close PHP sections. The shortened form <? also exists. This short delimiter makes script files less portable since support for them can be disabled in the local PHP configuration and it is therefore discouraged.[208][209] Conversely, there is no recommendation against the echo short tag <?=.[210] Prior to PHP 5.4.0, this short syntax for echo only works with the short_open_tag configuration setting enabled, while for PHP 5.4.0 and later it is always available.[211][212][208] The purpose of all these delimiters is to separate PHP code from non-PHP content, such as JavaScript code or HTML markup.[213] So the shortest "Hello, World!" program written in PHP is:
 The first form of delimiters, <?php and ?>, in XHTML and other XML documents, creates correctly formed XML processing instructions.[214] This means that the resulting mixture of PHP code and other markups in the server-side file is itself well-formed XML.
 
Variables are prefixed with a dollar symbol, and a type does not need to be specified in advance. PHP 5 introduced type declarations that allow functions to force their parameters to be objects of a specific class, arrays, interfaces or callback functions. However, before PHP 7, type declarations could not be used with scalar types such as integers or strings.[71]
 Below is an example of how PHP variables are declared and initialized.
 Unlike function and class names, variable names are case-sensitive. Both double-quoted ("") and heredoc strings provide the ability to interpolate a variable's value into the string.[215] PHP treats newlines as whitespace in the manner of a free-form language, and statements are terminated by a semicolon.[216] PHP has three types of comment syntax: /* */ marks block and inline comments; // or # are used for one-line comments.[217] The echo statement is one of several facilities PHP provides to output text.[citation needed]
 In terms of keywords and language syntax, PHP is similar to C-style syntax. if conditions, for and while loops and function returns are similar in syntax to languages such as C, C++, C#, Java and Perl.[citation needed]
 PHP is loosely typed. It stores integers in a platform-dependent range, either as a 32, 64 or 128-bit signed integer equivalent to the C-language long type. Unsigned integers are converted to signed values in certain situations, which is different behaviour to many other programming languages.[218] Integer variables can be assigned using decimal (positive and negative), octal, hexadecimal, and binary notations.[citation needed]
 Floating-point numbers are also stored in a platform-specific range. They can be specified using floating-point notation, or two forms of scientific notation.[219] PHP has a native Boolean type that is similar to the native Boolean types in Java and C++. Using the Boolean type conversion rules, non-zero values are interpreted as true and zero as false, as in Perl and C++.[219]
 The null data type represents a variable that has no value; NULL is the only allowed value for this data type.[219]
 Variables of the "resource" type represent references to resources from external sources. These are typically created by functions from a particular extension, and can only be processed by functions from the same extension; examples include file, image, and database resources.[219]
 Arrays can contain elements of any type that PHP can handle, including resources, objects, and even other arrays. Order is preserved in lists of values and in hashes with both keys and values, and the two can be intermingled.[219] PHP also supports strings, which can be used with single quotes, double quotes, nowdoc or heredoc syntax.[220]
 The Standard PHP Library (SPL) attempts to solve standard problems and implements efficient data access interfaces and classes.[221]
 PHP defines a large array of functions in the core language and many are also available in various extensions; these functions are well documented online PHP documentation.[222] However, the built-in library has a wide variety of naming conventions and associated inconsistencies, as described under history above.
 Custom functions may be defined by the developer:
 As of 2024, the output of the above sample program is "I am currently 29 years old."
 In lieu of function pointers, functions in PHP can be referenced by a string containing their name. In this manner, normal PHP functions can be used, for example, as callbacks or within function tables.[223] User-defined functions may be created at any time without being prototyped.[222][223] Functions may be defined inside code blocks, permitting a run-time decision as to whether or not a function should be defined. There is a function_exists function that determines whether a function with a given name has already been defined. Function calls must use parentheses, with the exception of zero-argument class constructor functions called with the PHP operator new, in which case parentheses are optional.[citation needed]
 Since PHP 4.0.1 create_function(), a thin wrapper around eval(), allowed normal PHP functions to be created during program execution; it was deprecated in PHP 7.2 and removed in PHP 8.0[224] in favor of syntax for anonymous functions or "closures"[225] that can capture variables from the surrounding scope, which was added in PHP 5.3. Shorthand arrow syntax was added in PHP 7.4:[226]
 In the example above, getAdder() function creates a closure using passed argument $x, which takes an additional argument $y, and returns the created closure to the caller. Such a function is a first-class object, meaning that it can be stored in a variable, passed as a parameter to other functions, etc.[227]
 Unusually for a dynamically typed language, PHP supports type declarations on function parameters, which are enforced at runtime. This has been supported for classes and interfaces since PHP 5.0, for arrays since PHP 5.1, for "callables" since PHP 5.4, and scalar (integer, float, string and boolean) types since PHP 7.0.[71] PHP 7.0 also has type declarations for function return types, expressed by placing the type name after the list of parameters, preceded by a colon.[70] For example, the getAdder function from the earlier example could be annotated with types like so in PHP 7:
 By default, scalar type declarations follow weak typing principles. So, for example, if a parameter's type is int, PHP would allow not only integers, but also convertible numeric strings, floats or booleans to be passed to that function, and would convert them.[71] However, PHP 7 has a "strict typing" mode which, when used, disallows such conversions for function calls and returns within a file.[71]
 Basic object-oriented programming functionality was added in PHP 3 and improved in PHP 4.[11] This allowed for PHP to gain further abstraction, making creative tasks easier for programmers using the language. Object handling was completely rewritten for PHP 5, expanding the feature set and enhancing performance.[228] In previous versions of PHP, objects were handled like value types.[228] The drawback of this method was that code had to make heavy use of PHP's "reference" variables if it wanted to modify an object it was passed rather than creating a copy of it. In the new approach, objects are referenced by handle, and not by value.[citation needed]
 PHP 5 introduced private and protected member variables and methods, along with abstract classes, final classes, abstract methods, and final methods. It also introduced a standard way of declaring constructors and destructors, similar to that of other object-oriented languages such as C++, and a standard exception handling model. Furthermore, PHP 5 added interfaces and allowed for multiple interfaces to be implemented. There are special interfaces that allow objects to interact with the runtime system. Objects implementing ArrayAccess can be used with array syntax and objects implementing Iterator or IteratorAggregate can be used with the foreach language construct. There is no virtual table feature in the engine, so static variables are bound with a name instead of a reference at compile time.[229]
 If the developer creates a copy of an object using the reserved word clone, the Zend engine will check whether a __clone() method has been defined. If not, it will call a default __clone() which will copy the object's properties. If a __clone() method is defined, then it will be responsible for setting the necessary properties in the created object. For convenience, the engine will supply a function that imports the properties of the source object, so the programmer can start with a by-value replica of the source object and only override properties that need to be changed.[230]
 The visibility of PHP properties and methods is defined using the keywords public, private, and protected. The default is public, if only var is used; var is a synonym for public. Items declared public can be accessed everywhere. protected limits access to inherited classes (and to the class that defines the item). private limits visibility only to the class that defines the item.[231] Objects of the same type have access to each other's private and protected members even though they are not the same instance.[citation needed]
 The following is a basic example of object-oriented programming in PHP 8:
 This program outputs the following:
 The only complete PHP implementation is the original, known simply as PHP. It is the most widely used and is powered by the Zend Engine. To disambiguate it from other implementations, it is sometimes unofficially called "Zend PHP". The Zend Engine compiles PHP source code on-the-fly into an internal format that it can execute, thus it works as an interpreter.[232][233] It is also the "reference implementation" of PHP, as PHP has no formal specification, and so the semantics of Zend PHP define the semantics of PHP. Due to the complex and nuanced semantics of PHP, defined by how Zend works, it is difficult for competing implementations to offer complete compatibility.[citation needed]
 PHP's single-request-per-script-execution model, and the fact that the Zend Engine is an interpreter, leads to inefficiency; as a result, various products have been developed to help improve PHP performance. In order to speed up execution time and not have to compile the PHP source code every time the web page is accessed, PHP scripts can also be deployed in the PHP engine's internal format by using an opcode cache, which works by caching the compiled form of a PHP script (opcodes) in shared memory to avoid the overhead of parsing and compiling the code every time the script runs. An opcode cache, Zend Opcache, is built into PHP since version 5.5.[234] Another example of a widely used opcode cache is the Alternative PHP Cache (APC), which is available as a PECL extension.[235]
 While Zend PHP is still the most popular implementation, several other implementations have been developed. Some of these are compilers or support JIT compilation, and hence offer performance benefits over Zend PHP at the expense of lacking full PHP compatibility.[citation needed] Alternative implementations include the following:
 PHP is free software released under the PHP License, which stipulates that:[240]
  Products derived from this software may not be called "PHP", nor may "PHP" appear in their name, without prior written permission from group@php.net. You may indicate that your software works in conjunction with PHP by saying "Foo for PHP" instead of calling it "PHP Foo" or "phpfoo".
 This restriction on the use of "PHP" makes the PHP License incompatible with the General Public License (GPL), while the Zend License is incompatible due to an advertising clause similar to that of the original BSD license.[241]
 PHP includes various free and open-source libraries in its source distribution or uses them in resulting PHP binary builds. PHP is fundamentally an Internet-aware system with built-in modules for accessing File Transfer Protocol (FTP) servers and many database servers, including PostgreSQL, MySQL, Microsoft SQL Server and SQLite (which is an embedded database), LDAP servers, and others. Numerous functions are familiar to C programmers, such as those in the stdio family, are available in standard PHP builds.[242]
 PHP allows developers to write extensions in C to add functionality to the PHP language. PHP extensions can be compiled statically into PHP or loaded dynamically at runtime. Numerous extensions have been written to add support for the Windows API, process management on Unix-like operating systems, multibyte strings (Unicode), cURL, and several popular compression formats. Other PHP features made available through extensions include integration with Internet Relay Chat (IRC), dynamic generation of images and Adobe Flash content, PHP Data Objects (PDO) as an abstraction layer used for accessing databases,[243][244][245][246][247][248][249] and even speech synthesis. Some of the language's core functions, such as those dealing with strings and arrays, are also implemented as extensions.[250] The PHP Extension Community Library (PECL) project is a repository for extensions to the PHP language.[251]
 Some other projects, such as Zephir, provide the ability for PHP extensions to be created in a high-level language and compiled into native PHP extensions. Such an approach, instead of writing PHP extensions directly in C, simplifies the development of extensions and reduces the time required for programming and testing.[252]
 By December 2018 the PHP Group consisted of ten people: Thies C. Arntzen, Stig Bakken, Shane Caraveo, Andi Gutmans, Rasmus Lerdorf, Sam Ruby, Sascha Schumann, Zeev Suraski, Jim Winstead, and Andrei Zmievski.[253]
 Zend Technologies provides a PHP Certification based on PHP 7[254] exam (and previously based on PHP 5.5) for programmers to become certified PHP developers.
 On 26 November 2021, the JetBrains blog announced the creation of The PHP Foundation, which will sponsor the design and development of PHP.[256]
 There are two primary ways for adding support for PHP to a web server – as a native web server module, or as a CGI executable. PHP has a direct module interface called server application programming interface (SAPI), which is supported by many web servers including Apache HTTP Server, Microsoft IIS and iPlanet Web Server. Some other web servers, such as OmniHTTPd, support the Internet Server Application Programming Interface (ISAPI), which is Microsoft's web server module interface. If PHP has no module support for a web server, it can always be used as a Common Gateway Interface (CGI) or FastCGI processor; in that case, the web server is configured to use PHP's CGI executable to process all requests to PHP files.[257]
 PHP-FPM (FastCGI Process Manager) is an alternative FastCGI implementation for PHP, bundled with the official PHP distribution since version 5.3.3.[258] When compared to the older FastCGI implementation, it contains some additional features, mostly useful for heavily loaded web servers.[259]
 When using PHP for command-line scripting, a PHP command-line interface (CLI) executable is needed. PHP supports a CLI server application programming interface (SAPI) since PHP 4.3.0.[260] The main focus of this SAPI is developing shell applications using PHP. There are quite a few differences between the CLI SAPI and other SAPIs, although they do share many of the same behaviours.[261]
 PHP has a direct module interface called SAPI for different web servers;[262] in case of PHP 5 and Apache 2.0 on Windows, it is provided in form of a DLL file called php5apache2.dll,[263] which is a module that, among other functions, provides an interface between PHP and the web server, implemented in a form that the server understands. This form is what is known as a SAPI.[citation needed]
 There are different kinds of SAPIs for various web server extensions. For example, in addition to those listed above, other SAPIs for the PHP language include the Common Gateway Interface and command-line interface.[262][264]
 PHP can also be used for writing desktop graphical user interface (GUI) applications, by using the PHP-GTK extension. PHP-GTK is not included in the official PHP distribution,[257] and as an extension, it can be used only with PHP versions 5.1.0 and newer. The most common way of installing PHP-GTK is by compiling it from the source code.[265]
 When PHP is installed and used in cloud environments, software development kits (SDKs) are provided for using cloud-specific features.[citation needed] For example:
 Numerous configuration options are supported, affecting both core PHP features and extensions.[268][269] Configuration file php.ini is searched for in different locations, depending on the way PHP is used.[270] The configuration file is split into various sections,[271] while some of the configuration options can be also set within the web server configuration.[272]
 PHP is a general-purpose scripting language that is especially suited to server-side web development, in which case PHP generally runs on a web server. Any PHP code in a requested file is executed by the PHP runtime, usually to create dynamic web page content or dynamic images used on websites or elsewhere.[273] It can also be used for command-line scripting and client-side graphical user interface (GUI) applications. PHP can be deployed on most web servers, many operating systems and platforms, and can be used with many relational database management systems (RDBMS). Most web hosting providers support PHP for use by their clients. It is available free of charge, and the PHP Group provides the complete source code for users to build, customize and extend for their own use.[17]
 Originally designed to create dynamic web pages, PHP now focuses mainly on server-side scripting,[274] and it is similar to other server-side scripting languages that provide dynamic content from a web server to a client, such as Python, Microsoft's ASP.NET, Sun Microsystems' JavaServer Pages,[275] and mod_perl. PHP has also attracted the development of many software frameworks that provide building blocks and a design structure to promote rapid application development (RAD).[citation needed] Some of these include PRADO, CakePHP, Symfony, CodeIgniter, Laravel, Yii Framework, Phalcon and Laminas, offering features similar to other web frameworks.
 The LAMP architecture has become popular in the web industry as a way of deploying web applications.[276] PHP is commonly used as the P in this bundle alongside Linux, Apache and MySQL, although the P may also refer to Python, Perl, or some mix of the three. Similar packages, WAMP and MAMP, are also available for Windows and macOS, with the first letter standing for the respective operating system. Although both PHP and Apache are provided as part of the macOS base install, users of these packages seek a simpler installation mechanism that can be more easily kept up to date.[citation needed]
 For specific and more advanced usage scenarios, PHP offers a well-defined and documented way for writing custom extensions in C or C++.[277][278][279][280][281][282][283][non-primary source needed] Besides extending the language itself in form of additional libraries, extensions are providing a way for improving execution speed where it is critical and there is room for improvements by using a true compiled language.[284][285] PHP also offers well-defined ways for embedding itself into other software projects. That way PHP can be easily used as an internal scripting language for another project, also providing tight interfacing with the project's specific internal data structures.[286]
 PHP received mixed reviews due to lacking support for multithreading at the core language level,[287] though using threads is made possible by the "pthreads" PECL extension.[288][289]
 A command line interface, php-cli, and two ActiveX Windows Script Host scripting engines for PHP have been produced.[citation needed]
 Usage share of PHP versions on 23 February 2024: three months after PHP 8.3's release[18][290][291][292][293]
 PHP is used for Web content management systems including MediaWiki,[294] WordPress,[295] Joomla,[296] Drupal,[297] Moodle,[298] eZ Publish, eZ Platform, and SilverStripe.[299]
 As of January 2013[update], PHP was used in more than 240 million websites (39% of those sampled) and was installed on 2.1 million web servers.[300]
 As of 23 February 2024[update] (three months after PHP 8.3's release), PHP is used as the server-side programming language on 76.5% of websites where the language could be determined; PHP 7 is the most used version of the language with 58.1% of websites using PHP being on that version, while 25.1% use PHP 8, 16.6% use PHP 5 and 0.2% use PHP 4.[18]
 In 2019, 11% of all vulnerabilities listed by the National Vulnerability Database were linked to PHP;[301] historically, about 30% of all vulnerabilities listed since 1996 in this database are linked to PHP. Technical security flaws of the language itself or of its core libraries are not frequent (22 in 2009, about 1% of the total although PHP applies to about 20% of programs listed).[302] Recognizing that programmers make mistakes, some languages include taint checking to automatically detect the lack of input validation which induces many issues. Such a feature is being developed for PHP,[303] but its inclusion into a release has been rejected several times in the past.[304][305]
 There are advanced protection patches such as Suhosin and Hardening-Patch, specially designed for web hosting environments.[306]
 Historically, old versions of PHP had some configuration parameters and default values for such runtime settings that made some PHP applications prone to security issues. Among these, magic_quotes_gpc and register_globals[307] configuration directives were the best known; the latter made any URL parameters become PHP variables, opening a path for serious security vulnerabilities by allowing an attacker to set the value of any uninitialized global variable and interfere with the execution of a PHP script. Support for "magic quotes" and "register globals" settings has been deprecated since PHP 5.3.0, and removed from PHP 5.4.0.[308]
 Another example for the potential runtime-settings vulnerability comes from failing to disable PHP execution (for example by using the engine configuration directive)[309] for the directory where uploaded files are stored; enabling it can result in the execution of malicious code embedded within the uploaded files.[310][311][312] The best practice is to either locate the image directory outside of the document root available to the web server and serve it via an intermediary script or disable PHP execution for the directory which stores the uploaded files.[citation needed]
 Also, enabling the dynamic loading of PHP extensions (via enable_dl configuration directive)[313] in a shared web hosting environment can lead to security issues.[314][315]
 Implied type conversions that result in different values being treated as equal, sometimes against the programmer's intent, can lead to security issues. For example, the result of the comparison '0e1234' == '0' is true, because strings that are parsable as numbers are converted to numbers; in this case, the first compared value is treated as scientific notation having the value (0×101234), which is zero. Errors like this resulted in authentication vulnerabilities in Simple Machines Forum,[316] Typo3[317] and phpBB[318] when MD5 password hashes were compared. The recommended way is to use hash_equals() (for timing attack safety), strcmp or the identity operator (===), as '0e1234' === '0' results in false.[319]
 In a 2013 analysis of over 170,000 website defacements, published by Zone-H, the most frequently (53%) used technique was the exploitation of file inclusion vulnerability, mostly related to insecure usage of the PHP language constructs include, require, and allow_url_fopen.[320][321]
 As of 23 February 2024,[update] (3 months after PHP 8.3 release) W3Techs reports that 85.32% of websites using PHP, use versions 8.0 or older (which are no longer supported by The PHP Development Team).[322] PHP Version 5 is still used by 16.6% of all websites.[291] It is highly recommended to migrate to PHP 8.1 or later and use random_int()[323] instead of rand()[324] or mt_rand(),[325] as the latter functions are not cryptographically secure. There are two attacks that can be performed over PHP entropy sources: "seed attack" and "state recovery attack".[citation needed] With current[when?] GPU technologies, an attacker can perform up to 230 MD5 calculations per second with a $250 GPU, while with an additional $500 can reach up to 232 calculations.[326] In combination with a "birthday attack" this can lead to serious security vulnerabilities.[citation needed]


Source: https://en.wikipedia.org/wiki/Dart_(Programmiersprache)
Content: Other reasons this message may be displayed:


Source: https://en.wikipedia.org/wiki/Ruby_(Programmiersprache)
Content: Other reasons this message may be displayed:


Source: https://en.wikipedia.org/wiki/Donald_Knuth
Content: 
 Donald Ervin Knuth (/kəˈnuːθ/[3] kə-NOOTH; born January 10, 1938) is an American computer scientist and mathematician. He is a professor emeritus at Stanford University. He is the 1974 recipient of the ACM Turing Award, informally considered the Nobel Prize of computer science.[4] Knuth has been called the "father of the analysis of algorithms".[5]
 Knuth is the author of the multi-volume work The Art of Computer Programming. He contributed to the development of the rigorous analysis of the computational complexity of algorithms and systematized formal mathematical techniques for it. In the process, he also popularized the asymptotic notation. In addition to fundamental contributions in several branches of theoretical computer science, Knuth is the creator of the TeX computer typesetting system, the related METAFONT font definition language and rendering system, and the Computer Modern family of typefaces.
 As a writer and scholar, Knuth created the WEB and CWEB computer programming systems designed to encourage and facilitate literate programming, and designed the MIX/MMIX instruction set architectures. He strongly opposes the granting of software patents, and has expressed his opinion to the United States Patent and Trademark Office and European Patent Organisation.
 Donald Knuth was born in Milwaukee, Wisconsin, to Ervin Henry Knuth and Louise Marie Bohning.[6] He describes his heritage as "Midwestern Lutheran German".[7]: 66  His father owned a small printing business and taught bookkeeping.[8] While a student at Milwaukee Lutheran High School, Knuth thought of ingenious ways to solve problems. For example, in eighth grade, he entered a contest to find the number of words that the letters in "Ziegler's Giant Bar" could be rearranged to create; the judges had identified 2,500 such words. With time gained away from school due to a fake stomach ache Knuth used an unabridged dictionary and determined whether each dictionary entry could be formed using the letters in the phrase. Using this algorithm, he identified over 4,500 words, winning the contest.[7]: 3  As prizes, the school received a new television and enough candy bars for all of his schoolmates to eat.[9]
 Knuth received a scholarship in physics to the Case Institute of Technology (now part of Case Western Reserve University) in Cleveland, Ohio, enrolling in 1956.[10] He also joined the Beta Nu Chapter of the Theta Chi fraternity. While studying physics at Case, Knuth was introduced to the IBM 650, an early commercial computer. After reading the computer's manual, Knuth decided to rewrite the assembly and compiler code for the machine used in his school because he believed he could do it better.[11]
 In 1958, Knuth created a program to help his school's basketball team win its games.[12] He assigned "values" to players in order to gauge their probability of scoring points, a novel approach that Newsweek and CBS Evening News later reported on.[11]
 Knuth was one of the founding editors of the Case Institute's Engineering and Science Review, which won a national award as best technical magazine in 1959.[13][14] He then switched from physics to mathematics, and received two degrees from Case in 1960:[10] his Bachelor of Science, and simultaneously a master of science by a special award of the faculty, who considered his work exceptionally outstanding.[4][11]
 In 1963, with mathematician Marshall Hall as his adviser,[2] he earned a PhD in mathematics from the California Institute of Technology, with a thesis titled Finite Semifields and Projective Planes.[15]
 In 1963, after receiving his PhD, Knuth joined Caltech's faculty as an assistant professor.[16]
 Knuth accepted a commission to write a book on computer programming language compilers. While working on this project, he decided that he could not adequately treat the topic without first developing a fundamental theory of computer programming, which became The Art of Computer Programming. He originally planned to publish this as a single book, but as he developed his outline for the book, he concluded that he required six volumes, and then seven, to thoroughly cover the subject. He published the first volume in 1968.[17]
 Just before publishing the first volume of The Art of Computer Programming, Knuth left Caltech to accept employment with the Institute for Defense Analyses' Communications Research Division,[18] then situated on the Princeton campus, which was performing mathematical research in cryptography to support the National Security Agency.
 In 1967, Knuth attended a Society for Industrial and Applied Mathematics conference and someone asked what he did. At the time, computer science was partitioned into numerical analysis, artificial intelligence, and programming languages. Based on his study and The Art of Computer Programming book, Knuth decided the next time someone asked he would say, "Analysis of algorithms".[19]
 In 1969, Knuth left his position at Princeton to join the Stanford University faculty,[20] where he became Fletcher Jones Professor of Computer Science in 1977. He became Professor of The Art of Computer Programming in 1990, and has been emeritus since 1993.[21][22]
 Knuth is a writer, as well as a computer scientist.[16]
 "The best way to communicate from one human being to another is through story." In the 1970s, Knuth called computer science "a totally new field with no real identity. And the standard of available publications was not that high. A lot of the papers coming out were quite simply wrong. ... So one of my motivations was to put straight a story that had been very badly told."[23]
 From 1972 to 1973, Knuth spent a year at the University of Oslo among people such as Ole-Johan Dahl. This is where he had originally intended to write the seventh volume in his book series, which was to deal with programming languages. But Knuth had finished only the first two volumes when he came to Oslo, and thus spent the year on the third volume, next to teaching. The third volume came out just after Knuth returned to Stanford in 1973.[24]
 By 2011, Volume 4A had been published.[17] Concrete Mathematics: A Foundation for Computer Science 2nd ed., which originated with an expansion of the mathematical preliminaries section of Volume 1 of TAoCP, was published in 1994. In April 2020, Knuth said he anticipated that Volume 4 will have at least parts A through F.[19] Volume 4B was published in October 2022.
 Knuth is also the author of Surreal Numbers,[25] a mathematical novelette on John Conway's set theory construction of an alternate system of numbers. Instead of simply explaining the subject, the book seeks to show the development of the mathematics. Knuth wanted the book to prepare students for doing original, creative research.
 In 1995, Knuth wrote the foreword to the book A=B by Marko Petkovšek, Herbert Wilf and Doron Zeilberger.[26] He also occasionally contributes language puzzles to Word Ways: The Journal of Recreational Linguistics.[27]
 Knuth has delved into recreational mathematics. He contributed articles to the Journal of Recreational Mathematics beginning in the 1960s, and was acknowledged as a major contributor in Joseph Madachy's Mathematics on Vacation.[28]
 Knuth also appears in a number of Numberphile[29] and Computerphile videos on YouTube, where he discusses topics from writing Surreal Numbers[30] to why he does not use email.[31]
 In addition to his writings on computer science, Knuth, a Lutheran,[32] is also the author of 3:16 Bible Texts Illuminated,[33] in which he examines the Bible by a process of systematic sampling, namely an analysis of chapter 3, verse 16 of each book.  Each verse is accompanied by a rendering in calligraphic art, contributed by a group of calligraphers led by Hermann Zapf. Knuth was invited to give a set of lectures at MIT on the views on religion and computer science behind his 3:16 project, resulting in another book, Things a Computer Scientist Rarely Talks About, where he published the lectures God and Computer Science.[34]
 Knuth strongly opposes granting software patents to trivial solutions that should be obvious, but has expressed more nuanced views for nontrivial solutions such as the interior-point method of linear programming.[35] He has expressed his disagreement directly to both the United States Patent and Trademark Office and European Patent Organisation.[36]
 In the 1970s the publishers of TAOCP abandoned Monotype in favor of phototypesetting. Knuth became so frustrated with the inability of the latter system to approach the quality of the previous volumes, which were typeset using the older system, that he took time out to work on digital typesetting and created TeX and Metafont.[37]
 While developing TeX, Knuth created a new methodology of programming, which he called literate programming, because he believed that programmers should think of programs as works of literature:
 Instead of imagining that our main task is to instruct a computer what to do, let us concentrate rather on explaining to human beings what we want a computer to do.[38] Knuth embodied the idea of literate programming in the WEB system. The same WEB source is used to weave a TeX file, and to tangle a Pascal source file. These in their turn produce a readable description of the program and an executable binary respectively. A later iteration of the system, CWEB, replaces Pascal with C, C++, and Java.[39]
 Knuth used WEB to program TeX and METAFONT, and published both programs as books, both originally published the same year: TeX: The Program (1986); and METAFONT: The Program (1986).[40]  Around the same time, LaTeX, the now-widely adopted macro package based on TeX, was first developed by Leslie Lamport, who later published its first user manual in 1986.[41]
 Donald Knuth married Nancy Jill Carter on 24 June 1961, while he was a graduate student at the California Institute of Technology. They have two children: John Martin Knuth and Jennifer Sierra Knuth.[42]
 Knuth gives informal lectures a few times a year at Stanford University, which he calls "Computer Musings". He was a visiting professor at the Oxford University Department of Computer Science in the United Kingdom until 2017 and an Honorary Fellow of Magdalen College.[43][44]
 Knuth is an organist and a composer. He and his father served as organists for Lutheran congregations. Knuth and his wife have a 16-rank organ in their home.[45] In 2016 he completed a piece for organ, Fantasia Apocalyptica, which he calls a "translation of the Greek text of the Revelation of Saint John the Divine into music". It was premièred in Sweden on January 10, 2018.[46]
 Knuth's Chinese name is Gao Dena (simplified Chinese: 高德纳; traditional Chinese: 高德納; pinyin: Gāo Dénà).[47][3] He was given this name in 1977 by Frances Yao shortly before making a three-week trip to China.[3][48] In the 1980 Chinese translation of Volume 1 of The Art of Computer Programming (simplified Chinese: 计算机程序设计艺术; traditional Chinese: 計算機程式設計藝術; pinyin: Jìsuànjī chéngxù shèjì yìshù), Knuth explains that he embraced his Chinese name because he wanted to be known by the growing numbers of computer programmers in China at the time. In 1989, his Chinese name was placed atop the Journal of Computer Science and Technology's header, which Knuth says "makes me feel close to all Chinese people although I cannot speak your language".[48]
 In 2006, Knuth was diagnosed with prostate cancer. He underwent surgery in December that year and said, "a little bit of radiation therapy ... as a precaution but the prognosis looks pretty good" in his video autobiography.[49]
 Knuth used to pay a finder's fee of $2.56 for any typographical errors or mistakes discovered in his books, because "256 pennies is one hexadecimal dollar", and $0.32 for "valuable suggestions". According to an article in the Massachusetts Institute of Technology's Technology Review, these Knuth reward checks are "among computerdom's most prized trophies". Knuth had to stop sending real checks in 2008 due to bank fraud, and now gives each error finder a "certificate of deposit" from a publicly listed balance in his fictitious "Bank of San Serriffe".[50]
 He once warned a correspondent, "Beware of bugs in the above code; I have only proved it correct, not tried it."[3]
 Knuth published his first "scientific" article in a school magazine in 1957 under the title "The Potrzebie System of Weights and Measures". In it, he defined the fundamental unit of length as the thickness of Mad No. 26, and named the fundamental unit of force "whatmeworry". Mad published the article in issue No. 33 (June 1957).[51][52]
 To demonstrate the concept of recursion, Knuth intentionally referred "Circular definition" and "Definition, circular" to each other in the index of The Art of Computer Programming, Volume 1.
 
The preface of Concrete Mathematics has the following paragraph: When DEK taught Concrete Mathematics at Stanford for the first time, he explained the somewhat strange title by saying that it was his attempt to teach a math course that was hard instead of soft.  He announced that, contrary to the expectations of his colleagues, he was not going to teach the Theory of Aggregates, nor Stone's Embedding Theorem, nor even the Stone–Čech compactification. (Several students from the civil engineering department got up and quietly left the room.) At the TUG 2010 Conference, Knuth announced a satirical XML-based successor to TeX, titled "iTeX" (pronounced [iː˨˩˦tɛks˧˥], performed with a bell ringing), which would support features such as arbitrarily scaled irrational units, 3D printing, input from seismographs and heart monitors, animation, and stereophonic sound.[53][54][55]
 In 1971, Knuth received the first ACM Grace Murray Hopper Award.[4] He has received various other awards, including the Turing Award, the National Medal of Science, the John von Neumann Medal, and the Kyoto Prize.[4]
 Knuth was elected a Distinguished Fellow of the British Computer Society (DFBCS) in 1980 in recognition of his contributions to the field of computer science.[56]
 In 1990 he was awarded the one-of-a-kind academic title Professor of The Art of Computer Programming, which has since been revised to Professor Emeritus of The Art of Computer Programming.
 Knuth was elected to the National Academy of Sciences in 1975. He was also elected a member of the National Academy of Engineering in 1981 for organizing vast subject areas of computer science so that they are accessible to all segments of the computing community. In 1992, he became an associate of the French Academy of Sciences. Also that year, he retired from regular research and teaching at Stanford University in order to finish The Art of Computer Programming. He was elected a Foreign Member of the Royal Society (ForMemRS) in 2003.[1]
 Knuth was elected as a Fellow (first class of Fellows) of the Society for Industrial and Applied Mathematics in 2009 for his outstanding contributions to mathematics.[57] He is a member of the Norwegian Academy of Science and Letters.[58] In 2012, he became a fellow of the American Mathematical Society[59] and a member of the American Philosophical Society.[60] Other awards and honors include:
 A short list of his publications include:[75]
 The Art of Computer Programming:
 Computers and Typesetting (all books are hardcover unless otherwise noted):
 Books of collected papers:
 Other books:


Source: https://en.wikipedia.org/wiki/Tony_Hoare
Content: 
 Sir Charles Antony Richard Hoare (Tony Hoare or C. A. R. Hoare)[pronunciation?] FRS FREng[3] (born 11 January 1934)[4] is a British computer scientist who has made foundational contributions to programming languages, algorithms, operating systems, formal verification, and concurrent computing.[5] His work earned him the Turing Award, usually regarded as the highest distinction in computer science, in 1980.
 Hoare developed the sorting algorithm quicksort in 1959–1960.[6] He developed Hoare logic, an axiomatic basis for verifying program correctness. In the semantics of concurrency, he introduced the formal language communicating sequential processes (CSP) to specify the interactions of concurrent processes, and along with Edsger Dijkstra, formulated the dining philosophers problem.[7][8][9][10][11][12] Since 1977, he has held positions at the University of Oxford and Microsoft Research in Cambridge.
 Tony Hoare was born in Colombo, Ceylon (now Sri Lanka) to British parents; his father was a colonial civil servant and his mother was the daughter of a tea planter. Hoare was educated in England at the Dragon School in Oxford and the King's School in Canterbury.[13] He then studied Classics and Philosophy ("Greats") at Merton College, Oxford.[14] On graduating in 1956 he did 18 months National Service in the Royal Navy,[14] where he learned Russian.[15] He returned to the University of Oxford in 1958 to study for a postgraduate certificate in statistics,[14] and it was here that he began computer programming, having been taught Autocode on the Ferranti Mercury by Leslie Fox.[16] He then went to Moscow State University as a British Council exchange student,[14] where he studied machine translation under Andrey Kolmogorov.[15]
 In 1960, Hoare left the Soviet Union and began working at Elliott Brothers Ltd,[14] a small computer manufacturing firm located in London. There, he implemented the language ALGOL 60 and began developing major algorithms.[17][18]
 He was involved with developing international standards in programming and informatics, as a member of the International Federation for Information Processing (IFIP) Working Group 2.1 on Algorithmic Languages and Calculi,[19] which specified, maintains, and supports the languages ALGOL 60 and ALGOL 68.[20]
 He became the Professor of Computing Science at the Queen's University of Belfast in 1968, and in 1977 returned to Oxford as the Professor of Computing to lead the Programming Research Group in the Oxford University Computing Laboratory (now Department of Computer Science, University of Oxford), following the death of Christopher Strachey. He became the first Christopher Strachey Professor of Computing on its establishment in 1988 until his retirement at Oxford in 2000.[21] He is now an Emeritus Professor there, and is also a principal researcher at Microsoft Research in Cambridge, England.[22][23][24]
 Hoare's most significant work has been in the following areas: his sorting and selection algorithm (Quicksort and Quickselect), Hoare logic, the formal language communicating sequential processes (CSP) used to specify the interactions between concurrent processes (and implemented in various programming languages such as occam), structuring computer operating systems using the monitor concept, and the axiomatic specification of programming languages.[25][26]
 Speaking at a software conference in 2009, Tony Hoare hyperbolically apologized for "inventing" the null reference:[27]
[28]
 I call it my billion-dollar mistake. It was the invention of the null reference in 1965. At that time, I was designing the first comprehensive type system for references in an object oriented language (ALGOL W). My goal was to ensure that all use of references should be absolutely safe, with checking performed automatically by the compiler. But I couldn't resist the temptation to put in a null reference, simply because it was so easy to implement. This has led to innumerable errors, vulnerabilities, and system crashes, which have probably caused a billion dollars of pain and damage in the last forty years.[29] For many years under his leadership, Hoare's Oxford department worked on formal specification languages such as CSP and Z. These did not achieve the expected take-up by industry, and in 1995 Hoare was led to reflect upon the original assumptions:[30]
 Ten years ago, researchers into formal methods (and I was the most mistaken among them) predicted that the programming world would embrace with gratitude every assistance promised by formalisation to solve the problems of reliability that arise when programs get large and more safety-critical. Programs have now got very large and very critical – well beyond the scale which can be comfortably tackled by formal methods. There have been many problems and failures, but these have nearly always been attributable to inadequate analysis of requirements or inadequate management control. It has turned out that the world just does not suffer significantly from the kind of problem that our research was originally intended to solve. In 1962, Hoare married Jill Pym, a member of his research team.[45]
  This article incorporates text available under the CC BY 4.0 license.


Source: https://en.wikipedia.org/wiki/Just-in-time-Kompilierung
Content: Other reasons this message may be displayed:


Source: https://en.wikipedia.org/wiki/PyPy
Content: PyPy (/ˈpaɪpaɪ/) is an implementation of the Python programming language.[2] PyPy often runs faster than the standard implementation CPython because PyPy uses a just-in-time compiler.[3] Most Python code runs well on PyPy except for code that depends on CPython extensions, which either does not work or incurs some overhead when run in PyPy.
 PyPy itself is built using a technique known as meta-tracing, which is a mostly automatic transformation that takes an interpreter as input and produces a tracing just-in-time compiler as output. Since interpreters are usually easier to write than compilers, but run slower, this technique can make it easier to produce efficient implementations of programming languages. PyPy's meta-tracing toolchain is called RPython.
 PyPy does not have full compatibility with more recent versions of the CPython ecosystem. While it claims compatibility with Python 2.7, 3.7, 3.8 and 3.9 ("a drop-in replacement for CPython"), it lacks some of the newer features and syntax in Python 3.10, such as syntax for pattern matching.[4]
 PyPy aims to provide a common translation and support framework for producing implementations of dynamic languages, emphasizing a clean separation between language specification and implementation aspects. It also aims to provide a compliant, flexible and fast implementation of the Python programming language using the above framework to enable new advanced features without having to encode low-level details into it.[5][6]
 The PyPy interpreter itself is written in a restricted subset of Python called RPython (Restricted Python).[7] RPython puts some constraints on the Python language such that a variable's type can be inferred at compile time.[8]
 The PyPy project has developed a toolchain that analyzes RPython code and translates it into a form of byte code, which can be lowered into C. There used to be other backends in addition to C (Java, C#, and Javascript), but those suffered from bitrot and have been removed. Thus, the recursive logo of PyPy is a snake swallowing itself since the RPython is translated by a Python interpreter. The code can also be run untranslated for testing and analysis, which provides a nice test-bed for research into dynamic languages.
 It allows for pluggable garbage collectors, as well as optionally enabling Stackless Python features. Finally, it includes a just-in-time (JIT) generator that builds a just-in-time compiler into the interpreter, given a few annotations in the interpreter source code. The generated JIT compiler is a tracing JIT.[9]
 RPython is now also used to write non-Python language implementations, such as Pixie.[10]
 PyPy as of version 7.3.7 is compatible with three CPython versions: 2.7, 3.7 and 3.8.[11][12] The first PyPy version compatible with CPython v3 is PyPy v2.3.1 (2014).[13] The PyPy interpreter compatible with CPython v3 is also known as PyPy3.
 PyPy has JIT compilation support on 32-bit/64-bit x86 and 32-bit/64-bit ARM processors.[14] It is tested nightly on Windows, Linux, OpenBSD and Mac OS X. PyPy is able to run pure Python software that does not rely on implementation-specific features.[15]
 There is a compatibility layer for CPython C API extensions called CPyExt, but it is incomplete and experimental. The preferred way of interfacing with C shared libraries is through the built-in C foreign function interface (CFFI) or ctypes libraries.
 PyPy is a followup to the Psyco project, a just-in-time specializing compiler for Python, developed by Armin Rigo between 2002 and 2010. PyPy's aim is to have a just-in-time specializing compiler with scope, which was not available for Psyco.[clarification needed] Initially, the RPython could also be compiled into Java bytecode, CIL and JavaScript, but these backends were removed due to lack of interest.
 PyPy was initially a research and development-oriented project. Reaching a mature state of development and an official 1.0 release in mid-2007, its next focus was on releasing a production-ready version with more CPython compatibility. Many of PyPy's changes have been made during coding sprints.
 PyPy was funded by the European Union being a Specific Targeted Research Project[31] between December 2004 and March 2007. In June 2008, PyPy announced funding being part of the Google Open Source programs and has agreed to focus on making PyPy more compatible with CPython. In 2009 Eurostars, a European Union funding agency specially focused on SMEs,[32] accepted a proposal from PyPy project members titled "PYJIT – a fast and flexible toolkit for dynamic programming languages based on PyPy". Eurostars funding lasted until August 2011.[33]
At PyCon US 2011, the Python Software Foundation provided a $10,000 grant for PyPy to continue work on performance and compatibility with newer versions of the language.[34]
The port to ARM architecture was sponsored in part by the Raspberry Pi Foundation.[22]
 The PyPy project also accepts donations through its status blog pages.[35] As of 2013, a variety of sub-projects had funding: Python 3 version compatibility, built-in optimized NumPy support for numerical calculations and software transactional memory support to allow better parallelism.[22]


Source: https://en.wikipedia.org/wiki/C_(Programmiersprache)
Content: Other reasons this message may be displayed:


Source: https://en.wikipedia.org/wiki/Cython
Content: 
 Cython (/ˈsaɪθɒn/) is a superset of the programming language Python, which allows developers to write Python code (with optional, C-inspired syntax extensions) that yields performance comparable to that of C.[5][6]
 Cython is a compiled language that is typically used to generate CPython extension modules. Annotated Python-like code is compiled to C (also usable from e.g. C++) and then automatically wrapped in interface code, producing extension modules that can be loaded and used by regular Python code using the import statement, but with significantly less computational overhead at run time. Cython also facilitates wrapping independent C or C++ code into python-importable modules.
 Cython is written in Python and C and works on Windows, macOS, and Linux, producing C source files compatible with CPython 2.6, 2.7, and 3.3 and later versions. The Cython source code that Cython compiles (to C) can use both Python 2 and Python 3 syntax, defaulting to Python 2 syntax in Cython 0.x (and Python 3 syntax in Cython 3.x). The default can be overridden (e.g. in source code comment) to Python 3 (or 2) syntax. Since Python 3 syntax has changed in recent versions, Cython may not be up to date with latest addition. Cython has "native support for most of the C++ language" and "compiles almost all existing Python code".[7]
 Cython 3.0.0 was released on 17 July 2023.[8]
 Cython works by producing a standard Python module. However, the behavior differs from standard Python in that the module code, originally written in Python, is translated into C. While the resulting code is fast, it makes many calls into the CPython interpreter and CPython standard libraries to perform actual work. Choosing this arrangement saved considerably on Cython's development time, but modules have a dependency on the Python interpreter and standard library.
 Although most of the code is C-based, a small stub loader written in interpreted Python is usually required (unless the goal is to create a loader written entirely in C, which may involve work with the undocumented internals of CPython). However, this is not a major problem due to the presence of the Python interpreter.[9]
 Cython has a foreign function interface for invoking C/C++ routines and the ability to declare the static type of subroutine parameters and results, local variables, and class attributes.
 A Cython program that implements the same algorithm as a corresponding Python program may consume fewer computing resources such as core memory and processing cycles due to differences between the CPython and Cython execution models. A basic Python program is loaded and executed by the CPython virtual machine, so both the runtime and the program itself consume computing resources. A Cython program is compiled to C code, which is further compiled to machine code, so the virtual machine is used only briefly when the program is loaded.[10][11][12][13]
 Cython employs:
 Performance depends both on what C code is generated by Cython and how that code is compiled by the C compiler.[16]
 Cython is a derivative of the Pyrex language, and supports more features and optimizations than Pyrex.[17][18] Cython was forked from Pyrex in 2007 by developers of the Sage computer algebra package, because they were unhappy with Pyrex's limitations and could not get patches accepted by Pyrex's maintainer Greg Ewing, who envisioned a much smaller scope for his tool than the Sage developers had in mind. They then forked Pyrex as SageX. When they found people were downloading Sage just to get SageX, and developers of other packages (including Stefan Behnel, who maintains the XML library LXML) were also maintaining forks of Pyrex, SageX was split off the Sage project and merged with cython-lxml to become Cython.[19]
 Cython files have a .pyx extension. At its most basic, Cython code looks exactly like Python code. However, whereas standard Python is dynamically typed, in Cython, types can optionally be provided, allowing for improved performance, allowing loops to be converted into C loops where possible. For example:
 A sample hello world program for Cython is more complex than in most languages because it interfaces with the Python C API and setuptools or other PEP517-compliant extension building facilities. At least three files are required for a basic project:
 The following code listings demonstrate the build and launch process:
 These commands build and launch the program:
 A more straightforward way to start with Cython is through command-line IPython (or through in-browser python console called Jupyter notebook):
 which gives a 95 times improvement over the pure-python version. More details on the subject in the official quickstart page.[20]
 Cython is particularly popular among scientific users of Python,[12][21][22] where it has "the perfect audience" according to Python creator Guido van Rossum.[23] Of particular note:
 Cython's domain is not limited to just numerical computing. For example, the lxml XML toolkit is written mostly in Cython, and like its predecessor Pyrex, Cython is used to provide Python bindings for many C and C++ libraries such as the messaging library ZeroMQ.[28] Cython can also be used to develop parallel programs for multi-core processor machines; this feature makes use of the OpenMP library.


Source: https://en.wikipedia.org/wiki/Spezial:ISBN-Suche/9783955618063
Content: Other reasons this message may be displayed:


Source: https://en.wikipedia.org/wiki/Spezial:ISBN-Suche/9783446441330
Content: Other reasons this message may be displayed:


Source: https://en.wikipedia.org/wiki/Spezial:ISBN-Suche/9783836228619
Content: Other reasons this message may be displayed:


Source: https://en.wikipedia.org/wiki/Spezial:ISBN-Suche/9781449355739
Content: Other reasons this message may be displayed:


Source: https://en.wikipedia.org/wiki/Spezial:ISBN-Suche/9783642549588
Content: Other reasons this message may be displayed:


Source: https://en.wikipedia.org/wiki/Spezial:ISBN-Suche/9783642043765
Content: Other reasons this message may be displayed:


Source: https://en.wikipedia.org/wiki/Spezial:ISBN-Suche/1430224150
Content: Other reasons this message may be displayed:


Source: https://en.wikipedia.org/wiki/Spezial:ISBN-Suche/9783527711482
Content: Other reasons this message may be displayed:


Source: https://en.wikipedia.org/wiki/Spezial:ISBN-Suche/9783831027002
Content: Other reasons this message may be displayed:


Source: https://en.wikipedia.org/wiki/Spezial:ISBN-Suche/9783446438064
Content: Other reasons this message may be displayed:


Source: https://en.wikipedia.org/wiki/Spezial:ISBN-Suche/9783966450072
Content: Other reasons this message may be displayed:


Source: https://en.wikipedia.org/wiki/Spezial:ISBN-Suche/9783826687266
Content: Other reasons this message may be displayed:


Source: https://en.wikipedia.org/wiki/Spezial:ISBN-Suche/9783836258647
Content: Other reasons this message may be displayed:


Source: https://en.wikipedia.org/wiki/Spezial:ISBN-Suche/9783955617707
Content: Other reasons this message may be displayed:


Source: https://en.wikipedia.org/wiki/Spezial:ISBN-Suche/9781491946008
Content: Other reasons this message may be displayed:


Source: https://en.wikipedia.org/wiki/Spezial:ISBN-Suche/382668673X
Content: Other reasons this message may be displayed:


Source: https://en.wikipedia.org/wiki/Spezial:ISBN-Suche/9783827325433
Content: Other reasons this message may be displayed:


Source: https://en.wikipedia.org/wiki/Spezial:ISBN-Suche/3540435085
Content: Other reasons this message may be displayed:


Source: https://en.wikipedia.org/wiki/Spezial:ISBN-Suche/3826609662
Content: Other reasons this message may be displayed:


Source: https://en.wikipedia.org/wiki/Books_on_Demand
Content: 
Self-publishing is the publication of media by its author at their own cost, without the involvement of a publisher. The term usually refers to written media, such as books and magazines, either as an ebook or as a physical copy using print on demand technology. It may also apply to albums, pamphlets, brochures, games, video content, artwork, and zines. Web fiction is also a major medium for self-publishing.
 Although self-publishing is not a new phenomenon, dating back to the 18th century, it has transformed during the internet age with new technologies and services providing increasing alternatives to traditional publishing, becoming a $1 billion market.[1]   However, with the increased ease of publishing and the range of services available, confusion has arisen as to what constitutes self-publishing.  In 2022, the Society of Authors and the Writers Guild of Great Britain produced a free downloadable guide to the various distinct types of publishing currently available.[2]
 In self publishing, authors publish their own book.  It is possible for an author to single-handedly carry out the whole process.  However increasingly, authors are recognizing that to compete effectively, they need to produce a high quality product, and they are engaging professionals for specific services as needed (such as editors or cover designers).[3] A growing number of companies offer a one-stop shop where an author can source a whole range of services required to self-publish a book (sometimes called "Assisted Self-publishing Providers" or "Self-publishing Service Providers").[4]
 Not to be confused with 
 It has been suggested that the best test for whether a company offers "Assisted Self-publishing Services" or "Hybrid/vanity publishing" is to apply a variant of "Yog's Law",[5] which states the following:
 Therefore if a company offers services to the author without claiming any rights, and allows the author to control the entire process, they are assisting the author to self-publish. Whereas if the company takes some rights, and/or takes control of artistic decisions, they are a hybrid publisher or a vanity publisher, depending on the degree of involvement.
 Historically, some authors have chosen to self-publish. Successful examples are John Locke,[6] Jane Austen, Emily Dickinson, Nathaniel Hawthorne, Martin Luther, Marcel Proust, Derek Walcott, and Walt Whitman. In 1759, British satirist Laurence Sterne's self-published the first two volumes of Tristram Shandy. In 1908, Ezra Pound sold A Lume Spento for six pence each. Franklin Hiram King's book Farmers of Forty Centuries was self-published in 1911, and was subsequently published commercially. In 1931, Irma S. Rombauer, the author of The Joy of Cooking paid a local printing company to print 3000 copies; the Bobbs-Merrill Company acquired the rights, and since then the book has sold over 18 million copies. In 1941, writer Virginia Woolf chose to self-publish her final novel Between the Acts on her Hogarth Press, in effect starting her own press.[7] Self-publication was also known in music: Joseph Haydn self-published his oratorio The Creation in 1800.[8]
 Five years ago, self-publishing was a scar. Now it's a tattoo. Traditional publishers are extremely selective in what they publish, and reject most of the manuscripts submitted to them.[10] In spite of that rigorous selection, they then assign an editor to polish the work even further, a proof-reader to check for errors and a designer to produce the cover.[11]  It can be challenging for a self-publishing author to produce a book to traditional professional standards.
 Before the advent of the internet and POD (Print on Demand), most self-publishing authors had to resort to a vanity press, which was very costly and acted as a barrier to publication.  Now, ebooks can be published at virtually no cost and the market has been flooded with poorly produced books.  Some estimate that as much as 70% of published ebooks are so bad, they are unreadable.[12]
 However, some self-published authors are now taking a professional approach, using services like critique groups, beta readers, professional editors and designers to polish their work to a professional standard equivalent to traditional publishing. Such authors are achieving success equivalent to traditionally published writers, lending respectability to self-publishing.[13]
 Self-publishing is also common among editors of academic journals. The study showed that a quarter of them publish 10% of their own articles in the same journals they edit (which is problematic for ethical reasons).[14]
 A huge impetus to self-publishing has been rapid advances in technology.  Print-On-Demand (or POD) technology, which became available in the mid-1990s,[15] makes it possible for a book to be printed after an order has been placed, so there are no costs for storing inventory.   Further, the Internet provides access to global distribution channels via online retailers, so a self-published book can be instantly available to book buyers worldwide. Advances in e-book readers and tablet computers have improved readability, making ebooks more popular.[16]
 Amazon's introduction of the Kindle and its self-publishing platform, Kindle Direct Publishing or KDP, in 2007 has been described as a tipping point in self-publishing, which "opened the floodgates" for self-publishing authors.[1]
 The Espresso Book Machine (a POD device) was first demonstrated at the New York Public Library in 2007. This machine prints, collates, covers, and binds a single book. It is in libraries and bookstores throughout the world, and it can make copies of out-of-print editions. Small bookstores sometimes use it to compete with large bookstore chains. It works by taking two pdf files, one for the text and one for the cover, and then prints an entire paperback book in a matter of minutes, which then drops down a chute.[17]
 The Library Journal and Biblioboard worked together to create a self-publishing platform called Self-e in which authors submitted books online which were made available to readers. These books are reviewed by Library Journal, and the best ones are published nationwide; authors do not make money this way but it serves as a marketing tool.[18]
 In order to be purchased by a customer, the completed book must be hosted on a publishing platform.  Amazon's Kindle is the largest of these but there are others. 
 Kindle Direct Publishing or KDP is Amazon's e-book publishing unit (see main article)
 IngramSpark lets authors publish digital, hardback and paperback editions of their books. It distributes books to most online bookstores. Bricks-and-mortar stores can also order books from IngramSpark at wholesale prices for sale in their own venues. It is run by Ingram Content Group.
 Apple sells books via its App Store which is a digital distribution platform for its mobile apps on its iOS operating system. Apps can be downloaded to its devices such as the iPhone, the iPod Touch handheld computer, and the iPad. Apple pays authors 70 percent of its proceeds at its Apple iBookstore where it sells iBooks.[16]
 Smashwords is a California-based company founded by Mark Coker which allows authors and independent publishers to upload their manuscripts electronically to the Smashwords service, which then converts them into multiple e-book formats which can be read on various devices.
 Barnes & Noble pays 65 percent of the list price of e-books purchased through its online store called Pubit.
 Kobo is a Canadian company which sells e-books, audiobooks, e-readers and tablet computers which originated as a cloud e-reading service.
 Scribd is an open publishing platform which features a digital library, an e-book and audiobook subscription service.
 Lulu is an online print-on-demand, self-publishing and distribution platform.
 Books on Demand [de; fr; fi] GmbH[25] BoD (2001), (since 1997 as Libri[26] GmbH),[27] is the "original" in self-publishing.[28][29][30][31]
 A major development in this century has been the growth of web fiction. A common type is the web serial. Unlike most modern novels, web fiction novels are frequently published in parts over time. Web fiction is especially popular in China, with revenues topping US$2.5 billion,[32] as well as in South Korea. Online literature in China plays a much more important role than in the United States and the rest of the world.[33] Most books are available online, where the most popular novels find millions of readers. They cost an average of 2 CNY, or roughly a tenth of the average price of a printed book.[34][35] Shanda Literature Ltd. is an online publishing company that claims to publish 8,000 Chinese literary works daily. Joara is S. Korea's largest web novel platform with 1.1 million members, 140,000 writers, an average of 2,400 serials per day and 420,000 works.[36] Joara's users have almost the same gender ratio, and both fantasy and romance genres are popular.
 While most self-published books do not make much money,[37] there are self-published authors who have achieved success, particularly in the early years of online self-publishing.[38] The number of authors who had sold more than one million e-books on Amazon from 2011 to 2016 was 40, according to one estimate.[39]


Source: https://en.wikipedia.org/wiki/Spezial:ISBN-Suche/9783751900584
Content: Other reasons this message may be displayed:


Source: https://en.wikipedia.org/wiki/Guido_van_Rossum
Content: 
 Guido van Rossum (Dutch: [ˈɣido vɑn ˈrɔsʏm, -səm]; born 31 January 1956) is a Dutch programmer best known as the creator of the Python programming language, for which he was the "benevolent dictator for life" (BDFL) until he stepped down from the position on 12 July 2018.[4][5] He remained a member of the Python Steering Council through 2019, and withdrew from nominations for the 2020 election.[6]
 Van Rossum was born and raised in the Netherlands, where he received a master's degree in mathematics and computer science from the University of Amsterdam in 1982. He received a bronze medal in 1974 in the International Mathematical Olympiad.[7] He has a brother, Just van Rossum, who is a type designer and programmer who designed the typeface used in the "Python Powered" logo.[8]
 Van Rossum lives in Belmont, California, with his wife, Kim Knapp,[9] and their son.[10][11][12] According to his home page and Dutch naming conventions, the "van" in his name is capitalized when he is referred to by surname alone, but not when using his first and last name together.[13]
 While working at the Centrum Wiskunde & Informatica (CWI), Van Rossum wrote and contributed a glob() routine to BSD Unix in 1986[14][15] and helped develop the ABC programming language. He once stated, "I try to mention ABC's influence because I'm indebted to everything I learned during that project and to the people who worked on it."[16] He also created Grail, an early web browser written in Python, and engaged in discussions about the HTML standard.[17]
 He has worked for various research institutes, including the Centrum Wiskunde & Informatica (CWI) in the Netherlands, the U.S. National Institute of Standards and Technology (NIST), and the Corporation for National Research Initiatives (CNRI). In May 2000, he left CNRI along with three other Python core developers to work for tech startup BeOpen.com, which subsequently collapsed by October of the same year.[18][19] From late 2000 until 2003 he worked for Zope Corporation. In 2003 Van Rossum left Zope for Elemental Security. While there he worked on a custom programming language for the organization.[20]
 From 2005 to December 2012, he worked at Google, where he spent half of his time developing the Python language. 
At Google, Van Rossum developed Mondrian, a web-based code review system written in Python and used within the company. He named the software after the Dutch painter Piet Mondrian.[21] He named another related software project after Gerrit Rietveld, a Dutch designer.[22] On 7 December 2012, Van Rossum left Google.[23]
 In January 2013, Van Rossum started working at the cloud file storage company Dropbox.[24][25]
 In October 2019, Van Rossum left Dropbox and officially retired.[26][27][28]
 On 12 November 2020 Van Rossum announced that he was coming out of retirement to join the Developer Division at Microsoft. He currently holds the title Distinguished Engineer at Microsoft.[29][30][31]
 In December 1989, Van Rossum had been looking for a "'hobby' programming project that would keep [him] occupied during the week around Christmas" as his office was closed when he decided to write an interpreter for a "new scripting language [he] had been thinking about lately: a descendant of ABC that would appeal to Unix/C hackers". He attributes choosing the name "Python" to "being in a slightly irreverent mood (and a big fan of Monty Python's Flying Circus)".[32]
 He has explained that Python's predecessor, ABC, was inspired by SETL, noting that ABC co-developer Lambert Meertens had "spent a year with the SETL group at NYU before coming up with the final ABC design".[33]
 On 12 July 2018, Van Rossum announced that he would be stepping down from the position of BDFL of the Python programming language.[34]
 In 1999, Van Rossum submitted a funding proposal to DARPA called "Computer Programming for Everybody", in which he further defined his goals for Python:
 In 2019, Python became the second most popular language on GitHub, the largest source code management website on the internet, second only to JavaScript.[35] According to a programming language popularity survey[36] it is consistently among the top 10 most mentioned languages in job postings. Furthermore, Python has been among the 10 most popular programming languages every year since 2004 according to the TIOBE Programming Community Index and got the number one spot on the index in October 2021.[37]


Source: https://en.wikipedia.org/wiki/Spezial:ISBN-Suche/9781430227571
Content: Other reasons this message may be displayed:


Source: https://en.wikipedia.org/wiki/Guido_van_Rossum
Content: 
 Guido van Rossum (Dutch: [ˈɣido vɑn ˈrɔsʏm, -səm]; born 31 January 1956) is a Dutch programmer best known as the creator of the Python programming language, for which he was the "benevolent dictator for life" (BDFL) until he stepped down from the position on 12 July 2018.[4][5] He remained a member of the Python Steering Council through 2019, and withdrew from nominations for the 2020 election.[6]
 Van Rossum was born and raised in the Netherlands, where he received a master's degree in mathematics and computer science from the University of Amsterdam in 1982. He received a bronze medal in 1974 in the International Mathematical Olympiad.[7] He has a brother, Just van Rossum, who is a type designer and programmer who designed the typeface used in the "Python Powered" logo.[8]
 Van Rossum lives in Belmont, California, with his wife, Kim Knapp,[9] and their son.[10][11][12] According to his home page and Dutch naming conventions, the "van" in his name is capitalized when he is referred to by surname alone, but not when using his first and last name together.[13]
 While working at the Centrum Wiskunde & Informatica (CWI), Van Rossum wrote and contributed a glob() routine to BSD Unix in 1986[14][15] and helped develop the ABC programming language. He once stated, "I try to mention ABC's influence because I'm indebted to everything I learned during that project and to the people who worked on it."[16] He also created Grail, an early web browser written in Python, and engaged in discussions about the HTML standard.[17]
 He has worked for various research institutes, including the Centrum Wiskunde & Informatica (CWI) in the Netherlands, the U.S. National Institute of Standards and Technology (NIST), and the Corporation for National Research Initiatives (CNRI). In May 2000, he left CNRI along with three other Python core developers to work for tech startup BeOpen.com, which subsequently collapsed by October of the same year.[18][19] From late 2000 until 2003 he worked for Zope Corporation. In 2003 Van Rossum left Zope for Elemental Security. While there he worked on a custom programming language for the organization.[20]
 From 2005 to December 2012, he worked at Google, where he spent half of his time developing the Python language. 
At Google, Van Rossum developed Mondrian, a web-based code review system written in Python and used within the company. He named the software after the Dutch painter Piet Mondrian.[21] He named another related software project after Gerrit Rietveld, a Dutch designer.[22] On 7 December 2012, Van Rossum left Google.[23]
 In January 2013, Van Rossum started working at the cloud file storage company Dropbox.[24][25]
 In October 2019, Van Rossum left Dropbox and officially retired.[26][27][28]
 On 12 November 2020 Van Rossum announced that he was coming out of retirement to join the Developer Division at Microsoft. He currently holds the title Distinguished Engineer at Microsoft.[29][30][31]
 In December 1989, Van Rossum had been looking for a "'hobby' programming project that would keep [him] occupied during the week around Christmas" as his office was closed when he decided to write an interpreter for a "new scripting language [he] had been thinking about lately: a descendant of ABC that would appeal to Unix/C hackers". He attributes choosing the name "Python" to "being in a slightly irreverent mood (and a big fan of Monty Python's Flying Circus)".[32]
 He has explained that Python's predecessor, ABC, was inspired by SETL, noting that ABC co-developer Lambert Meertens had "spent a year with the SETL group at NYU before coming up with the final ABC design".[33]
 On 12 July 2018, Van Rossum announced that he would be stepping down from the position of BDFL of the Python programming language.[34]
 In 1999, Van Rossum submitted a funding proposal to DARPA called "Computer Programming for Everybody", in which he further defined his goals for Python:
 In 2019, Python became the second most popular language on GitHub, the largest source code management website on the internet, second only to JavaScript.[35] According to a programming language popularity survey[36] it is consistently among the top 10 most mentioned languages in job postings. Furthermore, Python has been among the 10 most popular programming languages every year since 2004 according to the TIOBE Programming Community Index and got the number one spot on the index in October 2021.[37]


Source: https://en.wikipedia.org/wiki/Guido_van_Rossum
Content: 
 Guido van Rossum (Dutch: [ˈɣido vɑn ˈrɔsʏm, -səm]; born 31 January 1956) is a Dutch programmer best known as the creator of the Python programming language, for which he was the "benevolent dictator for life" (BDFL) until he stepped down from the position on 12 July 2018.[4][5] He remained a member of the Python Steering Council through 2019, and withdrew from nominations for the 2020 election.[6]
 Van Rossum was born and raised in the Netherlands, where he received a master's degree in mathematics and computer science from the University of Amsterdam in 1982. He received a bronze medal in 1974 in the International Mathematical Olympiad.[7] He has a brother, Just van Rossum, who is a type designer and programmer who designed the typeface used in the "Python Powered" logo.[8]
 Van Rossum lives in Belmont, California, with his wife, Kim Knapp,[9] and their son.[10][11][12] According to his home page and Dutch naming conventions, the "van" in his name is capitalized when he is referred to by surname alone, but not when using his first and last name together.[13]
 While working at the Centrum Wiskunde & Informatica (CWI), Van Rossum wrote and contributed a glob() routine to BSD Unix in 1986[14][15] and helped develop the ABC programming language. He once stated, "I try to mention ABC's influence because I'm indebted to everything I learned during that project and to the people who worked on it."[16] He also created Grail, an early web browser written in Python, and engaged in discussions about the HTML standard.[17]
 He has worked for various research institutes, including the Centrum Wiskunde & Informatica (CWI) in the Netherlands, the U.S. National Institute of Standards and Technology (NIST), and the Corporation for National Research Initiatives (CNRI). In May 2000, he left CNRI along with three other Python core developers to work for tech startup BeOpen.com, which subsequently collapsed by October of the same year.[18][19] From late 2000 until 2003 he worked for Zope Corporation. In 2003 Van Rossum left Zope for Elemental Security. While there he worked on a custom programming language for the organization.[20]
 From 2005 to December 2012, he worked at Google, where he spent half of his time developing the Python language. 
At Google, Van Rossum developed Mondrian, a web-based code review system written in Python and used within the company. He named the software after the Dutch painter Piet Mondrian.[21] He named another related software project after Gerrit Rietveld, a Dutch designer.[22] On 7 December 2012, Van Rossum left Google.[23]
 In January 2013, Van Rossum started working at the cloud file storage company Dropbox.[24][25]
 In October 2019, Van Rossum left Dropbox and officially retired.[26][27][28]
 On 12 November 2020 Van Rossum announced that he was coming out of retirement to join the Developer Division at Microsoft. He currently holds the title Distinguished Engineer at Microsoft.[29][30][31]
 In December 1989, Van Rossum had been looking for a "'hobby' programming project that would keep [him] occupied during the week around Christmas" as his office was closed when he decided to write an interpreter for a "new scripting language [he] had been thinking about lately: a descendant of ABC that would appeal to Unix/C hackers". He attributes choosing the name "Python" to "being in a slightly irreverent mood (and a big fan of Monty Python's Flying Circus)".[32]
 He has explained that Python's predecessor, ABC, was inspired by SETL, noting that ABC co-developer Lambert Meertens had "spent a year with the SETL group at NYU before coming up with the final ABC design".[33]
 On 12 July 2018, Van Rossum announced that he would be stepping down from the position of BDFL of the Python programming language.[34]
 In 1999, Van Rossum submitted a funding proposal to DARPA called "Computer Programming for Everybody", in which he further defined his goals for Python:
 In 2019, Python became the second most popular language on GitHub, the largest source code management website on the internet, second only to JavaScript.[35] According to a programming language popularity survey[36] it is consistently among the top 10 most mentioned languages in job postings. Furthermore, Python has been among the 10 most popular programming languages every year since 2004 according to the TIOBE Programming Community Index and got the number one spot on the index in October 2021.[37]


Source: https://en.wikipedia.org/wiki/Wiki
Content: 
 A wiki (/ˈwɪki/ ⓘ WI-kee) is a form of online hypertext publication that is collaboratively edited and managed by its own audience directly through a web browser. A typical wiki contains multiple pages for the subjects or scope of the project, and could be either open to the public or limited to use within an organization for maintaining its internal knowledge base.
 Wikis are enabled by wiki software, otherwise known as wiki engines. A wiki engine, being a form of a content management system, differs from other web-based systems such as blog software or static site generators, in that the content is created without any defined owner or leader, and wikis have little inherent structure, allowing structure to emerge according to the needs of the users.[1] Wiki engines usually allow content to be written using a simplified markup language and sometimes edited with the help of a rich-text editor.[2] There are dozens of different wiki engines in use, both standalone and part of other software, such as bug tracking systems. Some wiki engines are free and open-source, whereas others are proprietary. Some permit control over different functions (levels of access); for example, editing rights may permit changing, adding, or removing material. Others may permit access without enforcing access control. Further rules may be imposed to organize content.
 There are hundreds of thousands of wikis in use, both public and private, including wikis functioning as knowledge management resources, note-taking tools, community websites, and intranets. Ward Cunningham, the developer of the first wiki software, WikiWikiWeb, originally described wiki as "the simplest online database that could possibly work".[3] "Wiki" (pronounced [wiki][note 1]) is a Hawaiian word meaning "quick".[4][5][6]
 The online encyclopedia project Wikipedia is the most popular wiki-based website, and is one of the most widely viewed sites in the world, having been ranked in the top twenty since 2007.[7] Wikipedia is not a single wiki but rather a collection of hundreds of wikis, with each one pertaining to a specific language. The English-language Wikipedia has the largest collection of articles: as of January 2024,[update] it has over six million articles.[8]
 In their 2001 book The Wiki Way: Quick Collaboration on the Web, Ward Cunningham and co-author Bo Leuf described the essence of the Wiki concept:[9][10][page needed]
 A wiki enables communities of editors and contributors to write documents collaboratively. All that people require to contribute is a computer, Internet access, a web browser, and a basic understanding of a simple markup language (e.g. MediaWiki markup language). A single page in a wiki website is referred to as a "wiki page", while the entire collection of pages, which are usually well-interconnected by hyperlinks, is "the wiki". A wiki is essentially a database for creating, browsing, and searching through information. A wiki allows non-linear, evolving, complex, and networked text, while also allowing for editor argument, debate, and interaction regarding the content and formatting.[11] A defining characteristic of wiki technology is the ease with which pages can be created and updated. Generally, there is no review by a moderator or gatekeeper before modifications are accepted and thus lead to changes on the website. Many wikis are open to alteration by the general public without requiring registration of user accounts. Many edits can be made in real-time and appear almost instantly online, but this feature facilitates abuse of the system. Private wiki servers require user authentication to edit pages, and sometimes even to read them. Maged N. Kamel Boulos, Cito Maramba, and Steve Wheeler write that the open wikis produce a process of Social Darwinism. "... because of the openness and rapidity that wiki pages can be edited, the pages undergo an evolutionary selection process, not unlike that which nature subjects to living organisms. 'Unfit' sentences and sections are ruthlessly culled, edited and replaced if they are not considered 'fit', which hopefully results in the evolution of a higher quality and more relevant page."[12]
 Some wikis have an edit button or link directly on the page being viewed if the user has permission to edit the page. This can lead to a text-based editing page where participants can structure and format wiki pages with a simplified markup language, sometimes known as wikitext, wiki markup or wikicode (it can also lead to a WYSIWYG editing page; see the paragraph after the table below). For example, starting lines of text with asterisks could create a bulleted list. The style and syntax of wikitexts can vary greatly among wiki implementations,[example  needed] some of which also allow HTML tags.
 Wikis have traditionally employed plain-text editing, utilizing simpler conventions than HTML to denote style and structure. Restricting access to HTML and Cascading Style Sheets (CSS) within wikis hinders users from modifying content layout and formatting. However, this restriction offers advantages. It fosters uniformity in appearance by curbing CSS modifications and ensures that users cannot introduce JavaScript code that might impede access for others.
 "I've had nothing yet," Alice replied in an offended tone, "so I can't take more."
 "You mean you can't take less," said the Hatter. "It's very easy to take more than nothing."
   
 Wikis can also make WYSIWYG editing available to users, usually through a JavaScript control that translates graphically entered formatting instructions into the corresponding HTML tags or wikitext. In those implementations, the markup of a newly edited, marked-up version of the page is generated and submitted to the server transparently, shielding the user from this technical detail. An example of this is the VisualEditor on Wikipedia. WYSIWYG controls do not, however, always provide all the features available in wikitext, and some users prefer not to use a WYSIWYG editor. Hence, many of these sites offer some means to edit the wikitext directly.
 Some wikis keep a record of changes made to wiki pages; often, every version of the page is stored. This means that authors can revert to an older version of the page should it be necessary because a mistake has been made, such as the content accidentally being deleted or the page has been vandalized to include offensive or malicious text or other inappropriate content.
 Many wiki implementations, such as MediaWiki, the software that powers Wikipedia, allow users to supply an edit summary when they edit a page. This is a short piece of text summarizing the changes they have made (e.g. "Corrected grammar" or "Fixed formatting in table"). It is not inserted into the article's main text but is stored along with that revision of the page, allowing users to explain what has been done and why. This is similar to a log message when making changes in a revision-control system. This enables other users to see which changes have been made by whom and why, often in a list of summaries, dates and other short, relevant content, a list which is called a "log" or "history".
 Within the text of most pages, there are usually many hypertext links to other pages within the wiki. This form of non-linear navigation is more "native" to a wiki than structured/formalized navigation schemes. Users can also create any number of index or table-of-contents pages, with hierarchical categorization or whatever form of organization they like. These may be challenging to maintain "by hand", as multiple authors and users may create and delete pages in an ad hoc, unorganized manner. Wikis can provide one or more ways to categorize or tag pages to support the maintenance of such index pages. Some wikis, including the original, have a backlink feature, which displays all pages that link to a given page. It is also typically possible in a wiki to create links to pages that do not yet exist, as a way to invite others to share what they know about a subject new to the wiki. Wiki users can typically "tag" pages with categories or keywords, to make it easier for other users to find the article. For example, a user creating a new article on cold-weather biking might "tag" this page under the categories of commuting, winter sports and bicycling. This would make it easier for other users to find the article.
 Links are created using a specific syntax, the so-called "link pattern". Originally, most wikis[citation needed] used CamelCase to name pages and create links. These are produced by capitalizing words in a phrase and removing the spaces between them (the word "CamelCase" is itself an example). While CamelCase makes linking easy, it also leads to links in a form that deviates from the standard spelling. To link to a page with a single-word title, one must abnormally capitalize one of the letters in the word (e.g. "WiKi" instead of "Wiki"). CamelCase-based wikis are instantly recognizable because they have many links with names such as "TableOfContents" and "BeginnerQuestions". A wiki can render the visible anchor of such links "pretty" by reinserting spaces, and possibly also reverting to lower case. This reprocessing of the link to improve the readability of the anchor is, however, limited by the loss of capitalization information caused by CamelCase reversal. For example, "RichardWagner" should be rendered as "Richard Wagner", whereas "PopularMusic" should be rendered as "popular music". There is no easy way to determine which capital letters should remain capitalized. As a result, many wikis now have "free linking" using brackets, and some disable CamelCase by default.
 Most wikis offer at least a title search, and sometimes a full-text search. The scalability of the search depends on whether the wiki engine uses a database. Some wikis, such as PmWiki, use flat files.[13] MediaWiki's first versions used flat files, but it was rewritten by Lee Daniel Crocker in the early 2000s (decade) to be a database application.[citation needed] Indexed database access is necessary for high speed searches on large wikis. Alternatively, external search engines such as Google Search can sometimes be used on wikis with limited searching functions to obtain more precise results.
 WikiWikiWeb was the first wiki.[14] Ward Cunningham started developing WikiWikiWeb in Portland, Oregon, in 1994, and installed it on the Internet domain c2.com on March 25, 1995. It was named by Cunningham, who remembered a Honolulu International Airport counter employee telling him to take the "Wiki Wiki Shuttle" bus that runs between the airport's terminals. According to Cunningham, "I chose wiki-wiki as an alliterative substitute for 'quick' and thereby avoided naming this stuff quick-web."[15][16]
 Cunningham was, in part, inspired by the Apple HyperCard, which he had used. HyperCard, however, was single-user.[17] Apple had designed a system allowing users to create virtual "card stacks" supporting links among the various cards. Cunningham developed Vannevar Bush's ideas by allowing users to "comment on and change one another's text."[2][18] Cunningham says his goals were to link together people's experiences to create a new literature to document programming patterns, and to harness people's natural desire to talk and tell stories with a technology that would feel comfortable to those not used to "authoring".[17]
 Wikipedia became the most famous wiki site, launched in January 2001 and entering the top ten most popular websites in 2007. In the early 2000s (decade), wikis were increasingly adopted in enterprise as collaborative software. Common uses included project communication, intranets, and documentation, initially for technical users. Some companies use wikis as their only collaborative software and as a replacement for static intranets, and some schools and universities use wikis to enhance group learning. There may be greater use of wikis behind firewalls than on the public Internet. On March 15, 2007, the word wiki was listed in the online Oxford English Dictionary.[19]
 In the late 1990s and early 2000s, the word "wiki" was used to refer to both user-editable websites and the software that powers them; the latter definition is still occasionally in use.[1] Wiki inventor Ward Cunningham wrote in 2014[20] that the word "wiki" should not be used to refer to a single website, but rather to a mass of user-editable pages or sites so that a single website is not "a wiki" but "an instance of wiki". He wrote that the concept of wiki federation, in which the same content can be hosted and edited in more than one location in a manner similar to distributed version control, meant that the concept of a single discrete "wiki" no longer made sense.[21]
 Wiki software is a type of collaborative software that runs a wiki system, allowing web pages to be created and edited using a common web browser. It may be implemented as a series of scripts behind an existing web server or as a standalone application server that runs on one or more web servers. The content is stored in a file system, and changes to the content are stored in a relational database management system. A commonly implemented software package is MediaWiki, which runs Wikipedia. Alternatively, personal wikis run as a standalone application on a single computer.
 Wikis can also be created on a "wiki farm", where the server-side software is implemented by the wiki farm owner. Some wiki farms can also make private, password-protected wikis. Free wiki farms generally contain advertising on every page. For more information, see Comparison of wiki hosting services.
 Wikis are generally designed with the philosophy of making it easy to correct mistakes, rather than making it difficult to make them. Thus, while wikis are very open, they provide a means to verify the validity of recent additions to the body of pages. The most prominent, on almost every wiki, is the "Recent Changes" page—a specific list showing recent edits, or a list of edits made within a given time frame.[22] Some wikis can filter the list to remove minor edits and edits made by automatic importing scripts ("bots").[23] From the change log, other functions are accessible in most wikis: the revision history shows previous page versions and the diff feature highlights the changes between two revisions. Using the revision history, an editor can view and restore a previous version of the article. This gives great power to the author to eliminate edits. The diff feature can be used to decide whether or not this is necessary. A regular wiki user can view the diff of an edit listed on the "Recent Changes" page and, if it is an unacceptable edit, consult the history, restoring a previous revision; this process is more or less streamlined, depending on the wiki software used.[24]
 In case unacceptable edits are missed on the "recent changes" page, some wiki engines provide additional content control. It can be monitored to ensure that a page, or a set of pages, keeps its quality. A person willing to maintain pages will be warned of modifications to the pages, allowing them to verify the validity of new editions quickly. This can be seen as a very pro-author and anti-editor feature.[25] A watchlist is a common implementation of this. Some wikis also implement "patrolled revisions", in which editors with the requisite credentials can mark some edits as not vandalism. A "flagged revisions" system can prevent edits from going live until they have been reviewed.[26]
 Critics of publicly editable wiki systems argue that these systems could be easily tampered with by malicious individuals ("vandals") or even by well-meaning but unskilled users who introduce errors into the content, while proponents maintain that the community of users can catch such malicious or erroneous content and correct it.[2] Lars Aronsson, a data systems specialist, summarizes the controversy as follows: "Most people when they first learn about the wiki concept, assume that a Web site that can be edited by anybody would soon be rendered useless by destructive input. It sounds like offering free spray cans next to a grey concrete wall. The only likely outcome would be ugly graffiti and simple tagging and many artistic efforts would not be long lived. Still, it seems to work very well."[14] High editorial standards in medicine and health sciences articles, in which users typically use peer-reviewed journals or university textbooks as sources, have led to the idea of expert-moderated wikis.[27] Some wikis allow one to link to specific versions of articles, which has been useful to the scientific community, in that expert peer reviewers could analyse articles, improve them and provide links to the trusted version of that article.[28] Noveck points out that "participants are accredited by members of the wiki community, who have a vested interest in preserving the quality of the work product, on the basis of their ongoing participation." On controversial topics that have been subject to disruptive editing, a wiki author may restrict editing to registered users.[29]
 The open philosophy of wiki – allowing anyone to edit content – does not ensure that every editor's intentions are well-mannered. For example, vandalism (changing wiki content to something offensive, adding nonsense, maliciously removing content, or deliberately adding incorrect information, such as hoax information) can be a major problem. On larger wiki sites, such as those run by the Wikimedia Foundation, vandalism can go unnoticed for some period of time. Wikis, because of their open nature, are susceptible to intentional disruption, known as "trolling".
Wikis tend to take a soft-security approach to the problem of vandalism, making damage easy to undo rather than attempting to prevent damage. Larger wikis often employ sophisticated methods, such as bots that automatically identify and revert vandalism and JavaScript enhancements that show characters that have been added in each edit. In this way, vandalism can be limited to just "minor vandalism" or "sneaky vandalism", where the characters added/eliminated are so few that bots do not identify them and users do not pay much attention to them.[30][unreliable source] An example of a bot that reverts vandalism on Wikipedia is ClueBot NG. ClueBot NG, which uses machine learning to identify likely vandalism, can revert edits, often within minutes, if not seconds.[31]
 The amount of vandalism a wiki receives depends on how open the wiki is. For instance, some wikis allow unregistered users, identified by their IP addresses, to edit content, while others limit this function to just registered users.[32]
 Edit wars can also occur as users repetitively revert a page to the version they favor. In some cases, editors with opposing views of which content should appear or what formatting style should be used will change and re-change each other's edits. This results in the page being "unstable" from a general user's perspective, because each time a general user comes to the page, it may look different. Some wiki software allows an administrator to stop such edit wars by locking a page from further editing until a decision has been made on what version of the page would be most appropriate.[11] Some wikis are in a better position than others to control behavior due to governance structures existing outside the wiki. For instance, a college teacher can create incentives for students to behave themselves on a class wiki they administer by limiting editing to logged-in users and pointing out that all contributions can be traced back to the contributors. Bad behavior can then be dealt with under university policies.[13]
 Malware can also be a problem for wikis, as users can add links to sites hosting malicious code. For example, a German Wikipedia article about the Blaster Worm was edited to include a hyperlink to a malicious website. Users of vulnerable Microsoft Windows systems who followed the link would be infected.[11] A countermeasure is the use of software that prevents users from saving an edit that contains a link to a site listed on a blacklist of malicious sites.
 The English Wikipedia has the largest user base among wikis on the World Wide Web[33] and ranks in the top 10 among all Web sites in terms of traffic.[34] Other large wikis include the WikiWikiWeb, Memory Alpha, Wikivoyage, and previously Susning.nu, a Swedish-language knowledge base. Medical and health-related wiki examples include Ganfyd, an online collaborative medical reference that is edited by medical professionals and invited non-medical experts.[12] Many wiki communities are private, particularly within enterprises. They are often used as internal documentation for in-house systems and applications. Some companies use wikis to allow customers to help produce software documentation.[35] A study of corporate wiki users found that they could be divided into "synthesizers" and "adders" of content. Synthesizers' frequency of contribution was affected more by their impact on other wiki users, while adders' contribution frequency was affected more by being able to accomplish their immediate work.[36] From a study of thousands of wiki deployments, Jonathan Grudin concluded careful stakeholder analysis and education are crucial to successful wiki deployment.[37]
 In 2005, the Gartner Group, noting the increasing popularity of wikis, estimated that they would become mainstream collaboration tools in at least 50% of companies by 2009.[38][needs update] Wikis can be used for project management.[39][40][unreliable source] Wikis have also been used in the academic community for sharing and dissemination of information across institutional and international boundaries.[41] In those settings, they have been found useful for collaboration on grant writing, strategic planning, departmental documentation, and committee work.[42] In the mid-2000s, the increasing trend among industries toward collaboration placed a heavier impetus upon educators to make students proficient in collaborative work, inspiring even greater interest in wikis being used in the classroom.[11]
 Wikis have found some use within the legal profession and within the government. Examples include the Central Intelligence Agency's Intellipedia, designed to share and collect intelligence, DKospedia, which was used by the American Civil Liberties Union to assist with review of documents about the internment of detainees in Guantánamo Bay;[43] and the wiki of the United States Court of Appeals for the Seventh Circuit, used to post court rules and allow practitioners to comment and ask questions. The United States Patent and Trademark Office operates Peer-to-Patent, a wiki to allow the public to collaborate on finding prior art relevant to the examination of pending patent applications. Queens, New York has used a wiki to allow citizens to collaborate on the design and planning of a local park. Cornell Law School founded a wiki-based legal dictionary called Wex, whose growth has been hampered by restrictions on who can edit.[29]
 In academic contexts, wikis have also been used as project collaboration and research support systems.[44][45]
 A city wiki (or local wiki) is a wiki used as a knowledge base and social network for a specific geographical locale.[46][47][48] The term 'city wiki' or its foreign language equivalent (e.g. German 'Stadtwiki') is sometimes also used for wikis that cover not just a city, but a small town or an entire region. A city wiki contains information about specific instances of things, ideas, people and places. Much of this information might not be appropriate for encyclopedias such as Wikipedia (e.g. articles on every retail outlet in a town), but might be appropriate for a wiki with more localized content and viewers. A city wiki could also contain information about the following subjects, that may or may not be appropriate for a general knowledge wiki, such as:
 WikiNodes are pages on wikis that describe related wikis. They are usually organized as neighbors and delegates. A neighbor wiki is simply a wiki that may discuss similar content or may otherwise be of interest. A delegate wiki is a wiki that agrees to have certain content delegated to that wiki.[49] One way of finding a wiki on a specific subject is to follow the wiki-node network from wiki to wiki.
 The four basic types of users who participate in wikis are reader, author, wiki administrator and system administrator. The system administrator is responsible for the installation and maintenance of the wiki engine and the container web server. The wiki administrator maintains wiki content and is provided additional functions about pages (e.g. page protection and deletion), and can adjust users' access rights by, for instance, blocking them from editing.[50]
 A study of several hundred wikis showed that a relatively high number of administrators for a given content size is likely to reduce growth;[51] that access controls restricting editing to registered users tends to reduce growth; that a lack of such access controls tends to fuel new user registration; and that higher administration ratios (i.e. admins/user) have no significant effect on content or population growth.[52]
 Active conferences and meetings about wiki-related topics include:
 Former wiki-related events include:
 Joint authorship of articles, in which different users participate in correcting, editing, and compiling the finished product, can also cause editors to become tenants in common of the copyright, making it impossible to republish without permission of all co-owners, some of whose identities may be unknown due to pseudonymous or anonymous editing.[11] Where persons contribute to a collective work such as an encyclopedia, there is, however, no joint ownership if the contributions are separate and distinguishable.[57] Despite most wikis' tracking of individual contributions, the action of contributing to a wiki page is still arguably one of jointly correcting, editing, or compiling, which would give rise to joint ownership. Some copyright issues can be alleviated through the use of an open content license. Version 2 of the GNU Free Documentation License includes a specific provision for wiki relicensing; Creative Commons licenses are also popular. When no license is specified, an implied license to read and add content to a wiki may be deemed to exist on the grounds of business necessity and the inherent nature of a wiki, although the legal basis for such an implied license may not exist in all circumstances.[citation needed]
 Wikis and their users can be held liable for certain activities that occur on the wiki. If a wiki owner displays indifference and forgoes controls (such as banning copyright infringers) that they could have exercised to stop copyright infringement, they may be deemed to have authorized infringement, especially if the wiki is primarily used to infringe copyrights or obtains a direct financial benefit, such as advertising revenue, from infringing activities.[11] In the United States, wikis may benefit from Section 230 of the Communications Decency Act, which protects sites that engage in "Good Samaritan" policing of harmful material, with no requirement on the quality or quantity of such self-policing.[58] It has also been argued, however, that a wiki's enforcement of certain rules, such as anti-bias, verifiability, reliable sourcing, and no-original-research policies, could pose legal risks.[59] When defamation occurs on a wiki, theoretically, all users of the wiki can be held liable, because any of them had the ability to remove or amend the defamatory material from the "publication". It remains to be seen whether wikis will be regarded as more akin to an internet service provider, which is generally not held liable due to its lack of control over publications' contents, than a publisher.[11] It has been recommended that trademark owners monitor what information is presented about their trademarks on wikis, since courts may use such content as evidence pertaining to public perceptions. Joshua Jarvis notes, "Once misinformation is identified, the trademark owner can simply edit the entry".[60]


Source: https://en.wikipedia.org/wiki/Web-Archivierung#Begrifflichkeiten
Content: Other reasons this message may be displayed:


Source: https://en.wikipedia.org/wiki/Internet_Archive
Content: 
 The Internet Archive is an American nonprofit digital library founded on May 10, 1996, and chaired by free information advocate Brewster Kahle.[1][2][4] It provides free access to collections of digitized materials including websites, software applications, music, audiovisual and print materials. The Archive also advocates for a free and open Internet. As of February 4, 2024[update], the Internet Archive holds more than 44 million print materials, 10.6 million videos, 1 million software programs, 15 million audio files, 4.8 million images, 255,000 concerts, and over 835 billion web pages in its Wayback Machine.[5] Its mission is committing to provide "universal access to all knowledge".[5]
 The Internet Archive allows the public to upload and download digital material to its data cluster, but the bulk of its data is collected automatically by its web crawlers, which work to preserve as much of the public web as possible. Its web archive, the Wayback Machine, contains hundreds of billions of web captures.[6][7] The Archive also oversees numerous book digitization projects, collectively one of the world's largest book digitization efforts.
 Brewster Kahle founded the Archive in May 1996 around the same time that he began the for-profit web crawling company Alexa Internet.[8][9] In October of that year, the Internet Archive had begun to archive and preserve the World Wide Web in large amounts,[10] though it saved the earliest known page on May 10, 1996, at 2:42 pm.[11][12][13][14] The archived content first became available to the general public in 2001, when it developed the Wayback Machine.
 In late 1999, the Archive expanded its collections beyond the web archive, beginning with the Prelinger Archives. Now, the Internet Archive includes texts, audio, moving images, and software. It hosts a number of other projects: the NASA Images Archive, the contract crawling service Archive-It, and the wiki-editable library catalog and book information site Open Library. Soon after that, the Archive began working to provide specialized services relating to the information access needs of the print-disabled; publicly accessible books were made available in a protected Digital Accessible Information System (DAISY) format.[15]
 According to its website:[16]
 Most societies place importance on preserving artifacts of their culture and heritage. Without such artifacts, civilization has no memory and no mechanism to learn from its successes and failures. Our culture now produces more and more artifacts in digital form. The Archive's mission is to help preserve those artifacts and create an Internet library for researchers, historians, and scholars. In August 2012, the Archive announced[17] that it has added BitTorrent to its file download options for more than 1.3 million existing files, and all newly uploaded files.[18][19] This method is the fastest means of downloading media from the Archive, as files are served from two Archive data centers, in addition to other torrent clients which have downloaded and continue to serve the files.[18][20] On November 6, 2013, the Internet Archive's headquarters in San Francisco's Richmond District caught fire,[21] destroying equipment and damaging some nearby apartments.[22] According to the Archive, it lost a side-building housing one of 30 of its scanning centers; cameras, lights, and scanning equipment worth hundreds of thousands of dollars; and "maybe 20 boxes of books and film, some irreplaceable, most already digitized, and some replaceable".[23] The nonprofit Archive sought donations to cover the estimated $600,000 in damage.[24]
 An overhaul of the site was launched as beta in November 2014, and the legacy layout was removed in March 2016.[25][26]
 In November 2016, Kahle announced that the Internet Archive was building the Internet Archive of Canada, a copy of the Archive to be based somewhere in Canada. The announcement received widespread coverage due to the implication that the decision to build a backup archive in a foreign country was because of the upcoming presidency of Donald Trump.[27][28][29] Kahle was quoted as saying:
 On November 9th in America, we woke up to a new administration promising radical change. It was a firm reminder that institutions like ours, built for the long-term, need to design for change. For us, it means keeping our cultural materials safe, private and perpetually accessible. It means preparing for a Web that may face greater restrictions. It means serving patrons in a world in which government surveillance is not going away; indeed it looks like it will increase. Throughout history, libraries have fought against terrible violations of privacy—where people have been rounded up simply for what they read. At the Internet Archive, we are fighting to protect our readers' privacy in the digital world.[27] Beginning in 2017, OCLC and the Internet Archive have collaborated to make the Archive's records of digitized books available in WorldCat.[30]
 Since 2018, the Internet Archive visual arts residency, which is organized by Amir Saber Esfahani and Andrew McClintock, helps connect artists with the Archive's over 48 petabytes[31] of digitized materials. Over the course of the yearlong residency, visual artists create a body of work which culminates in an exhibition. The hope is to connect digital history with the arts and create something for future generations to appreciate online or off.[32] Previous artists in residence include Taravat Talepasand, Whitney Lynn, and Jenny Odell.[33]
 The Internet Archive acquires most materials from donations,[34] such as hundreds of thousands of 78 rpm discs from Boston Public Library in 2017,[35] a donation of 250,000 books from Trent University in 2018,[36] and the entire collection of Marygrove College's library in 2020 after it closed.[37] All material is then digitized and retained in digital storage, while a digital copy is returned to the original holder and the Internet Archive's copy, if not in the public domain, is lent to patrons worldwide one at a time under the controlled digital lending (CDL) theory of the first-sale doctrine.[38]
 The Archive is a 501(c)(3) nonprofit operating in the United States. In 2019, it had an annual budget of $36 million, derived from revenue from its Web crawling services, various partnerships, grants, donations, and the Kahle-Austin Foundation.[39] The Internet Archive also manages periodic funding campaigns. For instance, a December 2019 campaign had a goal of reaching $6 million in donations.[40]
 The Archive is headquartered in San Francisco, California. From 1996 to 2009, its headquarters were in the Presidio of San Francisco, a former U.S. military base. Since 2009, its headquarters have been at 300 Funston Avenue in San Francisco, a former Christian Science Church.  At one time, most of its staff worked in its book-scanning centers; as of 2019, scanning is performed by 100 paid operators worldwide.[41]  The Archive also has data centers in three Californian cities: San Francisco, Redwood City, and Richmond. To reduce the risk of data loss, the Archive creates copies of parts of its collection at more distant locations, including the Bibliotheca Alexandrina[42][43] in Egypt and a facility in Amsterdam.[44]
 The Archive is a member of the International Internet Preservation Consortium[45] and was officially designated as a library by the state of California in 2007.[46][47]
 The Internet Archive capitalized on the popular use of the term "WABAC Machine" from a segment of The Adventures of Rocky and Bullwinkle cartoon (specifically, Peabody's Improbable History), and uses the name "Wayback Machine" for its service that allows archives of the World Wide Web to be searched and accessed.[48] This service allows users to view some of the archived web pages. The Wayback Machine was created as a joint effort between Alexa Internet (owned by Amazon.com) and the Internet Archive when a three-dimensional index was built to allow for the browsing of archived web content.[49] Hundreds of billions of web sites and their associated data (images, source code, documents, etc.) are saved in a database. The service can be used to see what previous versions of web sites used to look like, to grab original source code from web sites that may no longer be directly available, or to visit web sites that no longer even exist. Not all web sites are available because many web site owners choose to exclude their sites. As with all sites based on data from web crawlers, the Internet Archive misses large areas of the web for a variety of other reasons. A 2004 paper found international biases in the coverage, but deemed them "not intentional".[50] In 2017, the Wayback Machine director announced that its crawlers would ignore robots.txt instructions and archive pages even if website owners asked bots not to access them.[51]
 A "Save Page Now" archiving feature was made available in October 2013,[52] accessible on the lower right of the Wayback Machine's main page.[53] Once a target URL is entered and saved, the web page will become part of the Wayback Machine.[52]
Through the Internet address web.archive.org,[54] users can upload to the Wayback Machine a large variety of contents, including PDF and data compression file formats. The Wayback Machine creates a permanent local URL of the upload content, that is accessible in the web, even if not listed while searching in the https://archive.org official website.
 In October 2016, it was announced that the way web pages are counted would be changed, resulting in the decrease of the archived pages counts shown. Embedded objects such as pictures, videos, style sheets, JavaScripts are no longer counted as a "web page", whereas HTML, PDF, and plain text documents remain counted.[55]
 
In September 2020, the Internet Archive announced a partnership with Cloudflare to automatically index websites served via its "Always Online" services.[78] Created in early 2006, Archive-It[79] is a web archiving subscription service that allows institutions and individuals to build and preserve collections of digital content and create digital archives. Archive-It allows the user to customize their capture or exclusion of web content they want to preserve for cultural heritage reasons. Through a web application, Archive-It partners can harvest, catalog, manage, browse, search, and view their archived collections.[80]
 In terms of accessibility, the archived web sites are full text searchable within seven days of capture.[81] Content collected through Archive-It is captured and stored as a WARC file. A primary and back-up copy is stored at the Internet Archive data centers. A copy of the WARC file can be given to subscribing partner institutions for geo-redundant preservation and storage purposes to their best practice standards.[82] Periodically, the data captured through Archive-It is indexed into the Internet Archive's general archive.
 As of March 2014[update], Archive-It had more than 275 partner institutions in 46 U.S. states and 16 countries that have captured more than 7.4 billion URLs for more than 2,444 public collections. Archive-It partners are universities and college libraries, state archives, federal institutions, museums, law libraries, and cultural organizations, including the Electronic Literature Organization, North Carolina State Archives and Library, Stanford University, Columbia University, American University in Cairo, Georgetown Law Library, and many others.
 In September 2020 Internet Archive announced a new initiative to archive and preserve open access academic journals, called Internet Archive Scholar.[83][84][85] Its full-text search index includes over 25 million research articles and other scholarly documents preserved in the Internet Archive. The collection spans from digitized copies of eighteenth century journals through the latest open access conference proceedings and pre-prints crawled from the World Wide Web.
 In 2021, the Internet Archive announced the initial version of the General Index, a publicly available index to a collection of 107 million academic journal articles.[86][87]
 The scanning performed by the Internet Archive is financially supported by libraries and foundations.[88] As of November 2008[update], when there were approximately 1 million texts, the entire collection was greater than 0.5 petabytes, which included raw camera images, cropped and skewed images, PDFs, and raw OCR data.[89]
 As of July 2013[update], the Internet Archive was operating 33 scanning centers in five countries, digitizing about 1,000 books a day for a total of more than 2 million books, in a total collection of 4.4 million books – including material digitized by others and fed into the Internet Archive; at that time, users were performing more than 15 million downloads per month.[90]
 The material digitized by others includes more than 300,000 books that were contributed to the collection, between about 2006 and 2008, by Microsoft through its Live Search Books project, which also included financial support and scanning equipment directly donated to the Internet Archive.[91] On May 23, 2008, Microsoft announced it would be ending its Live Book Search project and would no longer be scanning books, donating its remaining scanning equipment to its former partners.[91]
 Around October 2007, Archive users began uploading public domain books from Google Book Search.[92] As of November 2013[update], there were more than 900,000 Google-digitized books in the Archive's collection;[93] the books are identical to the copies found on Google, except without the Google watermarks, and are available for unrestricted use and download.[a] Brewster Kahle revealed in 2013 that this archival effort was coordinated by Aaron Swartz, who, with a "bunch of friends", downloaded the public domain books from Google slowly enough and from enough computers to stay within Google's restrictions. They did this to ensure public access to the public domain. The Archive ensured the items were attributed and linked back to Google, which never complained, while libraries "grumbled". According to Kahle, this is an example of Swartz's "genius" to work on what could give the most to the public good for millions of people.[94]
 In addition to books, the Archive offers free and anonymous public access to more than four million court opinions, legal briefs, or exhibits uploaded from the United States Federal Courts' PACER electronic document system via the RECAP web browser plugin. These documents had been kept behind a federal court paywall. On the Archive, they had been accessed by more than six million people by 2013.[94]
 The Archive's BookReader web app,[95] built into its website, has features such as single-page, two-page, and thumbnail modes; fullscreen mode; page zooming of high-resolution images; and flip page animation.[95][96]
 The Open Library is another project of the Internet Archive. The project seeks to include a web page for every book ever published: it holds 25 million catalog records of editions. It also seeks to be a web-accessible public library: it contains the full texts of approximately 1,600,000 public domain books (out of the more than five million from the main texts collection), as well as in-print and in-copyright books,[97] many of which are fully readable, downloadable[98][99] and full-text searchable;[100] it offers a two-week loan of e-books in its controlled digital lending program for over 647,784 books not in the public domain, in partnership with over 1,000 library partners from six countries[90][101] after a free registration on the web site. Open Library is a free and open-source software project, with its source code freely available on GitHub.
 The Open Library faces objections from some authors and the Society of Authors, who hold that the project is distributing books without authorization and is thus in violation of copyright laws,[102] and four major publishers initiated a copyright infringement lawsuit against the Internet Archive in June 2020 to stop the Open Library project.[103]
 Many large institutional sponsors have helped the Internet Archive provide millions of scanned publications (text items).[104] Some sponsors that have digitized large quantities of texts include the University of Toronto's Robarts Library, the University of Alberta Libraries, the University of Ottawa, the Library of Congress, Boston Library Consortium member libraries, the Boston Public Library, the Princeton Theological Seminary Library, and many others.[105]
 In 2017, the MIT Press authorized the Internet Archive to digitize and lend books from the press's backlist,[106] with financial support from the Arcadia Fund.[107][108] A year later, the Internet Archive received further funding from the Arcadia Fund to invite some other university presses to partner with the Internet Archive to digitize books, a project called "Unlocking University Press Books".[109][110]
 The Library of Congress created numerous handle system identifiers that pointed to free digitized books in the Internet Archive.[111] The Internet Archive and Open Library are listed on the Library of Congress website as a source of e-books.[112]
 In addition to web archives, the Internet Archive maintains extensive collections of digital media that are attested by the uploader to be in the public domain in the United States or licensed under a license that allows redistribution, such as Creative Commons licenses. Media are organized into collections by media type (moving images, audio, text, etc.), and into sub-collections by various criteria. Each of the main collections includes a "Community" sub-collection (formerly named "Open Source") where general contributions by the public are stored.
 The Audio Archive is an audio archive that includes music, audiobooks, news broadcasts, old time radio shows, podcasts, and a wide variety of other audio files. As of January 2023[update], there are more than 15,000,000 free digital recordings in the collection. The subcollections include audio books and poetry, podcasts, non-English audio, and many others.[113] The sound collections are curated by B. George, director of the ARChive of Contemporary Music.[114]
 Next to the stock HTML5 audio player, Winamp-resembling Webamp is available.
 A project to preserve recordings of amateur radio transmissions, with funding from the Amateur Radio Digital Communications foundation.[115][116]
 The Live Music Archive sub-collection includes more than 170,000 concert recordings from independent musicians, as well as more established artists and musical ensembles with permissive rules about recording their concerts, such as the Grateful Dead, and more recently, The Smashing Pumpkins. Also, Jordan Zevon has allowed the Internet Archive to host a definitive collection of his father Warren Zevon's concert recordings. The Zevon collection ranges from 1976 to 2001 and contains 126 concerts including 1,137 songs.[117]
 The Great 78 Project aims to digitize 250,000 78 rpm singles (500,000 songs) from the period between 1880 and 1960, donated by various collectors and institutions. It has been developed in collaboration with the Archive of Contemporary Music and George Blood Audio, responsible for the audio digitization.[114]
 The Archive has a collection of freely distributable music that is streamed and available for download via its Netlabels service. The music in this collection generally has Creative Commons-license catalogs of virtual record labels.[118][119]
 This collection contains more than 3.5 million items.[120] Cover Art Archive, Metropolitan Museum of Art – Gallery Images, NASA Images, Occupy Wall Street Flickr Archive, and USGS Maps are some sub-collections of Image collection.
 The Cover Art Archive is a joint project between the Internet Archive and MusicBrainz, whose goal is to make cover art images on the Internet. As of April 2021,[update] this collection contains more than 1,400,000 items.[121]
 The images of this collection are from the Metropolitan Museum of Art. This collection contains more than 140,000 items.[122]
 The NASA Images archive was created through a Space Act Agreement between the Internet Archive and NASA to bring public access to NASA's image, video, and audio collections in a single, searchable resource. The IA NASA Images team worked closely with all of the NASA centers to keep adding to the ever-growing collection.[123] The nasaimages.org site launched in July 2008 and had more than 100,000 items online at the end of its hosting in 2012.
 This collection contains Creative Commons-licensed photographs from Flickr related to the Occupy Wall Street movement. This collection contains more than 15,000 items.[124]
 This collection contains more than 59,000 items from Libre Map Project.[125]
 One of the sub-collections of the Internet Archive's Video Archive is the Machinima Archive. This small section hosts many Machinima videos. Machinima is a digital artform in which computer games, game engines, or software engines are used in a sandbox-like mode to create motion pictures, recreate plays, or even publish presentations or keynotes. The archive collects a range of Machinima films from internet publishers such as Rooster Teeth and Machinima.com as well as independent producers. The sub-collection is a collaborative effort among the Internet Archive, the How They Got Game research project at Stanford University, the Academy of Machinima Arts and Sciences, and Machinima.com.[126]
 This collection contains approximately 160,000 microfilmed items from a variety of libraries including the University of Chicago Libraries, the University of Illinois at Urbana-Champaign, the University of Alberta, Allen County Public Library, and the National Technical Information Service.[127][128]
 The Internet Archive holds a collection of approximately 3,863 feature films.[129] Additionally, the Internet Archive's Moving Image collection includes: newsreels, classic cartoons, pro- and anti-war propaganda, The Video Cellar Collection, Skip Elsheimer's "A.V. Geeks" collection, early television, and ephemeral material from Prelinger Archives, such as advertising, educational, and industrial films, as well as amateur and home movie collections.
 Subcategories of this collection include:
 Open Educational Resources is a digital collection at archive.org. This collection contains hundreds of free courses, video lectures, and supplemental materials from universities in the United States and China. The contributors of this collection are ArsDigita University, Hewlett Foundation, MIT, Monterey Institute, and Naropa University.[132]
 In September 2012, the Internet Archive launched the TV News Search & Borrow service for searching U.S. national news programs.[133] The service is built on closed captioning transcripts and allows users to search and stream 30-second video clips. Upon launch, the service contained "350,000 news programs collected over 3 years from national U.S. networks and stations in San Francisco and Washington D.C."[134] According to Kahle, the service was inspired by the Vanderbilt Television News Archive, a similar library of televised network news programs.[135] In contrast to Vanderbilt, which limits access to streaming video to individuals associated with subscribing colleges and universities, the TV News Search & Borrow allows open access to its streaming video clips. In 2013, the Archive received an additional donation of "approximately 40,000 well-organized tapes" from the estate of a Philadelphia woman, Marion Stokes. Stokes "had recorded more than 35 years of TV news in Philadelphia and Boston with her VHS and Betamax machines."[136]
 Brooklyn Museum collection contains approximately 3,000 items from Brooklyn Museum.[137] In December 2020, the film research library of Lillian Michelson was donated to the archive.[138]
 Voicing a strong reaction to the idea of books simply being thrown away, and inspired by the Svalbard Global Seed Vault, Kahle now envisions collecting one copy of every book ever published. "We're not going to get there, but that's our goal", he said. Alongside the books, Kahle plans to store the Internet Archive's old servers, which were replaced in 2010.[139]
 The Internet Archive has "the largest collection of historical software online in the world", spanning 50 years of computer history in terabytes of computer magazines and journals, books, shareware discs, FTP sites, video games, etc. The Internet Archive has created an archive of what it describes as "vintage software", as a way to preserve them.[140] The project advocated for an exemption from the United States Digital Millennium Copyright Act to permit them to bypass copy protection, which the United States Copyright Office approved in 2003 for a period of three years.[141] The Archive does not offer the software for download, as the exemption is solely "for the purpose of preservation or archival reproduction of published digital works by a library or archive."[142] The Library of Congress renewed the exemption in 2006, and in 2009 indefinitely extended it pending further rulemakings.[143] The Library reiterated the exemption as a "Final Rule" with no expiration date in 2010.[144] In 2013, the Internet Archive began to provide select video games browser-playable via MESS, for instance the Atari 2600 game E.T. the Extra-Terrestrial.[145] Since December 23, 2014, the Internet Archive presents, via a browser-based DOSBox emulation, thousands of DOS/PC games[146][147][148][149] for "scholarship and research purposes only".[150][151][152] In November 2020, the Archive introduced a new emulator for Adobe Flash called Ruffle, and began archiving Flash animations and games ahead of the December 31, 2020, end-of-life for the Flash plugin across all computer systems.[153]
 A combined hardware software system has been developed that performs a safe method of digitizing content.[154][155]
 From 2012 to November 2015, the Internet Archive operated the Internet Archive Federal Credit Union, a federal credit union based in New Brunswick, New Jersey, with the goal of providing access to low- and middle-income people. Throughout its short existence, the IAFCU experienced significant conflicts with the National Credit Union Administration, which severely limited the IAFCU's loan portfolio and concerns over serving Bitcoin firms. At the time of its dissolution, it consisted of 395 members and was worth $2.5 million.[156][157]
 Since 2019,[158] the Internet Archive organizes an event called Decentralized Web Camp (DWeb Camp). It is an annual camp that brings together a diverse global community of contributors in a natural setting. The camp aims to tackle real-world challenges facing the web and co-create decentralized technologies for a better internet. It aims to foster collaboration, learning, and fun while promoting principles of trust, human agency, mutual respect, and ecological awareness.[159]
 On 30 September 2021, as a part of its 25th anniversary celebration, Internet Archive launched the "Wayforward Machine", a satirical, fictional website covered with pop-ups asking for personal information. The site was intended to depict a fictional dystopian timeline of real-world events leading to such a future, such as the repeal of Section 230 of the United States Code in 2022 and the introduction of advertising implants in 2041.[160][161]
 The Great Room of the Internet Archive features a collection of more than 100 ceramic figures representing employees of the Internet Archive, with the 100th statue immortalizing Aaron Swartz. This collection, inspired by the statues of the Xian warriors in China, was commissioned by Brewster Kahle, sculpted by Nuala Creed, and as of 2014, is ongoing.[162]
 The Internet Archive visual arts residency,[163] organized by Amir Saber Esfahani, is designed to connect emerging and mid-career artists with the Archive's millions of collections and to show what is possible when open access to information intersects with the arts. During this one-year residency, selected artists develop a body of work that responds to and utilizes the Archive's collections in their own practice.[164]
 
 On May 8, 2008, it was revealed that the Internet Archive had successfully challenged an FBI national security letter asking for logs on an undisclosed user.[168][169]
 On November 28, 2016, it was revealed that a second FBI national security letter had been successfully challenged that had been asking for logs on another undisclosed user.[170]
 The Internet Archive blacked out its web site for 12 hours on January 18, 2012, in protest of the Stop Online Piracy Act and the PROTECT IP Act bills, two pieces of legislation in the United States Congress that they argued would "negatively affect the ecosystem of web publishing that led to the emergence of the Internet Archive". This occurred in conjunction with the English Wikipedia blackout, as well as numerous other protests across the Internet.[171]
 The Internet Archive is a member of the Open Book Alliance, which has been among the most outspoken critics of the Google Book Settlement. The Archive advocates an alternative digital library project.[172]
 In November 2005, free downloads of Grateful Dead concerts were removed from the site, following what seemed to be disagreements between some of the former band members. John Perry Barlow identified Bob Weir, Mickey Hart, and Bill Kreutzmann as the instigators of the change, according to an article in The New York Times.[173] Phil Lesh, a founding member of the band, commented on the change in a November 30, 2005, posting to his personal web site:
 It was brought to my attention that all of the Grateful Dead shows were taken down from Archive.org right before Thanksgiving. I was not part of this decision making process and was not notified that the shows were to be pulled. I do feel that the music is the Grateful Dead's legacy and I hope that one way or another all of it is available for those who want it.[174] A November 30 forum post from Brewster Kahle summarized what appeared to be the compromise reached among the band members. Audience recordings could be downloaded or streamed, but soundboard recordings were to be available for streaming only. Concerts have since been re-added.[175]
 In February 2016, Internet Archive users had begun archiving digital copies of Nintendo Power, Nintendo's official magazine for their games and products, which ran from 1988 to 2012. The first 140 issues had been collected, before Nintendo had the archive removed on August 8, 2016. In response to the take-down, Nintendo told gaming website Polygon, "[Nintendo] must protect our own characters, trademarks and other content. The unapproved use of Nintendo's intellectual property can weaken our ability to protect and preserve it, or to possibly use it for new projects".[176]
 In August 2017, the Department of Telecommunications of the Government of India blocked the Internet Archive along with other file-sharing websites, in accordance with two court orders issued by the Madras High Court,[177] citing piracy concerns after copies of two Bollywood films were allegedly shared via the service.[178] The HTTP version of the Archive was blocked but it remained accessible using the HTTPS protocol.[177]
 In 2023, the Internet Archive became a popular site for Indians to watch the first episode of India: The Modi Question, a BBC documentary.[179] The video was reported to have been removed by the Archive on January 23.[179] The Internet Archive then stated, on January 27, that they had removed the video in response to a BBC request under the Digital Millennium Copyright Act.[180]
 The Great 78 Project had been started on the Internet Archive to store digitized versions of pre-1972 songs and albums from 78 rpm phonograph records, for the stated purpose of "the preservation, research and discovery of 78rpm records". The project had started in 2016, at which time the copyright on pre-1972 recordings only had limited duration; in 2019, the U.S. Congress passed the Music Modernization Act which extended pre-1972 recording copyrights to 2067. In August 2023, Sony Music Entertainment and five other major music publishers sued the Internet Archive over the Great 78 Project, asserting the project was engaged in copyright theft, denying the claim about research purposes since all the music was available via their respective digital and streaming music services. The companies were seeking the statutory damages for nearly 2500 songs named in the suit, for a total of $347 million.[181]
 On October 9, 2016, the Internet Archive was temporarily blocked in Turkey after it was used (amongst other file hosting services) by hackers to host 17 GB of leaked government emails.[182][183]
 Because the Internet Archive only lightly moderates uploads, it includes resources that may be valued by extremists or may be used by them to evade block listing. In February 2018, the Counter Extremism Project said that the Archive hosted terrorist videos, including the beheading of Alan Henning, and had declined to respond to requests about the videos.[184] In May 2018, a report published by the cyber-security firm Flashpoint stated that the Islamic State was using the Internet Archive to share its propaganda.[185] Chris Butler, from the Internet Archive, responded that they regularly spoke to the US and EU governments about sharing information on terrorism.[185] In April 2019, Europol, acting on a referral from French police, asked the Internet Archive to remove 550 sites of "terrorist propaganda".[186] The Archive rejected the request, saying that the reports were wrong about the content they pointed to, or were too broad for the organization to comply with.[186] On July 14, 2021, the Internet Archive held a joint "Referral Action Day" with Europol to target terrorist videos.[187]
 A 2021 article said that jihadists regularly used the Internet Archive for "dead drops" of terrorist videos.[188] In January 2022, a former UCLA lecturer's 800-page manifesto, containing racist ideas and threats against UCLA staff, was uploaded to the Internet Archive.[189] The manifesto was removed by the Internet Archive after a week, amidst discussion about whether such documents should be preserved by archivists or not.[189] Another 2022 paper found "an alarming volume of terrorist, extremist, and racist material on the Internet Archive".[190] A 2023 paper reported that Neo-Nazis collect links to online, publicly available resources to be shared with new recruits. As the Internet Archive hosts uploaded texts that are not allowed on other websites, Nazi and neo-Nazi books in the Archive (e.g., The Turner Diaries) frequently appear on these lists. These lists also feature older, public domain material created when white supremacist views were mainstream.[191]
 In the midst of the COVID-19 pandemic which closed many schools, universities, and libraries, the Archive announced on March 24, 2020, that it was creating the National Emergency Library by removing the lending restrictions it had in place for 1.4 million digitized books in its Open Library but otherwise limiting users to the number of books they could check out and enforcing their return; normally, the site would only allow one digital lending for each physical copy of the book they had, by use of an encrypted file that would become unusable after the lending period was completed.[4] This Library would remain as such until at least June 30, 2020, or until the US national emergency was over, whichever came later.[192] At launch, the Internet Archive allowed authors and rightholders to submit opt-out requests for their works to be omitted from the National Emergency Library.[193][194][195]
 The Internet Archive said the National Emergency Library addressed an "unprecedented global and immediate need for access to reading and research material" due to the closures of physical libraries worldwide.[196] They justified the move in a number of ways. Legally, they said they were promoting access to those inaccessible resources, which they claimed was an exercise in fair use principles. The Archive continued implementing their controlled digital lending policy that predated the National Emergency Library, meaning they still encrypted the lent copies and it was no easier for users to create new copies of the books than before. An ultimate determination of whether or not the National Emergency Library constituted fair use could only be made by a court. Morally, they also pointed out that the Internet Archive was a registered library like any other, that they either paid for the books themselves or received them as donations, and that lending through libraries predated copyright restrictions.[193][197]
 The Archive had already been criticized by authors and publishers for its prior lending approach, and upon announcement of the National Emergency Library, authors, publishers, and groups representing both took further issue, equating the move to copyright infringement and digital piracy, and using the COVID-19 pandemic as a reason to push the boundaries of copyright (see also: Open Library § Copyright violation accusations).[195][198][199][200] After the works of some of these authors were ridiculed in responses, the Internet Archive's Jason Scott requested that supporters of the National Emergency Library not denigrate anyone's books: "I realize there's strong debate and disagreement here, but books are life-giving and life-changing and these writers made them."[201]
 The operation of the National Emergency Library was part of a lawsuit filed against the Internet Archive by four major book publishers—Hachette, HarperCollins, John Wiley & Sons, and Penguin Random House—in June 2020, challenging the copyright validity of the controlled digital lending program.[4][103][202] In response, the Internet Archive closed the National Emergency Library on June 16, 2020, rather than the planned June 30, 2020, due to the lawsuit.[203][204] The plaintiffs, supported by the Copyright Alliance,[205] claimed in their lawsuit that the Internet Archive's actions constituted a "willful mass copyright infringement".[206] In August 2020 the lawsuit trial was tentatively scheduled to begin in November 2021.[207] By June 2022, both parties to the case requested summary judgment for the case, each favoring their respective sides, which Judge John G. Koeltl approved of a summary judgment hearing to take place later in 2022.[208] No summary judgment was issued, and instead a first hearing was held on March 20, 2023.[209] Over the course of the hearing, Judge John G. Koeltl appeared unmoved by the IA's fair use claims and unconvinced that the publishers' market for library e-books was not impacted by their practice.[210]
 Senator Thom Tillis of North Carolina, chairman of the intellectual property subcommittee on the Senate Judiciary Committee, said in a letter to the Internet Archive that he was "concerned that the Internet Archive thinks that it—not Congress—gets to determine the scope of copyright law".[206]
 As part of its response to the publishers' lawsuit, in late 2020 the Archive launched a campaign called Empowering Libraries (hashtag #EmpoweringLibraries) that portrayed the lawsuit as a threat to all libraries.[211]
 In a 2021 preprint article, Argyri Panezi argued that the case "presents two important, but separate questions related to the electronic access to library works; first, it raises questions around the legal practice of digital lending, and second, it asks whether emergency use of copyrighted material might be fair use" and argued that libraries have a public service role to enable "future generations to keep having equal access—or opportunities to access—a plurality of original sources".[212]
 In December 2020, Publishers Weekly included the lawsuit among its "Top 10 Library Stories of 2020".[213]
 Judge Koeltl ruled on March 24, 2023, against Internet Archive in the case, saying the National Emergency Library concept was not fair use, so the Archive infringed their copyrights by lending out the books without the waitlist restriction. An agreement was then reached for the Internet Archive to pay an undisclosed amount to the publishers.[214] The Internet Archive said afterwards it would appeal this ruling, but otherwise would continue other digital book services which have been previously cleared under case law, such as books for reading-impaired users.[215][216] An updated report of the appeal process involving The Internet Archive was published on December 18, 2023, by TorrentFreak News.[217]


Source: https://en.wikipedia.org/wiki/Wikipedia:Defekte_Weblinks
Content: Other reasons this message may be displayed:


Source: https://en.wikipedia.org/wiki/Portal:Gesprochene_Wikipedia
Content: Other reasons this message may be displayed:


Source: https://en.wikipedia.org/wiki/Datei:Qsicon_lesenswert.svg
Content: Other reasons this message may be displayed:


Source: https://en.wikipedia.org/wiki/Spezial:Permanenter_Link/10220285
Content: Other reasons this message may be displayed:


Source: https://en.wikipedia.org/wiki/Wikipedia:Lesenswerte_Artikel
Content: Other reasons this message may be displayed:


Source: https://en.wikipedia.org/wiki/Gemeinsame_Normdatei
Content: 
 
 The Gemeinsame Normdatei (translated as Integrated Authority File, also known as the Universal Authority File) or GND is an international authority file for the organisation of personal names, subject headings and corporate bodies from catalogues. It is used mainly for documentation in libraries and increasingly also by archives and museums. The GND is managed by the German National Library (German: Deutsche Nationalbibliothek; DNB) in cooperation with various regional library networks in German-speaking Europe and other partners. The GND falls under the Creative Commons Zero (CC0) licence.[1]
 The GND specification provides a hierarchy of high-level entities and sub-classes, useful in library classification, and an approach to unambiguous identification of single elements. It also comprises an ontology intended for knowledge representation in the semantic web, available in the RDF format.[2]
 The GND became operational in April 2012 and integrates the content of the following authority files, which have since been discontinued:
 It is referred to by identifiers named GND-ID.
 At the time of its introduction on 5 April 2012, the GND held 9,493,860 files, including 2,650,000 personalised names.[citation needed]
 There are six main types of GND entities:[3]


Source: https://en.wikipedia.org/wiki/Library_of_Congress_Control_Number
Content: The Library of Congress Control Number (LCCN) is a serially based system of numbering cataloged records in the Library of Congress, in the United States.  It is not related to the contents of any book, and should not be confused with Library of Congress Classification (LCC).
 The LCCN numbering system has been in use since 1898, at which time the acronym LCCN originally stood for Library of Congress Card Number.[1][2] It has also been called the Library of Congress Catalog Card Number, among other names. The Library of Congress prepared cards of bibliographic information for their library catalog and would sell duplicate sets of the cards to other libraries for use in their catalogs. This is known as centralized cataloging. Each set of cards was given a serial number to help identify it.
 Although most of the bibliographic information is now electronically created, stored, and shared with other libraries, there is still a need to identify each unique record, and the LCCN continues to perform that function.
 Librarians all over the world use this unique identifier in the process of cataloging most books which have been published in the United States.  It helps them reach the correct cataloging data (known as a cataloging record), which the Library of Congress and third parties make available on the Web and through other media.
 In February 2008, the Library of Congress created the LCCN Permalink service, providing a stable URL for all Library of Congress Control Numbers.[3][4]
 In its most elementary form, the number includes a year and a serial number.  The year has two digits for 1898 to 2000, and four digits beginning in 2001.  The three ambiguous years (1898, 1899, and 1900) are distinguished by the size of the serial number.  There are also some peculiarities in numbers beginning with a "7" because of an experiment applied between 1969 and 1972 which added a check digit.[2]
 Serial numbers are six digits long and should include leading zeros.[5] The leading zeros padding the number are a more recent addition to the format, so many older works will show less-full codes. The hyphen that is often seen separating the year and serial number is optional. More recently, the Library of Congress has instructed publishers not to include a hyphen.


Source: https://en.wikipedia.org/wiki/Wikipedia:Kategorien
Content: Other reasons this message may be displayed:


Source: https://en.wikipedia.org/wiki/Kategorie:Wikipedia:Gesprochener_Artikel
Content: Other reasons this message may be displayed:


Source: https://en.wikipedia.org/wiki/Kategorie:Wikipedia:Lesenswert
Content: Other reasons this message may be displayed:


Source: https://en.wikipedia.org/wiki/Kategorie:Python_(Programmiersprache)
Content: Other reasons this message may be displayed:


Source: https://en.wikipedia.org/wiki/Kategorie:Skriptsprache
Content: Other reasons this message may be displayed:


Source: https://en.wikipedia.org/wiki/Kategorie:Objektorientierte_Programmiersprache
Content: Other reasons this message may be displayed:


Source: https://en.wikipedia.org/wiki/Kategorie:Imperative_Programmiersprache
Content: Other reasons this message may be displayed:


Source: https://en.wikipedia.org/wiki/Kategorie:Funktionale_Programmiersprache
Content: Other reasons this message may be displayed:


Source: https://en.wikipedia.org/wiki/Kategorie:Wikipedia:Weblink_offline
Content: Other reasons this message may be displayed:


Source: https://en.wikipedia.org/wiki/Spezial:Meine_Diskussionsseite
Content: Other reasons this message may be displayed:


Source: https://en.wikipedia.org/wiki/Spezial:Meine_Beitr%C3%A4ge
Content: Other reasons this message may be displayed:


Source: https://en.wikipedia.org/wiki/Python_(Programmiersprache)
Content: Other reasons this message may be displayed:


Source: https://en.wikipedia.org/wiki/Diskussion:Python_(Programmiersprache)
Content: Other reasons this message may be displayed:


Source: https://en.wikipedia.org/wiki/Python_(Programmiersprache)
Content: Other reasons this message may be displayed:


Source: https://en.wikipedia.org/wiki/Wikipedia:Hauptseite
Content: Other reasons this message may be displayed:


Source: https://en.wikipedia.org/wiki/Wikipedia:Hauptseite
Content: Other reasons this message may be displayed:


Source: https://en.wikipedia.org/wiki/Portal:Wikipedia_nach_Themen
Content: Other reasons this message may be displayed:


Source: https://en.wikipedia.org/wiki/Spezial:Zuf%C3%A4llige_Seite
Content: Other reasons this message may be displayed:


Source: https://en.wikipedia.org/wiki/Wikipedia:Beteiligen
Content: Other reasons this message may be displayed:


Source: https://en.wikipedia.org/wiki/Hilfe:Neuen_Artikel_anlegen
Content: Other reasons this message may be displayed:


Source: https://en.wikipedia.org/wiki/Wikipedia:Autorenportal
Content: Other reasons this message may be displayed:


Source: https://en.wikipedia.org/wiki/Hilfe:%C3%9Cbersicht
Content: Other reasons this message may be displayed:


Source: https://en.wikipedia.org/wiki/Spezial:Letzte_%C3%84nderungen
Content: Other reasons this message may be displayed:


Source: https://en.wikipedia.org/wiki/Wikipedia:Kontakt
Content: Other reasons this message may be displayed:


Source: https://en.wikipedia.org/wiki/Spezial:Linkliste/Python_(Programmiersprache)
Content: Other reasons this message may be displayed:


Source: https://en.wikipedia.org/wiki/Spezial:%C3%84nderungen_an_verlinkten_Seiten/Python_(Programmiersprache)
Content: Other reasons this message may be displayed:


Source: https://en.wikipedia.org/wiki/Spezial:Spezialseiten
Content: Other reasons this message may be displayed:


Source: https://en.wikipedia.org/wiki/Wikipedia:%C3%9Cber_Wikipedia
Content: Other reasons this message may be displayed:


Source: https://en.wikipedia.org/wiki/Wikipedia:Impressum
Content: Other reasons this message may be displayed:


